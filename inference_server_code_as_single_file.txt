./setup_swiss_army_llama.py
---
import os
import subprocess
import httpx
import re
from logger_config import logger

def get_external_ip_func():
    providers = [
        "https://ipinfo.io/ip",
        "https://api.ipify.org",
        "https://checkip.amazonaws.com",
        "https://icanhazip.com"
    ]
    for provider in providers:
        try:
            response = httpx.get(provider)
            response.raise_for_status()
            return response.text.strip()
        except httpx.RequestError as e:
            logger.warning("Failed to retrieve external IP address from {}: {}".format(provider, e))
    logger.warning("Failed to retrieve external IP address from all providers.")
    return "Unknown"

def run_command(command, env=None, capture_output=False, check=False, timeout=None):
    shell = '/bin/zsh' if os.path.exists('/bin/zsh') else '/bin/bash'
    full_env = {**os.environ, **env} if env else os.environ.copy()
    command = ' '.join(command) if isinstance(command, list) else command
    try:
        result = subprocess.run(command, shell=True, env=full_env, capture_output=capture_output, text=True, executable=shell, check=check, timeout=timeout)
        if capture_output:
            if result.stdout:
                logger.info(result.stdout)
            if result.stderr:
                logger.error(result.stderr)
        return result
    except subprocess.TimeoutExpired:
        logger.warning("Command '{}' timed out after {} seconds".format(command, timeout))
    except subprocess.CalledProcessError as e:
        logger.error("Command '{}' failed with exit code {}".format(command, e.returncode))
        if capture_output:
            logger.error(e.output)
        raise

def is_port_available(port):
    result = run_command(["lsof", "-i", ":{}".format(port)], capture_output=True)
    return result.returncode != 0

def is_swiss_army_llama_responding(external_ip, port, security_token):
    try:
        url = "http://{}:{}/get_list_of_available_model_names/".format(external_ip, port)
        params = {'token': security_token}
        response = httpx.get(url, params=params)
        return response.status_code == 200
    except Exception as e:
        print("Error: {}".format(e))
        return False

def update_security_token(file_path, token):
    with open(file_path, 'r+') as file:
        content = file.read()
        file.seek(0)
        file.truncate()
        content = re.sub(r'SECURITY_TOKEN\s*=\s*"[^"]*"', 'SECURITY_TOKEN = "{}"'.format(token), content)
        file.write(content)

def is_pyenv_installed():
    result = run_command(["pyenv --version"], capture_output=True)
    return result.returncode == 0

def is_python_3_12_installed():
    result = run_command(["pyenv versions"], capture_output=True)
    return "3.12" in result.stdout

def is_rust_installed():
    try:
        result = run_command(["rustc", "--version"], capture_output=True)
        return result.returncode == 0
    except FileNotFoundError:
        return False

def setup_virtual_environment(swiss_army_llama_path):
    venv_path = os.path.join(swiss_army_llama_path, 'venv')
    if not os.path.exists(venv_path):
        os.makedirs(venv_path)
        run_command(['python3 -m venv {}'.format(venv_path)], check=True)
    pip_executable = os.path.join(venv_path, 'bin', 'pip')
    run_command(['{} install --upgrade pip'.format(pip_executable)], check=True)
    run_command(['{} install wheel'.format(pip_executable)], check=True)
    run_command(['{} install -r {}/requirements.txt'.format(pip_executable, swiss_army_llama_path)], check=True)
    return os.path.join(venv_path, 'bin', 'python')

def set_timezone_utc():
    os.environ['TZ'] = 'UTC'
    shell_profile_path = os.path.expanduser('~/.zshrc') if os.path.exists(os.path.expanduser('~/.zshrc')) else os.path.expanduser('~/.bashrc')
    if 'export TZ=UTC' not in open(shell_profile_path, 'r').read():
        run_command(['echo "export TZ=UTC" >> {}'.format(shell_profile_path)])

def check_systemd_service_exists(service_name):
    result = run_command("systemctl is-enabled {}".format(service_name), capture_output=True)
    return result.returncode == 0 and 'enabled' in result.stdout

def create_systemd_service(service_name, user, working_directory, exec_start):
    service_content = """[Unit]
Description=Swiss Army Llama service
After=network.target

[Service]
Type=simple
User={}
WorkingDirectory={}
ExecStart={}
Restart=always

[Install]
WantedBy=multi-user.target
""".format(user, working_directory, exec_start)
    service_path = "/etc/systemd/system/{}.service".format(service_name)
    temp_service_path = "/tmp/{}.service".format(service_name)
    with open(temp_service_path, 'w') as file:
        file.write(service_content)
    run_command("sudo mv {} {}".format(temp_service_path, service_path), check=True)
    logger.info("Systemd service file created at {}".format(service_path))
    run_command("sudo systemctl daemon-reload", check=True)
    run_command("sudo systemctl enable {}".format(service_name), check=True)
    run_command("sudo systemctl start {}".format(service_name), check=True)
    status_output = run_command("sudo systemctl status {}".format(service_name), capture_output=True, timeout=5)
    logger.info("Status of {} service:\n{}".format(service_name, status_output.stdout))

def ensure_pyenv_setup():
    if not is_pyenv_installed():
        logger.info("Installing pyenv...")
        run_command(["sudo apt-get update && sudo apt-get install -y build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python3-openssl git redis redis-server redis-tools"])
        run_command(["curl https://pyenv.run | bash"])
    if not is_python_3_12_installed():
        logger.info("Installing Python 3.12 using pyenv...")
        run_command(["pyenv install 3.12"])
        run_command(["pyenv global 3.12"])

def configure_shell_for_pyenv():
    shell_rc_path = os.path.expanduser("~/.zshrc") if os.path.exists(os.path.expanduser("~/.zshrc")) else os.path.expanduser("~/.bashrc")
    pyenv_init_str = 'export PYENV_ROOT="$HOME/.pyenv"\nexport PATH="$PYENV_ROOT/bin:$PATH"\neval "$(pyenv init --path)"\n'
    if pyenv_init_str not in open(shell_rc_path).read():
        with open(shell_rc_path, "a") as shell_rc:
            shell_rc.write(pyenv_init_str)
    os.environ["PYENV_ROOT"] = os.path.expanduser("~/.pyenv")
    os.environ["PATH"] = "{}:{}/bin".format(os.environ['PYENV_ROOT'], os.environ.get('PATH', ''))

def has_repo_been_updated(repo_path):
    try:
        run_command("git -C {} fetch".format(repo_path), check=True)
        local_commit = run_command("git -C {} rev-parse HEAD".format(repo_path), capture_output=True).stdout.strip()
        remote_commit = run_command("git -C {} rev-parse origin/main".format(repo_path), capture_output=True).stdout.strip()
        return local_commit != remote_commit
    except Exception as e:
        logger.error("Error checking repository updates: {}".format(e))
        return False

def setup_swiss_army_llama(security_token):
    set_timezone_utc()
    swiss_army_llama_path = os.path.expanduser("~/swiss_army_llama")
    swiss_army_llama_script = os.path.join(swiss_army_llama_path, "swiss_army_llama.py")
    if not os.path.exists(swiss_army_llama_path):
        logger.info("Cloning the Swiss Army Llama repository.")
        run_command("git clone https://github.com/Dicklesworthstone/swiss_army_llama {}".format(swiss_army_llama_path), check=True)
    else:
        logger.info("Swiss Army Llama repository already exists.")
    logger.info("Checking for updates to the Swiss Army Llama repository.")
    run_command("git -C {} fetch".format(swiss_army_llama_path), check=True)
    local_commit = run_command("git -C {} rev-parse HEAD".format(swiss_army_llama_path), capture_output=True).stdout.strip()
    remote_commit = run_command("git -C {} rev-parse origin/main".format(swiss_army_llama_path), capture_output=True).stdout.strip()
    if local_commit != remote_commit:
        logger.info("Updates found. Stashing local changes and pulling new changes from the repository.")
        run_command("git -C {} stash".format(swiss_army_llama_path), check=True)
        run_command("git -C {} pull".format(swiss_army_llama_path), check=True)
        update_security_token(swiss_army_llama_script, security_token)
    configure_shell_for_pyenv()
    if not is_pyenv_installed():
        ensure_pyenv_setup()
    if not is_python_3_12_installed():
        logger.info("Python 3.12 is not installed. Installing Python 3.12 using pyenv.")
        run_command("pyenv install 3.12", check=True)
        run_command("pyenv global 3.12", check=True)
    venv_path = os.path.join(swiss_army_llama_path, 'venv')
    if not os.path.exists(venv_path):
        python_executable = setup_virtual_environment(swiss_army_llama_path)
    else:
        python_executable = os.path.join(venv_path, 'bin', 'python')
        run_command(['pip install --upgrade -r {}/requirements.txt'.format(swiss_army_llama_path)], check=True) 
    if not is_rust_installed():
        logger.info("Rust is not installed. Installing Rust.")
        run_command("curl https://sh.rustup.rs -sSf | sh -s -- -y", check=True)
        os.environ.update({
            'PATH': "{}/.cargo/bin:{}".format(os.environ.get('HOME'), os.environ.get('PATH'))
        })
        run_command("rustup default nightly && rustup update nightly", check=True)
    if not check_systemd_service_exists("swiss_army_llama"):
        create_systemd_service("swiss_army_llama", os.getlogin(), swiss_army_llama_path, "{} {}".format(python_executable, swiss_army_llama_script))
    else:
        logger.info("Swiss Army Llama systemd service already exists; skipping installation, reloading systemd, and starting/enabling the service.")
        run_command("sudo systemctl daemon-reload", check=True)
        run_command("sudo systemctl enable swiss_army_llama", check=True)
        run_command("sudo systemctl start swiss_army_llama", check=True)
        status_output = run_command("sudo systemctl status swiss_army_llama", capture_output=True, timeout=5)
        logger.info("Status of swiss_army_llama service:\n{}".format(status_output.stdout))

def kill_running_instances_of_swiss_army_llama():
    logger.info("Stopping Swiss Army Llama service...")
    run_command("sudo systemctl stop swiss_army_llama", check=False)
    logger.info("Killing any remaining Swiss Army Llama processes...")
    run_command("ps -ef | grep 'swiss_army' | grep -v grep | awk '{print $2}' | xargs -r kill -9", check=False)

def check_and_setup_swiss_army_llama(security_token):
    swiss_army_llama_port = 8089
    remote_swiss_army_llama_mapped_port = 8087  # Local port for remote SSH tunnel (not used here)   # noqa: F841
    external_ip = get_external_ip_func()
    if external_ip == "Unknown":
        logger.error("Unable to reach external network providers. Network may be unreachable.")
        return
    repo_path = os.path.expanduser("~/swiss_army_llama")
    repo_updated = has_repo_been_updated(repo_path)
    service_responding = is_swiss_army_llama_responding("localhost", swiss_army_llama_port, security_token)
    swiss_army_llama_port_available = is_port_available(swiss_army_llama_port)
    if service_responding:
        logger.info("Swiss Army Llama service is responding on port {}.".format(swiss_army_llama_port))
        if not repo_updated:
            logger.info("Swiss Army Llama is already set up and running, and the repository has not been updated.")
            return
    elif not swiss_army_llama_port_available:
        logger.error("Swiss Army Llama service is NOT responding on port {}.".format(swiss_army_llama_port))
        logger.error("Port {} is not available. It may be in use by another process.".format(swiss_army_llama_port))
    if repo_updated or not service_responding:
        if repo_updated:
            logger.info("Repository has updates. Running setup.")
        else:
            logger.info("Repository has not been updated, but the service is not responding. Running setup.")
        kill_running_instances_of_swiss_army_llama()  # Ensure service is stopped before setup
        setup_swiss_army_llama(security_token)


---
./README.md
---
# Introduction and Overview of Pastel Inference Layer

![Illustration](https://raw.githubusercontent.com/pastelnetwork/inference_layer_documentation/master/inference_illustration.webp)


The new Pastel Inference Layer is [implemented](https://github.com/pastelnetwork/python_inference_layer_server) in Python on the server side. It works entirely with regular pasteld (`pasteld` is the Pastel Daemon binary, which is a compiled C++ binary that is the heart of the Pastel system) features accessed via the RPC (though some of this functionality has since been directly integrated as new or expanded pasteld RPC methods instead of being done externally all in Python).

There are 2 main components of the Inference Layer:


- **Credit Pack Tickets:** these are new Pastel blockchain tickets that can be created by users with the cooperation of a single responding Supernode (with verification/validation from other Supernodes before the ticket is finalized).
    - The purpose of the credit pack tickets is for the user to pre-pay for a certain amount of future inference requests ahead of time by paying a certain amount of PSL (which is actually burned) to create a credit pack ticket with a certain number of credits, where this number is determined by market prices and must be agreed upon by the majority of Supernodes. 
    - Once created, the credit pack ticket makes it simple to account for inference requests, the cost of which are denominated in terms of credits as opposed to in PSL.
        - They do this by specifying a particular PSL address as the “tracking address” for the credit pack, as well as a list of one or more PastelIDs that are authorized to spend credits from the credit pack.
        - Essentially, to pay for the cost of the credit pack, the end user has to send the full PSL cost of the credit pack to the central Pastel burn address, and this burn transaction must originate from the tracking address.
        - Once the credit pack has been created in the blockchain specifying an initial credit pack balance, credits can be consumed from the credit pack by the end user by creating inference requests. Each particular inference request has a certain cost measured in credits (e.g., generating a ChatGPT-type “text completion” of max length of 1,000 tokens might cost 41.2 inference credits), and this specific cost is told to the end user by the Responding Supernode before the request is actually done. 
        - If the user wants to move forward with the inference request at the quoted price in credits, the user does this by sending a particular small amount of PSL to the central burn address from the designated tracking address that is specified in the credit pack ticket they are using. The amount sent is a tiny bit of PSL that corresponds exactly to the number of credits that the user is authorizing to be spent from their credit pack ticket. For example, to authorize their text completion request for a cost of 41.2 credits, the end user would burn exactly 0.00412 PSL (or 412 “Patoshis”, the smallest individual unit of PSL) worth a small fraction of a penny.
        - The beauty of this system is that, beyond the initial credit pack ticket (the data for which is immutably stored forever in the Pastel blockchain as coin transactions), we don’t need to spend a lot of space in the blockchain and involve addition ticket overhead and complexity for tracking the current remaining balance of credits in a particular credit pack ticket; we can instead “encode” this data as tiny coin transactions which take up practically no space and which we already have various RPC methods in pasteld to keep track of (and we added additional methods for this purpose, such as the new `scanburntransactions` RPC method).
    - Unlike previous blockchain tickets in Pastel for Sense, Cascade, and NFTs, these new credit pack tickets work in a fundamentally different way: 
        - The cost of the credit pack ticket is paid by the end user, but this payment is burned rather than going to one or more specific Supernodes. This simplifies things a lot because we don’t need to have separate registration and activation stages and wait for block confirmations. 
        - The creation of a credit pack creates an obligation of the entire Pastel Network to honor future inference requests that are paid for using the credits in the credit pack; this is different from Cascade for example, where the registering Supernode (which receives most of the storage fee from the user rather than it being burned) is responsible for generating the storage symbols and spreading them through the network.
        - The new credit pack tickets are implemented in pasteld in a new “generic” way using a new ticket type called `contract`; the actual contents of these new `contract` tickets can be changed at will without requiring any changes in pasteld (or Gonode for that matter); pasteld is simply indexing them and making it quick and easy to retrieve them. This also means that we can use the same generic `contract` ticket for all sorts of new applications besides credit pack tickets without having to change anything in pasteld going forward.


- **Inference Requests:** these are simply REST requests made by the inference client (there is a reference client in Python, but the [primary client](https://github.com/pastelnetwork/pastel_inference_js_client) is written in JS and has a visual UI) to the Inference Layer server, which is implemented as a FastAPI application. 
    - Inference Requests can take many different forms, including:
        - Text completion (aka, LLMs, similar to ChatGPT) from a large variety of models/services, such as:
            - Various API-based services, including OpenAI, Claude3, Groq, Mistral, OpenRouter, etc. These services are *optional* according to the wishes of the Supernode operator. If the operator wants to support those models, they can include encrypted API keys for them in the [.env file](https://github.com/pastelnetwork/python_inference_layer_server/blob/master/.env) for the inference server, and they will automatically be used (and these API keys are automatically tested periodically to determine if they still work; if they don’t work, then that Supernode stops advertising support for that service). 
                - Note that there is a master list of supported models/services stored in github as the [model_menu.json](https://github.com/pastelnetwork/python_inference_layer_server/blob/master/model_menu.json) file; the local Supernode than filters this master list down to the list of models that it specifically supports, and these are advertised from a particular endpoint in the inference server code so that end users and other Supernodes can determine which models/services are supported by which specific Supernodes. In addition to specifying which models/services a Supernode supports, the model_menu.json has much more: it completely specifies in a uniform way which model parameters are available and which inference types they apply to, and the form of input and output that a model or service expects. The model_menu.json is also used in the JS Inference Client to automatically populate the UI elements instead of specifying these in a fixed way, so that updating the model_menu.json file means that everything updates automatically across the entire inference layer. It also makes adding support for new models and services easy, since you can just add new Swiss Army Llama models to the file and they will automatically work. For adding new services and model types, you also need to implement a function to submit the request to the particular service/model, but this is easy to do now with over 6 fully worked examples that all work in a similar way.
            - “Locally Hosted” LLMs using [Swiss Army Llama](https://github.com/Dicklesworthstone/swiss_army_llama); these do not require any API key and are completely decentralized/distributed, since they can be run on the Supernode machine itself doing inference on the CPU (which is slow, but still works). Such models allow for absolutely no censorship on the input prompts you can supply, and can also support models for which all the “safety” features have been removed so that they will answer questions which would be rejected by services such as OpenAI or Claude3 (e.g., “How do you hotwire a car?”). These models also support additional useful options that give you more control over the inference, such as being able to specify a grammar file to constrain the nature of the output of the model.
                - The reason why “Locally Hosted” is in quotes is because we also support running Swiss Army Llama on a remote machine. This is very useful because GPU enabled cloud instances are extremely expensive, often costing nearly $2/hour (over $1,000/month), which is much much more than the cost of running a regular CPU-only cloud instance for a Supernode, which can be as low as $30/month. But of course inference goes many times faster on a GPU enabled machine. Thus, Supernode operators can instead optionally use a service such as [vast.ai](https://vast.ai/) to rent a cheap GPU enabled instance with an Nvidia 4090 GPU for as little as $0.34/hour (around $250/month), which can be spun up and set up in just a couple minutes using a pre-made [Docker image template](https://cloud.vast.ai/?ref_id=78066&template_id=3c711e5ddd050882f0eb6f4ce1b8cc28) (which itself is mostly just using a regular [Docker image](https://hub.docker.com/repository/docker/jemanuel82/vastai_swiss_army_llama_template/general)) of Swiss Army Llama with full GPU mode enabled for everything. Then the details of this machine can be specified easily in the .env file for the inference server, and requests for Swiss Army Llama models will be seamlessly forwarded to the remote machine for execution (with a fallback to the local machine in case of an error). This means that a single GPU enabled instance can be shared between multiple Supernodes. This gives the best of both worlds in terms of decentralization/censorship resistance, inference performance, and economical low-cost operation.
        - Ask a question about an image that supports both Swiss Army Llama (via the Llava-Llama3 multimodal model) and via the GPT4o vision model.
        - Generate an image using Stable diffusion (with support easily added for others, such as Dall-e)
        - Using Swiss Army Llama to get embedding vectors for a whole document (e.g., text, html, pdf, doc, etc.) and do a semantic search against it.
        - Same for an audio file with speech, which will be automatically transcribed using Whisper and embeddings calculated optionally (along with an optional semantic search across those embeddings).


We explain above the main parts of the system, but not much about how they are actually implemented, which is quite involved and complex. The reason for the complexity is that we want the entire process to be very robust and secure. In particular, we want to ensure that:


- Users never spend money (in the form of either PSL or in Inference Credits) irreversibly but then don’t properly get the thing they paid for— either a new credit pack ticket that works, or the results of their inference request.

- Because obligations arising from the Inference Layer are “network wide” and thus shared by all Supernodes, this makes it a lot easier in theory to deal with problems. Even if a particular Supernode doesn’t follow through with what it is supposed to do for any reason (e.g., it’s being malicious, or it experienced technical issues like running out of disk space or poor network connectivity), then we want the other Supernodes to automatically realize that and have a robust process for automatically having the next Supernode step up to complete what the user has already paid for.

- We want to have confidence that each part of the complex processes involved in creating a new credit pack ticket or in creating and fulfilling a new inference request is 100% valid and correct and hasn’t been changed by any malicious parties, including the end user, the responding Supernode, or another Supernode, or even a “man in the middle”, like one of the ISPs involved in the process for the end user or the Supernodes. 
    - This means that each step in these processes is mediated by messages, and the messages are verified and validated in very particular ways. For example, the relevant fields of each message are combined in a repeatable way and hashed together, and this hash included in the message “envelope”; these message hashes are then signed by all parties involved using their PastelID private keys. This allows all parties to know for sure that the contents of the message hasn’t been altered in any way, because otherwise the hash wouldn’t match the hash that each party independently computes based on all the relevant message fields, and thus the signatures wouldn’t validate either. 
    - In addition to this, each message has the current UTC timestamp and current Pastel block height included. If a message arrives more than a minute after this timestamp or more than 2 blocks away from the specified block height, then the message is deemed invalid and is ignored.
    - For example, this means that if a user makes a request for a credit pack contain 1,000 credits at a cost of 30,000 PSL, say, then there is no way for a malicious responding Supernode to alter that request so that the end user pays the full 30,000 PSL but only gets a credit pack containing 100 credits. And the same goes for inference requests— the user is guaranteed to get what they requested and nothing more.
    
- We also want to avoid any pricing surprises for users. Before a user spends any PSL or Inference Credits, they need to fully understand what they are getting for their money. 
    - This means that requests for new credit packs are first given a price quote from the responding Supernode, and the end user has to agree to this price before moving forward, and the end user can specify a maximum PSL cost to the credit pack. Also, since end users might not know what a “reasonable” price level is for a credit pack in PSL terms, we automatically validate the pricing quote on the client side by checking the price quote using the current market price of PSL and the same rules that the Supernodes are supposed to use in arriving at the pricing of credits; if the offered price quote is too far from what it’s supposed to be, the client will automatically refuse to move forward even if the total credit pack price is below the maximum price specified by the end user.  
    - In terms of inference requests, this means that the user first fully specifies what they want to do in their inference request, and the responding Supernode does some calculations to determine how many credits the inference request will cost and tells the user; the user thus has a chance to see how much it will cost them before moving forward and spending any of their inference credits from their credit pack ticket until they are sure they want to do it at that price. Once they are ready, they indicate this by sending a confirmation transaction (we will describe this fully below), and only then are the credits permanently deducted from their credit pack.
    
- To make the system simpler and more reliable, the fundamental interaction is one between the end user and a single Supernode, called the “Responding Supernode”; in the case where we are creating a new credit pack, this is simply the Supernode that is “closest” to the end user’s PastelID using XOR distance. In the case of a new inference request, this would be the “closest” Supernode using XOR distance to the end user that also supports the exact model/service specified in the user’s inference request. But just because the end user primarily communicates with this single Responding Supernode, it doesn’t mean that the other Supernodes are kept in the dark, or that the single Responding Supernode is able to do whatever it wants without consulting the other Supernodes.
    - In the case of creating a new credit pack ticket, the Responding Supernode can propose a price quote to the end user, but then when the user confirms the price quote, that doesn’t mean that the process is over; the problem of course is that the creation of a new credit pack creates an obligation on the part of the entire network of Supernodes to honor those inference credits to complete future inference requests. 
    - So if the Responding Supernode decided to price the new credits at way below cost or the normal market price, while that might be great for that specific end user that got a special deal, it would be bad for the broader network because all the other Supernodes would then be stuck fulfilling inference requests that are unprofitable on a network level; i.e., where the users pro-rata amount of PSL burned in USD terms is much less than the USD cost of providing a particular inference request for those credits. This is an important concept here, since both the entire credit pack cost is burned, so nothing is *directly* profitable to the Supernode, but the Supernode benefits *indirectly* to the extent that PSL is burned and the supply reduced. 
    - In order to deal with this issue, there is a whole procedure by which the Responding Supernode, after giving the intial credit pack price quote in PSL to the end user and getting the end user’s agreement to the initial quote, must then communicate all the details of the new credit pack request to all the other Supernodes. Then a certain portion of all the Supernodes need to respond (the quorum percentage), and of the Supernodes that respond, a majority of those (the agreement percentage) must agree to the proposed pricing terms (i.e., agree that it’s OK to sell X inference credits for Y PSL). They indicate this agreement by signing their pastelid to the details of the entire credit pack proposal, and these signatures are included in the final credit pack data that is stored in the blockchain. 
    - Later, when the end user attempts to actually use the credit pack for an inference request, all of this information is re-validated to determine that it’s all correct and proper— that is, that the original Responding Supernode didn’t “go rogue” and offer terms that the other Supernodes wouldn’t agree to.

With those preliminaries out of the way, we can now get into the specifics of the implementations of both parts. We begin with the creation of credit pack tickets.

----------

## Credit Pack Ticket Implementation Details

Note: The following python message models are written using the [SQLModel library](https://sqlmodel.tiangolo.com/). This allows us to create a single class that can be used both as a database ORM (using sqlite via [SQLAlchemy](https://www.sqlalchemy.org/)) AND simultaneously as a response/validation model (using [Pydantic](https://docs.pydantic.dev/latest/)). Normally, we would need to specify both an ORM data model and a separate Pydantic model, since they are used for different things (ORM models are for adding, selecting, and deleting data from a database without having to use raw SQL queries, and Pydantic models are for automatically validating the form of inputs and responses to and from FastAPI endpoint functions and handling all the serialization to/from JSON).

As mentioned above, the entire flow can be viewed as an intricate exchange of messages between the end user and the Responding Supernode, and between the Responding Supernode and the rest of the Supernodes. These message exchanges are facilitated using standard REST API endpoints on the Supernodes.

For security purposes, all these endpoints require a challenge/response system to access them at all; in this process, before an end user or other Supernode (or anyone else— just call them the “client”) can access any of these endpoints, the client must first request a challenge string from the specific Supernode they want to connect to, and in this challenge request they must specify their PastelID. The Supernode then responds with a challenge, which has a random challenge ID and challenge string; the client then has to quickly sign this challenge string using their PastelID (which must match what they said their PastelID was in the original challenge request) and return both the challenge ID, challenge string, and signature, all before the challenge expires automatically. The Supernode then verifies that the signature is valid and matches the right PastelID, and only THEN will it allow the client to access the endpoint. 

Beyond that, there is security at the individual message level; you will see in the classes below that they all contain fields with names like `sha3_256_hash_of_credit_pack_purchase_request_fields` and `requesting_end_user_pastelid_signature_on_request_hash`. These are used to verify and validate ALL details of ALL messages at EVERY point in the whole process. 

Now, let’s introduce the actual flow for a user to request the creation of a new credit pack ticket:

1) First the end user picks a certain Supernode from the list ofSupernodes (the one whose pastelid has XOR distance closest to the best block hash on pastel now; call this Supernode the `responding_supernode`) and requests to purchase an credit pack ticket for a certain price using this message that is sent via a POST to the `/credit_purchase_initial_request` endpoint:

```python
    class CreditPackPurchaseRequest(SQLModel, table=True):
        id: uuid.UUID = Field(default_factory=uuid.uuid4, index=True, nullable=True)
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
        requesting_end_user_pastelid: str = Field(index=True)
        requested_initial_credits_in_credit_pack: int
        list_of_authorized_pastelids_allowed_to_use_credit_pack: str = Field(sa_column=Column(JSON))
        credit_usage_tracking_psl_address: str = Field(index=True)
        request_timestamp_utc_iso_string: str
        request_pastel_block_height: int
        credit_purchase_request_message_version_string: str
        requesting_end_user_pastelid_signature_on_request_hash: str
```

2) The `responding_supernode` evaluates the message from the end user to check that all fields are valid. If anything in the request is invalid, then it responds to the POST request to the `/credit_purchase_initial_request` endpoint with a rejection message:

```python
    class CreditPackPurchaseRequestRejection(SQLModel, table=True):
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
        credit_pack_purchase_request_fields_json_b64: str
        rejection_reason_string: str
        rejection_timestamp_utc_iso_string: str
        rejection_pastel_block_height: int
        credit_purchase_request_rejection_message_version_string: str
        responding_supernode_pastelid: str = Field(index=True)
        sha3_256_hash_of_credit_pack_purchase_request_rejection_fields: str = Field(unique=True, index=True)
        responding_supernode_signature_on_credit_pack_purchase_request_rejection_hash: str
```

3) If the purchase request message fields are all valid, then the `responding_supernode` first determines the best price (in PSL per credit) that they would be willing to accept for the credit pack. To avoid wasting time and communication overhead, this preliminary price quote (preliminary because it is not considered valid by the network until it has been agreed to by enough other Supernodes) is first sent to the end user to determine if the end user is willing to agree to the quoted price (since if the end user doesn't like the price or doesn't have enough PSL to buy the credit pack for the total price, then there is no point in continuing any further). So in that case the POST request from the end user is responded to with this message:

```python
    class CreditPackPurchaseRequestPreliminaryPriceQuote(SQLModel, table=True):
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
        credit_usage_tracking_psl_address: str = Field(index=True)
        credit_pack_purchase_request_fields_json_b64: str
        preliminary_quoted_price_per_credit_in_psl: float
        preliminary_total_cost_of_credit_pack_in_psl: float
        preliminary_price_quote_timestamp_utc_iso_string: str
        preliminary_price_quote_pastel_block_height: int
        preliminary_price_quote_message_version_string: str
        responding_supernode_pastelid: str = Field(index=True)
        sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields: str = Field(unique=True, index=True)
        responding_supernode_signature_on_credit_pack_purchase_request_preliminary_price_quote_hash: str
```

4) The user then calls another endpoint on the`responding_supernode` (the `/credit_purchase_preliminary_price_quote_response` endpoint) to POST their response to the preliminary price quote with this message:

```python
    class CreditPackPurchaseRequestPreliminaryPriceQuoteResponse(SQLModel, table=True):
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
        sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields: str = Field(index=True)
        credit_pack_purchase_request_fields_json_b64: str
        agree_with_preliminary_price_quote: bool
        credit_usage_tracking_psl_address: str = Field(index=True)
        preliminary_quoted_price_per_credit_in_psl: float
        preliminary_price_quote_response_timestamp_utc_iso_string: str
        preliminary_price_quote_response_pastel_block_height: int
        preliminary_price_quote_response_message_version_string: str
        requesting_end_user_pastelid: str = Field(index=True)
        sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_response_fields: str = Field(unique=True, index=True)
        requesting_end_user_pastelid_signature_on_preliminary_price_quote_response_hash: str
```

5) If the end user rejects the price quote (i.e., `agree_with_preliminary_price_quote` is false), then the process terminates now. The end user can repeat the process on the next block to try with a new `responding_supernode` that might be willing to accept a lower price. If the end user accepts the preliminary price quote (i.e., `agree_with_preliminary_price_quote` is true), then the `/credit_purchase_preliminary_price_quote_response` endpoint on the `responding_supernode` must now determine if enough other supernodes in the pastel network agree with the pricing it proposed to the end user. The `responding_supernode` does this by selecting the 12 supernodes whose hash(pastelid) has XOR distance closest to the best pastel block's merkle root; collectively these are known as the `potentially_agreeing_supernodes`. The `responding_supernode` calls a REST endpoint on each of the potentially_agreeing_supernodes called the `/credit_pack_price_agreement_request` endpoint and POSTs a message of the form:

```python
    class CreditPackPurchasePriceAgreementRequest(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        sha3_256_hash_of_credit_pack_purchase_request_response_fields: str = Field(index=True)
        supernode_requesting_price_agreement_pastelid: str = Field(index=True)
        credit_pack_purchase_request_fields_json_b64: str
        credit_usage_tracking_psl_address: str = Field(index=True)
        proposed_psl_price_per_credit: float
        price_agreement_request_timestamp_utc_iso_string: str
        price_agreement_request_pastel_block_height: int
        price_agreement_request_message_version_string: str
        sha3_256_hash_of_price_agreement_request_fields: str = Field(index=True)
        supernode_requesting_price_agreement_pastelid_signature_on_request_hash: str
```

6) Each of the `potentially_agreeing_supernodes` checks all the fields of the message, including the fields of the related messages (i.e., `credit_pack_purchase_request_response_fields_json`) to determine if all fields and signatures are valid so far. Then it determines if it is willing to go along with the quoted credit pricing proposed by the `responding_supernode`. It does this by responding to the POST request to `/credit_purchase_preliminary_price_quote_response` endpoint with the following message structure:

```python
    class CreditPackPurchasePriceAgreementRequestResponse(SQLModel, table=True):
        sha3_256_hash_of_price_agreement_request_fields: str = Field(primary_key=True, index=True)
        credit_pack_purchase_request_fields_json_b64: str
        agree_with_proposed_price: bool
        credit_usage_tracking_psl_address: str = Field(unique=True,index=True)
        proposed_psl_price_per_credit: float
        proposed_price_agreement_response_timestamp_utc_iso_string: str
        proposed_price_agreement_response_pastel_block_height: int
        proposed_price_agreement_response_message_version_string: str
        responding_supernode_signature_on_credit_pack_purchase_request_fields_json_b64: str
        responding_supernode_pastelid: str = Field(index=True)
        sha3_256_hash_of_price_agreement_request_response_fields: str = Field(unique=True, index=True)
        responding_supernode_signature_on_price_agreement_request_response_hash: str
```

7) The `responding_supernode` waits for as many of the `potentially_agreeing_supernodes` to respond as possible in a set period of time (say, 30 seconds) and then aggregates all of their responses for processing. It counts up the number of valid responses received (the `valid_price_agreement_request_responses` from all of the `potentially_agreeing_supernodes` and computes
`(len(valid_price_agreement_request_responses)/len(potentially_agreeing_supernodes))` and checks that this result exceeds `0.51`. If it does not, then the entire process terminates now and the `responding_supernode` prepares a message for the end user for the next time the end user calls the REST endpoint `/check_status_of_credit_purchase_request` on the `responding_supernode`; first the end user POSTs the message:

```python
    class CreditPackRequestStatusCheck(SQLModel, table=True):
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
        requesting_end_user_pastelid: str = Field(index=True)
        requesting_end_user_pastelid_signature_on_sha3_256_hash_of_credit_pack_purchase_request_fields: str
```

and in the case of a termination, the `responding_supernode` responds with this message detailing why the request failed (in this case because not enough of the `potentially_agreeing_supernodes` responded in time with a valid response to the `responding_supernode`):

```python
    class CreditPackPurchaseRequestResponseTermination(SQLModel, table=True):
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
        credit_pack_purchase_request_fields_json_b64: str
        termination_reason_string: str
        termination_timestamp_utc_iso_string: str
        termination_pastel_block_height: int
        credit_purchase_request_termination_message_version_string: str
        responding_supernode_pastelid: str = Field(index=True)
        sha3_256_hash_of_credit_pack_purchase_request_termination_fields: str = Field(unique=True, index=True)
        responding_supernode_signature_on_credit_pack_purchase_request_termination_hash: str
```

8) If enough of the `potentially_agreeing_supernodes` respond in time with valid responses, the next step is for the `responding_supernode` to tally up the responses to determine the number of `potentially_agreeing_supernodes agree` to the quoted credit price; call them the `agreeing_supernodes`. If `len(agreeing_supernodes)/len(valid_price_agreement_request_responses)` exceeds `0.85` then the price quote is deemed to be valid by the network as a whole. If `len(agreeing_supernodes)/len(valid_price_agreement_request_responses)` is LESS than `0.85`, then the entire process terminates, and when the end user next calls the `responding_supernode`'s `/check_status_of_credit_purchase_request` endpoint, the `responding_supernode` will again respond with a `CreditPackPurchaseRequestResponseTermination` (but now the `termination_reason_string` field will explain that the request terminated for a different reason— because not enough of the other Supernodes agreed to the proposed pricing). But if enough of them agreed to the pricing so that the propose credit pack is deemed to be valid by the network, then the `responding_supernode responds` with this message the next time the end user calls its `/check_status_of_credit_purchase_request` endpoint:

```python
    class CreditPackPurchaseRequestResponse(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(foreign_key="creditpackpurchaserequest.sha3_256_hash_of_credit_pack_purchase_request_fields", index=True)
        credit_pack_purchase_request_fields_json_b64: str
        psl_cost_per_credit: float
        proposed_total_cost_of_credit_pack_in_psl: float
        credit_usage_tracking_psl_address: str = Field(index=True)
        request_response_timestamp_utc_iso_string: str
        request_response_pastel_block_height: int
        best_block_merkle_root: str
        best_block_height: int
        credit_purchase_request_response_message_version_string: str
        responding_supernode_pastelid: str = Field(index=True)
        list_of_blacklisted_supernode_pastelids: str = Field(sa_column=Column(JSON))
        list_of_potentially_agreeing_supernodes: str = Field(sa_column=Column(JSON))
        list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms: str = Field(sa_column=Column(JSON))
        list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion: str = Field(sa_column=Column(JSON))
        selected_agreeing_supernodes_signatures_dict: str = Field(sa_column=Column(JSON))
        sha3_256_hash_of_credit_pack_purchase_request_response_fields: str = Field(unique=True, index=True)
        responding_supernode_signature_on_credit_pack_purchase_request_response_hash: str
```
, and the `responding_supernode` will also call the `/credit_pack_purchase_request_final_response_announcement` endpoint on each of the `agreeing_supernodes` and will also POST the same `CreditPackPurchaseRequestResponse` message so that all of the `agreeing_supernodes` know all the details of the ticket.

9) At this point, the deal is agreed to in all particulars, and all that is left is for the end user to actually burn `proposed_total_cost_of_credit_pack_in_psl` PSL coins by sending exactly this many coins to the burn address from the end user's `credit_usage_tracking_psl_address` within 50 blocks of `request_response_pastel_block_height`. Once the end user does this, the transaction's UTXO is then communicated to the responding_supernode by the end user calling the responding_supernode's `/confirm_credit_purchase_request` endpoint by POSTing a message of the form:

```python
    class CreditPackPurchaseRequestConfirmation(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(foreign_key="creditpackpurchaserequest.sha3_256_hash_of_credit_pack_purchase_request_fields", index=True)
        sha3_256_hash_of_credit_pack_purchase_request_response_fields: str = Field(foreign_key="creditpackpurchaserequestresponse.sha3_256_hash_of_credit_pack_purchase_request_response_fields", index=True)
        credit_pack_purchase_request_fields_json_b64: str
        requesting_end_user_pastelid: str = Field(index=True)
        txid_of_credit_purchase_burn_transaction: str = Field(index=True)
        credit_purchase_request_confirmation_utc_iso_string: str
        credit_purchase_request_confirmation_pastel_block_height: int
        credit_purchase_request_confirmation_message_version_string: str
        sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str = Field(unique=True, index=True)
        requesting_end_user_pastelid_signature_on_sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str
```

The end user also calls the `/credit_pack_purchase_completion_announcement` endpoint on each of the `agreeing_supernodes` and POSTs the same `CreditPackPurchaseRequestConfirmation` message to them to let them know that the payment was sent.

10) The `responding_supernode` then checks the details of this message and also checks the pastel blockchain directly to confirm that the transaction with txid `txid_of_credit_purchase_burn_transaction` really exists in the blockchain, was mined and confirmed by at least 3 new blocks (this check can be disabled to speed things up during debugging), and matches the expected amount exactly, and that it was indeed sent by the `credit_usage_tracking_psl_address` within 50 blocks of `request_response_pastel_block_height`. If all those things are true, then the `responding_supernode` writes the “combined ticket data” to the Pastel blockchain.

Essentially, this combined data is a nested JSON string that contains within it the JSON serialized messages for `CreditPackPurchaseRequest`, `CreditPackPurchaseRequestResponse`, and `CreditPackPurchaseRequestConfirmation`, which between them contain all the many salient pieces of information that are required to fully validate the legitimacy of the credit pack ticket— even by a newly joined Supernode that wasn’t around when the original credit pack was purchase and so never directly saw any of the many announcement messages that would have been sent to it if it were part of the network when the credit pack ticket was originally created. The process of writing this data to the blockchain, which is done by storing the Z-standard compressed JSON data in the form of Pay2FakeMultiSig coin transactions, is now completely handled within pasteld itself using a new RPC method, which is access in the Python Inference server code like this:

```python
    ticket_register_command_response = await rpc_connection.tickets('register', 'contract', ticket_json_b64, ticket_type_identifier, ticket_input_data_fully_parsed_sha3_256_hash)
```

This command returns a single Pastel TXID which uniquely picks out the credit pack ticket and is the primary way we refer to the completed credit pack ticket for all future operations (note that, of course, this TXID is itself not in the credit pack data, since that would create a “chicken and egg” problem.) For example, if we want to retrieve the credit pack from the blockchain we can use the RPC method in pasteld from Python like this and quickly parse out all the relevant data to get back a Python dict variable that has everything we’d need to fully validate the ticket contents:

```python
    ticket_get_command_response = await rpc_connection.tickets('get', ticket_txid , 1)
```

Because it would be wasteful to constantly get data from the blockchain and parse it, we only need to do this the first time (or if there is a chain re-org or rollback, which is detected and handled automatically in the Python Inference Server code) and the parsed data is serialized and turned into records in the Supernode’s local SQLite database. This is also true for all messages that the Supernode receives from end users or other Supernodes, as well as announcement messages that are sent using the built-in Pastel Masternode messaging system; all of these are automatically added to the database for ease of querying and efficiency (this is the beauty of using SQLmodel— all messages that are sent/received are automatically validated against the model and can be losslessly turned into the underlying ORM data models from the raw JSON, and can then be inserted easily into the database without a lot of annoying and verbose conversion/ingestion code).

After the ticket has been written to the blockchain successfully by the `responding_supernode`, it finally responds to the end user with this message:

```python
    class CreditPackPurchaseRequestConfirmationResponse(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(foreign_key="creditpackpurchaserequest.sha3_256_hash_of_credit_pack_purchase_request_fields", index=True)
        sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str = Field(foreign_key="creditpackpurchaserequestconfirmation.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields", index=True)
        credit_pack_confirmation_outcome_string: str
        pastel_api_credit_pack_ticket_registration_txid: str = Field(index=True)
        credit_pack_confirmation_failure_reason_if_applicable: str
        credit_purchase_request_confirmation_response_utc_iso_string: str
        credit_purchase_request_confirmation_response_pastel_block_height: int
        credit_purchase_request_confirmation_response_message_version_string: str
        responding_supernode_pastelid: str = Field(index=True)
        sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields: str = Field(unique=True, index=True)
        responding_supernode_signature_on_credit_pack_purchase_request_confirmation_response_hash: str
```

In addition, the `responding_supernode` calls the `/credit_pack_storage_completion_announcement` endpoint on all of the `agreeing_supernodes` and POSTs the same `CreditPackPurchaseRequestConfirmationResponse` message to them to let them know that the process has been successfully completed and everything is done and the credit pack ticket has been written to the blockchain correctly and is now valid and ready to be used, as well as the final TXID for the credit pack ticket.

11) If, for whatever reason, the `responding_supernode` is unable or unwilling to actually store the ticket data in the blockchain, but the end user has already burned the required PSL as needed, then we still need to deal with this situation, because otherwise it's very unfair to the end user. Luckily, any of the `agreeing_supernodes` can be called upon if needed by the end user to do this. But the end user is only permitted to ask one of the `agreeing_supernodes` to do this for them if more than 10 Pastel blocks have elapsed since the `CreditPackPurchaseRequestConfirmation` was sent by the end user without the original `responding_supernode` sending the `CreditPackPurchaseRequestConfirmationResponse` to the `agreeing_supernodes`. In that case, the end user chooses the "closest" of the `agreeing_supernodes` to the end user's pastelid (i.e., the XOR distance of `hash(agreeing_supernode_pastelid)` to `hash(end_user_pastelid)` is smallest of all the `agreeing_supernodes`; call this the `closest_agreeing_supernode`) and then the end user calls the `/credit_pack_storage_retry_request` endpoint on the `closest_agreeing_supernode` and POSTs the message:

```python
    class CreditPackStorageRetryRequest(SQLModel, table=True):
        sha3_256_hash_of_credit_pack_purchase_request_response_fields: str = Field(primary_key=True, index=True)
        credit_pack_purchase_request_fields_json_b64: str
        requesting_end_user_pastelid: str = Field(index=True)
        closest_agreeing_supernode_to_retry_storage_pastelid: str = Field(index=True)
        credit_pack_storage_retry_request_timestamp_utc_iso_string: str
        credit_pack_storage_retry_request_pastel_block_height: int
        credit_pack_storage_retry_request_message_version_string: str
        sha3_256_hash_of_credit_pack_storage_retry_request_fields: str
        requesting_end_user_pastelid_signature_on_credit_pack_storage_retry_request_hash: str
```

12) At this point, the `closest_agreeing_supernode` will check all the details to ensure that the retry request is valid (particularly, that the original `responding_supernode` has still not confirmed that the ticket storage took place successfully) in all respects, and if so, it will store the ticket in the Pastel blockchain itself. It can do this, including having the original `responding_supernode`'s pastelid signature on the ticket, because earlier in the process, the `responding_supernode` called the `closest_agreeing_supernode`'s `credit_pack_purchase_completion_announcement` endpoint, thus supplying all these details automatically. So now, the `closest_agreeing_supernode` simply stores that exact ticket data itself in the blockchain. When done, it responds to the end user's request to its `/credit_pack_storage_retry_request` endpoint with the following message:

```python
    class CreditPackStorageRetryRequestResponse(SQLModel, table=True):
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
        sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str
        credit_pack_storage_retry_confirmation_outcome_string: str
        pastel_api_credit_pack_ticket_registration_txid: str
        credit_pack_storage_retry_confirmation_failure_reason_if_applicable: str
        credit_pack_storage_retry_confirmation_response_utc_iso_string: str
        credit_pack_storage_retry_confirmation_response_pastel_block_height: int
        credit_pack_storage_retry_confirmation_response_message_version_string: str
        closest_agreeing_supernode_to_retry_storage_pastelid: str = Field(index=True)
        sha3_256_hash_of_credit_pack_storage_retry_confirmation_response_fields: str
        closest_agreeing_supernode_to_retry_storage_pastelid_signature_on_credit_pack_storage_retry_confirmation_response_hash: str    
```

That completes the entire flow from start to finish of creating a new credit pack ticket. 


----------

In this next section, we will go into more detail about how the various service functions that enable this functionality are implemented, with a particular focus on the provisioning and validation of new and existing credit pack tickets. 

First off, we should discuss how credit pack tickets are priced in PSL terms in the first place; that is, “for X number of inference credits, the total cost should be Y PSL.”  The basic idea here is to keep the price of inference credits relatively stable in USD terms even if the market price of PSL (i.e,. how much USD 1.0 PSL is worth) is moving around quite a bit. Another goal in the pricing is to keep the cost of inference credits in line with the underlying cost to serve requests, plus a 10% "theoretical profit margin" for the Supernodes that are serving these inference requests. Note that this isn't a *real* profit margin, since the Supernodes don't actually receive *any* of the cost of the credit pack tickets— this is all burned by the end user. However, this coin burning does reduce the total supply of PSL outstanding which benefits all holders, including and especially Supernode operators. The profit margin concept is more to ensure that the intrinsic economics of the broader pastel inference system are economically viable.

Various service functions are used to do this. Let's go through each function and explain their purpose and how they contribute to the estimation process:


- **`fetch_current_psl_market_price`**:
    - This function retrieves the current market price of PSL (Pastel) from two sources: CoinMarketCap and CoinGecko.
    - It sends HTTP requests to the respective APIs and extracts the PSL price in USD from the responses.
    - If the price cannot be retrieved from either source, it retries after a short delay.
    - It calculates the average price based on the available prices from the sources.
    - The function validates the average price to ensure it falls within a reasonable range.
    - It returns the average PSL price in USD.
    
- **`estimated_market_price_of_inference_credits_in_psl_terms`**:
    - This function estimates the market price of inference credits in PSL terms.
    - It first retrieves the current PSL market price in USD using the `fetch_current_psl_market_price` function.
    - It then calculates the cost per credit in USD, considering a target value per credit and a target profit margin.
    - The target value per credit represents the underlying cost to serve inference requests, while the profit margin ensures the economic viability of the Pastel inference system.
    - The cost per credit in USD is converted to PSL terms by dividing it by the current PSL market price.
    - The function returns the estimated market price of 1.0 inference credit in PSL.
    
- **`calculate_price_difference_percentage`** (in the inference client):
    - This function calculates the percentage difference between a quoted price and an estimated price.
    - It takes the quoted price and estimated price as input and computes the absolute difference between them.
    - The difference is then divided by the estimated price to obtain the percentage difference.
    - It raises an error if the estimated price is zero to avoid division by zero.
    - The function returns the price difference percentage.
    
- **`confirm_preliminary_price_quote`** (in the inference client):
    - This function confirms the preliminary price quote for a credit pack purchase.
    - It takes the preliminary price quote, maximum total credit pack price, and maximum per credit price as input.
    - If the maximum prices are not provided, it uses default values.
    - It extracts the quoted price per credit, quoted total price, and requested credits from the preliminary price quote.
    - It estimates the fair market price for the credits using the `estimated_market_price_of_inference_credits_in_psl_terms` function.
    - It calculates the price difference percentage between the quoted price and the estimated fair price using the `calculate_price_difference_percentage` function.
    - The function compares the quoted prices with the maximum prices and the estimated fair price.
    - If the quoted prices are within the acceptable range and the price difference percentage is below a certain threshold, it confirms the preliminary price quote.
    - Otherwise, it logs a warning message and rejects the price quote.
    internal_estimate_of_credit_pack_ticket_cost_in_psl (in the inference client):
    - This function provides an internal estimate of the cost of a credit pack ticket in PSL terms.
    - It takes the desired number of credits and a price cushion percentage as input.
    - It retrieves the estimated market price per credit in PSL using the `estimated_market_price_of_inference_credits_in_psl_terms` function.
    - It calculates the estimated total cost of the ticket by multiplying the desired number of credits, estimated price per credit, and the price cushion percentage.
    - The price cushion percentage allows for some flexibility in the pricing to account for market fluctuations or other factors.
    - The function returns the estimated total cost of the credit pack ticket in PSL.

These functions work together to estimate the cost of a credit pack ticket in PSL terms, considering the desired number of credits and the underlying cost to serve inference requests. The `fetch_current_psl_market_price` function retrieves the current PSL market price, which is used by the `estimated_market_price_of_inference_credits_in_psl_terms` function to estimate the fair market price of inference credits in PSL.

The `confirm_preliminary_price_quote` function in the inference client uses these estimates to validate the quoted prices from the responding supernode. It ensures that the quoted prices are within acceptable ranges and not significantly different from the estimated fair market price. This helps protect the user from potential price manipulation or uninformed pricing.

The `internal_estimate_of_credit_pack_ticket_cost_in_psl` function provides an internal estimate of the total cost of a credit pack ticket based on the desired number of credits and a price cushion percentage. This estimate can be used by the client to set reasonable maximum prices when requesting a credit pack. Overall, these functions contribute to the estimation and validation of credit pack ticket costs in PSL terms, ensuring that the prices are economically viable for the Pastel inference system and fair for the users.


----------

Next, we go into the various service functions for validating existing credit pack tickets. Before any credit pack ticket can be used, it’s not enough for the end user to have a TXID that points to a credit pack ticket in the Pastel Blockchain; theoretically, something that looks like a credit pack ticket could be created by ANY user without it being valid. So the mere fact that a ticket exists in the blockchain doesn’t mean that it’s valid. We also can’t rely on pasteld to automatically validate the ticket in the same way that it does for Sense, Cascade, and NFT tickets, because in this new `contract` ticket type, pasteld just stores the raw ticket data without ever having to decode and parse it, let alone validate it.

This is what gives us the flexibility and power to dynamically revise the credit pack ticket fields/structure (which is also facilitated by the inclusion of a “message_version_string” in nearly every message, so we can deal with changes in a robust way to the inference protocol) and also to create completely new kinds of `contract` tickets for new applications (e.g., some kind of generic “smart contract” application that can run little WASM containers, similar to AWS Lambda functions). But it also means that all of the detailed level validation of the credit pack ticket data itself needs to be done by us in the Inference Layer server code in Python.

That’s no problem though— it’s easy enough to validate a credit pack ticket in Python. We just need to get the ticket data from the blockchain, parse out the component messages, and run them through the same validation process that we apply when the Supernode receives a message via a REST endpoint. In fact, we should review that function, because it’s quite long and intricate, because we use the same validation function to check ALL the various kinds of messages that are sent around during the process of creating a new credit pack (in fact, we also reuse this same function for inference request message validation). You can read the entire code for the function [here](https://github.com/pastelnetwork/python_inference_layer_server/blob/4176bb6e3e52cdeb6e3c61f291b2944e800b764f/service_functions.py#L5885).




Below is a detailed explanation in words of how the function works:

The `validate_credit_pack_ticket_message_data_func` is a comprehensive validation function that performs several checks on a received SQLModel instance to ensure the integrity and validity of the data. Here's a detailed explanation of how the function verifies the correctness of the message and ensures it hasn't been tampered with:


1. Timestamp Validation:
    - The function iterates through all the fields in the model instance and checks for fields ending with "_timestamp_utc_iso_string".
    - For each timestamp field, it attempts to convert the field value to a datetime object using `pd.to_datetime()`. If the conversion fails, it indicates an invalid timestamp format, and an error is appended to the `validation_errors` list.
    - Additionally, the function compares the timestamp in the field with the current timestamp using the `compare_datetimes()` function. If the difference between the timestamps exceeds a certain threshold (`MAXIMUM_LOCAL_PASTEL_BLOCK_HEIGHT_DIFFERENCE_IN_BLOCKS`), it suggests that the timestamp is too far from the current time, and an error is appended to the `validation_errors` list.
    
2. Pastel Block Height Validation:
    - The function retrieves the best block hash, Merkle root, and block height using the `get_best_block_hash_and_merkle_root_func()` function.
    - It then iterates through the fields in the model instance and checks for fields ending with "_pastel_block_height".
    - For each block height field, it compares the field value with the current block height obtained from the Pastel network. If the absolute difference between the field value and the current block height exceeds `MAXIMUM_LOCAL_PASTEL_BLOCK_HEIGHT_DIFFERENCE_IN_BLOCKS`, it indicates a discrepancy, and an error is appended to the `validation_errors` list.
    
3. Hash Validation:
    - The function computes the expected SHA3-256 hash of the response fields using the `compute_sha3_256_hash_of_sqlmodel_response_fields()` function.
    - It then searches for a field name starting with "sha3_256_hash_of" and ending with "_fields" to locate the hash field in the model instance.
    - If a hash field is found, the function compares the actual hash value stored in the field with the computed expected hash. If the hashes don't match, it indicates that the response fields have been modified, and an error is appended to the `validation_errors` list.
    
4. Pastel ID Signature Validation:
    - The function handles signature validation differently depending on the type of the model instance.
    - For `CreditPackPurchasePriceAgreementRequestResponse` instances:
        - It identifies the first Pastel ID field and signature fields based on naming conventions.
        - If the corresponding Pastel ID field is found, it verifies each signature field using the `verify_message_with_pastelid_func()` function, passing the Pastel ID, the message to verify (either the hash or the JSON-encoded request fields), and the signature.
        - If any signature verification fails, an error is appended to the `validation_errors` list.
    - For other model instances:
        - It identifies the last signature field and the last hash field based on naming conventions.
        - If both fields are found, it checks for the presence of the corresponding Pastel ID field.
        - If the Pastel ID field is found or not applicable (in case of combined Pastel ID and signature fields), it extracts the Pastel ID and signature from the fields.
        - It then verifies the signature using the `verify_message_with_pastelid_func()` function, passing the Pastel ID, the message to verify (the hash field value), and the signature.
        - If the signature verification fails, an error is appended to the `validation_errors` list.
        

Finally, the function returns the list of validation errors (`validation_errors`). If the list is empty, it indicates that the message passed all the validation checks successfully. If there are any errors, they are included in the returned list for further handling or reporting. By performing these comprehensive validations, the function ensures that the received message adheres to the expected format, timestamps are within acceptable ranges, block heights match the current network state, hashes are consistent, and signatures are valid. This helps in detecting any modifications or tampering of the message during transmission or by malicious actors.

In addition to the main validation function, several helper functions support the detailed validation process:


1. `compare_datetimes(datetime_input1, datetime_input2)`:
    - This function ensures that the datetime inputs are converted to timezone-aware datetime objects. It calculates the difference in seconds between the two datetime inputs and checks if they are within an acceptable range (`MAXIMUM_LOCAL_UTC_TIMESTAMP_DIFFERENCE_IN_SECONDS`). If the difference is too large, a warning is logged.
    
2. `get_sha256_hash_of_input_data_func(input_data_or_string)`:
    - This function computes the SHA3-256 hash of the input data. If the input is a string, it is encoded to bytes before hashing. The function returns the hexadecimal representation of the hash.
    
3. `sort_dict_by_keys(input_dict)`:
    - This function sorts a dictionary and its nested dictionaries by keys. It converts the sorted dictionary into a JSON string, which can be useful for consistent and reproducible hash calculations.
    
4. `extract_response_fields_from_credit_pack_ticket_message_data_as_json_func(model_instance: SQLModel)`:
    - This asynchronous function extracts the response fields from a SQLModel instance and converts them into a JSON string. It handles various data types (e.g., datetime, list, dict, decimal) and ensures the fields are sorted for consistent hash computation.
    
5. `async def compute_sha3_256_hash_of_sqlmodel_response_fields(model_instance: SQLModel)`:
    - This asynchronous function uses `extract_response_fields_from_credit_pack_ticket_message_data_as_json_func` to get the JSON representation of the response fields and then computes the SHA3-256 hash of this JSON string.
    
6. `validate_credit_pack_blockchain_ticket_data_field_hashes(model_instance: SQLModel)`:
    - This asynchronous function validates the hashes of the response fields in a SQLModel instance. It compares the expected hash (computed from the response fields) with the actual hash stored in the model instance. If the hashes don't match, it appends an error to the `validation_errors` list.
    

Now we can also look at the full function for validating an existing credit pack ticket (`validate_existing_credit_pack_ticket`); the code for this function can be found [here](https://github.com/pastelnetwork/python_inference_layer_server/blob/4176bb6e3e52cdeb6e3c61f291b2944e800b764f/service_functions.py#L2969).


And a detailed explanation of how it works in words is below:

The `validate_existing_credit_pack_ticket` function is an asynchronous function that takes a `credit_pack_ticket_txid` as input and performs a comprehensive validation of an existing Pastel credit pack ticket. The purpose of this function is to ensure that the credit pack ticket is valid and has not been tampered with, even if the Supernode performing the validation was not present when the ticket was initially created. Here's a detailed breakdown of how the function works:


1. The function starts by retrieving the credit pack ticket data from the blockchain using the `retrieve_credit_pack_ticket_from_blockchain_using_txid` function. This function returns three objects: `credit_pack_purchase_request`, `credit_pack_purchase_request_response`, and `credit_pack_purchase_request_confirmation`.
2. It initializes a `validation_results` dictionary to store the validation results, including the overall validity of the ticket, individual validation checks, and any validation failure reasons.
3. Payment Validation:
    - The function calls the `check_burn_transaction` function to validate the payment associated with the credit pack ticket.
    - It checks if a matching or exceeding burn transaction exists for the specified `txid_of_credit_purchase_burn_transaction`, `credit_usage_tracking_psl_address`, `proposed_total_cost_of_credit_pack_in_psl`, and `request_response_pastel_block_height`.
    - If a matching or exceeding transaction is found, the payment is considered valid. Otherwise, the ticket is marked as invalid, and the reason is added to the `validation_failure_reasons_list`.
4. Supernode Validation:
    - The function retrieves the count and details of active supernodes at the time of the credit pack purchase using the `fetch_active_supernodes_count_and_details` function, based on the `request_response_pastel_block_height`.
    - It checks if all the potentially agreeing supernodes and agreeing supernodes listed in the ticket were valid supernodes at the time of the purchase by comparing their pastelids with the list of active supernodes.
    - If any supernode is not found in the list of active supernodes, the ticket is marked as invalid, and the reason is added to the `validation_failure_reasons_list`.
5. Ticket Response Hash Validation:
    - The function validates the hashes of the credit pack purchase request response and confirmation objects using the `validate_credit_pack_blockchain_ticket_data_field_hashes` function.
    - It checks if the computed hashes match the hashes included in the ticket objects.
    - If any hash mismatch is detected, the ticket is marked as invalid, and the reason is added to the `validation_failure_reasons_list`.
6. Signature Validation:
    - The function verifies the signatures of the agreeing supernodes using the `verify_message_with_pastelid_func` function.
    - It iterates over each agreeing supernode pastelid and verifies their signature on the `credit_pack_purchase_request_fields_json_b64` field.
    - If any signature fails validation, the ticket is marked as invalid, and the reason is added to the `validation_failure_reasons_list`.
7. Agreeing Supernodes Validation:
    - The function calculates the quorum percentage and agreeing percentage based on the number of potentially agreeing supernodes, agreeing supernodes, and total active supernodes at the time of the purchase.
    - It checks if the quorum percentage and agreeing percentage meet the required thresholds (`SUPERNODE_CREDIT_PRICE_AGREEMENT_QUORUM_PERCENTAGE` and `SUPERNODE_CREDIT_PRICE_AGREEMENT_MAJORITY_PERCENTAGE`).
    - If either the quorum or agreeing percentage is not met, the ticket is marked as invalid, and the reason is added to the `validation_failure_reasons_list`.
8. Handling Validation Failures:
    - If any validation failures are detected, the function logs the failure reasons and adds the invalid credit pack ticket TXID to a "known bad" table in the database using the `insert_credit_pack_ticket_txid_into_known_bad_table_in_db` function.
9. Finally, the function returns the `validation_results` dictionary, which includes the overall validity of the ticket, individual validation checks, and any validation failure reasons.

This function ensures the integrity and validity of an existing Pastel credit pack ticket by performing comprehensive validations, including payment validation, supernode validation, hash validation, signature validation, and agreeing supernodes validation. By checking against historical data and verifying the consistency of the ticket data, the function can determine the validity of a credit pack ticket even if the validating supernode was not present during the ticket's creation.

We also offer an convenience endpoint for use by an end user where the user can specify their PastelID and get back a list of all their valid credit pack tickets (this function can also determine the current credit balance of those tickets; how this works will be explained in more detail below when we explain the flow for Inference Requests). The code for that function can be found [here](https://github.com/pastelnetwork/python_inference_layer_server/blob/4176bb6e3e52cdeb6e3c61f291b2944e800b764f/service_functions.py#L3091).

And here is the breakdown of how that function works:

The `get_valid_credit_pack_tickets_for_pastelid` function is an asynchronous function that retrieves a list of valid credit pack tickets for a given PastelID. It provides a convenient way for an end user to retrieve all their credit pack tickets and their current credit balances. Here's a detailed breakdown of how the function works:


1. The function starts by querying the database using SQLAlchemy's `select` statement to retrieve all the `CreditPackPurchaseRequestConfirmation` records associated with the given `pastelid`. This is done within an asynchronous database session using `db_code.Session()`.
2. It initializes an empty list called `complete_tickets` to store the complete credit pack tickets.
3. For each `request_confirmation` retrieved from the database, the function performs the following steps:
    - Extracts the `sha3_256_hash_of_credit_pack_purchase_request_fields` from the `request_confirmation`.
    - Queries the database to find the corresponding `CreditPackPurchaseRequestResponseTxidMapping` record using the extracted hash. This mapping contains the `pastel_api_credit_pack_ticket_registration_txid` (TXID) associated with the credit pack ticket.
    - If a `txid_mapping` is found, the function proceeds to retrieve the complete credit pack ticket data.
4. Retrieving Complete Credit Pack Ticket Data:
    - If a `txid_mapping` is found, the function queries the database to check if there is an existing `CreditPackCompleteTicketWithBalance` record associated with the TXID.
    - If an existing record is found (`existing_data`), it means the complete credit pack ticket data is already stored in the database. In this case:
        - The function loads the `complete_credit_pack_data_json` from the `existing_data` record and parses it into a `complete_ticket` dictionary.
        - It calls the `determine_current_credit_pack_balance_based_on_tracking_transactions` function to determine the current credit balance and the number of confirmation transactions for the credit pack ticket based on the TXID.
        - The `credit_pack_current_credit_balance` and `balance_as_of_datetime` fields are added to the `complete_ticket` dictionary.
        - The updated `complete_ticket` is then converted back to JSON format and stored in the `existing_data` record, along with the updated `datetime_last_updated`.
        - The changes are committed to the database within an asynchronous database session.
    - If an existing record is not found, it means the complete credit pack ticket data needs to be retrieved and stored in the database. In this case:
        - The function calls the `determine_current_credit_pack_balance_based_on_tracking_transactions` function to determine the current credit balance and the number of confirmation transactions for the credit pack ticket based on the TXID.
        - It retrieves the `credit_pack_purchase_request_response` and `credit_pack_purchase_request_confirmation` using the `retrieve_credit_pack_ticket_using_txid` function.
        - If both the response and confirmation are successfully retrieved, the function proceeds to create a `complete_ticket` dictionary containing the `credit_pack_purchase_request`, `credit_pack_purchase_request_response`, `credit_pack_purchase_request_confirmation`, `credit_pack_registration_txid`, `credit_pack_current_credit_balance`, and `balance_as_of_datetime`.
        - The `complete_ticket` dictionary is then processed by converting UUIDs to strings and normalizing the data using the `convert_uuids_to_strings` and `normalize_data` functions.
        - The processed `complete_ticket` is converted to JSON format.
        - If an `existing_data` record exists, the `complete_credit_pack_data_json` and `datetime_last_updated` fields are updated with the new data.
        - If an `existing_data` record doesn't exist, a new `CreditPackCompleteTicketWithBalance` record is created with the TXID, `complete_credit_pack_data_json`, and `datetime_last_updated`.
        - The changes are committed to the database within an asynchronous database session.
5. Finally, the `complete_ticket` is appended to the `complete_tickets` list.
6. After processing all the `request_confirmations`, the function returns the `complete_tickets` list containing all the valid credit pack tickets for the given `pastelid`.

This function efficiently retrieves and processes credit pack tickets for a specific PastelID. It leverages the database to store and retrieve complete ticket data, including the current credit balance. By caching the ticket data in the database, subsequent requests for the same ticket can be served faster without the need to retrieve the data from the blockchain every time. The function also ensures that the ticket data is updated with the latest credit balance and timestamp whenever it is retrieved.

The process of retrieving valid credit pack tickets for a specific PastelID using the `get_valid_credit_pack_tickets_for_pastelid` function relies on the local Supernode's SQLite database. However, to ensure that the local database contains up-to-date information about all the credit pack tickets in the blockchain, the Supernode periodically runs background tasks to gather ticket data from the blockchain, parse it, and ingest it into the local database, including the following:


1. `retrieve_generic_ticket_data_from_blockchain`:
    - This function retrieves the ticket data for a specific ticket TXID from the blockchain using the `rpc_connection.tickets('get', ...)` command.
    - It retrieves the ticket's input data, including the fully parsed SHA3-256 hash and the ticket input data dictionary.
    - It computes the SHA3-256 hash of the retrieved ticket input data and compares it with the retrieved fully parsed hash to ensure data integrity.
    - If the hashes match, it returns the credit pack combined blockchain ticket data as a JSON string.
2. `get_list_of_credit_pack_ticket_txids_already_in_db`:
    - This function retrieves the list of credit pack ticket TXIDs that are already stored in the local SQLite database.
    - It queries the `CreditPackPurchaseRequestResponseTxidMapping` table to get the list of TXIDs.
    - It returns a list of unique TXIDs that are already stored in the database.
3. `list_generic_tickets_in_blockchain_and_parse_and_validate_and_store_them`:
    - This function retrieves all the blockchain tickets of a specific type (e.g., "INFERENCE_API_CREDIT_PACK_TICKET") starting from a given block height.
    - It uses the `rpc_connection.tickets('list', ...)` command to retrieve the ticket data from the blockchain.
    - It checks the internal consistency of the retrieved tickets by comparing the computed SHA3-256 hashes of the ticket input data with the retrieved fully parsed hashes.
    - It retrieves the list of already stored credit pack TXIDs and the list of known bad credit pack TXIDs from the local database.
    - If `force_revalidate_all_tickets` is set to `True`, it attempts to validate all retrieved tickets, even those already stored in the database or in the known bad list.
    - If `force_revalidate_all_tickets` is set to `False`, it only attempts to validate tickets that are not already stored in the database or in the known bad list.
    - For each ticket TXID that needs validation, it calls the `validate_existing_credit_pack_ticket` function to perform in-depth validation of all aspects of the ticket.
    - If a ticket passes validation, it is saved to the local database using the `retrieve_credit_pack_ticket_using_txid` function.
    - It returns the list of retrieved ticket input data as JSON strings and the list of fully validated ticket TXIDs.
4. `periodic_ticket_listing_and_validation`:
    - This function is an infinite loop that periodically calls the `list_generic_tickets_in_blockchain_and_parse_and_validate_and_store_them` function.
    - It runs every 30 minutes (adjustable) to continuously update the local database with the latest credit pack ticket data from the blockchain.
    - If an error occurs during the periodic execution, it logs the error and continues the loop.

The `startup` function is called when the Supernode starts up. It initializes the database, generates or loads the encryption key, decrypts sensitive fields, and creates background tasks for various operations, including the `periodic_ticket_listing_and_validation` task.
By periodically running the `periodic_ticket_listing_and_validation` task, the Supernode ensures that its local SQLite database is continuously updated with the latest credit pack ticket data from the blockchain. This allows the `get_valid_credit_pack_tickets_for_pastelid` function to retrieve valid credit pack tickets for a specific PastelID efficiently from the local database, without having to query the blockchain every time.

This approach optimizes performance and reduces the load on the blockchain by caching the ticket data in the local database and periodically synchronizing it with the blockchain data. It enables fast retrieval of valid credit pack tickets for a given PastelID while ensuring data consistency and integrity through periodic updates and validations.

One obvious issue with this approach is that we need it to be robust to possible Pastel blockchain level re-orgs and rollbacks; essentially, the local database can go “out of sync” with the underlying blockchain if the blockchain itself changes (this obviously doesn’t happen in normal operation). We do this by using various additional functions and background services which are periodically running:

The code provided includes several functions that work together to handle chain reorgs and rollbacks in the process of tracking and storing burn transactions and block hashes. Let's break down how these functions contribute to handling chain reorgs and rollbacks:


1. `detect_chain_reorg_and_rescan`:
    - This function runs continuously in the background, periodically checking for chain reorgs.
    - It retrieves the latest stored block hash from the database and compares it with the block hash at the same height in the blockchain.
    - If the stored block hash doesn't match the block hash in the blockchain, it indicates that a chain reorg has occurred.
    - When a chain reorg is detected, the function triggers a full rescan of burn transactions by deleting all existing records in the `BurnAddressTransaction` and `BlockHash` tables and calling the `full_rescan_burn_transactions` function.
    - The function sleeps for a specified interval (e.g., 6000 seconds) before checking for chain reorgs again.
2. `full_rescan_burn_transactions`:
    - This function is called when a chain reorg is detected or when no burn transaction records are found in the database.
    - It performs a full rescan of burn transactions starting from the genesis block.
    - It uses the `rpc_connection.scanburntransactions("*")` method to retrieve all burn transactions from any address.
    - The retrieved burn transactions are then processed in chunks using the `process_transactions_in_chunks` function, which inserts the transactions into the database.
    - After processing the burn transactions, the function checks if block hash records exist in the database. If not, it calls the `fetch_and_insert_block_hashes` function to retrieve and store block hashes for the entire blockchain.
3. `fetch_and_insert_block_hashes`:
    - This function fetches and inserts block hashes into the database for a specified range of block heights.
    - It uses the `rpc_connection.getblockhash` method to retrieve block hashes in batches.
    - The retrieved block hashes are processed to construct potential block insert tuples, including the previous and next block hashes if available.
    - The function filters out duplicate heights within the batch and checks the database for existing block heights to avoid inserting duplicates.
    - The final block hashes to be inserted are passed to the `bulk_insert_block_hashes` function for bulk insertion into the database.
    - The function logs progress after inserting a certain number of block hashes (e.g., every 500 inserts) and continues processing the next range of block heights.
4. `bulk_insert_block_hashes`:
    - This function performs the bulk insertion of block hashes into the database.
    - It rechecks the existing block heights just before inserting to handle potential race conditions.
    - The function creates new `BlockHash` instances for the block hashes that don't already exist in the database.
    - It adds the new block hashes to the database session and attempts to commit the changes.
    - If an exception occurs during the bulk insert, the changes are rolled back, and an error is logged.
    

By using these functions together, the code can handle chain reorgs and rollbacks effectively:


- The `detect_chain_reorg_and_rescan` function continuously monitors for chain reorgs by comparing the stored block hashes with the actual block hashes in the blockchain.
- When a chain reorg is detected, the function triggers a full rescan of burn transactions and block hashes by calling the `full_rescan_burn_transactions` function.
- The `full_rescan_burn_transactions` function retrieves all burn transactions from the genesis block and processes them in chunks, inserting them into the database.
- If block hash records are missing, the `fetch_and_insert_block_hashes` function is called to retrieve and store block hashes for the entire blockchain.
- The `fetch_and_insert_block_hashes` function fetches block hashes in batches, filters out duplicates, and passes them to the `bulk_insert_block_hashes` function for bulk insertion into the database.

By performing a full rescan of burn transactions and block hashes when a chain reorg is detected, the code ensures that the stored data is consistent with the current state of the blockchain, effectively handling chain reorgs and rollbacks. Also, as we will see in the next section, these background tasks cache certain information about burn transactions in the local database that are critical for efficiently computing the remaining available balance on a credit pack ticket, so that Supernodes don’t accept inference requests from credit pack tickets that don’t have enough credit value left on them to cover the full cost of the inference request. 

OK, we are now ready to discuss how Inference Requests work and how they interact with credit pack tickets in Pastel.


----------
## Inference Requests

We already gave a pretty detailed background on how inference requests work in the abstract, and in particular, how the system tracks the usage of credits from credit packs in order to determine the remaining available balance of each credit pack (again, only the credit pack’s initial balance amount is stored in the original blockchain ticket— all the information for tracking the current remaining available credit balance must be dynamically computed by the Supernodes by keeping track of all the various burn transactions sent from the designated PSL tracking address that is specified in the original credit pack ticket to determine how many credits have already been used from that specific credit pack ticket). 

We now introduce the detailed flow involved in an end user creating a new inference request and how that request is fulfilled by the responding_supernode:

**Inference Request Flow:**

1. **Initiating the Inference Request:**
    - The user creates a new inference request by sending a POST request to the `/make_inference_api_usage_request` endpoint with an `InferenceAPIUsageRequest` message, which looks like this:

```python
    class InferenceAPIUsageRequest(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        inference_request_id: str = Field(unique=True, index=True)
        requesting_pastelid: str = Field(index=True)
        credit_pack_ticket_pastel_txid: str = Field(index=True)
        requested_model_canonical_string: str
        model_inference_type_string: str
        model_parameters_json_b64: str
        model_input_data_json_b64: str
        inference_request_utc_iso_string: str
        inference_request_pastel_block_height: int
        status: str = Field(index=True)
        inference_request_message_version_string: str
        sha3_256_hash_of_inference_request_fields: str
        requesting_pastelid_signature_on_request_hash: str
```

    - This message contains essential information such as the user's PastelID, the credit pack ticket TXID, the requested model, inference type, model parameters, input data, and various other details.
    - To avoid any issues with quoted JSON or escaping text, all model inputs and all model parameters are supplied as base64 encoded JSON. This also provides flexibility to work with binary inputs, such as input images for the “Ask a Question About an Image” inference requests.
    - The user signs the hash of the inference request fields using their PastelID to ensure the integrity and authenticity of the request.
    - The inclusion of the credit pack ticket TXID allows the responding supernode to verify that the user has sufficient credits to cover the cost of the inference request.
    
2. **Processing the Inference Request:**
    - The responding supernode, determined by the XOR distance between the user's PastelID and the supernode's PastelID, receives the inference request.
    - The supernode first validates the request by checking the signature, ensuring that the request comes from the claimed PastelID and that the data hasn't been tampered with.
    - Next, the supernode checks if it supports the requested model and inference type. This is crucial because different supernodes may have different capabilities and support different models/services.
    - If the supernode supports the requested model, it then ensures that the provided input data matches the expected format for that specific model/service. For example, if the inference request is for text completion, the supernode verifies that the input is indeed a text prompt and not an image file.
    - The supernode then calculates the cost of the inference request based on the specific details provided. This cost calculation varies depending on the type of inference:
        - For text completion requests, the supernode uses the relevant tokenizer for the requested model/service to count the number of tokens in the input prompt. The cost is determined based on the token count and the specific pricing implemented by the API provider in the case of service based offerings; a target “profit margin” is added on top of this to determine the cost of the inference request in credit pack credits. 
        - In the case of models that are hosted locally or remotely using Swiss Army Llama, where there is no API based pricing, we estimate the cost using parameters stored in the model_menu.json file particular to each model that takes into account the memory and processing load of running the model. Eventually, when we have gathered more operating data, we will convert these into estimates of total time used on a GPU enabled instance, and use the average live pricing level for a service like vast.ai to come up with more accurate pricing that covers the true underlying cost of using each model locally or remotely.
        - For audio transcription and embedding requests, the cost is calculated based on the duration of the audio file supplied by the user, measured in seconds.
        - For image generation requests, the relevant API based pricing is again used to arrive at an estimated cost in credits that should cover the Supernode owner’s operating expenses from using the relevant API for that request.
        - For “Ask a Question About an Image” requests, the relevant pricing for the API is again used, taking into account the actual resolution of the input image. 
    - Once the cost is determined, the supernode generates an `InferenceAPIUsageResponse` message, which includes the proposed cost in inference credits, the remaining credits in the user's credit pack after processing the request, and other relevant details, which looks like this:

```python
    class InferenceAPIUsageResponse(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        inference_response_id: str = Field(unique=True, index=True)
        inference_request_id: str = Field(foreign_key="inferenceapiusagerequest.inference_request_id", index=True)
        proposed_cost_of_request_in_inference_credits: float
        remaining_credits_in_pack_after_request_processed: float
        credit_usage_tracking_psl_address: str = Field(index=True)
        request_confirmation_message_amount_in_patoshis: int
        max_block_height_to_include_confirmation_transaction: int
        inference_request_response_utc_iso_string: str
        inference_request_response_pastel_block_height: int
        inference_request_response_message_version_string: str    
        sha3_256_hash_of_inference_request_response_fields: str
        supernode_pastelid_and_signature_on_inference_request_response_hash: str
    ```

    - The supernode sends this response back to the user, providing them with the cost estimate for their inference request.
    
2. **Broadcasting the Inference Request and Response:**
    - After sending the `InferenceAPIUsageResponse` back to the user, the responding supernode broadcasts a combined message containing both the `InferenceAPIUsageRequest` and `InferenceAPIUsageResponse` to the nearest supernodes based on the XOR distance to the user's PastelID.
    - This broadcast ensures that multiple supernodes are aware of the inference request and can assist in processing it if needed, providing redundancy and fault tolerance.
    - The broadcast message includes the original inference request details, the proposed cost, and the remaining credits in the user's credit pack.
    
3. **Confirming the Inference Request:**
    - Upon receiving the `InferenceAPIUsageResponse` from the responding supernode, the user reviews the proposed cost and decides whether to proceed with the inference request.
    - If the user agrees to the cost, they confirm the inference request by first sending the corresponding number Patoshis from the designated PSL tracking address to the Pastel burn address; once they do that and get a TXID for that burn transactions, they send a POST request to the `/confirm_inference_request` endpoint with an `InferenceConfirmation` message:

```python
    class InferenceConfirmation(SQLModel):
        inference_request_id: str
        requesting_pastelid: str
        confirmation_transaction: dict
```

    - The `InferenceConfirmation` message includes the inference request ID, the user's PastelID, and a confirmation transaction.
    - The confirmation transaction serves as proof that the user has agreed to the proposed cost and has authorized the deduction of the required inference credits from their credit pack. Since only the authorized creator of the credit pack would have control over the tracking address, everyone knows for sure that the request originated with the user for all intents and purposes (ignoring cases where the user got hacked, for example). 
    - Note that multiple different PastelIDs (the ones specifically listed in the original credit pack ticket data) can create and authorize inference requests from the credit pack ticket; this means that multiple users (each with their own PastelID) could in theory share a single credit pack and easily keep track of who has used what in terms of credits from the pack, but each of these would need to have the private key of the tracking address imported into their local wallet. 
    - However, since only a very tiny amount of PSL is required for the tracking transactions, this address could contain, say, 10 PSL or less, so there would be very little risk in sharing the private key, since there isn’t much PSL value to steal. Critically, even if a third party were able to get the private key to the tracking address, they still wouldn’t be able to “steal” inference requests using it unless they had ALSO stolen the PastelID private keys for one of the PastelIDs included in the list of authorized PastelIDs for that specific credit pack.
    
4. **Executing the Inference Request:**
    - Once the `responding_supernode` receives the `InferenceConfirmation` from the user, it proceeds to process and execute the inference request.
    - The Supernode verifies the confirmation transaction to ensure that the user has authorized the deduction of the inference credits.
    - If the confirmation is valid, the Supernode begins executing the inference request using the specified model and parameters.
    - The execution process varies depending on the type of inference; for example:
        - For text completion requests, the Supernode feeds the input prompt to the specified language model or service using the (optional) user-supplied parameters (e.g., number of completions; number of tokens in each completion; sampling temperature) and generates the completion text based on the model's output.
        - For image generation requests, the Supernode feeds the image prompt and specific user parameters to the image generation model (e.g., Stable Diffusion) and generates an output image to be sent back to the user.
        - For “Ask a Question About an Image” requests, the user supplies an input image and question string, and the Supernode sends that to the multi-modal model (e.g., GPT4o, Llava, etc.) and returns the output response text to the user.
        - For document embedding requests, the supernode uses the specified embedding model to generate vector representations of the extracted sentences in the input document, which can be extracted from various common document formats such as PDF, DOC, HTML, TXT, and even scanned images and PDF using OCR. The user can also optionally supply a semantic query string which will be used to search across the embeddings, returning the most relevant parts of the document text.
        - For audio transcription requests, the Supernode uses the specified audio transcription model to convert the provided audio file into written text and optionally computes embedding vectors for the transcribed text using the specified LLM for calculating the embeddings. The user can also optionally supply a semantic query string which will be used to search across the embeddings, returning the most relevant parts of the transcribed text.
    - During the execution process, the Supernode may need to communicate with external APIs or services, depending on the requested model/service.
    - The Supernode monitors the progress of the inference execution and handles any errors or exceptions that may occur.
    
5. **Generating the Inference Output Result:**
    - Once the inference execution is completed, the `responding_supernode` generates an `InferenceAPIOutputResult` message, which looks like this:

```python
    class InferenceAPIOutputResult(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        inference_result_id: str = Field(unique=True, index=True)
        inference_request_id: str = Field(foreign_key="inferenceapiusagerequest.inference_request_id", index=True)
        inference_response_id: str = Field(foreign_key="inferenceapiusageresponse.inference_response_id", index=True)
        responding_supernode_pastelid: str = Field(index=True)
        inference_result_json_base64: str
        inference_result_file_type_strings: str
        inference_result_utc_iso_string: str
        inference_result_pastel_block_height: int
        inference_result_message_version_string: str    
        sha3_256_hash_of_inference_result_fields: str    
        responding_supernode_signature_on_inference_result_id: str
```

    - This message contains the inference result ID, the original inference request ID, the inference response ID, the `responding_supernode`'s PastelID, and the actual inference output data.
    - The inference output data is serialized and stored in the `inference_result_json_base64` field as a base64-encoded JSON string.
    - The Supernode also includes the file type of the inference output (e.g., JSON, text, image) in the `inference_result_file_type_strings` field.
    - The Supernode signs the hash of the inference result fields using its PastelID to ensure the integrity and authenticity of the output.
    
6. **Checking the Status of Inference Request Results:**
    - The user can check the status of their inference request by sending a GET request to the `/check_status_of_inference_request_results/{inference_response_id}` endpoint.
    - The endpoint expects the `inference_response_id` as a path parameter, which uniquely identifies the inference response associated with the user's request.
    - The `responding_supernode` receives the status check request and verifies that the `inference_response_id` exists and is associated with the requesting user.
    - If the inference request is still being processed, the Supernode responds with a status indicating that the results are not yet available.
    - If the inference request has been completed, the Supernode responds with a status indicating that the results are ready for retrieval.
    
7. **Retrieving the Inference Output Results:**
    - When the inference results are available, the user can retrieve them by sending a POST request to the `/retrieve_inference_output_results` endpoint.
    - The user provides the `inference_response_id`, their PastelID, and a challenge-response signature for authentication purposes.
    - The `responding_supernode` verifies the user's authentication by checking the provided PastelID and challenge-response signature.
    - If the authentication is successful, the Supernode retrieves the `InferenceAPIOutputResult` associated with the provided `inference_response_id`.
    - The Supernode sends the `InferenceAPIOutputResult` back to the user, allowing them to access the inference output data.
    - Additionally, the Supernode broadcasts the `InferenceAPIOutputResult` to the nearest Supernodes based on the XOR distance to the user's PastelID.
    - This broadcast ensures that multiple Supernodes have a record of the inference output and can provide the results to the user if the original `responding_supernode` becomes unavailable.
    
8. **Auditing the Inference Request Response:**
    - The user has the option to audit the inference request response by sending a POST request to the `/audit_inference_request_response` endpoint.
    - The user provides the `inference_response_id`, their PastelID, and a signature to prove their identity and authorization.
    - The `responding_supernode` verifies the user's signature and checks if the provided PastelID matches the one associated with the inference request.
    - If the authentication and authorization are successful, the Supernode retrieves the `InferenceAPIUsageResponse` associated with the provided `inference_response_id`.
    - The Supernode returns the `InferenceAPIUsageResponse` to the user, allowing them to review the details of the inference request response, such as the proposed cost and remaining credits.
    - This audit process provides transparency and allows the user to verify that the inference request was processed correctly and that the appropriate amount of credits were deducted from their credit pack.
    
9. **Auditing the Inference Request Result:**
    - Similar to auditing the inference request response, the user can also audit the inference request result by sending a POST request to the `/audit_inference_request_result` endpoint.
    - The user provides the `inference_response_id`, their PastelID, and a signature for authentication and authorization.
    - The `responding_supernode` verifies the user's signature and checks if the provided PastelID matches the one associated with the inference request.
    - If the authentication and authorization are successful, the supernode retrieves the `InferenceAPIOutputResult` associated with the provided `inference_response_id`.
    - The Supernode returns the `InferenceAPIOutputResult` to the user, allowing them to review the details of the inference output, such as the actual inference result data and file type.
    - This audit process ensures that the user can verify the correctness and integrity of the inference output received from the Supernode.
    

**Additional Endpoints:**

- `/get_inference_model_menu`:
    - This endpoint allows users to retrieve information about the available inference models and their parameters.
    - The `responding_supernode` maintains a menu of supported models and their corresponding parameters, such as input formats, output formats, and any specific options or configurations.
    - When a user sends a GET request to this endpoint, the Supernode responds with the inference model menu, providing the user with the necessary information to construct a valid inference request.
    - The model menu helps users understand the capabilities and requirements of each supported model, enabling them to select the appropriate model for their specific inference needs.
    
- `/download/{file_name}`:
    - This endpoint facilitates the download of inference result files, particularly when the inference output is not easily representable as a JSON string (e.g., generated images or audio files).
    - When an inference request results in a file output, the `responding_supernode` stores the file temporarily and provides a unique file name to the user as part of the `InferenceAPIOutputResult`.
    - The user can then send a GET request to the `/download/{file_name}` endpoint, specifying the file name they received.
    - The Supernode verifies that the requested file exists and is associated with the user's inference request.
    - If the file is found and the user is authorized to access it, the Supernode initiates a file download response, allowing the user to retrieve the inference output file.
    - The Supernode may implement additional security measures, such as file expiration or authentication, to ensure that only authorized users can download the inference output files.


----------

Now let’s get into more details about how this functionality is actually implemented in the Inference Layer server code.  We will introduce a category of inference related service functions, and then give detailed breakdowns of the names of these functions and how they work (inputs, outputs, and purpose/rationale). The code for all these functions can be found [here](https://github.com/pastelnetwork/python_inference_layer_server/blob/master/service_functions.py).


**Messaging Related Service Functions:**

These functions use the built-in Masternode messaging system that is already part of pasteld, and build additional abstractions on top of these so we can use PastelIDs as the basis of identity and security/signing. These messages are used internally by the Supernodes for helping with the flows for both credit pack creation and inference request processing, but we also expose “user oriented” messaging functionality that permits any Pastel user (i.e., not just a Supernode) to use their PastelID as a sort of “email address” for sending and receiving validated and signed messages (that is, users can know for sure that a given message really came from the specified PastelID sender and that the message wasn’t modified in transmit by one or more Supernodes; it does so by checking the PastelID signature on the signed message hash). In this scenario, the Supernodes (which are the only nodes that are able to actually send and receive the lower level messages using the built-in masternode messaging system) act as a kind of “[SMTP](https://en.wikipedia.org/wiki/Simple_Mail_Transfer_Protocol)” mail server on behalf of regular Pastel users.

Note that messages are not currently encrypted, so any Supernode can see the contents of a message sent between two ordinary users. However, it should be relatively simple to add some form of encryption using the pastelIDs of the users to automatically create shared secrets between pairs of users (this is on the list of future features). 

There are several functions crucial for handling inference requests and processing broadcast messages in the Inference Layer server. Let's go through each function and explain how they tie into the various steps of the inference request flow:


- **`broadcast_message_to_n_closest_supernodes_to_given_pastelid`**
    - This function is responsible for broadcasting a message to the closest supernodes based on a given PastelID. It takes the input PastelID, message body, and message type as parameters. First, it retrieves the list of supernodes and filters them based on their support for the desired model using the `is_model_supported` function. It excludes the local supernode from the list of supported supernodes. If no other supported supernodes are found, it falls back to using all supernodes except the local one. The function then selects the N closest supernodes to the input PastelID from the supported supernodes list. It signs the message using the local supernode's PastelID and broadcasts it to the selected supernodes. This function is used in the "Broadcasting the Inference Request and Response" step to inform nearby supernodes about the inference request and response.
    
- **`process_broadcast_messages`**
    This function processes broadcast messages received by the supernode. It takes the message and a database session as input. The function first checks the message type to determine how to handle it.
    For `inference_request_response_announcement_message` messages:
    - It checks if the corresponding `InferenceAPIUsageRequest` and `InferenceAPIUsageResponse` records already exist in the database.
    - If neither record exists, it creates new entries for both the request and response in the database.
    - If either record already exists, it skips the insertion to avoid duplicates.
    For `inference_request_result_announcement_message` messages:
    - It checks if the corresponding `InferenceAPIOutputResult` record already exists in the database.
    - If the record doesn't exist, it creates a new entry for the output result in the database.
    - If the record already exists, it skips the insertion to avoid duplicates.
    This function is used to process and store the information received from broadcast messages related to inference requests and results.
    
- **`monitor_new_messages`**
    This function continuously monitors for new messages received by the Supernode. It runs in an infinite loop, periodically checking for new messages. It retrieves the last processed timestamp from the database to determine the starting point for processing new messages. The function fetches new messages from the masternode messaging system using the `list_sn_messages_func`. For each new message, it checks if the message already exists in the database to avoid duplicates. If the message is new, it updates various metadata tables in the database, including:
    - `MessageSenderMetadata`: Tracks the total messages sent and data sent by each sending Supernode.
    - `MessageReceiverMetadata`: Tracks the total messages received and data received by each receiving Supernode.
    - `MessageSenderReceiverMetadata`: Tracks the total messages and data exchanged between each pair of sending and receiving Supernodes.
    - `MessageMetadata`: Tracks the overall total messages, senders, and receivers in the system.
    After updating the metadata, the function processes the new messages concurrently using the `process_broadcast_messages` function. This function ensures that the Supernode stays up to date with new messages and processes them accordingly, tying into the overall flow of handling inference requests and responses.

**Corresponding SQLModel Data Models**
The data models used to support these functions are defined using SQLModel, which combines SQLAlchemy ORM models with Pydantic response models. Some of the key models include:


- `Message` - Represents a message exchanged between Supernodes:

```python
    class Message(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        sending_sn_pastelid: str = Field(index=True)
        receiving_sn_pastelid: str = Field(index=True)
        sending_sn_txid_vout: str = Field(index=True)
        receiving_sn_txid_vout: str = Field(index=True)
        message_type: str = Field(index=True)
        message_body: str = Field(sa_column=Column(JSON))
        signature: str
        timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
```

- `MessageMetadata` - Tracks the overall total messages, senders, and receivers in the system:

```python
    class MessageMetadata(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        total_messages: int
        total_senders: int
        total_receivers: int
        timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
```

- `MessageSenderMetadata` - Tracks the total messages sent and data sent by each sending supernode:

```python
    class MessageSenderMetadata(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        sending_sn_pastelid: str = Field(index=True)
        sending_sn_txid_vout: str = Field(index=True)
        sending_sn_pubkey: str = Field(index=True)
        total_messages_sent: int
        total_data_sent_bytes: float
        timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
```

- `MessageReceiverMetadata` - Tracks the total messages received and data received by each receiving supernode:

```python
    class MessageReceiverMetadata(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        receiving_sn_pastelid: str = Field(index=True)
        receiving_sn_txid_vout: str = Field(index=True)
        total_messages_received: int
        total_data_received_bytes: float
        timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
```

- `MessageSenderReceiverMetadata` - Tracks the total messages and data exchanged between each pair of sending and receiving Supernodes:

```python
    class MessageSenderReceiverMetadata(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        sending_sn_pastelid: str = Field(index=True)
        receiving_sn_pastelid: str = Field(index=True)
        total_messages: int
        total_data_bytes: float
        timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
```

These models work together with the functions to facilitate the processing of inference requests, broadcasting of messages, and handling of received broadcast messages in the Inference Layer server. They enable the Supernodes to communicate and coordinate effectively, ensuring that inference requests are properly processed and the necessary information is stored and tracked in the database.


----------

**Model Menu and API Based Service Related Functions:** 

The next group of service functions we will review are related to how Supernodes determine which models they themselves support, and which ones are supported by other Supernodes. It also includes functions which are used by the Supernodes to automatically test and verify API keys for use with API based services such as OpenAI, Stability, Groq, Mistral, etc. These functions include the following:

- **`is_model_supported`**:
    - This function checks if a desired model is supported by a Supernode. It takes the model menu, desired model canonical string, desired model inference type string, and desired model parameters JSON as input. 
    - The function compares the desired model information with the models available in the model menu, ensuring that the desired model canonical string matches any of the models in the menu with a similarity threshold of 95% using fuzzy matching (via the [FuzzyWuzzy](https://pypi.org/project/fuzzywuzzy/) library). 
    - If a match is found, it further checks if the desired inference type string is supported by the matched model and verifies that the desired model parameters are valid according to the parameter specifications in the model menu. 
    - This function is used in the "Processing the Inference Request" step to ensure that the `responding_supernode` supports the requested model and inference type.


- **`get_inference_model_menu`**:
    - This function is responsible for retrieving the inference model menu, which contains information about the available models and their supported parameters.
    - It first loads the API key test results from a file using the `load_api_key_tests` function.
    - It then fetches the latest model menu from a GitHub URL using an asynchronous HTTP client.
    - The function filters the model menu based on the availability of valid API keys for each model provider (e.g., Stability, OpenAI, Mistral, Groq, Anthropic, OpenRouter).
    - It checks the validity of each API key using the `is_api_key_valid` function, which either retrieves the test result from the loaded API key tests or runs a new test using the `run_api_key_test` function.
    - Models that don't require API keys are automatically included in the filtered model menu.
    - The filtered model menu is saved locally as a JSON file.
    - Finally, the function returns the filtered model menu.
    - This function is used in the "Processing the Inference Request" step to provide the available models and their parameters to the `is_model_supported` function.
    
- **`load_api_key_tests`**:
    - This function loads the API key test results from a JSON file.
    - It reads the contents of the file and returns the loaded JSON data.
    - If the file is not found, it returns an empty dictionary.
    - This function is used by `get_inference_model_menu` to load the existing API key test results.
    
- **`save_api_key_tests`**:
    - This function saves the API key test results to a JSON file.
    - It takes the `api_key_tests` dictionary as input and writes it to the specified file path.
    - This function is used by `get_inference_model_menu` to save the updated API key test results after running new tests.


- **`is_api_key_valid`**:
    - This function checks the validity of an API key for a specific API provider.
    - It takes the `api_name` and `api_key_tests` dictionary as input.
    - If the API name is not found in the `api_key_tests` dictionary or the existing test result is outdated (based on the `is_test_result_valid` function), it runs a new API key test using the `run_api_key_test` function.
    - If the test passes, it updates the `api_key_tests` dictionary with the new test result and timestamp.
    - It returns the validity status of the API key (True or False).
    - This function is used by `get_inference_model_menu` to determine which models should be included in the filtered model menu based on the availability of valid API keys.
    
- **`is_test_result_valid`**:
    - This function checks if an API key test result is still valid based on a specified validity duration.
    - It takes the `test_timestamp` as input and compares it with the current timestamp.
    - If the difference between the current timestamp and the test timestamp is less than the specified validity duration (e.g., 24 hours), it considers the test result as valid.
    - This function is used by `is_api_key_valid` to determine if a new API key test needs to be run.
    
- **`run_api_key_test`**:
    - This function runs an API key test for a specific API provider.
    - It takes the `api_name` as input and calls the corresponding test function based on the API name.
    - It supports testing API keys for Stability, OpenAI, Mistral, Groq, Anthropic (Claude), and OpenRouter.
    - Each test function sends a test request to the respective API endpoint using the provided API key and checks the response status or content to determine if the API key is valid.
    - If the test passes, it returns True; otherwise, it returns False.
    - This function is used by `is_api_key_valid` to run a new API key test when needed.
    

These functions work together to manage the inference model menu and ensure that only models with valid API keys are included. The `get_inference_model_menu` function is a key part of the "Processing the Inference Request" step, where it provides the available models and their parameters to the `is_model_supported` function. By regularly testing the API keys and filtering the model menu accordingly, the system ensures that inference requests are only processed for models with valid API keys, preventing potential errors or failures during the execution of the inference request.

It also allows Supernodes to accurately advertise the specific models and services they support. This allows the Inference Layer to provide a lot more options to users without introducing necessary centralization, since Supernode Operators are free to decide which API based services, if any, they want to support with their own Supernodes; if they do want to support a particular API service, such as *OpenAI* or *Groq*, then it’s totally up to the Supernode operators themselves to procure a valid API key and include it in their .env file in encrypted form. The result of all this are exposed by each Supernode at the following endpoint:

```python
    @router.get("/get_inference_model_menu")
    async def get_inference_model_menu_endpoint(
        rpc_connection=Depends(get_rpc_connection),
    ):
        model_menu = await service_functions.get_inference_model_menu()
        return model_menu 
```

which sends back the filtered model menu after removing any entries in the master model_menu.json that the particular Supernode doesn’t support. For instance, if that particular Supernode is controlled by a Supernode operator who never bothered to get a Stability API key, then the returned model menu from that Supernode will not include the “text_to_image” inference type and associated Stability models; if an end user wants to use that inference type, they will need to select a different Supernode as their closest model-supporting `responding_supernode`.

To really understand how the `model_menu.json`  file works and its importance in the wider system, we need to go into a lot more detail. For starters, here is what the beginning of this file looks like:

```JSON
    {
      "models": [
        {
          "model_name": "swiss_army_llama-Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M",
          "model_url": "https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf",
          "description": "Hermes 2 Pro is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house. This version excels at Function Calling, JSON Structured Outputs, and has improved on several other metrics, scoring 90% on function calling evaluation and 84% on structured JSON Output evaluation.",
          "supported_inference_type_strings": [
            "text_completion",
            "embedding",
            "embedding_document",
            "embedding_audio"
          ],
          "input_fields": [
            {
              "name": "input_prompt",
              "file_type": "text",
              "optional": false
            }
          ],
          "output_fields": [
            {
              "name": "generated_text",
              "file_type": "text",
              "optional": false
            }
          ],
          "model_parameters": [
            {
              "name": "number_of_tokens_to_generate",
              "type": "int",
              "default": 1000,
              "inference_types_parameter_applies_to": "['text_completion']",
              "description": "The maximum number of tokens to generate [Optional]"
            },
            {
              "name": "temperature",
              "type": "float",
              "default": 0.7,
              "inference_types_parameter_applies_to": "['text_completion']",
              "description": "The temperature for sampling [Optional]"
            },
            {
              "name": "number_of_completions_to_generate [Optional]",
              "type": "int",
              "default": 1,
              "inference_types_parameter_applies_to": "['text_completion']",
              "description": "The number of completions to generate"
            },
            {
              "name": "grammar_file_string",
              "type": "string",
              "default": "",
              "inference_types_parameter_applies_to": "['text_completion']",
              "description": "Grammar File String: The grammar file used to restrict text generation [Optional] (Default is to not use any grammar file. Examples: `json`, `list`)"
            },
            {
              "name": "corpus_identifier_string",
              "type": "string",
              "default": "",
              "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
              "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
            }
          ],
          "credit_costs": {
            "text_completion": {
              "input_tokens": 1.8,
              "output_tokens": 1.4,
              "compute_cost": 1.0,
              "memory_cost": 0.9
            },
            "embedding": {
              "input_tokens": 0.7,
              "compute_cost": 0.8,
              "memory_cost": 0.5
            },
            "embedding_document": {
              "average_tokens_per_sentence": 0.2,
              "total_sentences": 0.1,
              "query_string_included": 0.4,
              "compute_cost": 1.2,
              "memory_cost": 0.8
            },
            "embedding_audio": {
              "audio_file_length_in_seconds": 0.5,
              "query_string_included": 1.5,
              "compute_cost": 1.2,
              "memory_cost": 1.0
            }
          }
        },
```
Compare the above entry for a “locally hosted” Swiss Army Llama based LLM to the entry below for an API service based model, which has fewer options to specify and which uses a very different approach to calculating the cost of a specific inference request (the API service based models try to estimate the actual cost to the Supernode operator of making that particular inference request, whereas the Swiss Army Llama based models instead try to quantify the compute/memory usage of a particular inference request):

```JSON
        {
          "model_name": "openai-gpt-4o",
          "model_url": "",
          "description": "GPT-4o is OpenAI's most advanced multimodal model that\u2019s faster and cheaper than GPT-4 Turbo with stronger vision capabilities. The model has 128K context and an October 2023 knowledge cutoff.",
          "supported_inference_type_strings": [
            "text_completion"
          ],
          "input_fields": [
            {
              "name": "input_prompt",
              "file_type": "text",
              "optional": true
            }
          ],
          "output_fields": [
            {
              "name": "generated_text",
              "file_type": "text",
              "optional": false
            }
          ],
          "model_parameters": [
            {
              "name": "number_of_tokens_to_generate",
              "type": "int",
              "default": 1000,
              "inference_types_parameter_applies_to": "['text_completion']",
              "description": "The maximum number of tokens to generate [Optional]"
            },
            {
              "name": "temperature",
              "type": "float",
              "default": 0.7,
              "inference_types_parameter_applies_to": "['text_completion']",
              "description": "The temperature for sampling [Optional]"
            },
            {
              "name": "number_of_completions_to_generate [Optional]",
              "type": "int",
              "default": 1,
              "inference_types_parameter_applies_to": "['text_completion']",
              "description": "The number of completions to generate"
            }
          ],
          "credit_costs": {
            "api_based_pricing": 1
          }
        }, 
```

Or for an image generation (“text_to_image” inference type) model’s entry, which specifies wildly different input parameters and output type (a binary image encoded as base64 text rather than a plain text output like you would get from an LLM model doing a text_completion inference request):

```JSON
     {
          "model_name": "stability-core",
          "model_url": "",
          "description": "Stable Image Core is the best quality achievable at high speed for text-to-image generation.",
          "supported_inference_type_strings": [
            "text_to_image",
            "creative_upscale"
          ],
          "input_fields": [
            {
              "name": "prompt",
              "file_type": "text",
              "optional": false
            }
          ],
          "output_fields": [
            {
              "name": "generated_image",
              "file_type": "base64_image",
              "optional": false
            }
          ],
          "model_parameters": [
            {
              "name": "aspect_ratio",
              "type": "string",
              "default": "1:1",
              "inference_types_parameter_applies_to": "['text_to_image']",
              "description": "The aspect ratio of the generated image [Optional]"
            },
            {
              "name": "seed",
              "type": "int",
              "default": 0,
              "inference_types_parameter_applies_to": "['text_to_image']",
              "description": "Random seed to use for generation [Optional]"
            },
            {
              "name": "style_preset",
              "type": "string",
              "default": null,
              "inference_types_parameter_applies_to": "['text_to_image']",
              "description": "Style Preset: Guides the image model towards a particular style (e.g., '3d-model', 'analog-film', 'anime', etc.) [Optional]"
            },
            {
              "name": "output_format",
              "type": "string",
              "default": "jpeg",
              "inference_types_parameter_applies_to": "['text_to_image']",
              "description": "The format of the generated image (e.g., 'png', 'jpeg', 'webp', etc.) [Optional]"
            },
            {
              "name": "negative_prompt",
              "type": "string",
              "default": null,
              "inference_types_parameter_applies_to": "['text_to_image']",
              "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
            }
          ],
          "credit_costs": {
            "api_based_pricing": 1
          }
        },
```

As you can probably see from the examples, the structure is quite generic and covers all the functionality we might need from any kind of model, with clear methods indicated for how to expand the coverage to new and different inference types, services, and models. The `model_menu.json` file plays a crucial role in the Inference Layer server code. It serves as a central configuration file that defines the available models, their capabilities, and the parameters they support for different inference types. The file is designed to be flexible and extensible, allowing easy addition of new models and services without requiring significant changes to the codebase.

Let's take a detailed look at the structure of the `model_menu.json` file and explain the rationale behind its design:


1. The root of the JSON file contains a single key, "models", which is an array of model objects.


2. Each model object represents a specific model or service that the Inference Layer supports. It contains various properties that describe the model's characteristics and capabilities. The key properties of a model object are:
    - `model_name`: A unique identifier for the model, used throughout the system to refer to the specific model.
    - `model_url`: The URL or path to the model file, if applicable. This is used for locally hosted models using Swiss Army Llama.
    - `description`: A brief description of the model, providing information about its capabilities, training data, or any other relevant details.
    - `supported_inference_type_strings`: An array of strings indicating the inference types supported by the model. This allows the system to determine which models can be used for a particular inference request.
    - `input_fields`: An array of objects specifying the input fields required by the model for each inference type. Each input field object contains the following properties:
        - `name`: The name of the input field.
        - `file_type`: The type of data expected for the input field (e.g., text, base64, etc.).
        - `optional`: A boolean indicating whether the input field is optional or required.
    - `output_fields`: An array of objects specifying the output fields returned by the model for each inference type. Each output field object contains the following properties:
        - `name`: The name of the output field.
        - `file_type`: The type of data returned in the output field (e.g., text, base64_image, etc.).
        - `optional`: A boolean indicating whether the output field is optional or guaranteed to be present.
    - `model_parameters`: An array of objects defining the parameters supported by the model for each inference type. Each parameter object contains the following properties:
        - `name`: The name of the parameter.
        - `type`: The data type of the parameter (e.g., int, float, string, etc.).
        - `default`: The default value for the parameter, if applicable.
        - `inference_types_parameter_applies_to`: An array of inference types to which the parameter applies. This allows for parameters to be specific to certain inference types.
        - `description`: A description of the parameter, providing information about its purpose and usage.
    - `credit_costs`: An object that defines the credit costs associated with using the model for different inference types. It can contain either specific cost breakdowns for each inference type or a flag indicating that the model uses API-based pricing.
    

The rationale behind this design is to provide a declarative and data-driven approach to configuring the available models and their capabilities. By storing all the necessary information about models in the `model_menu.json` file, the Inference Layer server code can dynamically adapt to changes in the model lineup without requiring code modifications.

One of the key benefits of this design is that it enables the automatic generation of UI elements in the visual frontend for each model and inference type. The frontend code can iterate over the models defined in the `model_menu.json` file and dynamically create the appropriate input fields, parameter controls, and output displays based on the specifications provided in the JSON file.
For example, when a user selects a specific model and inference type in the frontend, the UI can be dynamically populated with the relevant input fields and parameter controls as defined in the `model_menu.json` file. This eliminates the need for hard-coded UI elements and allows for a highly flexible and adaptable user interface.

Furthermore, the `model_menu.json` file serves as a single source of truth for model configuration. It provides a clear and centralized location for defining the available models, their capabilities, and the parameters they support. This makes it easier to manage and update the model lineup, as well as maintain consistency across different parts of the system.



----------

**Inference Request Cost Estimation Service Functions**

The next batch of functions pertain to estimating the cost of performing specific inference requests depending on the nature of the request and specified models and model parameters. Let's go through each function and explain their purpose and how they contribute to the cost estimation process.


1. **`get_tokenizer`** **and** **`count_tokens`**:
- The `get_tokenizer` function maps model names to their corresponding tokenizer names using a predefined mapping. It uses fuzzy string matching to find the best match for the given model name and returns the appropriate tokenizer name.
- The `count_tokens` function takes a model name and input data as parameters and counts the number of tokens in the input data using the appropriate tokenizer for the specified model.
- It handles different types of tokenizers, such as GPT-2, GPT-Neo, BERT, RoBERTa, and specific tokenizers for models like Claude, Whisper, and CLIP.
- The token count is important for estimating the cost of inference requests, as many pricing models are based on the number of input and output tokens.


2. **`calculate_api_cost`**:
- This function calculates the estimated cost of an inference request based on the pricing data for each API service and model.
- It takes the model name, input data, and model parameters as input and uses fuzzy string matching to find the best match for the model name in the predefined pricing data.
- For Stability models, the cost is calculated based on the credits per call and the number of completions to generate.
- For OpenAI's GPT-4o-vision model, the cost is calculated based on the input image resolution, number of tiles, and tokens, as well as the question tokens.
- For other models, the cost is calculated based on the number of input tokens, output tokens to generate, number of completions, and per-call cost.
- The function returns the estimated cost in dollars.


3. **`convert_document_to_sentences`**:
- This function converts a document file to sentences using the Swiss Army Llama service.
- It checks if either the local or remote Swiss Army Llama service is responding and selects the appropriate port.
- It uploads the document file and retrieves the file metadata (URL, hash, size) using the `upload_and_get_file_metadata` function.
- It sends a POST request to the Swiss Army Llama service with the file metadata and security token to convert the document to sentences.
- If the remote Swiss Army Llama service fails, it falls back to the local service if available.
- The function returns a dictionary containing the individual sentences and the total number of sentences in the document.


4. **`calculate_proposed_inference_cost_in_credits`**:
- This function calculates the proposed cost of an inference request in credits based on the requested model, model parameters, inference type, and input data.
- It distinguishes between API-based models and local LLM models (Swiss Army Llama).
- For API-based models, it calls the `calculate_api_cost` function to estimate the cost in dollars and then converts it to credits based on a target value per credit and profit margin.
- For local LLM models, it retrieves the credit costs from the model data and calculates the cost based on the inference type:
- For text completion and question-answering on images, it considers the input tokens, output tokens, number of completions, compute cost, and memory cost.
- For embedding documents, it converts the document to sentences using the `convert_document_to_sentences` function and calculates the cost based on the total tokens, total sentences, query string inclusion, compute cost, and memory cost.
- For embedding audio, it calculates the cost based on the audio length in seconds, query string inclusion, compute cost, and memory cost.
- The function applies a credit cost multiplier factor and ensures a minimum cost in credits.
- It returns the final proposed cost in credits for the inference request.

These functions work together to estimate the cost of performing specific inference requests based on various factors such as the model, input data, model parameters, and inference type. They take into account the pricing models of different API services and local LLM models, converting the estimated costs to credits for a consistent pricing scheme. The `calculate_proposed_inference_cost_in_credits` function serves as the main entry point for estimating the cost of an inference request, utilizing the other functions as needed.


----------

**Misc Utility Functions for Inference Requests:**

Next up are a couple utility functions that are used with Swiss Army Llama:


- **`is_swiss_army_llama_responding`**:
    - This function checks if the Swiss Army Llama service is responding, either locally or remotely.
    - It takes a boolean parameter `local` to determine whether to check the local or remote service.
    - Based on the `local` parameter, it sets the appropriate port number for the service.
    - It sends a GET request to the `/get_list_of_available_model_names/` endpoint of the Swiss Army Llama service, including a security token as a parameter.
    - If the response status code is 200 (OK), it means the service is responding, and the function returns `True`. Otherwise, it returns `False`.
    - Checking the responsiveness of the Swiss Army Llama service is important to ensure that it is available for performing inference requests and other tasks.
    
- **`check_if_input_text_would_get_rejected_from_api_services`**:
    - This is an asynchronous function that checks if the input text for an inference request is likely to be rejected or cause problems with API services like OpenAI.
    - It first checks if the Swiss Army Llama service is responding, either locally or remotely, based on the configuration settings.
    - If neither the local nor the remote Swiss Army Llama service is responding, it logs an error message and returns `None`, indicating that the check cannot be performed.
    - If the Swiss Army Llama service is responding, it constructs an input prompt that asks whether the given inference request text is problematic or offensive and likely to be rejected by OpenAI.
    - It sends a POST request to the `/get_text_completions_from_input_prompt/` endpoint of the Swiss Army Llama service, passing the input prompt, model name, and other parameters.
    - The function expects the Swiss Army Llama service to respond with either "ACCEPT" or "REJECT" based on its assessment of the input text (it constrains the results of the model by using a grammar file which only allows responses that conform to either “ACCEPT” or “REJECT”
    - If the response is "ACCEPT", it means the inference request is not problematic, and the function returns `True`, indicating that the request can be passed on to the API service.
    - If the response is "REJECT", it means the inference request is determined to be problematic and likely to result in a rejection or ban from the API service. In this case, the function logs an error message and returns `False`.
    - If the response is neither "ACCEPT" nor "REJECT", it logs a warning message and returns the current value of `inference_request_allowed`.
    - In case of any exceptions during the Swiss Army Llama request, it logs an error message and attempts to fall back to the local Swiss Army Llama service if the remote service was being used.
    

The `check_if_input_text_would_get_rejected_from_api_services` function plays a crucial role in the inference request flow. Before sending an inference request to an external API service, it is important to ensure that the input text is not problematic or offensive, as sending such requests could result in the API key being banned or the request being rejected.

By leveraging the Swiss Army Llama service, this function performs a pre-check on the input text to determine its suitability for the API service. It uses a specific model and parameters to assess the input text and expects a clear "ACCEPT" or "REJECT" response.

This pre-check step helps to mitigate the risk of sending inappropriate or offensive requests to the API service, protecting the Pastel inference system (more specifically, protecting individual Supernode operators themselves) from potential bans or rejections. It allows the system to proactively filter out problematic requests and ensure a smoother and more reliable inference request flow.

The function also handles scenarios where the Swiss Army Llama service may not be responding and provides fallback mechanisms to ensure the pre-check can still be performed using the local service if available.

----------

**Validating and Processing New Inference Requests:**

The next batch of service functions play crucial roles in the inference request flow, specifically in validating and processing incoming inference requests. They also help us to determine how much a given credit pack ticket has been used so we can determine if it contains enough remaining credits to cover the cost of the currently contemplated inference request, as well as detecting the corresponding confirmation tracking transactions that are used by end users to authorize a new inference request. Let's go through each function and explain their purpose and how they tie into the inference flow:

- **`validate_inference_api_usage_request`**:
    - This function takes an `InferenceAPIUsageRequest` object as input and performs a series of validations to ensure the integrity and validity of the inference request.
    - It first calls the `validate_inference_request_message_data_func` to validate the request message data against predefined validation rules. If any validation errors are found, an exception is raised.
    - It extracts relevant information from the request, such as the requesting PastelID, credit pack ticket TXID, requested model, inference type, model parameters, and input data.
    - The function validates the credit pack ticket TXID using the `validate_pastel_txid_string` function to ensure it follows the expected format.
    - It retrieves the credit pack purchase request response using the `retrieve_credit_pack_ticket_using_txid` function and checks if the requesting PastelID is authorized to use the credit pack.
    - The function then verifies if the requested model is available in the model menu obtained using the `get_inference_model_menu` function. If the model is not found or the requested inference type is not supported by the model, the function returns `False`.
    - If the requested model is a Swiss Army Llama model (not API-based), the function checks if the Swiss Army Llama service is available, either locally or remotely. If the service is not responding, the function returns `False`.
    - The function decodes the model parameters and input data from base64 format and performs additional checks based on the detected data type.
    - If the inference request is for an API-based model and local checking is enabled, the function calls the `check_if_input_text_would_get_rejected_from_api_services` function to determine if the input text is likely to be rejected by the API service. If the request is deemed risky, the function returns `False`.
    - The function calculates the proposed cost of the inference request in credits using the `calculate_proposed_inference_cost_in_credits` function based on the requested model, model parameters, inference type, and input data.
    - It validates the credit pack ticket using the `validate_existing_credit_pack_ticket` function to ensure its validity.
    - The function determines the current credit balance of the credit pack ticket using the `determine_current_credit_pack_balance_based_on_tracking_transactions` function and checks if there are sufficient credits available for the requested inference. If the balance is insufficient, the function returns `False`.
    - Finally, the function returns a tuple indicating the validity of the request, the proposed cost in credits, and the remaining credits after the request.

- **`check_burn_address_for_tracking_transaction`**:
    - This function checks the burn address for a specific tracking transaction related to an inference request confirmation.
    - It takes the tracking address, expected amount, transaction ID (TXID), maximum block height, and optional retry parameters as input.
    - The function uses the RPC connection to retrieve transactions from the blockchain and searches for a transaction that matches the specified criteria.
    - If a transaction with the exact expected amount is found and has sufficient confirmations (or if the confirmation check is skipped), the function returns `True` along with the transaction details.
    - If a transaction with an amount greater than or equal to the expected amount is found, the function returns `False` for an exact match but `True` for an exceeding transaction, along with the transaction details.
    - If no matching transaction is found after the specified number of retries, the function returns `False`.
    - This function is used to verify that the user has indeed sent the required tracking transaction to the burn address, confirming their agreement to pay for the inference request using their credits.

-  **`determine_current_credit_pack_balance_based_on_tracking_transactions`**:
    - This function is a crucial part of the inference flow, as it calculates the current balance of credits available in a specific credit pack ticket. This function is essential for ensuring that users have sufficient credits to cover the cost of their inference requests and for keeping track of the credits consumed over time.
    - The function takes the `credit_pack_ticket_txid` as input, which uniquely identifies the credit pack ticket.
    - It retrieves the credit pack ticket data using the `retrieve_credit_pack_ticket_using_txid` function, which fetches the ticket information from the blockchain using the provided TXID.
    - From the retrieved ticket data, the function extracts the initial credit balance (`requested_initial_credits_in_credit_pack`) and the credit usage tracking PSL address (`credit_usage_tracking_psl_address`). These pieces of information are necessary for calculating the current credit balance.
    - The function then queries the database to retrieve all the burn address transactions associated with the specific credit usage tracking address. These transactions represent the inference requests that have consumed credits from the credit pack.
    - It calculates the total credits consumed by summing up the amounts of all the retrieved burn address transactions. The transaction amounts are adjusted using the `CREDIT_USAGE_TO_TRACKING_AMOUNT_MULTIPLIER` to convert them from the tracking amount to the actual credit amount.
    - The function then checks if there are any new blocks added to the blockchain since the last update of the database. It does this by comparing the current block height (`current_block_height`) with the latest block height stored in the database (`latest_db_block_height`).
    - If there are new blocks, the function retrieves the new burn transactions from those blocks using the `listsinceblock` RPC method. It filters out the transactions that are not relevant to the specific credit pack ticket based on the burn address and tracking address.
    - The function decodes the new burn transactions using the `process_transactions_in_chunks` function, which retrieves the detailed transaction data in chunks to handle a large number of transactions efficiently.
    - It then queries the database again to retrieve any new tracking transactions associated with the credit pack ticket that occurred in the new blocks or are still pending.
    - The total credits consumed is updated by adding the amounts of the new tracking transactions to the previously calculated total.
    - Optionally, the function can update the block hashes in the database for the new blocks using the `fetch_and_insert_block_hashes` function. This step is useful for maintaining a local copy of the block hashes for faster lookups.
    - Finally, the function calculates the current credit balance by subtracting the total credits consumed from the initial credit balance. It also counts the total number of confirmation transactions from the tracking address to the burn address.
    - The function returns the current credit balance and the number of confirmation transactions.
    - The `determine_current_credit_pack_balance_based_on_tracking_transactions` function is closely tied to the `check_burn_address_for_tracking_transaction` function, which is used to verify the tracking transactions sent by users to confirm their inference requests.
        - When a user sends a confirmation transaction to the burn address, the `check_burn_address_for_tracking_transaction` function checks if the transaction meets the expected criteria, such as the correct amount and sufficient confirmations. If the transaction is valid, it indicates that the user has authorized the consumption of credits from their credit pack for the inference request.
        - The `determine_current_credit_pack_balance_based_on_tracking_transactions` function, in turn, relies on these tracking transactions to calculate the current balance of the credit pack. By querying the database and retrieving the relevant tracking transactions, the function can accurately determine how many credits have been consumed and update the credit pack balance accordingly.
    - This function is crucial for maintaining the integrity of the credit system and ensuring that users can only consume the credits they have available in their credit packs. It allows the inference flow to proceed smoothly, with the `responding_supernode` being able to verify that the user has sufficient credits before executing the inference request.
    - Moreover, the function's ability to handle new blocks and transactions ensures that the credit balance remains up to date, even as new inference requests are processed and new tracking transactions are added to the blockchain.

- **`process_inference_api_usage_request`**:
    - This function takes an `InferenceAPIUsageRequest` object as input and processes the inference request.
    - It first calls the `validate_inference_api_usage_request` function to validate the request. If the request is invalid, an exception is raised.
    - If the request is valid, the function saves the inference API usage request using the `save_inference_api_usage_request` function.
    - It retrieves the credit pack purchase request response using the `retrieve_credit_pack_ticket_using_txid` function to obtain the credit usage tracking PSL address.
    - The function then calls the `create_and_save_inference_api_usage_response` function to create and save an `InferenceAPIUsageResponse` object based on the saved request, proposed cost in credits, remaining credits after the request, and credit usage tracking PSL address.
    - Finally, the function returns the created `InferenceAPIUsageResponse` object.

- **`create_and_save_inference_api_usage_response`**:
    - This is a helper function used by `process_inference_api_usage_request` to create and save an `InferenceAPIUsageResponse` object.
    - It takes the saved inference API usage request, proposed cost in credits, remaining credits after the request, and credit usage tracking PSL address as input.
    - The function generates a unique identifier for the inference response and creates an `InferenceAPIUsageResponse` instance with the relevant information.
    - It computes the hash of the response fields using the `compute_sha3_256_hash_of_sqlmodel_response_fields` function and signs the hash with the local Supernode's PastelID using the `sign_message_with_pastelid_func` function.
    - The function then saves the `InferenceAPIUsageResponse` object to the database using an asynchronous database session.
    - Finally, it returns the saved `InferenceAPIUsageResponse` object.

These functions play a critical role in the inference request flow by validating incoming requests, checking the availability of requested models and services, calculating the cost of the inference in credits, and ensuring sufficient credits are available in the associated credit pack ticket. They also handle the creation and saving of the `InferenceAPIUsageResponse` object, which represents the Supernode's response to the inference request, including the proposed cost and remaining credits.

The `validate_inference_api_usage_request` function acts as a gatekeeper, performing various checks and validations to ensure the integrity and validity of the inference request before proceeding with the actual processing. It helps prevent invalid or unauthorized requests from being processed and ensures that the requested model and inference type are supported.

The `process_inference_api_usage_request` function orchestrates the overall processing of the inference request. It relies on the `validate_inference_api_usage_request` function to validate the request and then saves the request, retrieves necessary information, and creates and saves the corresponding `InferenceAPIUsageResponse` object.

----------

**Processing Inference Confirmations and Saving Output Results:**

The next batch of service functions pertain to several important functions related to processing inference confirmations, saving inference output results. Let's go through each function and explain their purpose and how they tie into the inference flow:

- **`process_inference_confirmation`**:
    - This function processes the confirmation of an inference request sent by the user.
    - It takes the inference request ID and an `InferenceConfirmation` object as input.
    - The function retrieves the corresponding `InferenceAPIUsageRequest` and `InferenceAPIUsageResponse` from the database using the provided inference request ID.
    - It ensures that the burn address is tracked by the local wallet by importing it if necessary.
    - The function then calls the `check_burn_address_for_tracking_transaction` function to check if the burn address has received the expected tracking transaction from the user.
    - If a matching transaction is found, the function computes the current credit pack balance based on the tracking transactions using the `determine_current_credit_pack_balance_based_on_tracking_transactions` function.
    - It updates the status of the inference request to "confirmed" in the database.
    - Finally, the function triggers the execution of the inference request by creating a new task using `asyncio.create_task(execute_inference_request(inference_request_id))`.
    - If any error occurs during the process, the function logs the error and re-raises the exception.
    
- **`save_inference_output_results`**:
    - This function saves the output results of an inference request to the database.
    - It takes the inference request ID, inference response ID, output results dictionary, and output results file type strings as input.
    - The function generates a unique identifier for the inference result and creates an `InferenceAPIOutputResult` record without the hash and signature fields.
    - It populates the record with the relevant information, including the`responding_supernode`'s PastelID, the base64-encoded output results JSON, file type strings, timestamp, block height, and message version.
    - The function computes the hash of the inference result fields using the `compute_sha3_256_hash_of_sqlmodel_response_fields` function and signs the hash with the `responding_supernode`'s PastelID using the `sign_message_with_pastelid_func` function.
    - Finally, it saves the `InferenceAPIOutputResult` record to the database using an asynchronous database session.
    - If any error occurs during the process, the function logs the error and re-raises the exception.
    
These functions play important roles in the inference flow, specifically in the confirmation and result saving stages. The `process_inference_confirmation` function is responsible for handling the user's confirmation of an inference request. When the user sends a tracking transaction to the burn address, indicating their agreement to pay for the inference using their credits, this function verifies the transaction and updates the status of the inference request to "confirmed". It also triggers the execution of the inference request by creating a new task.

The `save_inference_output_results` function is called after the inference request has been executed and the output results are available. It takes the output results, along with other relevant information, and creates an `InferenceAPIOutputResult` record in the database. This record contains the inference results, file type information, `responding_supernode`'s PastelID, and other metadata. The function also computes a hash of the result fields and signs it with the Supernode's PastelID to ensure the integrity and authenticity of the results.


----------


**Inference Request Execution Functions for API-Based Services and Models**

The next batch of service functions we will review are related to how inference requests are executed for API-based services and models, such as those from Stability, OpenAI, Anthropic, Mistral, Groq, and OpenRouter. These functions are responsible for submitting the inference requests to the respective APIs, handling the responses, and processing the output results. Let's go through each function and explain their purpose and how they contribute to the execution of inference requests:

1. **`get_claude3_model_name`**:
    - This is a helper function specific to Anthropic's Claude API. It maps the model names used in the Pastel Inference Layer to the corresponding model names recognized by the Claude API.
    - It takes the `model_name` as input and returns the corresponding Claude API model name using a predefined mapping.
    - If the provided `model_name` is not found in the mapping, it returns an empty string.
    - This function ensures that the correct model name is used when submitting inference requests to the Claude API.
    
2. **`submit_inference_request_to_stability_api`**:
    - This function is responsible for submitting inference requests to the Stability API for image generation tasks.
    - It supports two types of inference requests: "text_to_image" and "creative_upscale".
    - For "text_to_image" requests:
        - It extracts the model parameters and input prompt from the `inference_request` object.
        - It constructs the necessary data payload based on the model parameters, such as aspect ratio, output format, negative prompt, seed, and style preset.
        - It sends a POST request to the Stability API endpoint with the appropriate headers and data payload.
        - If the response is successful (status code 200), it processes the response JSON and extracts the generated image as base64-encoded data.
        - It returns the output results and file type strings.
    - For "creative_upscale" requests:
        - It extracts the model parameters and input image from the `inference_request` object.
        - It constructs the necessary data payload based on the model parameters, such as prompt, output format, creativity, seed, and negative prompt.
        - It sends a POST request to the Stability API endpoint with the appropriate headers, files, and data payload.
        - If the response is successful (status code 200), it retrieves the `generation_id` from the response.
        - It then enters a loop to poll for the upscaled image result using the `generation_id`.
        - Once the upscaled image is available, it encodes the image as base64 and returns the output results and file type strings.
    - If any errors occur during the process or if the inference type is not supported, it logs an error message and returns `None`.
    
3. **`submit_inference_request_to_openai_api`**:
    - This function is responsible for submitting inference requests to the OpenAI API for various tasks, including text completion, embedding, and question-answering on images.
    - For "text_completion" requests:
        - It extracts the model parameters and input prompt from the `inference_request` object.
        - It determines the number of completions to generate based on the model parameters.
        - It sends a POST request to the OpenAI API endpoint with the appropriate headers and JSON payload, including the model, input prompt, max tokens, temperature, and number of completions.
        - If the response is successful (status code 200), it processes the response JSON and extracts the generated text completions.
        - It keeps track of the total input and output tokens used for the request.
        - It returns the output results and file type strings.
    - For "embedding" requests:
        - It extracts the input text from the `inference_request` object.
        - It sends a POST request to the OpenAI API endpoint with the appropriate headers and JSON payload, including the model and input text.
        - If the response is successful (status code 200), it processes the response JSON and extracts the embedding vector.
        - It returns the output results and file type strings.
    - For "ask_question_about_an_image" requests:
        - It extracts the model parameters, number of completions, input image, and question from the `inference_request` object.
        - It encodes the input image as base64.
        - It sends a POST request to the OpenAI API endpoint with the appropriate headers and JSON payload, including the model, question, image URL, and max tokens.
        - If the response is successful (status code 200), it processes the response JSON and extracts the generated text responses.
        - It keeps track of the total input and output tokens used for the request.
        - It returns the output results and file type strings.
    - If any errors occur during the process or if the inference type is not supported, it logs an error message and returns `None`.
    
4. **`submit_inference_request_to_openrouter`**:
    - This function is responsible for submitting inference requests to the OpenRouter API for text completion tasks.
    - It extracts the model parameters and input prompt from the `inference_request` object.
    - It constructs the necessary JSON payload based on the model parameters, such as model, input prompt, max tokens, and temperature.
    - It sends a POST request to the OpenRouter API endpoint with the appropriate headers and JSON payload.
    - If the response is successful (status code 200), it processes the response JSON and extracts the generated text completion.
    - It returns the output results and file type strings.
    - If any errors occur during the process or if the inference type is not supported, it logs an error message and returns `None`.
    
5. **`submit_inference_request_to_mistral_api`**:
    - This function is responsible for submitting inference requests to the Mistral API for text completion and embedding tasks.
    - For "text_completion" requests:
        - It extracts the model parameters, input prompt, and number of completions from the `inference_request` object.
        - It uses the Mistral API client to send a streaming request for text completion, providing the model, input prompt, max tokens, and temperature.
        - It processes the streaming response chunks and accumulates the generated text completion.
        - It keeps track of the total input and output tokens used for the request.
        - It returns the output results and file type strings.
    - For "embedding" requests:
        - It extracts the input text from the `inference_request` object.
        - It uses the Mistral API client to send a request for embedding, providing the model and input text.
        - It processes the response and extracts the embedding vector.
        - It returns the output results and file type strings.
    - If any errors occur during the process or if the inference type is not supported, it logs an error message and returns `None`.
    
6. **`submit_inference_request_to_groq_api`**:
    - This function is responsible for submitting inference requests to the Groq API for text completion and embedding tasks.
    - For "text_completion" requests:
        - It extracts the model parameters, input prompt, and number of completions from the `inference_request` object.
        - It uses the Groq API client to send a request for text completion, providing the model, input prompt, max tokens, and temperature.
        - It processes the response and extracts the generated text completions.
        - It keeps track of the total input and output tokens used for the request.
        - It returns the output results and file type strings.
    - For "embedding" requests:
        - It extracts the input text from the `inference_request` object.
        - It uses the Groq API client to send a request for embedding, providing the model and input text.
        - It processes the response and extracts the embedding vector.
        - It returns the output results and file type strings.
    - If any errors occur during the process or if the inference type is not supported, it logs an error message and returns `None`.
    
7. **`submit_inference_request_to_claude_api`**:
    - This function is responsible for submitting inference requests to the Claude API (by Anthropic) for text completion and embedding tasks.
    - It uses the `get_claude3_model_name` helper function to map the model name from the `inference_request` to the corresponding Claude API model name.
    - For "text_completion" requests:
        - It extracts the model parameters, input prompt, and number of completions from the `inference_request` object.
        - It uses the Claude API client to send a streaming request for text completion, providing the model, input prompt, max tokens, and temperature.
        - It processes the streaming response and accumulates the generated text completions.
        - It keeps track of the total input and output tokens used for the request.
        - It returns the output results and file type strings.
    - For "embedding" requests:
        - It extracts the input text from the `inference_request` object.
        - It uses the Claude API client to send a request for embedding, providing the model and input text.
        - It processes the response and extracts the embedding vector.
        - It returns the output results and file type strings.
    - If any errors occur during the process or if the inference type is not supported, it logs an error message and returns `None`.
    
These functions play an important role in executing inference requests for API-based services and models. They handle the communication with the respective APIs, construct the necessary payloads based on the inference request details, and process the API responses to extract the relevant output results.

The rationale behind having separate functions for each API is to provide a modular and extensible approach to integrating different APIs into the Pastel Inference Layer. Each API has its own specific requirements, endpoints, and response formats, so having dedicated functions allows for customized handling of each API's peculiarities.

These functions also ensure that the output results are properly formatted and returned along with the appropriate file type strings. This is important for the Pastel Inference Layer to correctly interpret and handle the output results based on their data types. The functions enable the system to seamlessly integrate with various APIs, execute inference requests based on user specifications, and retrieve the generated output results. By providing a consistent interface for different APIs, these functions contribute to the flexibility and extensibility of the Pastel Inference Layer, allowing it to support a wide range of models and services.

----------

**Inference Request Execution Functions for Swiss Army Llama**

The next batch of service functions we will review are related to how inference requests are executed for models hosted using [Swiss Army Llama](https://github.com/Dicklesworthstone/swiss_army_llama), which is an open-source framework for running large language models (LLMs) and computing and storing embedding vectors of documents (and also of audio files, which are transcribed to text using [Whisper](https://github.com/SYSTRAN/faster-whisper)) in one of two ways:

- **Locally**— i.e., running on the CPU on the actual Supernode server itself;

- **On a remote server**— i.e., a GPU-enabled instance set up by the Supernode using a cost effective service such as [vast.ai](https://vast.ai/) and a [template image](https://cloud.vast.ai/?ref_id=78066&template_id=3c711e5ddd050882f0eb6f4ce1b8cc28) already set up for Swiss Army Llama; this remote GPU instance can then be shared across all of the Supernode operators various Supernodes; 

These functions are responsible for submitting the inference requests to the Swiss Army Llama service, handling the responses, and processing the output results. Let's go through each function and explain their purpose and how they contribute to the execution of inference requests:

1. **`determine_swiss_army_llama_port`**:
    - This function determines the appropriate port to use for communicating with the Swiss Army Llama service.
    - It checks if the local Swiss Army Llama service is responding using the `is_swiss_army_llama_responding` function with the `local` parameter set to `True`.
    - If the `USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE` configuration is set to `True`, it also checks if the remote Swiss Army Llama service is responding by calling `is_swiss_army_llama_responding` with `local` set to `False`.
    - If the remote Swiss Army Llama service is responding and the configuration allows its usage, the function returns the `REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT`.
    - If the local Swiss Army Llama service is responding, it returns the `SWISS_ARMY_LLAMA_PORT`.
    - If neither service is responding, it returns `None`.
    - This function helps in selecting the appropriate port based on the availability and configuration of the Swiss Army Llama services.
    
2. **`handle_swiss_army_llama_exception`**:
    - This is an exception handling function specific to Swiss Army Llama requests.
    - It takes the exception object `e`, the `client` instance, the `inference_request`, `model_parameters`, `port`, `is_fallback` flag, and the `handler_function` as parameters.
    - If the exception occurs while using the remote Swiss Army Llama service (indicated by the `port` being `REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT`) and it's not a fallback attempt, the function logs a message indicating a fallback to the local Swiss Army Llama service.
    - It then recursively calls the `handler_function` with the `port` set to `SWISS_ARMY_LLAMA_PORT` and `is_fallback` set to `True`.
    - If the exception occurs in any other scenario, it returns `None` for both the output results and file type strings.
    - This function provides a mechanism to handle exceptions during Swiss Army Llama requests and fallback to the local service if necessary.
    
3. **`handle_swiss_army_llama_text_completion`**:
    - This function handles text completion requests using the Swiss Army Llama service.
    - It takes the `client` instance, `inference_request`, `model_parameters`, `port`, and `is_fallback` flag as parameters.
    - It constructs the payload for the text completion request, including the input prompt, LLM model name, temperature, number of tokens to generate, number of completions to generate, and grammar file string.
    - It sends a POST request to the Swiss Army Llama service endpoint `/get_text_completions_from_input_prompt/` with the payload and security token.
    - If the request is successful (status code 200), it processes the response JSON and extracts the generated text completion.
    - It identifies the data type of the output text using the `magika` library and sets the appropriate output file type strings.
    - If an exception occurs during the request, it calls the `handle_swiss_army_llama_exception` function to handle the exception and potentially fallback to the local service.
    - It returns the output text and file type strings.
    
4. **`handle_swiss_army_llama_image_question`**:
    - This function handles image question requests using the Swiss Army Llama service.
    - It takes the `client` instance, `inference_request`, `model_parameters`, `port`, and `is_fallback` flag as parameters.
    - It extracts the image data and question from the input data, which is assumed to be base64-encoded JSON.
    - It constructs the payload for the image question request, including the question, LLM model name, temperature, number of tokens to generate, and number of completions to generate.
    - It sends a POST request to the Swiss Army Llama service endpoint `/ask_question_about_image/` with the payload, image file, and security token.
    - If the request is successful (status code 200), it processes the response JSON and extracts the generated text answer.
    - It identifies the data type of the output text using the `magika` library and sets the appropriate output file type strings.
    - If an exception occurs during the request, it calls the `handle_swiss_army_llama_exception` function to handle the exception and potentially fallback to the local service.
    - It returns the output text and file type strings.
    
5. **`handle_swiss_army_llama_embedding_document`**:
    - This function handles document embedding requests using the Swiss Army Llama service.
    - It takes the `client` instance, `inference_request`, `model_parameters`, `port`, and `is_fallback` flag as parameters.
    - It extracts the document file data from the input data, which is assumed to be base64-encoded JSON.
    - It uploads the document file using the `upload_and_get_file_metadata` function and retrieves the file metadata (URL, hash, size).
    - It constructs the parameters for the document embedding request, including the LLM model name, embedding pooling method, corpus identifier string, JSON format, and whether to send back a JSON or ZIP file.
    - It sends a POST request to the Swiss Army Llama service endpoint `/get_all_embedding_vectors_for_document/` with the parameters, file metadata, and security token.
    - If the request is successful (status code 200), it processes the response based on the specified format (JSON or ZIP).
    - For JSON format, it directly returns the response JSON as the output results.
    - For ZIP format, it encodes the ZIP file content as base64 and sets the appropriate output file type strings.
    - If an exception occurs during the request, it calls the `handle_swiss_army_llama_exception` function to handle the exception and potentially fallback to the local service.
    - It returns the output results and file type strings.
    
6. **`handle_swiss_army_llama_embedding_audio`**:
    - This function handles audio embedding requests using the Swiss Army Llama service.
    - It takes the `client` instance, `inference_request`, `model_parameters`, `port`, and `is_fallback` flag as parameters.
    - It extracts the audio file data from the input data, which is assumed to be base64-encoded JSON.
    - It uploads the audio file using the `upload_and_get_file_metadata` function and retrieves the file metadata (URL, hash, size).
    - It constructs the parameters for the audio embedding request, including the option to compute embeddings for the resulting transcript document, LLM model name, embedding pooling method, and corpus identifier string.
    - It sends a POST request to the Swiss Army Llama service endpoint `/compute_transcript_with_whisper_from_audio/` with the parameters, file metadata, and security token.
    - If the request is successful (status code 200), it processes the response JSON as the output results.
    - If a query text is provided in the model parameters, it sends an additional request to the `/search_stored_embeddings_with_query_string_for_semantic_similarity/` endpoint to perform a semantic search on the embeddings.
    - It appends the search results to the output results.
    - If an exception occurs during the request, it calls the `handle_swiss_army_llama_exception` function to handle the exception and potentially fallback to the local service.
    - It returns the output results and file type strings.
    
7. **`handle_swiss_army_llama_semantic_search`**:
    - This function handles semantic search requests using the Swiss Army Llama service.
    - It takes the `client` instance, `inference_request`, `model_parameters`, `port`, and `is_fallback` flag as parameters.
    - It constructs the payload for the semantic search request, including the query text, number of most similar strings to return, LLM model name, embedding pooling method, and corpus identifier string.
    - It sends a POST request to the Swiss Army Llama service endpoint `/search_stored_embeddings_with_query_string_for_semantic_similarity/` with the payload and security token.
    - If the request is successful (status code 200), it processes the response JSON as the output results.
    - If an exception occurs during the request, it calls the `handle_swiss_army_llama_exception` function to handle the exception and potentially fallback to the local service.
    - It returns the output results and file type strings.
    
8. **`handle_swiss_army_llama_advanced_semantic_search`**:
    - This function handles advanced semantic search requests using the Swiss Army Llama service.
    - It takes the `client` instance, `inference_request`, `model_parameters`, `port`, and `is_fallback` flag as parameters.
    - It constructs the payload for the advanced semantic search request, including the query text, LLM model name, embedding pooling method, corpus identifier string, similarity filter percentage, number of most similar strings to return, and result sorting metric.
    - It sends a POST request to the Swiss Army Llama service endpoint `/advanced_search_stored_embeddings_with_query_string_for_semantic_similarity/` with the payload and security token.
    - If the request is successful (status code 200), it processes the response JSON as the output results.
    - If an exception occurs during the request, it calls the `handle_swiss_army_llama_exception` function to handle the exception and potentially fallback to the local service.
    - It returns the output results and file type strings.
    
9. **`submit_inference_request_to_swiss_army_llama`**:
    - This is the main function for submitting inference requests to the Swiss Army Llama service.
    - It takes the `inference_request` and an optional `is_fallback` flag as parameters.
    - It determines the appropriate port for the Swiss Army Llama service using the `determine_swiss_army_llama_port` function.
    - If no valid port is found (indicating that neither the local nor remote service is responding), it logs an error message and returns `None` for both the output results and file type strings.
    - It decodes the model parameters from base64-encoded JSON.
    - Based on the `model_inference_type_string` of the inference request, it calls the corresponding handler function:
        - For "text_completion", it calls `handle_swiss_army_llama_text_completion`.
        - For "embedding_document", it calls `handle_swiss_army_llama_embedding_document`.
        - For "embedding_audio", it calls `handle_swiss_army_llama_embedding_audio`.
        - For "ask_question_about_an_image", it calls `handle_swiss_army_llama_image_question`.
        - For "semantic_search", it calls `handle_swiss_army_llama_semantic_search`.
        - For "advanced_semantic_search", it calls `handle_swiss_army_llama_advanced_semantic_search`.
    - If the inference type is not supported, it logs a warning message and returns `None` for both the output results and file type strings.
    - It returns the output results and file type strings obtained from the respective handler function.
    
These functions play a crucial role in executing inference requests using the Swiss Army Llama service: they handle the communication with the Swiss Army Llama service, construct the necessary payloads based on the inference request details, and process the service responses to extract the relevant output results.

The rationale behind having separate handler functions for different inference types is to provide a modular and extensible approach to supporting various capabilities of the Swiss Army Llama service. Each inference type has its own specific requirements, parameters, and response formats, so having dedicated handler functions allows for customized handling of each type.

The `submit_inference_request_to_swiss_army_llama` function serves as the main entry point for submitting inference requests to the Swiss Army Llama service. It determines the appropriate port to use based on the availability and configuration of the local and remote services. It then delegates the request to the corresponding handler function based on the inference type.

The exception handling mechanism implemented in the `handle_swiss_army_llama_exception` function provides a way to gracefully handle errors that may occur during the communication with the Swiss Army Llama service. It allows for fallback to the local service if the remote service encounters an exception, ensuring that the inference request can still be processed.

Overall, these functions enable the Pastel Inference Layer to seamlessly integrate with the Swiss Army Llama service, providing users with the ability to run large language models locally or on a remote server. They handle the execution of various inference types, such as text completion, document and audio embedding, image question answering, and semantic search. By offering flexibility in the deployment options and supporting different inference capabilities, these functions contribute to the versatility and robustness of the Pastel Inference Layer.

----------

**Inference Request Execution and Result Retrieval Functions**

The final batch of service functions we will review are related to the overall execution of inference requests, checking the status of inference results, and retrieving the output results while verifying authorization. These functions tie together the various components of the inference request flow and provide the necessary endpoints for users to interact with the system. Let's go through each function and explain their purpose and how they contribute to the inference request execution and result retrieval process:

1. **`execute_inference_request`**:
    - This is the main function responsible for executing an inference request.
    - It takes the `inference_request_id` as a parameter, which uniquely identifies the inference request.
    - It retrieves the corresponding `InferenceAPIUsageRequest` from the database using the provided `inference_request_id`.
    - If the `InferenceAPIUsageRequest` is not found, it logs a warning message and returns, indicating an invalid inference request ID.
    - It also retrieves the associated `InferenceAPIUsageResponse` from the database using the same `inference_request_id`.
    - Based on the `requested_model_canonical_string` of the `InferenceAPIUsageRequest`, it determines the appropriate API or service to submit the inference request to:
        - If the model starts with "stability-", it calls the `submit_inference_request_to_stability_api` function.
        - If the model starts with "openai-", it calls the `submit_inference_request_to_openai_api` function.
        - If the model starts with "mistralapi-", it calls the `submit_inference_request_to_mistral_api` function.
        - If the model starts with "groq-", it calls the `submit_inference_request_to_groq_api` function.
        - If the model contains "claude" (case-insensitive), it calls the `submit_inference_request_to_claude_api` function.
        - If the model starts with "openrouter/", it calls the `submit_inference_request_to_openrouter` function.
        - If the model starts with "swiss_army_llama-", it calls the `submit_inference_request_to_swiss_army_llama` function.
        - If none of the above conditions match, it raises a `ValueError` indicating an unsupported provider or model.
    - The selected submission function is called with the `InferenceAPIUsageRequest` object, and it returns the `output_results` and `output_results_file_type_strings`.
    - If the `output_results` and `output_results_file_type_strings` are not `None`, it calls the `save_inference_output_results` function to save the inference output results to the database.
    - If any exception occurs during the execution process, it logs the error, prints the traceback, and re-raises the exception.
    
2. **`check_status_of_inference_request_results`**:
    - This function checks the status of an inference request's results.
    - It takes the `inference_response_id` as a parameter, which uniquely identifies the inference response.
    - It retrieves the corresponding `InferenceAPIOutputResult` from the database using the provided `inference_response_id`.
    - If the `InferenceAPIOutputResult` is found, it means the inference request has been processed and the results are available, so the function returns `True`.
    - If the `InferenceAPIOutputResult` is not found, it means the inference request is still being processed or has not yet started, so the function returns `False`.
    - If any exception occurs during the status check, it logs the error and re-raises the exception.
    
3. **`get_inference_output_results_and_verify_authorization`**:
    - This function retrieves the inference output results and verifies the authorization of the requesting user.
    - It takes the `inference_response_id` and `requesting_pastelid` as parameters.
    - It retrieves the corresponding `InferenceAPIOutputResult` from the database using the provided `inference_response_id`.
    - If the `InferenceAPIOutputResult` is not found, it raises a `ValueError` indicating that the inference output results are not found.
    - It then retrieves the associated `InferenceAPIUsageRequest` from the database using the `inference_request_id` from the `InferenceAPIOutputResult`.
    - If the `InferenceAPIUsageRequest` is not found or the `requesting_pastelid` does not match the `requesting_pastelid` of the `InferenceAPIUsageRequest`, it raises a `ValueError` indicating unauthorized access to the inference output results.
    - If the authorization is successful, it returns the `InferenceAPIOutputResult` object.
    
These functions play a critical role in the overall execution and management of inference requests in the Pastel Inference Layer. The `execute_inference_request` function serves as the central point of control for executing an inference request. It determines the appropriate API or service to submit the request to based on the model specified in the `InferenceAPIUsageRequest`. By delegating the actual submission to the respective functions for each API or service, it maintains a modular and extensible architecture.

The `check_status_of_inference_request_results` function provides a way for users to check the status of their inference requests. It allows them to determine whether the results are available or if the request is still being processed. This function is typically called by the user periodically to poll for the availability of the results.

The `get_inference_output_results_and_verify_authorization` function is responsible for retrieving the inference output results and verifying the authorization of the requesting user. It ensures that only the user who initiated the inference request can access the corresponding output results. This function is called when the user wants to retrieve the results after confirming their availability through the status check.

The use of database queries and transactions in these functions ensures data consistency and integrity. The functions retrieve the necessary objects from the database, such as `InferenceAPIUsageRequest`, `InferenceAPIUsageResponse`, and `InferenceAPIOutputResult`, to perform the required operations. The database transactions are managed using the `db_code.Session` context manager, which handles the session lifecycle and ensures proper resource management.

Error handling is also incorporated into these functions. If any exceptions occur during the execution or retrieval process, the functions log the errors, print the tracebacks, and re-raise the exceptions. This allows for proper error propagation and handling at higher levels of the system.

Overall, these functions provide the necessary functionality for executing inference requests, checking the status of results, and retrieving the output results while verifying authorization. They encapsulate the complexity of interacting with different APIs and services, and provide a consistent interface for users to interact with the Pastel Inference Layer. By leveraging the modular design and database transactions, these functions ensure a reliable and secure execution and retrieval process for inference requests.

----------

## Misc Other Infrastructure Code

Here, we explain other parts of the Inference Server system code that we haven't touched on yet above. First of these is the code used by the Inference Server to initially install, set up, configure, and update Swiss Army Llama for local operation on the Supernode server itself (setting Swiss Army Llama up on a remote machine for use with the Inference Server is a separate process that we will detail in a later section). This is handled from the code file `setup_swiss_army_llama.py`, which is called from the `main.py` entrypoint code file in the Inference Server as a startup task of the FastAPI server. The complete code for `setup_swiss_army_llama.py` can be found [here](https://github.com/pastelnetwork/python_inference_layer_server/blob/master/setup_swiss_army_llama.py).

The `setup_swiss_army_llama.py` file contains various utility functions and a main function `check_and_setup_swiss_army_llama` that orchestrates the entire setup process. Let's go through each function and explain their purpose and how they contribute to setting up and managing the Swiss Army Llama service:

1. `get_external_ip_func`:
    - This function attempts to retrieve the external IP address of the server by querying several public IP address providers.
    - It iterates through a list of providers and sends HTTP GET requests to each provider until a successful response is received.
    - If a provider returns a valid IP address, the function returns it.
    - If all providers fail to provide an IP address, the function returns "Unknown".
    
2. `run_command`:
    - This function is a wrapper around the `subprocess.run` function to execute shell commands.
    - It takes the command as a string or a list of command arguments, along with optional parameters for environment variables, output capture, error checking, and timeout.
    - It uses the default shell (`/bin/zsh` if available, otherwise `/bin/bash`) to execute the command.
    - If `capture_output` is set to `True`, the function captures and logs the command's stdout and stderr.
    - If the command times out or fails, the function logs the appropriate error message.
    - It returns the `subprocess.CompletedProcess` object representing the executed command.
    
3. `is_port_available`:
    - This function checks if a specified port is available (not in use) on the server.
    - It uses the `lsof` command to list open files and sockets associated with the port.
    - If the command's return code is non-zero, it means the port is available.
    
4. `is_swiss_army_llama_responding`:
    - This function checks if the Swiss Army Llama service is responding on a specified IP address and port.
    - It constructs a URL using the provided IP address, port, and the `/get_list_of_available_model_names/` endpoint.
    - It sends an HTTP GET request to the URL with the security token as a parameter.
    - If the response status code is 200, it means the service is responding.
    
5. `update_security_token`:
    - This function updates the security token in the Swiss Army Llama configuration file.
    - It reads the content of the specified file and uses regular expressions to replace the existing security token with the provided new token.
    - It writes the updated content back to the file.
    
6. `is_pyenv_installed`:
    - This function checks if the `pyenv` tool is installed on the server.
    - It runs the `pyenv --version` command and checks the return code.
    - If the return code is 0, it means `pyenv` is installed.
    
7. `is_python_3_12_installed`:
    - This function checks if Python 3.12 is installed using `pyenv`.
    - It runs the `pyenv versions` command and checks if "3.12" is present in the output.
    
8. `is_rust_installed`:
    - This function checks if the Rust programming language is installed on the server.
    - It runs the `rustc --version` command and checks the return code.
    - If the return code is 0, it means Rust is installed.
    - If the command throws a `FileNotFoundError`, it means Rust is not installed.
    
9. `setup_virtual_environment`:
    - This function sets up a Python virtual environment for Swiss Army Llama.
    - It creates a virtual environment directory if it doesn't exist.
    - It upgrades `pip`, installs the `wheel` package, and installs the dependencies from the `requirements.txt` file.
    - It returns the path to the Python executable within the virtual environment.
    
10. `set_timezone_utc`:
    - This function sets the timezone to UTC.
    - It sets the `TZ` environment variable to 'UTC'.
    - It adds the `export TZ=UTC` line to the user's shell profile file (`~/.zshrc` or `~/.bashrc`).
    
11. `check_systemd_service_exists`:
    - This function checks if a systemd service with the specified name exists and is enabled.
    - It runs the `systemctl is-enabled <service_name>` command and checks the return code and output.
    - If the return code is 0 and the output contains 'enabled', it means the service exists.
    
12. `create_systemd_service`:
    - This function creates a systemd service file for Swiss Army Llama.
    - It constructs the service file content with the provided service name, user, working directory, and command to execute.
    - It writes the service file content to a temporary file and moves it to the systemd directory.
    - It reloads the systemd daemon, enables the service, and starts the service.
    - It logs the status of the service after starting it.
    
13. `ensure_pyenv_setup`:
    - This function ensures that `pyenv` is installed and Python 3.12 is set up.
    - If `pyenv` is not installed, it installs the necessary dependencies and installs `pyenv`.
    - If Python 3.12 is not installed, it installs Python 3.12 using `pyenv` and sets it as the global Python version.
    
14. `configure_shell_for_pyenv`:
    - This function configures the shell environment for `pyenv`.
    - It adds the necessary `pyenv` initialization commands to the user's shell profile file (`~/.zshrc` or `~/.bashrc`).
    - It updates the `PYENV_ROOT` and `PATH` environment variables to include `pyenv` paths.
    
15. `has_repo_been_updated`:
    - This function checks if the Swiss Army Llama repository has been updated.
    - It fetches the latest changes from the remote repository.
    - It compares the local commit hash with the remote commit hash of the `main` branch.
    - If the commit hashes are different, it means the repository has been updated.
    
16. `setup_swiss_army_llama`:
    - This function performs the actual setup of Swiss Army Llama.
    - It sets the timezone to UTC.
    - It clones the Swiss Army Llama repository if it doesn't exist.
    - It checks for updates in the repository and pulls the latest changes if updates are available.
    - It configures the shell environment for `pyenv` and ensures `pyenv` and Python 3.12 are installed.
    - It sets up a Python virtual environment for Swiss Army Llama and installs the dependencies.
    - If Rust is not installed, it installs Rust and sets up the necessary environment variables.
    - If the Swiss Army Llama systemd service doesn't exist, it creates the service file and starts the service.
    - If the service already exists, it reloads the systemd daemon, enables the service, and starts it.
    
17. `kill_running_instances_of_swiss_army_llama`:
    - This function kills any running instances of Swiss Army Llama.
    - It stops the Swiss Army Llama systemd service.
    - It finds and kills any remaining Swiss Army Llama processes using the `ps` and `kill` commands.
    
18. `check_and_setup_swiss_army_llama`:
    - This is the main function that orchestrates the setup process.
    - It retrieves the external IP address of the server.
    - It checks if the Swiss Army Llama repository has been updated.
    - It checks if the Swiss Army Llama service is responding on the specified port.
    - It checks if the Swiss Army Llama port is available.
    - If the service is responding and the repository hasn't been updated, it considers Swiss Army Llama to be already set up and skips the setup process.
    - If the service is not responding or the repository has been updated, it kills any running instances of Swiss Army Llama and runs the setup process.

These functions work together to automate the setup and management of the Swiss Army Llama service on the Supernode server. The `check_and_setup_swiss_army_llama` function serves as the entry point and is called from the `main.py` file as a startup task of the FastAPI server.

By checking for updates in the Swiss Army Llama repository, ensuring the necessary dependencies are installed, configuring the environment, and managing the systemd service, this code ensures that the Swiss Army Llama service is properly set up and running on the Supernode server. It also handles scenarios where the service may need to be restarted or updated based on changes in the repository or the service's responsiveness.

----------

**Environment Variable Setup and Processing, Including Decryption of API Keys and Passwords**

The Pastel Inference Layer server relies on environment variables to store and manage various configuration settings, including sensitive information such as API keys and passwords. These environment variables are typically stored in a `.env` file, which is loaded and processed during the server startup.

The `.env` file contains a wide range of configuration settings, covering aspects such as:

- Server port numbers
- Timeout values
- Transaction settings
- Credit pack settings
- API keys for external services (e.g., OpenAI, Anthropic, Stability)
- Passphrases for local and remote Pastel IDs

You can see the sample .env file below. Not that the very long values are actually encrypted secrets that rely upon the existence of a saved decryption key stored somewhere on the machine:

```Dockerfile
    UVICORN_PORT=7123
    TEMP_OVERRIDE_LOCALHOST_ONLY=1
    CHALLENGE_EXPIRATION_TIME_IN_SECONDS=300
    NUMBER_OF_DAYS_BEFORE_MESSAGES_ARE_CONSIDERED_OBSOLETE=3
    GITHUB_MODEL_MENU_URL=https://raw.githubusercontent.com/pastelnetwork/python_inference_layer_server/master/model_menu.json
    MESSAGING_TIMEOUT_IN_SECONDS=60
    MAXIMUM_NUMBER_OF_PASTEL_BLOCKS_FOR_USER_TO_SEND_BURN_AMOUNT_FOR_CREDIT_TICKET=50
    MAXIMUM_LOCAL_CREDIT_PRICE_DIFFERENCE_TO_ACCEPT_CREDIT_PRICING=0.25
    MAXIMUM_LOCAL_PASTEL_BLOCK_HEIGHT_DIFFERENCE_IN_BLOCKS=1
    MINIMUM_NUMBER_OF_PASTEL_BLOCKS_BEFORE_TICKET_STORAGE_RETRY_ALLOWED=10
    MINIMUM_CONFIRMATION_BLOCKS_FOR_CREDIT_PACK_BURN_TRANSACTION=1
    MAXIMUM_LOCAL_UTC_TIMESTAMP_DIFFERENCE_IN_SECONDS=55.0
    BURN_TRANSACTION_MAXIMUM_AGE_IN_DAYS=3
    SUPERNODE_CREDIT_PRICE_AGREEMENT_QUORUM_PERCENTAGE=0.51
    SUPERNODE_CREDIT_PRICE_AGREEMENT_MAJORITY_PERCENTAGE=0.85
    CREDIT_USAGE_TO_TRACKING_AMOUNT_MULTIPLIER=10
    CREDIT_COST_MULTIPLIER_FACTOR=0.02
    API_KEY_TEST_VALIDITY_HOURS=72
    TARGET_VALUE_PER_CREDIT_IN_USD=0.01
    TARGET_PROFIT_MARGIN=0.15
    MAXIMUM_PER_CREDIT_PRICE_IN_PSL_FOR_CLIENT=100.0
    MINIMUM_COST_IN_CREDITS=0.5
    SWISS_ARMY_LLAMA_PORT=8089
    USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE=1
    REMOTE_SWISS_ARMY_LLAMA_INSTANCE_SSH_KEY_PATH=/home/ubuntu/vastai_privkey
    REMOTE_SWISS_ARMY_LLAMA_INSTANCE_IP_ADDRESS=106.185.159.136
    REMOTE_SWISS_ARMY_LLAMA_INSTANCE_PORT=20765
    REMOTE_SWISS_ARMY_LLAMA_EXPOSED_PORT=8089
    REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT=8087
    SWISS_ARMY_LLAMA_SECURITY_TOKEN=gAAAAABmEFF7K1lIlVGm3P8HxZoRJMdr0EIioE3Ug1x9PKaqLi620LOvSALMEcG7drPr61deEkcP2SRplE9qv-sjgwjE964axw%3D%3D
    OPENAI_API_KEY=gAAAAABmEXkqBi83ORWgoTMTvvg0d4ehGjTuCVFguKPVETPAsg7VhRE_Ecs4tb0WFylElFkhQTtHthcDQtzHQ_Do2M0SO2hYqG53vTANcBhPUbSAxwM0sRXcmqeEX1OWhaHbRze67DzkvzqBS1KnQtaUzBtUuusQ6w%3D%3D
    CLAUDE3_API_KEY=gAAAAABmEFF73nD1mrch6DKztNtk9-Kv6PvPpnHSYknatGR4ejc2tDyYc7aqqmxPR03AO23eR_VXIChhNt9q0ZSHB4A2tDVtTCRpsB3xywCe91_TTViyg0VDXj1hlKM-urXGHoKLnyn_xWqsrhTj6WP-ebjqWhdyK0cM-izqWsHFuXWvac1RGwmgZpadweFWYJOAvviTLClcBJaHANviG2_HGkvxfWr8GQ%3D%3D
    GROQ_API_KEY=gAAAAABmEJe9xW7Fejwg2buHRymIH5v7TcJGk7P_lLPEG91RJ28iLwHIJ7PCt77drz_Hz0z-aOAp-GgC6h6_DhZ0yw9H5vFsCtMAMmEOKRcJOMnbrGs-hcJRm6r3Cg3_tqwcSUG1XjVk4MeEYim9ZkbTBcG_AeHi0g%3D%3D
    MISTRAL_API_KEY=gAAAAABmEJe98LKZmYTux4IMIiSLHV3jftgxCr7zR_RKZx3xuMwiw5xv2rCy4hOY9Inyg2kkoFke8_o55RBLxNUe8UHOZIQmoa38o6XhHCxpydLlJORytFkb7fI6pKWv0hPNgnFiMjw6
    STABILITY_API_KEY=gAAAAABmEM0J9Y4HtTRPcqbhy5R3UZpDQvHZrUY6UKbcMRh-RsV7JROD1nCxGcDElDkdt61qgVZWOUiCpNDdE5w_hxV7wmAuQosh-Vsfapp25cb2qMA3xJzK8Aoo1X1gdyakDq2xHJWhhtFWpCDTBcrt6p799tMfCQ%3D%3D
    OPENROUTER_API_KEY=gAAAAABmFzAGXWhWQZox78SWuj5SUvesTkCU-ry5uf2YZyqfUaF1vZjkR6zwbghs9vdbqLEv4rSqu0FSGewDiIG-jFw8JpY2p9AIt9h2M0Mhc8JkRp9fXGVnCR8YFqaw3aQZmmbpwSobshjqw9oWdgnl3rmpd1SSRXtX5RLpXX7vWPiFbrBZj_8%3D
    LOCAL_PASTEL_ID_PASSPHRASE=gAAAAABmEFF72YuPdwL_cNZSu54NLVxiT9s6QwGpUn0dnHoBbHRq7T-vK_vAuBV0HFrpnlquzxrmknsYKuaiWktTjOSH4knDnA%3D%3D
    MY_LOCAL_PASTELID=jXYdog1FfN1YBphHrrRuMVsXT76gdfMTvDBo2aJyjQnLdz2HWtHUdE376imdgeVjQNK93drAmwWoc7A3G4t2Pj
    MY_PASTELID_PASSPHRASE=gAAAAABmEFF7yMxMmyz4xaSa7ykie-487B42GOgzlR0HGa0K08M7Ow3DZHNt6W46M_SQQQaKtFMM-OTVJDIE51AQh4CBchmU1g%3D%3D
```

The use of environment variables allows for flexible configuration of the server without the need to modify the codebase directly. It also enables easy management of different configurations for various environments (e.g., development, staging, production).

To process the environment variables, the Pastel Inference Layer server utilizes the `python-decouple` library. This library provides a convenient way to load environment variables from a file and access them throughout the application. The `Config` class from `python-decouple` is used to create a `config` object, which acts as a centralized point of access for the environment variables.
One critical aspect of the environment variable setup is the handling of sensitive information, such as API keys and passphrases. To ensure the security of these sensitive values, the Pastel Inference Layer server employs encryption techniques. The sensitive fields in the `.env` file are encrypted using the `Fernet` symmetric encryption algorithm from the `cryptography` library.

The encryption and decryption process is handled by a set of dedicated functions in the `service_functions.py` module. These functions include:


- `generate_or_load_encryption_key_sync`: Generates or loads the encryption key synchronously.
- `encrypt_sensitive_fields`: Encrypts the sensitive fields in the `.env` file.
- `decrypt_sensitive_data`: Decrypts the encrypted data using the provided encryption key.
- `encrypt_sensitive_data`: Encrypts the sensitive data using the provided encryption key.
- `decrypt_sensitive_fields`: Decrypts the sensitive fields and assigns them to global variables.

The encryption key is generated or loaded during the server startup process. If an encryption key file doesn't exist, a new key is generated using the `Fernet.generate_key()` method and saved to a file for future use. The encryption key is then used to encrypt the sensitive fields in the `.env` file, replacing the plain-text values with encrypted strings.

During runtime, the `decrypt_sensitive_fields` function is called to decrypt the sensitive fields using the encryption key. The decrypted values are assigned to global variables, making them accessible throughout the application. This approach ensures that the sensitive information remains protected at rest (in the `.env` file) and is only decrypted when needed during execution.
The encryption and decryption process strikes a balance between security and usability. By encrypting the sensitive fields, the Pastel Inference Layer server minimizes the risk of unauthorized access to critical information. At the same time, the decryption of these fields during runtime allows the server to utilize the necessary API keys and passphrases for its operations.

In summary, the environment variable setup and processing in the Pastel Inference Layer server play a vital role in the configuration and security of the application. The use of a `.env` file and the `python-decouple` library enables flexible and centralized management of configuration settings. The encryption and decryption of sensitive fields using the `Fernet` algorithm ensures the protection of critical information while still allowing seamless integration with the server's functionality. This setup demonstrates a thoughtful approach to balancing configuration flexibility, security, and ease of use in the Pastel Inference Layer server.

----------

**Ansible Playbook for Deploying Inference Server**

The Inference Server repo includes an ansible playbook called [pastel_inference_layer_deployment_playbook.yml](https://github.com/pastelnetwork/python_inference_layer_server/blob/master/setup_scripts_and_playbooks/pastel_inference_layer_deployment_playbook.yml) that largely automates the process of deploying the system on a fresh machine, or updating a configured inference server Supernode when the inference code is updated. 

The complete playbook is shown below, with explanation to follow:

```YAML
---
- name: Pastel Inference Layer Deployment
  hosts: all
  become: yes
  vars:
    ubuntu_user: ubuntu
    oh_my_zsh_install_script: "https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh"
    rust_install_script: "https://sh.rustup.rs"
    atuin_install_script: "https://setup.atuin.sh"
    zshrc_path: "/home/{{ ubuntu_user }}/.zshrc"
    bashrc_path: "/home/{{ ubuntu_user }}/.bashrc"
    oh_my_zsh_install_flag: "/home/{{ ubuntu_user }}/.oh-my-zsh"
    home_dir: "/home/{{ ubuntu_user }}"

  tasks:
    - name: Update and upgrade apt packages
      ansible.builtin.apt:
        update_cache: yes
        upgrade: dist
        autoremove: yes

    - name: Check if zsh is installed
      command: which zsh
      register: zsh_installed
      ignore_errors: yes

    - name: Ensure zsh_installed is defined
      set_fact:
        zsh_installed: { "rc": 1 }
      when: zsh_installed is not defined

    - name: Check if Oh My Zsh is installed
      stat:
        path: "{{ oh_my_zsh_install_flag }}"
      register: oh_my_zsh_installed

    - name: Install Oh My Zsh
      become_user: "{{ ubuntu_user }}"
      shell: >
        sh -c "$(curl -fsSL {{ oh_my_zsh_install_script }})" && touch {{ oh_my_zsh_install_flag }}
      when: not oh_my_zsh_installed.stat.exists

    - name: Install Rust
      become_user: "{{ ubuntu_user }}"
      shell: >
        curl -fsSL {{ rust_install_script }} | sh -s -- -y

    - name: Ensure Rust environment is loaded
      lineinfile:
        path: "{{ zshrc_path }}"
        regexp: 'source $HOME/.cargo/env'
        line: 'source $HOME/.cargo/env'
        state: present
      become_user: "{{ ubuntu_user }}"

    - name: Install Atuin
      become_user: "{{ ubuntu_user }}"
      shell: >
        /bin/bash -c "$(curl --proto '=https' --tlsv1.2 -sSf {{ atuin_install_script }})"

    - name: Ensure Atuin environment is loaded
      lineinfile:
        path: "{{ zshrc_path }}"
        regexp: 'eval "$(atuin init zsh)"'
        line: 'eval "$(atuin init zsh)"'
        state: present
      become_user: "{{ ubuntu_user }}"

    - name: Install dependencies for pyenv
      apt:
        name:
          - build-essential
          - libssl-dev
          - zlib1g-dev
          - libbz2-dev
          - libreadline-dev
          - libsqlite3-dev
          - wget
          - curl
          - llvm
          - libncurses5-dev
          - libncursesw5-dev
          - xz-utils
          - tk-dev
          - libffi-dev
          - liblzma-dev
          - python3-openssl
          - git
        state: present
        update_cache: yes

    - name: Clone pyenv repository
      git:
        repo: 'https://github.com/pyenv/pyenv.git'
        dest: '{{ home_dir }}/.pyenv'
        update: yes
        force: yes

    - name: Set pyenv environment variables in .zshrc if zsh is found
      blockinfile:
        path: '{{ zshrc_path }}'
        block: |
          export PYENV_ROOT="$HOME/.pyenv"
          export PATH="$PYENV_ROOT/bin:$PATH"
          eval "$(pyenv init --path)"
      when: zsh_installed.rc == 0

    - name: Set pyenv environment variables in .bashrc if zsh is not found
      blockinfile:
        path: '{{ bashrc_path }}'
        block: |
          export PYENV_ROOT="$HOME/.pyenv"
          export PATH="$PYENV_ROOT/bin:$PATH"
          eval "$(pyenv init --path)"
      when: zsh_installed.rc != 0

    - name: Ensure pyenv directories have correct permissions
      file:
        path: /home/ubuntu/.pyenv
        state: directory
        owner: ubuntu
        group: ubuntu
        mode: '0755'
        recurse: yes

    - name: Ensure pyenv is available and install Python 3.12
      become: no
      shell: |
        export PATH="/home/ubuntu/.pyenv/bin:$PATH"
        eval "$(pyenv init --path)"
        eval "$(pyenv init -)"
        pyenv install 3.12
      args:
        executable: /bin/bash
      environment:
        PYENV_ROOT: "/home/ubuntu/.pyenv"
      become_user: ubuntu

    - name: Check again if zsh is installed
      command: which zsh
      register: zsh_installed
      ignore_errors: yes
      failed_when: false

    - name: Replace /root/ with /home/ubuntu/ in .zshrc
      replace:
        path: "/home/{{ ubuntu_user }}/.zshrc"
        regexp: "/root/"
        replace: "/home/{{ ubuntu_user }}/"

    - name: Determine the user shell
      shell: echo $SHELL
      register: user_shell

    - name: Set shell path and profile file
      set_fact:
        shell_path: "{{ user_shell.stdout }}"
        profile_file: "{{ 'zshrc' if '/zsh' in user_shell.stdout else 'bashrc' }}"

    - name: Check if the application directory exists
      stat:
        path: /home/{{ ubuntu_user }}/python_inference_layer_server
      register: app_dir

    - name: Clone the repository if the directory doesn't exist
      git:
        repo: https://github.com/pastelnetwork/python_inference_layer_server
        dest: /home/{{ ubuntu_user }}/python_inference_layer_server
      when: not app_dir.stat.exists

    - name: Run initial setup script if the directory was just created
      shell: |
        chmod +x setup_scripts_and_playbooks/initial_inference_server_setup_script.sh
        ./setup_scripts_and_playbooks/initial_inference_server_setup_script.sh
      args:
        chdir: /home/{{ ubuntu_user }}/python_inference_layer_server
      when: not app_dir.stat.exists

    - name: Update code
      shell: |
        source /home/{{ ubuntu_user }}/.{{ profile_file }}
        git stash
        git pull
      args:
        chdir: /home/{{ ubuntu_user }}/python_inference_layer_server
        executable: "{{ shell_path }}"

    - name: Get the name of the existing tmux session
      shell: tmux list-sessions -F '#{session_name}' | head -1
      register: tmux_session_name
      ignore_errors: true

    - name: Create tmux session if it doesn't exist
      shell: tmux new-session -d -s default_session
      when: tmux_session_name.stdout == ""
      args:
        executable: "{{ shell_path }}"

    - name: Set the tmux session name
      set_fact:
        session_name: "{{ tmux_session_name.stdout if tmux_session_name.stdout else 'default_session' }}"

    - name: Check if supernode_script window exists
      shell: tmux list-windows -t {{ session_name }} -F '#{window_name}' | grep -q '^supernode_script$'
      register: window_exists
      ignore_errors: true

    - name: Kill supernode_script window if it exists
      shell: tmux kill-window -t {{ session_name }}:supernode_script
      when: window_exists.rc == 0

    - name: Create temporary script
      copy:
        content: |
          #!/bin/{{ 'zsh' if '/zsh' in shell_path else 'bash' }}
          source /home/{{ ubuntu_user }}/.{{ profile_file }}
          cd /home/{{ ubuntu_user }}/python_inference_layer_server
          pyenv local 3.12
          source venv/bin/activate
          python -m pip install --upgrade pip
          python -m pip install --upgrade setuptools
          python -m pip install wheel
          pip install -r requirements.txt
          python main.py
        dest: /home/{{ ubuntu_user }}/run_script.sh
        mode: '0755'

    - name: Launch script in new tmux window
      shell: |
        tmux new-window -t {{ session_name }}: -n supernode_script -d "{{ shell_path }} -c '/home/{{ ubuntu_user }}/run_script.sh'"
      args:
        executable: "{{ shell_path }}"

    - name: Remove temporary script
      file:
        path: /home/{{ ubuntu_user }}/run_script.sh
        state: absent

```

**Detailed Breakdown of the Ansible Playbook for the Pastel Inference Layer Deployment:**

The Ansible playbook above is designed to automate the deployment of the Pastel Inference Layer on a server. It performs various tasks to prepare the environment, install necessary software, and set up the application for running within a `tmux` session. Below is a comprehensive breakdown of each step:

#### Initial Setup and Package Management
The playbook begins by updating the package lists and upgrading the installed packages. This ensures that the system is up to date with the latest security patches and software versions. The `autoremove` option cleans up unnecessary packages, keeping the system lean.

#### Checking and Installing Oh My Zsh
The playbook checks if Oh My Zsh is already installed by looking for a specific file. If it is not installed, it uses a shell script to install Oh My Zsh, which provides an enhanced shell experience.

#### Installing Rust
Rust is installed using a provided script. Rust is essential for compiling certain dependencies and tools that may be required by the Pastel Inference Layer.

#### Configuring Shell Environments
The playbook ensures that the Rust environment variables are loaded by adding them to the `.zshrc` file. This allows the user to use Rust tools seamlessly.

#### Installing Atuin
Atuin, a tool for enhancing shell history, is installed. The playbook adds the necessary initialization commands to `.zshrc` to ensure it is available in the shell.

#### Setting Up pyenv and Python
The playbook installs dependencies required for building Python from source and then clones the `pyenv` repository. `pyenv` allows for easy management of multiple Python versions.

Depending on whether `zsh` or `bash` is the default shell, it adds `pyenv` initialization commands to either `.zshrc` or `.bashrc`. This ensures `pyenv` is set up correctly regardless of the shell in use.

#### Ensuring pyenv Initialization
The playbook runs commands to initialize `pyenv` in the respective shell and checks if `pyenv` is correctly installed by verifying its version. This step ensures that the environment is correctly set up for Python version management.

#### Installing Python 3.12 and Setting Up Virtual Environment
The playbook installs Python 3.12 using `pyenv`. It then creates and activates a virtual environment within the project directory. This isolated environment ensures that dependencies for the Pastel Inference Layer do not interfere with other Python projects on the system.

#### Cloning the Application Repository
The playbook checks if the application directory exists. If it does not, it clones the repository from GitHub and runs an initial setup script. This script likely sets up necessary configurations and dependencies specific to the Pastel Inference Layer.

#### Updating the Application Code
If the repository already exists, the playbook stashes any local changes and pulls the latest code from the repository. This ensures the application is up to date with the latest changes.

#### Setting Up and Managing tmux Session
The playbook uses `tmux` to create a new session or attach to an existing one. This allows the application to run in the background, even if the SSH session is disconnected.

It checks if a specific `tmux` window for running the application exists. If it does, it kills the window to prevent multiple instances from running. It then creates a new window within the `tmux` session to run the application.

A temporary script is created and executed within the `tmux` window to start the application. This script sets up the environment, activates the virtual environment, installs any missing dependencies, and starts the application.

#### Cleaning Up
After launching the application, the playbook removes the temporary script to keep the system clean.

----------

**Using a Remote GPU-Enabled Instance of Swiss Army Llama on Vast.ai to Speed Up Inference**
 
The Pastel Inference Layer server supports the use of a remote GPU-enabled instance of Swiss Army Llama hosted on Vast.ai to significantly speed up inference tasks. This approach leverages the power of GPU acceleration while maintaining a cost-effective and decentralized infrastructure.
The process involves using a custom Docker image that encapsulates the necessary dependencies and configurations for running Swiss Army Llama on a GPU-enabled instance. The Docker image is based on the PyTorch image with CUDA and cuDNN pre-installed, ensuring compatibility with GPU acceleration.

The Dockerfile performs the following steps:

1. Sets environment variables for FAISS GPU build and CUDA support.
2. Installs necessary packages and dependencies, including build tools, libraries, and utilities.
3. Installs Rust and Atuin for additional functionality.
4. Clones the Swiss Army Llama repository and sets the working directory.
5. Installs Python dependencies using conda and pip, including the GPU-enabled version of FAISS.
6. Exposes the necessary ports for the Swiss Army Llama server and Redis.
7. Creates a benchmark script to assess the performance of the host machine.
8. Defines the command to start the benchmark, Redis server, and Swiss Army Llama server when the container starts.
9. 

The resulting Docker image is published on Docker Hub [here](https://hub.docker.com/repository/docker/jemanuel82/vastai_swiss_army_llama_template/general), making it easily accessible for deployment. The exact Dockerfile contents is shown below:

```bash
    # Use the PyTorch image with CUDA and cuDNN pre-installed
    FROM pytorch/pytorch:2.2.0-cuda12.1-cudnn8-devel
    
    # Set environment variables for FAISS GPU build and CUDA support
    ENV DEBIAN_FRONTEND=noninteractive
    ENV PYTHONUNBUFFERED=1
    ENV DATA_DIRECTORY=/swiss_army_llama
    ENV FAISS_ENABLE_GPU=ON
    ENV LLAMA_CUDA=on
    ENV PATH="/root/.cargo/bin:${PATH}"
    
    # Pre-accept the Microsoft EULA for ttf-mscorefonts-installer and update/install packages
    RUN echo ttf-mscorefonts-installer msttcorefonts/accepted-mscorefonts-eula select true | debconf-set-selections && \
        apt-get update && \
        apt-get install -y \
        build-essential curl pkg-config ffmpeg gcc g++ gdb gdisk git cmake htop psmisc sysbench \
        libboost-all-dev libssl-dev make nano nginx nodejs npm openssh-client iperf3 \
        openssh-server openssl rsync libpulse-dev software-properties-common inetutils-ping \
        ubuntu-release-upgrader-core ubuntu-restricted-extras ufw unzip vcsh vim vpnc \
        zip zlib1g-dev zstd libpq-dev libmagic1 libxml2-dev libxslt1-dev antiword \
        unrtf poppler-utils tesseract-ocr flac lame libmad0 libsox-fmt-mp3 sox \
        libjpeg-dev swig redis-server wget libffi-dev libbz2-dev libreadline-dev \
        libsqlite3-dev llvm libncurses5-dev xz-utils tk-dev libxmlsec1-dev liblzma-dev \
        libncursesw5-dev python3-openssl libpoppler-cpp-dev pstotext net-tools iproute2 multitail && \
        apt-get upgrade -y && \
        apt-get autoremove -y && \
        rm -rf /var/lib/apt/lists/*
    
    # Install Rust and Atuin
    RUN curl https://sh.rustup.rs -sSf | sh -s -- -y && \
        /bin/bash -c "$(curl --proto '=https' --tlsv1.2 -sSf https://setup.atuin.sh)"
    
    # Clone the repository
    RUN git clone https://github.com/Dicklesworthstone/swiss_army_llama.git /swiss_army_llama
    WORKDIR /swiss_army_llama
    
    # Install Python dependencies using conda and pip
    RUN conda update -n base -c defaults conda && \
        conda install -c conda-forge python=3.12 pip setuptools wheel && \
        conda install -c conda-forge faiss-gpu && \
        CMAKE_ARGS="-DLLAMA_CUDA=on" /opt/conda/bin/pip install --no-cache-dir -r requirements.txt
    
    # Expose the port the app runs on and Redis default port
    EXPOSE 8089 6379
    
    # Create benchmark script
    RUN echo '#!/bin/bash' > /swiss_army_llama/benchmark.sh && \
        echo 'echo "CPU Benchmark:" > /swiss_army_llama/host_machine_benchmark_results.txt' >> /swiss_army_llama/benchmark.sh && \
        echo 'sysbench --test=cpu --cpu-max-prime=5000 run >> /swiss_army_llama/host_machine_benchmark_results.txt' >> /swiss_army_llama/benchmark.sh && \
        echo 'echo "\nMemory Benchmark:" >> /swiss_army_llama/host_machine_benchmark_results.txt' >> /swiss_army_llama/benchmark.sh && \
        echo 'sysbench --test=memory --memory-block-size=1M --memory-total-size=2G run >> /swiss_army_llama/host_machine_benchmark_results.txt' >> /swiss_army_llama/benchmark.sh && \
        echo 'echo "\nDisk Benchmark:" >> /swiss_army_llama/host_machine_benchmark_results.txt' >> /swiss_army_llama/benchmark.sh && \
        echo 'dd if=/dev/zero of=/tmp/test1.img bs=1G count=1 oflag=dsync 2>> /swiss_army_llama/host_machine_benchmark_results.txt' >> /swiss_army_llama/benchmark.sh && \
        echo 'rm -f /tmp/test1.img' >> /swiss_army_llama/benchmark.sh && \
        echo 'echo "\nNetwork Benchmark (localhost):" >> /swiss_army_llama/host_machine_benchmark_results.txt' >> /swiss_army_llama/benchmark.sh && \
        echo 'iperf3 -s & sleep 1' >> /swiss_army_llama/benchmark.sh && \
        echo 'iperf3 -c localhost -t 10 >> /swiss_army_llama/host_machine_benchmark_results.txt' >> /swiss_army_llama/benchmark.sh && \
        echo 'pkill iperf3' >> /swiss_army_llama/benchmark.sh && \
        chmod +x /swiss_army_llama/benchmark.sh
    
    # Start benchmark, Redis, and swiss_army_llama server when the container starts
    CMD /swiss_army_llama/benchmark.sh && service redis-server start && python /swiss_army_llama/swiss_army_llama.py
```


To utilize this GPU-enabled Swiss Army Llama instance, users can leverage the Vast.ai platform. Vast.ai is a decentralized marketplace that allows individuals to rent out their spare GPU capacity at affordable rates. Users can spin up a powerful machine with a 4090 GPU and ample RAM for as low as 34 cents per hour, significantly lower than traditional cloud providers like AWS, Azure, or Lambda. It’s sort of like the “UberX” of GPU cloud instance providers, with a disruptive and compelling business model. 

To get started with Vast.ai, users need to follow these steps:


1. Create an account on Vast.ai and purchase credits using a credit card (via Stripe) or by paying with cryptocurrency.
2. Generate an SSH key and add it to their Vast.ai account for secure access to provisioned instances.
3. Select the [Swiss Army Llama template](https://cloud.vast.ai/?ref_id=78066&template_id=3c711e5ddd050882f0eb6f4ce1b8cc28](https://cloud.vast.ai/?ref_id=78066&template_id=3c711e5ddd050882f0eb6f4ce1b8cc28) to speed up the deployment process. This template specifies the custom Docker image and the required storage of 60GB.
4. Provision the instance on Vast.ai, which will provide the IP address and port for SSH access.
5. Access the provisioned machine using SSH with the command provided by Vast.ai, similar to:

```bash
    ssh -i ~/vastai_privkey -p 20765 root@106.185.159.136
```

6. Once connected to the instance, navigate to the Swiss Army Llama directory and start the server:

```bash
    cd /swiss_army_llama/ 
    git update python swiss_army_llama.py
    cd /swiss_army_llama/ git update python swiss_army_llama.py
```

7. This will initiate the process of downloading the default Swiss Army Llama model files from Hugging Face.

To integrate the remote Swiss Army Llama instance with the local Supernode inference servers, users need to add the following optional fields to their `.env` file:

```ini
    USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE=1
    REMOTE_SWISS_ARMY_LLAMA_INSTANCE_SSH_KEY_PATH=/home/ubuntu/vastai_privkey
    REMOTE_SWISS_ARMY_LLAMA_INSTANCE_IP_ADDRESS=106.185.159.136
    REMOTE_SWISS_ARMY_LLAMA_INSTANCE_PORT=20765
    REMOTE_SWISS_ARMY_LLAMA_EXPOSED_PORT=8089
    REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT=8087
```

With these configurations in place, the Pastel Inference Layer server will automatically detect the availability of the remote Swiss Army Llama instance and establish an SSH tunnel to the Vast.ai machine. If the connection is successful, all Swiss Army Llama requests from the Supernode inference servers will be routed to the associated remote GPU-enabled instance.
This setup offers several benefits:

1. Improved Inference Performance: By utilizing a GPU-enabled instance, inference tasks can be accelerated significantly, reducing latency and improving overall performance.
2. Cost-Effectiveness: Vast.ai's decentralized marketplace allows users to rent GPU resources at affordable rates, making it more cost-effective compared to traditional cloud providers.
3. Decentralization and Censorship Resistance: By leveraging a decentralized platform like Vast.ai, the Pastel Inference Layer server maintains its decentralized nature and resists censorship attempts.
4. Resource Sharing: Multiple Supernode inference servers can share a single remote GPU instance, optimizing resource utilization and reducing costs.

By integrating a remote GPU-enabled instance of Swiss Army Llama hosted on Vast.ai, the Pastel Inference Layer server achieves a balance between inference performance, cost-effectiveness, and decentralization. This approach empowers users to harness the power of GPU acceleration while maintaining the core principles of the Pastel network.




---
./main.py
---
import warnings
from cryptography.utils import CryptographyDeprecationWarning
from logger_config import setup_logger
from endpoint_functions import router
import asyncio
import os
import random
import traceback
import fastapi
import threading
from fastapi import Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.exceptions import RequestValidationError
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.middleware.base import RequestResponseEndpoint
from starlette.responses import Response
import uvloop
from uvicorn import Config, Server
from decouple import Config as DecoupleConfig, RepositoryEnv
from database_code import initialize_db
from setup_swiss_army_llama import check_and_setup_swiss_army_llama
from service_functions import (monitor_new_messages, generate_or_load_encryption_key_sync, decrypt_sensitive_data, get_env_value, fetch_all_mnid_tickets_details, establish_ssh_tunnel, schedule_micro_benchmark_periodically,
                                list_generic_tickets_in_blockchain_and_parse_and_validate_and_store_them, generate_supernode_inference_ip_blacklist)
warnings.filterwarnings("ignore", category=CryptographyDeprecationWarning)
config = DecoupleConfig(RepositoryEnv('.env'))
UVICORN_PORT = config.get("UVICORN_PORT", cast=int)
USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE = config.get("USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE", default=0, cast=int)
SWISS_ARMY_LLAMA_SECURITY_TOKEN = config.get("SWISS_ARMY_LLAMA_SECURITY_TOKEN", cast=str)
os.environ['TZ'] = 'UTC' # Set timezone to UTC for the current session
asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
logger = setup_logger()

app = fastapi.FastAPI(
    title="Pastel-Supernode-Inference-Layer",
    description="Pastel Supernode Inference Layer API",
    docs_url="/",
    redoc_url="/redoc"
)

class LimitRequestSizeMiddleware(BaseHTTPMiddleware):
    def __init__(self, app, max_request_size: int):
        super().__init__(app)
        self.max_request_size = max_request_size

    async def dispatch(self, request: Request, call_next: RequestResponseEndpoint) -> Response:
        request_size = int(request.headers.get('content-length', 0))
        if request_size > self.max_request_size:
            return Response("Request size exceeds the limit", status_code=413)
        return await call_next(request)
    
app.add_middleware(LimitRequestSizeMiddleware, max_request_size=50 * 1024 * 1024)
app.include_router(router, prefix='', tags=['main'])

# Custom Exception Handling Middleware
@app.middleware("http")
async def custom_exception_handling(request: Request, call_next):
    try:
        return await call_next(request)
    except RequestValidationError as ve:
        logger.error(f"Validation error: {ve}")
        return JSONResponse(status_code=ve.status_code, content={"detail": ve.error_msg})
    except Exception as e:
        tb = traceback.format_exc()  # Get the full traceback
        logger.error(f"Unhandled exception: {e}\n{tb}")  # Log the exception with traceback
        return JSONResponse(status_code=500, content={"detail": str(e)})
    
# CORS Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=['*'],
    allow_credentials=True,
    allow_methods=['*'],
    allow_headers=['*'],
    expose_headers=["Authorization"]
)

def decrypt_sensitive_fields():
    global LOCAL_PASTEL_ID_PASSPHRASE, SWISS_ARMY_LLAMA_SECURITY_TOKEN, OPENAI_API_KEY, CLAUDE3_API_KEY, GROQ_API_KEY, MISTRAL_API_KEY, STABILITY_API_KEY, OPENROUTER_API_KEY, encryption_key
    LOCAL_PASTEL_ID_PASSPHRASE = decrypt_sensitive_data(get_env_value("LOCAL_PASTEL_ID_PASSPHRASE"), encryption_key)
    SWISS_ARMY_LLAMA_SECURITY_TOKEN = decrypt_sensitive_data(get_env_value("SWISS_ARMY_LLAMA_SECURITY_TOKEN"), encryption_key)
    OPENAI_API_KEY = decrypt_sensitive_data(get_env_value("OPENAI_API_KEY"), encryption_key)
    CLAUDE3_API_KEY = decrypt_sensitive_data(get_env_value("CLAUDE3_API_KEY"), encryption_key)
    GROQ_API_KEY = decrypt_sensitive_data(get_env_value("GROQ_API_KEY"), encryption_key)
    MISTRAL_API_KEY = decrypt_sensitive_data(get_env_value("MISTRAL_API_KEY"), encryption_key)
    STABILITY_API_KEY = decrypt_sensitive_data(get_env_value("STABILITY_API_KEY"), encryption_key)
    OPENROUTER_API_KEY = decrypt_sensitive_data(get_env_value("OPENROUTER_API_KEY"), encryption_key)
    
async def startup():
    global encryption_key  # Declare encryption_key as global
    try:
        db_init_complete = await initialize_db()
        logger.info(f"Database initialization complete: {db_init_complete}")
        encryption_key = generate_or_load_encryption_key_sync()  # Generate or load the encryption key synchronously    
        decrypt_sensitive_fields() # Now decrypt sensitive fields        
        asyncio.create_task(monitor_new_messages())  # Create a background task
        asyncio.create_task(fetch_all_mnid_tickets_details())
        asyncio.create_task(list_generic_tickets_in_blockchain_and_parse_and_validate_and_store_them())
        # asyncio.create_task(periodic_ticket_listing_and_validation())
        asyncio.create_task(asyncio.to_thread(check_and_setup_swiss_army_llama, SWISS_ARMY_LLAMA_SECURITY_TOKEN)) # Check and setup Swiss Army Llama asynchronously
        await generate_supernode_inference_ip_blacklist()  # Compile IP blacklist text file of unresponsive Supernodes for inference tasks
        asyncio.create_task(schedule_generate_supernode_inference_ip_blacklist())  # Schedule the task
        asyncio.create_task(schedule_micro_benchmark_periodically())  # Schedule the task
    except Exception as e:
        logger.error(f"Error during startup: {e}")
        
@app.on_event("startup")
async def startup_event():
    await startup()

async def schedule_generate_supernode_inference_ip_blacklist():
    while True:
        jitter = random.randint(-180, 180)  # Jitter of up to 3 minutes (180 seconds)
        interval_seconds = 300 + jitter  # 300 seconds = 5 minutes
        await asyncio.sleep(interval_seconds)
        await generate_supernode_inference_ip_blacklist()
        
async def main():
    uvicorn_config = Config(
        "main:app",
        host="0.0.0.0",
        port=UVICORN_PORT,
        loop="uvloop",
    )
    server = Server(uvicorn_config)
    await server.serve()

if __name__ == "__main__":
    if USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
        ssh_thread = threading.Thread(target=establish_ssh_tunnel, daemon=True)
        ssh_thread.start()
    generate_or_load_encryption_key_sync()
    config = DecoupleConfig(RepositoryEnv('.env'))
    asyncio.run(main())


---
./endpoint_functions.py
---
import service_functions
import database_code as db
from logger_config import logger
from fastapi import APIRouter, Depends, Query, Request, Body
from fastapi.responses import HTMLResponse, JSONResponse, StreamingResponse, FileResponse
from fastapi.exceptions import HTTPException
from fastapi.security import HTTPBasic, HTTPBasicCredentials
from starlette.background import BackgroundTask
from starlette.status import HTTP_401_UNAUTHORIZED
from json import JSONEncoder
from pathlib import Path
import json
import os
import asyncio
import tempfile
import uuid
import traceback
import pickle
import secrets
import pandas as pd
import plotly.express as px
from datetime import datetime, timedelta, timezone
from typing import Optional, List, Union, Dict, Any
from pydantic import SecretStr, BaseModel
from decouple import Config as DecoupleConfig, RepositoryEnv

config = DecoupleConfig(RepositoryEnv('.env'))
TEMP_OVERRIDE_LOCALHOST_ONLY = config.get("TEMP_OVERRIDE_LOCALHOST_ONLY", default=0)
SN_PERFORMANCE_STATS_PASSWORD = config.get("SN_PERFORMANCE_STATS_PASSWORD", default="CHANGEME")
pickle_file_path = Path('performance_data_history.pkl')
security = HTTPBasic()

def verify_password(credentials: HTTPBasicCredentials = Depends(security)):
    correct_password = secrets.compare_digest(credentials.password, SN_PERFORMANCE_STATS_PASSWORD)
    if not correct_password:
        raise HTTPException(
            status_code=HTTP_401_UNAUTHORIZED,
            detail="Incorrect password",
            headers={"WWW-Authenticate": "Basic"},
        )
    return credentials.password

# RPC Client Dependency
async def get_rpc_connection():
    rpc_host, rpc_port, rpc_user, rpc_password, other_flags = service_functions.get_local_rpc_settings_func() 
    return service_functions.AsyncAuthServiceProxy(f"http://{rpc_user}:{rpc_password}@{rpc_host}:{rpc_port}")

router = APIRouter()

def localhost_only(request: Request):
    client_host = request.client.host
    if not TEMP_OVERRIDE_LOCALHOST_ONLY:
        if client_host != "127.0.0.1":
            raise HTTPException(status_code=401, detail="Unauthorized")
    
class DateTimeEncoder(JSONEncoder):
    def default(self, obj):
        if isinstance(obj, (datetime.date, datetime.datetime)):
            return obj.isoformat()
        
@router.get("/liveness_ping", response_model=dict)
async def liveness_ping_function():
    current_utc_timestamp = datetime.now(timezone.utc)
    if service_functions.benchmark_results_cache:
        latest_benchmark = service_functions.benchmark_results_cache[-1]
        raw_benchmark_score = latest_benchmark[1]
        performance_ratio_score = latest_benchmark[2]
        last_benchmark_time = datetime.fromisoformat(latest_benchmark[0])
        if last_benchmark_time.tzinfo is None:
            last_benchmark_time = last_benchmark_time.replace(tzinfo=timezone.utc)
        last_benchmark_n_seconds_ago = (current_utc_timestamp - last_benchmark_time).total_seconds()
    else:
        raw_benchmark_score = None
        performance_ratio_score = None
        last_benchmark_n_seconds_ago = None

    response_dict = {
        'status': 'alive', 
        'timestamp': current_utc_timestamp, 
        'raw_benchmark_score': raw_benchmark_score, 
        'performance_ratio_score': performance_ratio_score,
        'last_benchmark_n_seconds_ago': last_benchmark_n_seconds_ago
    }
    return response_dict

@router.get("/supernode_list_json", response_model=dict)
async def get_supernode_list_json(
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Retrieves the list of Supernodes as JSON data.

    Returns a JSON object containing the Supernode list data.

    Raises:
    - HTTPException (status_code=500): If an error occurs while retrieving the Supernode list.

    Example response:
    {
        "1234567890abcdef": {
            "supernode_status": "ENABLED",
            "protocol_version": "1.0",
            "supernode_psl_address": "tPmkbohSbiocyAhXJVBZkBsKJiuVyNRp2GJ",
            "lastseentime": "2024-03-22T12:34:56.789000",
            "activeseconds": 3600,
            "lastpaidtime": "2024-03-22T11:34:56.789000",
            "lastpaidblock": 12345,
            "ipaddress:port": "127.0.0.1:9999",
            "rank": 1,
            "pubkey": "0x1234567890abcdef",
            "extAddress": "127.0.0.1:9999",
            "extP2P": "127.0.0.1:9998",
            "extKey": "1234567890abcdef",
            "activedays": 1.0
        },
        ...
    }
    """
    try:
        _, supernode_list_json = await service_functions.check_supernode_list_func()
        return JSONResponse(content=json.loads(supernode_list_json))
    except Exception as e:
        logger.error(f"Error getting supernode list JSON: {e}")
        raise HTTPException(status_code=500, detail="Internal Server Error")


@router.get("/supernode_list_csv")
async def get_supernode_list_csv(
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Retrieves the list of Supernodes as a normalized CSV file.

    Returns a StreamingResponse containing the CSV file data.

    Raises:
    - HTTPException (status_code=500): If an error occurs while retrieving or processing the Supernode list.

    Example response:
    A CSV file named "supernode_list.csv" will be downloaded containing the normalized Supernode list data.

    CSV file structure:
    supernode_status,protocol_version,supernode_psl_address,lastseentime,activeseconds,lastpaidtime,lastpaidblock,ipaddress:port,rank,pubkey,extAddress,extP2P,extKey,activedays
    ENABLED,1.0,tPmkbohSbiocyAhXJVBZkBsKJiuVyNRp2GJ,2024-03-22T12:34:56.789000,3600,2024-03-22T11:34:56.789000,12345,127.0.0.1:9999,1,0x1234567890abcdef,127.0.0.1:9999,127.0.0.1:9998,1234567890abcdef,1.0
    ...
    """
    try:
        supernode_list_df, _ = await service_functions.check_supernode_list_func()
        normalized_df = pd.json_normalize(supernode_list_df.to_dict(orient='records'))
        # Convert the normalized DataFrame to CSV
        csv_data = normalized_df.to_csv(index=False)
        # Create a StreamingResponse with the CSV data
        response = StreamingResponse(iter([csv_data]), media_type="text/csv")
        response.headers["Content-Disposition"] = "attachment; filename=supernode_list.csv"
        return response
    except Exception as e:
        logger.error(f"Error getting supernode list CSV: {e}")
        raise HTTPException(status_code=500, detail="Internal Server Error")


@router.get("/get_local_machine_sn_info", response_model=db.LocalMachineSupernodeInfo, dependencies=[Depends(localhost_only)])
async def get_local_machine_sn_info(
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Note: Endpoint only available on localhost.
        
    Retrieves information about the local machine's Supernode status.

    Returns a LocalMachineSupernodeInfo object containing the following fields:
    - `local_machine_supernode_data`: DataFrame containing the local machine's Supernode data (if it is a Supernode).
    - `local_sn_rank`: The rank of the local machine's Supernode (if it is a Supernode).
    - `local_sn_pastelid`: The PastelID of the local machine's Supernode (if it is a Supernode).
    - `local_machine_ip_with_proper_port`: The IP address and port of the local machine's Supernode (if it is a Supernode).

    Raises:
    - HTTPException (status_code=404): If the local machine is not a Supernode.
    - HTTPException (status_code=500): If an error occurs while retrieving the local machine's Supernode information.

    Example response:
    {
        "local_machine_supernode_data": {
            "supernode_status": "ENABLED",
            "protocol_version": "1.0",
            ...
        },
        "local_sn_rank": 1,
        "local_sn_pastelid": "1234567890abcdef",
        "local_machine_ip_with_proper_port": "127.0.0.1:9999"
    }
    """
    try:
        local_machine_supernode_data, local_sn_rank, local_sn_pastelid, local_machine_ip_with_proper_port = await service_functions.get_local_machine_supernode_data_func()
        if len(local_machine_supernode_data) > 0:
            return db.LocalMachineSupernodeInfo(
                local_machine_supernode_data=local_machine_supernode_data.to_dict(orient='records')[0],
                local_sn_rank=local_sn_rank,
                local_sn_pastelid=local_sn_pastelid,
                local_machine_ip_with_proper_port=local_machine_ip_with_proper_port
            )
        else:
            raise HTTPException(status_code=404, detail="Local machine is not a Supernode")
    except Exception as e:
        logger.error(f"Error getting local machine Supernode info: {e}")
        raise HTTPException(status_code=500, detail="Internal Server Error")
    
    
@router.get("/get_sn_data_from_pastelid", response_model=db.SupernodeData, dependencies=[Depends(localhost_only)])
async def get_sn_data_from_pastelid(
    pastelid: str = Query(..., description="The PastelID of the Supernode"),
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Note: Endpoint only available on localhost.
        
    Retrieves Supernode data based on the specified PastelID.

    - `pastelid`: The PastelID of the Supernode.

    Returns a SupernodeData object containing the Supernode data.

    Raises:
    - HTTPException (status_code=404): If the specified machine is not a Supernode.
    - HTTPException (status_code=500): If an error occurs while retrieving the Supernode data.

    Example response:
    {
        "supernode_status": "ENABLED",
        "protocol_version": "1.0",
        "supernode_psl_address": "tPmkbohSbiocyAhXJVBZkBsKJiuVyNRp2GJ",
        ...
    }
    """
    try:
        supernode_data = await service_functions.get_sn_data_from_pastelid_func(pastelid)
        if not supernode_data.empty:
            return db.SupernodeData(**supernode_data.to_dict(orient='records')[0])
        else:
            raise HTTPException(status_code=404, detail="Specified machine is not a Supernode")
    except Exception as e:
        logger.error(f"Error getting Supernode data from PastelID: {e}")
        raise HTTPException(status_code=500, detail="Internal Server Error")    
    
    
@router.get("/get_sn_data_from_sn_pubkey", response_model=db.SupernodeData, dependencies=[Depends(localhost_only)])
async def get_sn_data_from_sn_pubkey(
    pubkey: str = Query(..., description="The public key of the Supernode"),
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Note: Endpoint only available on localhost.
        
    Retrieves Supernode data based on the specified Supernode public key.

    - `pubkey`: The public key of the Supernode.

    Returns a SupernodeData object containing the Supernode data.

    Raises:
    - HTTPException (status_code=404): If the specified machine is not a Supernode.
    - HTTPException (status_code=500): If an error occurs while retrieving the Supernode data.

    Example response:
    {
        "supernode_status": "ENABLED",
        "protocol_version": "1.0",
        "supernode_psl_address": "tPmkbohSbiocyAhXJVBZkBsKJiuVyNRp2GJ",
        ...
    }
    """
    try:
        supernode_data = await service_functions.get_sn_data_from_sn_pubkey_func(pubkey)
        if not supernode_data.empty:
            return db.SupernodeData(**supernode_data.to_dict(orient='records')[0])
        else:
            raise HTTPException(status_code=404, detail="Specified machine is not a Supernode")
    except Exception as e:
        logger.error(f"Error getting Supernode data from public key: {e}")
        raise HTTPException(status_code=500, detail="Internal Server Error")
        

@router.get("/get_messages", response_model=List[db.Message], dependencies=[Depends(localhost_only)])
async def get_messages(
    last_k_minutes: Optional[int] = Query(100, description="Number of minutes to retrieve messages from"),
    message_type: Optional[str] = Query("all", description="Type of messages to retrieve ('all' or specific type)"),
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Note: Endpoint only available on localhost.
        
    Retrieves Supernode messages from the last specified minutes.

    - `last_k_minutes`: Number of minutes to retrieve messages from (default: 100).
    - `message_type`: Type of messages to retrieve ('all' or specific type) (default: 'all').

    Returns a list of Message objects containing the message, message_type, sending_sn_pastelid, and timestamp.
    """
    try:
        messages = await service_functions.parse_sn_messages_from_last_k_minutes_func(last_k_minutes, message_type)
        return [
            db.Message(
                message=msg["message"],
                message_type=msg["message_type"],
                sending_sn_pastelid=msg["sending_sn_pastelid"],
                timestamp=msg["timestamp"]
            )
            for msg in messages
        ]
    except Exception as e:
        logger.error(f"Error retrieving messages: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error retrieving messages: {str(e)}")
    

@router.post("/broadcast_message_to_all_sns", response_model=db.SendMessageResponse, dependencies=[Depends(localhost_only)])
async def broadcast_message_to_all_sns(
    message: str = Query(..., description="Message to broadcast"),
    message_type: str = Query(..., description="Type of the message"),
    pastelid_passphrase: SecretStr = Query(..., description="Passphrase for the sending PastelID"),
    verbose: Optional[int] = Query(0, description="Verbose mode (0 or 1)"),
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Note: Endpoint only available on localhost.
    
    Broadcasts a message to a list of Supernodes.

    - `message`: Message to broadcast.
    - `message_type`: Type of the message.
    - `pastelid_passphrase`: Passphrase for the sending PastelID.
    - `verbose`: Verbose mode (0 or 1) (default: 0).

    Returns a SendMessageResponse object containing the status and message.
    """
    try:
        signed_message = await service_functions.broadcast_message_to_all_sns_using_pastelid_func(
            message, message_type, pastelid_passphrase.get_secret_value(), verbose
        )
        return db.SendMessageResponse(status="success", message=f"Message broadcasted: {signed_message}")
    except Exception as e:
        logger.error(f"Error broadcasting message: {str(e)}")
        return db.SendMessageResponse(status="error", message=f"Error broadcasting message: {str(e)}")


@router.get("/request_challenge/{pastelid}")
async def request_challenge(
    pastelid: str,
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Request a challenge string for authentication.

    - `pastelid`: The PastelID requesting the challenge.

    Returns a dictionary containing the challenge string and the challenge ID.
    """
    try:
        challenge, challenge_id = await service_functions.generate_challenge(pastelid)
        return {"challenge": challenge, "challenge_id": challenge_id}
    except Exception as e:
        logger.error(f"Error generating challenge: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error generating challenge: {str(e)}")


@router.post("/send_user_message", response_model=db.SupernodeUserMessage)
async def send_user_message(
    user_message: db.UserMessage = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Send a user message via Supernodes.

    This endpoint allows a user to send a message to another user via the Supernode network.
    The sender must provide a valid challenge signature to authenticate their identity.

    Parameters:
    - `user_message` (UserMessage): The user message to be sent, including:
        - `from_pastelid` (str): The PastelID of the sender.
        - `to_pastelid` (str): The PastelID of the recipient.
        - `message_body` (str): The content of the message.
        - `message_signature` (str): The signature of the message by the sender's PastelID.
    - `challenge` (str): The challenge string obtained from the `/request_challenge` endpoint.
    - `challenge_id` (str): The ID of the challenge string.
    - `challenge_signature` (str): The signature of the PastelID on the challenge string.

    Returns:
    - `SupernodeUserMessage`: The sent message details, including:
        - `message` (str): The content of the sent message.
        - `message_type` (str): The type of the message (always "user_message").
        - `sending_sn_pastelid` (str): The PastelID of the Supernode that sent the message.
        - `timestamp` (datetime): The timestamp of when the message was sent.
        - `id` (int): The unique identifier of the Supernode user message.
        - `user_message` (UserMessage): The details of the user message, including:
            - `from_pastelid` (str): The PastelID of the sender.
            - `to_pastelid` (str): The PastelID of the recipient.
            - `message_body` (str): The content of the message.
            - `message_signature` (str): The signature of the message by the sender's PastelID.
            - `id` (int): The unique identifier of the user message.
            - `timestamp` (datetime): The timestamp of when the user message was created.

    Raises:
    - HTTPException (status_code=401): If the provided challenge signature is invalid.
    - HTTPException (status_code=500): If an error occurs while sending the user message.
    """
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            user_message.from_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        supernode_user_message = await service_functions.send_user_message_via_supernodes(
            user_message.from_pastelid, user_message.to_pastelid, user_message.message_body, user_message.message_signature
        )
        return supernode_user_message 
    except Exception as e:
        logger.error(f"Error sending user message: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error sending user message: {str(e)}")


@router.get("/get_user_messages", response_model=List[db.UserMessage])
async def get_user_messages(
    pastelid: str = Query(..., description="The PastelID to retrieve messages for"),
    challenge: str = Query(..., description="The challenge string"),
    challenge_id: str = Query(..., description="The ID of the challenge string"),
    challenge_signature: str = Query(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Retrieve all user messages (sent and received) for a given PastelID.

    This endpoint allows a user to retrieve all messages associated with their PastelID.
    The user must provide a valid challenge signature to authenticate their identity.

    Parameters:
    - `pastelid` (str): The PastelID of the user to retrieve messages for.
    - `challenge` (str): The challenge string obtained from the `/request_challenge` endpoint.
    - `challenge_id` (str): The ID of the challenge string.
    - `challenge_signature` (str): The signature of the PastelID on the challenge string.

    Returns:
    - List[UserMessage]: A list of user messages associated with the provided PastelID, each including:
        - `from_pastelid` (str): The PastelID of the sender.
        - `to_pastelid` (str): The PastelID of the recipient.
        - `message_body` (str): The content of the message.
        - `message_signature` (str): The signature of the message by the sender's PastelID.
        - `id` (int): The unique identifier of the user message.
        - `timestamp` (datetime): The timestamp of when the user message was created.

    Raises:
    - HTTPException (status_code=401): If the provided challenge signature is invalid.
    - HTTPException (status_code=500): If an error occurs while retrieving the user messages.
    """
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        user_messages = await service_functions.get_user_messages_for_pastelid(pastelid)
        return [
            db.UserMessage(
                from_pastelid=message.from_pastelid,
                to_pastelid=message.to_pastelid,
                message_body=message.message_body,
                message_signature=message.message_signature,
                id=message.id,
                timestamp=message.timestamp
            )
            for message in user_messages
        ]
    except Exception as e:
        logger.error(f"Error retrieving user messages: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error retrieving user messages: {str(e)}")


#__________________________________________________________________________________________________________
# Endpoints related to Credit pack purchasing and provisioning:

class CreditPackTicketResponse(BaseModel):
    credit_pack_purchase_request_response: db.CreditPackPurchaseRequestResponse
    credit_pack_purchase_request_confirmation: db.CreditPackPurchaseRequestConfirmation
    
    
@router.get("/get_credit_pack_ticket_from_txid", response_model=CreditPackTicketResponse)
async def get_credit_pack_ticket_from_txid_endpoint(
    txid: str = Query(..., description="The transaction ID of the credit pack ticket"),
    pastelid: str = Query(..., description="The PastelID of the requesting party"),
    challenge: str = Query(..., description="The challenge string"),
    challenge_id: str = Query(..., description="The ID of the challenge string"),
    challenge_signature: str = Query(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        _, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation = await service_functions.retrieve_credit_pack_ticket_using_txid(txid)
        if not all((credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation)):
            raise HTTPException(status_code=404, detail=f"Credit pack ticket with TXID {txid} not found or was invalid!")
        credit_pack_ticket = CreditPackTicketResponse(
            credit_pack_purchase_request_response=credit_pack_purchase_request_response,
            credit_pack_purchase_request_confirmation=credit_pack_purchase_request_confirmation
        )        
        return credit_pack_ticket
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"Error retrieving credit pack ticket from txid: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error retrieving credit pack ticket from txid: {str(e)}")
    
    
@router.post("/credit_purchase_initial_request", response_model=Union[db.CreditPackPurchaseRequestPreliminaryPriceQuote, db.CreditPackPurchaseRequestRejection])
async def credit_purchase_initial_request_endpoint(
    credit_pack_request: db.CreditPackPurchaseRequest = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            credit_pack_request.requesting_end_user_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        preliminary_price_quote = await service_functions.process_credit_purchase_initial_request(credit_pack_request)
        return preliminary_price_quote
    except Exception as e:
        logger.error(f"Error encountered with credit purchase initial request: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error encountered with credit purchase initial request: {str(e)}")


@router.post("/credit_purchase_preliminary_price_quote_response", response_model=Union[db.CreditPackPurchaseRequestResponse, db.CreditPackPurchaseRequestResponseTermination])
async def credit_purchase_preliminary_price_quote_response_endpoint(
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    preliminary_price_quote_response: db.CreditPackPurchaseRequestPreliminaryPriceQuoteResponse = Body(...),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            preliminary_price_quote_response.requesting_end_user_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        result = await service_functions.process_credit_purchase_preliminary_price_quote_response(preliminary_price_quote_response)
        if isinstance(result, db.CreditPackPurchaseRequestResponse):
            result_dict = result.model_dump()
            result_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in result_dict.items()}
            service_functions.log_action_with_payload("processed", "credit purchase preliminary price quote response", result_dict)
            return result
        elif isinstance(result, db.CreditPackPurchaseRequestResponseTermination):
            result_dict = result.model_dump()
            result_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in result_dict.items()}
            logger.warning(f"Credit purchase preliminary price quote response terminated: {result_dict}")
            return result
        else:
            raise HTTPException(status_code=500, detail="Unexpected response type")
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"Error processing credit purchase preliminary price quote response: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing credit purchase preliminary price quote response: {str(e)}")


@router.post("/credit_pack_price_agreement_request", response_model=db.CreditPackPurchasePriceAgreementRequestResponse)
async def credit_pack_price_agreement_request_endpoint(
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    credit_pack_price_agreement_request: db.CreditPackPurchasePriceAgreementRequest = Body(...),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            credit_pack_price_agreement_request.supernode_requesting_price_agreement_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        response = await service_functions.process_credit_pack_price_agreement_request(credit_pack_price_agreement_request)
        if isinstance(response, db.CreditPackPurchasePriceAgreementRequestResponse):
            response_dict = response.model_dump()
            response_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in response_dict.items()}
            service_functions.log_action_with_payload("processed", "credit pack price agreement request", response_dict)
            return response
        else:
            raise HTTPException(status_code=400, detail=response)
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"Error processing credit pack price agreement request: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing credit pack price agreement request: {str(e)}")
    

@router.post("/check_status_of_credit_purchase_request", response_model=db.CreditPackPurchaseRequestStatus)
async def check_status_of_credit_purchase_request_endpoint(
    credit_pack_request_status_check: db.CreditPackRequestStatusCheck = Body(...), 
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            credit_pack_request_status_check.requesting_end_user_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        status = await service_functions.get_credit_purchase_request_status(credit_pack_request_status_check)
        status_dict = status.model_dump()
        status_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in status_dict.items()}
        service_functions.log_action_with_payload("checking status of", "credit purchase request", status_dict)
        return status
    except Exception as e:
        logger.error(f"Error checking status of credit purchase request: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error checking status of credit purchase request: {str(e)}")


@router.post("/confirm_credit_purchase_request", response_model=db.CreditPackPurchaseRequestConfirmationResponse)
async def confirm_credit_purchase_request_endpoint(
    confirmation: db.CreditPackPurchaseRequestConfirmation = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            confirmation.requesting_end_user_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        response = await service_functions.process_credit_purchase_request_confirmation(confirmation)
        response_dict = response.model_dump()
        response_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in response_dict.items()}
        service_functions.log_action_with_payload("processed", "credit purchase request confirmation", response_dict)
        return response
    except Exception as e:
        logger.error(f"Error processing credit purchase request confirmation: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing credit purchase request confirmation: {str(e)}")


@router.post("/credit_pack_purchase_request_final_response_announcement")
async def credit_pack_purchase_request_final_response_announcement_endpoint(
    response: db.CreditPackPurchaseRequestResponse = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            response.responding_supernode_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        await service_functions.process_credit_pack_purchase_request_final_response_announcement(response)
        service_functions.log_action_with_payload("processed", "credit pack purchase request final response announcement", response)
        return {"message": "Announcement processed successfully"}
    except Exception as e:
        logger.error(f"Error processing credit pack purchase request final response announcement: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing credit pack purchase request final response announcement: {str(e)}")


@router.post("/credit_pack_purchase_completion_announcement")
async def credit_pack_purchase_completion_announcement_endpoint(
    confirmation: db.CreditPackPurchaseRequestConfirmation = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            confirmation.requesting_end_user_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        await service_functions.process_credit_pack_purchase_completion_announcement(confirmation)
        service_functions.log_action_with_payload("processed", "credit pack purchase completion announcement", confirmation)
        return {"message": "Announcement processed successfully"}
    except Exception as e:
        logger.error(f"Error processing credit pack purchase completion announcement: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing credit pack purchase completion announcement: {str(e)}")


@router.post("/credit_pack_storage_completion_announcement")
async def credit_pack_storage_completion_announcement_endpoint(
    storage_completion_announcement: db.CreditPackPurchaseRequestConfirmationResponse = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            storage_completion_announcement.responding_supernode_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        await service_functions.process_credit_pack_storage_completion_announcement(storage_completion_announcement)
        service_functions.log_action_with_payload("processed", "credit pack storage completion announcement", storage_completion_announcement)
        return {"message": "Announcement processed successfully"}
    except Exception as e:
        logger.error(f"Error processing credit pack storage completion announcement: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing credit pack storage completion announcement: {str(e)}")


@router.post("/credit_pack_storage_retry_request", response_model=db.CreditPackStorageRetryRequestResponse)
async def credit_pack_storage_retry_request_endpoint(
    request: db.CreditPackStorageRetryRequest = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            request.requesting_end_user_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        response = await service_functions.process_credit_pack_storage_retry_request(request)
        response_dict = response.model_dump()
        response_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in response_dict.items()}
        service_functions.log_action_with_payload("processed", "credit pack storage retry request", response_dict)
        return response
    except Exception as e:
        logger.error(f"Error processing credit pack storage retry request: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing credit pack storage retry request: {str(e)}")
    
    
@router.post("/credit_pack_storage_retry_completion_announcement")
async def credit_pack_storage_retry_completion_announcement_endpoint(
    response: db.CreditPackStorageRetryRequestResponse = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            response.responding_supernode_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        await service_functions.process_credit_pack_storage_retry_completion_announcement(response)
        service_functions.log_action_with_payload("processed", "credit pack storage retry completion announcement", json.dumps(response))
        return {"message": "Announcement processed successfully"}
    except Exception as e:
        logger.error(f"Error processing credit pack storage retry completion announcement: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error processing credit pack storage retry completion announcement: {str(e)}")


@router.post("/get_valid_credit_pack_tickets_for_pastelid", response_model=List[dict])
async def get_valid_credit_pack_tickets_for_pastelid_endpoint(
    pastelid: str = Body(..., description="The PastelID to retrieve credit pack tickets for"),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(pastelid, challenge_signature, challenge_id)
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        valid_tickets = await service_functions.get_valid_credit_pack_tickets_for_pastelid(pastelid)
        return valid_tickets
    except Exception as e:
        logger.error(f"Error retrieving valid credit pack tickets for PastelID {pastelid}: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error retrieving valid credit pack tickets: {str(e)}")
    
    
@router.post("/check_credit_pack_balance", response_model=Dict[str, Any])
async def check_credit_pack_balance_endpoint(
    credit_pack_ticket_txid: str = Body(..., description="The transaction ID of the credit pack ticket"),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(credit_pack_ticket_txid, challenge_signature, challenge_id)
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        current_credit_balance, number_of_transactions = await service_functions.determine_current_credit_pack_balance_based_on_tracking_transactions_new(credit_pack_ticket_txid)
        balance_info = {
            "current_credit_balance": current_credit_balance,
            "number_of_confirmation_transactions": number_of_transactions
        }
        logger.info(f"Checked credit pack balance for txid {credit_pack_ticket_txid}: {balance_info}")
        return balance_info
    except Exception as e:
        logger.error(f"Error checking credit pack balance for txid {credit_pack_ticket_txid}: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error checking credit pack balance: {str(e)}")
    
    
@router.post("/retrieve_credit_pack_ticket_from_purchase_burn_txid", response_model=Dict[str, Any])
async def retrieve_credit_pack_ticket_endpoint(
    purchase_burn_txid: str = Body(..., description="The transaction ID of the credit pack purchase burn"),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(purchase_burn_txid, challenge_signature, challenge_id)
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation = await service_functions.retrieve_credit_pack_ticket_from_purchase_burn_txid(purchase_burn_txid)
        if credit_pack_purchase_request is None or credit_pack_purchase_request_response is None or credit_pack_purchase_request_confirmation is None:
            raise HTTPException(status_code=404, detail="Credit pack ticket not found")
        ticket_info = {
            "credit_pack_purchase_request": credit_pack_purchase_request.model_dump(),
            "credit_pack_purchase_request_response": credit_pack_purchase_request_response.model_dump(),
            "credit_pack_purchase_request_confirmation": credit_pack_purchase_request_confirmation.model_dump()
        }
        logger.info(f"Retrieved credit pack ticket for purchase burn txid {purchase_burn_txid}: {ticket_info}")
        return ticket_info
    except Exception as e:
        logger.error(f"Error retrieving credit pack ticket for purchase burn txid {purchase_burn_txid}: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error retrieving credit pack ticket: {str(e)}")
    
    
@router.post("/get_final_credit_pack_registration_txid_from_credit_purchase_burn_txid", response_model=Dict[str, Any])
async def get_final_credit_pack_registration_txid_endpoint(
    purchase_burn_txid: str = Body(..., description="The transaction ID of the credit pack purchase burn"),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(purchase_burn_txid, challenge_signature, challenge_id)
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        sha3_256_hash_of_credit_pack_purchase_request_fields = await service_functions.get_final_credit_pack_registration_txid_from_credit_purchase_burn_txid(purchase_burn_txid)
        if sha3_256_hash_of_credit_pack_purchase_request_fields is None:
            raise HTTPException(status_code=404, detail="Credit pack ticket not found")
        logger.info(f"Retrieved final credit pack registration txid for purchase burn txid {purchase_burn_txid}: {sha3_256_hash_of_credit_pack_purchase_request_fields}")
        return {"final_credit_pack_registration_txid": sha3_256_hash_of_credit_pack_purchase_request_fields}
    except Exception as e:
        logger.error(f"Error retrieving final credit pack registration txid for purchase burn txid {purchase_burn_txid}: {str(e)}")
        traceback.print_exc()        
        raise HTTPException(status_code=500, detail=f"Error retrieving final credit pack registration txid: {str(e)}")

    
#__________________________________________________________________________________________________________


@router.post("/make_inference_api_usage_request", response_model=db.InferenceAPIUsageResponse)
async def make_inference_api_usage_request_endpoint(
    inference_api_usage_request: db.InferenceAPIUsageRequest = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            inference_api_usage_request.requesting_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        # Validate and process the inference API usage request
        inference_response = await service_functions.process_inference_api_usage_request(inference_api_usage_request)
        inference_request_dict = inference_api_usage_request.model_dump()
        inference_request_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in inference_request_dict.items()}
        # Abbreviate the 'model_input_data_json_b64' field to the first 32 characters
        inference_request_dict['model_input_data_json_b64'] = inference_request_dict['model_input_data_json_b64'][:32]        
        inference_response_dict = inference_response.model_dump()
        inference_response_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in inference_response_dict.items()}
        combined_message_dict = {**inference_request_dict, **inference_response_dict}
        # Broadcast message to nearest SNs to requester's pastelid containing inference request/response message 
        response_message_body = json.dumps(combined_message_dict)
        response_message_type = "inference_request_response_announcement_message"
        _ = await service_functions.broadcast_message_to_n_closest_supernodes_to_given_pastelid(inference_api_usage_request.requesting_pastelid, response_message_body, response_message_type) 
        # Return the InferenceAPIUsageResponse as the API response
        return inference_response
    except Exception as e:
        logger.error(f"Error encountered with inference API usage request: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error encountered with inference API usage request: {str(e)}")
    

@router.post("/confirm_inference_request", response_model=db.InferenceConfirmation)
async def confirm_inference_request_endpoint(
    inference_confirmation: db.InferenceConfirmation = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature_from_inference_request_id(
            inference_confirmation.inference_request_id, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        # Process the inference confirmation
        is_processed = await service_functions.process_inference_confirmation(
            inference_confirmation.inference_request_id, inference_confirmation
        )
        if is_processed:
            logger.info(f"Inference request {inference_confirmation.inference_request_id} confirmed successfully")
        else:
            logger.error(f"Error confirming inference request {inference_confirmation.inference_request_id}")
        return inference_confirmation
    except Exception as e:
        logger.error(f"Error sending inference confirmation: {str(e)}")
        traceback.print_exc()       
        raise HTTPException(status_code=500, detail=f"Error sending inference confirmation: {str(e)}")


@router.get("/check_status_of_inference_request_results/{inference_response_id}")
async def check_status_of_inference_request_results_endpoint(inference_response_id: str):
    try:
        request_result_is_available = await service_functions.check_status_of_inference_request_results(inference_response_id)
        if request_result_is_available is None:
            raise HTTPException(status_code=404, detail="Inference request result not found")
        return request_result_is_available
    except ValueError as ve:
        logger.error(f"Validation error: {str(ve)}")
        raise HTTPException(status_code=400, detail=f"Invalid request: {str(ve)}")
    except Exception as e:
        logger.error(f"Error checking status of inference request results: {str(e)}")
        traceback.print_exc()        
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@router.post("/retrieve_inference_output_results", response_model=db.InferenceAPIOutputResult)
async def retrieve_inference_output_results_endpoint(
    inference_response_id: str = Query(..., description="The ResponseID Associated with the Inference Request"),
    pastelid: str = Query(..., description="The PastelID of the requesting party"),
    challenge: str = Query(..., description="The challenge string"),
    challenge_id: str = Query(..., description="The ID of the challenge string"),
    challenge_signature: str = Query(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(pastelid, challenge_signature, challenge_id)
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        inference_output_results = await service_functions.get_inference_output_results_and_verify_authorization(inference_response_id, pastelid)
        # Broadcast message to nearest SNs to requester's pastelid containing inference results
        inference_output_results_dict = inference_output_results.model_dump()
        inference_output_results_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in inference_output_results_dict.items()}
        # Retrieve the inference API usage request from the database
        inference_usage_request_object = await service_functions.get_inference_api_usage_request_for_audit(inference_output_results_dict['inference_request_id'])
        inference_usage_request_dict = inference_usage_request_object.model_dump()
        inference_usage_request_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in inference_usage_request_dict.items()}
        # Add model_parameters_json_b64 and other fields to the inference output results dict:
        inference_output_results_dict['model_parameters_json_b64'] = inference_usage_request_dict['model_parameters_json_b64']
        inference_output_results_dict['requested_model_canonical_string'] = inference_usage_request_dict['requested_model_canonical_string']
        inference_output_results_dict['model_inference_type_string'] = inference_usage_request_dict['model_inference_type_string']
        # Abbreviate the 'inference_result_json_base64' field to the first 32 characters
        inference_output_results_dict['inference_result_json_base64'] = inference_output_results_dict['inference_result_json_base64'][:32]
        result_message_body = json.dumps(inference_output_results_dict)
        result_message_type = "inference_request_result_announcement_message"
        _ = await service_functions.broadcast_message_to_n_closest_supernodes_to_given_pastelid(pastelid, result_message_body, result_message_type) 
        return inference_output_results
    except Exception as e:
        logger.error(f"Error retrieving inference output results: {str(e)}")
        traceback.print_exc()        
        raise HTTPException(status_code=500, detail=f"Error retrieving inference output results: {str(e)}")


@router.post("/audit_inference_request_response", response_model=db.InferenceAPIUsageResponse)
async def audit_inference_request_response_endpoint(
    inference_response_id: str = Body(..., description="The inference response ID"),
    pastel_id: str = Body(..., description="The PastelID of the requester"),
    signature: str = Body(..., description="The signature of the PastelID on the inference_response_id"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        # Retrieve the InferenceAPIOutputResult from the local database
        api_usage_response = await service_functions.get_inference_api_usage_response_for_audit(inference_response_id)
        if api_usage_response is None:
            raise HTTPException(status_code=404, detail="Inference result not found")
        # Verify the signature
        is_valid_signature = await service_functions.verify_message_with_pastelid_func(pastel_id, inference_response_id, signature)
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        api_usage_request = await service_functions.get_inference_api_usage_request_for_audit(api_usage_response.inference_request_id)
        # Verify that the PastelID matches the one in the response
        if api_usage_request.requesting_pastelid != pastel_id:
            raise HTTPException(status_code=403, detail="PastelID does not match the one in the inference request")
        # Return the InferenceAPIOutputResult as the API response
        return api_usage_response
    except Exception as e:
        logger.error(f"Error auditing inference request result: {str(e)}")
        traceback.print_exc()        
        raise HTTPException(status_code=500, detail=f"Error auditing inference request result: {str(e)}")
    
    
@router.post("/audit_inference_request_result", response_model=db.InferenceAPIOutputResult)
async def audit_inference_request_result_endpoint(
    inference_response_id: str = Body(..., description="The inference response ID"),
    pastel_id: str = Body(..., description="The PastelID of the requester"),
    signature: str = Body(..., description="The signature of the PastelID on the inference_response_id"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        # Retrieve the InferenceAPIOutputResult from the local database
        api_usage_result = await service_functions.get_inference_api_usage_result_for_audit(inference_response_id)
        if api_usage_result is None:
            raise HTTPException(status_code=404, detail="Inference result not found")
        # Verify the signature
        is_valid_signature = await service_functions.verify_message_with_pastelid_func(pastel_id, inference_response_id, signature)
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        api_usage_request = await service_functions.get_inference_api_usage_request_for_audit(api_usage_result.inference_request_id)
        # Verify that the PastelID matches the one in the response
        if api_usage_request.requesting_pastelid != pastel_id:
            raise HTTPException(status_code=403, detail="PastelID does not match the one in the inference request")
        # Return the InferenceAPIOutputResult as the API response
        return api_usage_result
    except Exception as e:
        logger.error(f"Error auditing inference request result: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error auditing inference request result: {str(e)}")
    
    
@router.get("/get_inference_model_menu")
async def get_inference_model_menu_endpoint(
    rpc_connection=Depends(get_rpc_connection),
):
    model_menu = await service_functions.get_inference_model_menu()
    return model_menu


@router.get("/favicon.ico", include_in_schema=False)
async def favicon():
    return FileResponse("favicon.ico")


@router.get("/download/{file_name}")
async def download_file(file_name: str):
    file_location = os.path.join(tempfile.gettempdir(), file_name)
    if file_location in service_functions.file_store and service_functions.file_store[file_location] > datetime.utcnow():
        return FileResponse(file_location, background=BackgroundTask(service_functions.remove_file, file_location))
    else:
        service_functions.remove_file(file_location)
        raise HTTPException(status_code=404, detail="File not found or expired")


@router.post("/update_inference_sn_reputation_score")
async def update_inference_sn_reputation_score_endpoint(
    reputation_score_data: db.ReputationScoreUpdate,
    rpc_connection=Depends(get_rpc_connection),
):
    is_updated = await service_functions.update_inference_sn_reputation_score(reputation_score_data.supernode_pastelid, reputation_score_data.reputation_score)
    return {"is_updated": is_updated}


@router.get("/show_logs/{minutes}", response_class=HTMLResponse)
async def show_logs(minutes: int = 5):
    logger.info("Reading logs for the last %d minutes", minutes)
    # Read the entire log file and generate HTML with logs up to `minutes` minutes from now
    with open("pastel_supernode_inference_layer.log", "r") as f:
        lines = f.readlines()
    logs = []
    now = datetime.now(timezone.utc)  # Get current time, make it timezone-aware
    for line in lines:
        if line.strip() == "":
            continue
        if line[0].isdigit():
            try:  # Try to parse the datetime
                log_datetime_str = line.split(" - ")[0]  # Assuming the datetime is at the start of each line
                log_datetime = datetime.strptime(log_datetime_str, "%Y-%m-%d %H:%M:%S,%f")  # Parse the datetime string to a datetime object
                log_datetime = log_datetime.replace(tzinfo=timezone.utc)  # Set the datetime object timezone to UTC to match `now`
                if now - log_datetime <= timedelta(minutes=minutes):  # If the log is within `minutes` minutes from now
                    logs.append(service_functions.highlight_rules_func(line.rstrip('\n')))  # Add the highlighted log to the list and strip any newline at the end
            except ValueError:
                pass  # If the line does not start with a datetime, ignore the ValueError and process the line anyway
    logs_as_string = "<br>".join(logs)  # Joining with <br> directly
    logs_as_string_newlines_rendered = logs_as_string.replace("\n", "<br>")
    logs_as_string_newlines_rendered_font_specified = f"""
    <html>
    <head>
    <link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" rel="stylesheet">
    <script>
    var logContainer;
    var lastLogs = `{logs_as_string_newlines_rendered}`;
    var shouldScroll = true;
    var userScroll = false;
    var lastPosition = 0;
    var minutes = {minutes};
    function fetchLogs() {{
        if (typeof minutes !== 'undefined' && typeof lastPosition !== 'undefined') {{
            fetch('/show_logs_incremental/' + minutes + '/' + lastPosition)
            .then(response => response.json())
            .then(data => {{
                if (logContainer) {{
                    var div = document.createElement('div');
                    div.innerHTML = data.logs;
                    if (div.innerHTML) {{
                        lastLogs += div.innerHTML;
                        lastPosition = data.last_position;
                    }}
                    logContainer.innerHTML = lastLogs;
                    if (shouldScroll) {{
                        logContainer.scrollTop = logContainer.scrollHeight;
                    }}
                }}
            }});
        }}
    }}
    function checkScroll() {{
        if(logContainer.scrollTop + logContainer.clientHeight < logContainer.scrollHeight) {{
            userScroll = true;
            shouldScroll = false;
        }} else {{
            userScroll = false;
        }}
        if (!userScroll) {{
            setTimeout(function(){{ shouldScroll = true; }}, 10000);
        }}
    }}
    window.onload = function() {{
        let p = document.getElementsByTagName('p');
        for(let i = 0; i < p.length; i++) {{
            let color = window.getComputedStyle(p[i]).getPropertyValue('color');
            p[i].style.textShadow = `0 0 5px ${{color}}, 0 0 10px ${{color}}, 0 0 15px ${{color}}, 0 0 20px ${{color}}`;
        }}
        document.querySelector('#copy-button').addEventListener('click', function() {{
            var text = document.querySelector('#log-container').innerText;
            navigator.clipboard.writeText(text).then(function() {{
                console.log('Copying to clipboard was successful!');
            }}, function(err) {{
                console.error('Could not copy text: ', err);
            }});
        }});
        document.querySelector('#download-button').addEventListener('click', function() {{
            var text = document.querySelector('#log-container').innerText;
            var element = document.createElement('a');
            element.setAttribute('href', 'data:text/plain;charset=utf-8,' + encodeURIComponent(text));
            element.setAttribute('download', 'pastel_supernode_inference_layer_log__' + new Date().toISOString() + '.txt');
            element.style.display = 'none';
            document.body.appendChild(element);
            element.click();
            document.body.removeChild(element);
        }});
    }}
    document.addEventListener('DOMContentLoaded', (event) => {{
        logContainer = document.getElementById('log-container');
        logContainer.innerHTML = lastLogs;
        logContainer.addEventListener('scroll', checkScroll);
        fetchLogs();
        setInterval(fetchLogs, 1000);  // Fetch the logs every 1 second
    }});
    </script>
    </head>        
    <style>
    .log-container {{
        scroll-behavior: smooth;
        background-color: #2b2b2b; /* dark gray */
        color: #d3d3d3; /* light gray */
        background-image: linear-gradient(rgba(0,0,0,0.1) 1px, transparent 1px), linear-gradient(90deg, rgba(0,0,0,0.1) 1px, transparent 1px);
        background-size: 100% 10px, 10px 100%;
        background-position: 0 0, 0 0;
        animation: scan 1s linear infinite;
        @keyframes scan {{
            0% {{
                background-position: 0 0, 0 0;
            }}
            100% {{
                background-position: -10px -10px, -10px -10px;
            }}
        }}
        font-size: 14px;
        font-family: monospace;
        padding: 10px;
        height: 100vh;
        margin: 0;
        box-sizing: border-box;
        overflow: auto;
    }}
    .icon-button {{
        position: fixed;
        right: 10px;
        margin: 10px;
        background-color: #555;
        color: white;
        border: none;
        cursor: pointer;
        border-radius: 50%;
        width: 60px;
        height: 60px;
        font-size: 30px;
        display: flex;
        align-items: center;
        justify-content: center;
        text-decoration: none;
    }}
    #copy-button {{
        bottom: 80px;  // Adjust this value as needed
    }}
    #download-button {{
        bottom: 10px;
    }}
    </style>
    <body>
    <pre id="log-container" class="log-container"></pre>
    <button id="copy-button" class="icon-button"><i class="fas fa-copy"></i></button>
    <button id="download-button" class="icon-button"><i class="fas fa-download"></i></button>
    </body>
    </html>"""
    return HTMLResponse(content=logs_as_string_newlines_rendered_font_specified)


@router.get("/show_logs_incremental/{minutes}/{last_position}")
def show_logs_incremental(minutes: int, last_position: int):
    new_logs = []
    now = datetime.now(timezone.utc)  # Get current time, make it timezone-aware
    try:
        with open("pastel_supernode_inference_layer.log", "r") as f:
            f.seek(last_position)  # Seek to `last_position`
            while True:
                line = f.readline()
                if line == "":  # If EOF
                    break
                if line.strip() == "":
                    continue
                try:  # Try to parse the datetime
                    log_datetime_str = line.split(" - ")[0]  # Assuming the datetime is at the start of each line
                    log_datetime = datetime.strptime(log_datetime_str, "%Y-%m-%d %H:%M:%S,%f")  # Parse the datetime string to a datetime object
                    log_datetime = log_datetime.replace(tzinfo=timezone.utc)  # Set the datetime object timezone to UTC to match `now`
                    if now - log_datetime > timedelta(minutes=minutes):  # If the log is older than `minutes` minutes from now
                        continue  # Ignore the log and continue with the next line
                except ValueError:
                    pass  # If the line does not start with a datetime, ignore the ValueError and process the line anyway
                new_logs.append(service_functions.highlight_rules_func(line.rstrip('\n')))  # Add the highlighted log to the list and strip any newline at the end
            last_position = f.tell()  # Get the last position
        new_logs_as_string = "<br>".join(new_logs)  # Joining with <br> directly
    except FileNotFoundError:
        new_logs_as_string = ""
        last_position = 0
    return {"logs": new_logs_as_string, "last_position": last_position}  # Also return the last position


@router.get("/show_logs", response_class=HTMLResponse)
async def show_logs_default():
    return await show_logs(5)


async def read_performance_data():
    retries = 3
    for _ in range(retries):
        try:
            if pickle_file_path.exists():
                with open(pickle_file_path, 'rb') as f:
                    performance_data_history = pickle.load(f)
                # Replace pd.NA with None to avoid serialization issues
                for df in performance_data_history.values():
                    df.replace({pd.NA: None}, inplace=True)
                return performance_data_history
        except (EOFError, pickle.UnpicklingError) as e:
            logger.error(f"Error reading pickle file: {e}", exc_info=True)
            await asyncio.sleep(2)
    raise HTTPException(status_code=500, detail="Could not read performance data.")


@router.post("/clear_out_known_bad_credit_pack_table")
async def clear_out_known_bad_credit_pack_table_endpoint(password: str = Depends(verify_password)):
    try:
        await service_functions.clear_out_all_credit_packs_from_known_bad_table()
        return {"message": "Successfully cleared all credit packs from the known bad table."}
    except Exception as e:
        logger.error(f"Error clearing known bad credit pack table: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error clearing known bad credit pack table: {str(e)}")


@router.get("/get_supernode_inference_server_benchmark_plots")
async def get_supernode_inference_server_benchmark_plots(password: str = Depends(verify_password)):
    performance_data_history = await read_performance_data()
    if not performance_data_history:
        raise HTTPException(status_code=404, detail="No performance data available.")
    data_frames = []
    for timestamp, df in performance_data_history.items():
        df['Timestamp'] = timestamp
        df['Smoothed Performance Ratio'] = df['Performance Ratio'].rolling(window=20, min_periods=5).mean()
        data_frames.append(df)
    if not data_frames:
        raise HTTPException(status_code=404, detail="No data available for plotting.")
    combined_df = pd.concat(data_frames)
    non_summary_df = combined_df[~combined_df['IP Address'].isin(['Min', 'Average', 'Median', 'Max'])]
    summary_df = combined_df[combined_df['IP Address'].isin(['Min', 'Average', 'Median', 'Max'])]

    # Generate the main plot with smoothed line charts for each supernode
    fig_main = px.line(non_summary_df, x='Timestamp', y='Smoothed Performance Ratio', color='IP Address',
                       title="Supernode Inference Server Benchmark Performance",
                       labels={'Smoothed Performance Ratio': 'Performance Ratio', 'Timestamp': 'Timestamp'},
                       template='plotly_white')

    # Set all traces except the first one to be initially invisible
    for i, trace in enumerate(fig_main.data):
        trace.visible = True if i == 0 else "legendonly"

    fig_main.update_layout(
        font=dict(family="Montserrat", size=14, color="black"),
        title=dict(font=dict(size=20)),
        xaxis=dict(showgrid=True, gridcolor='LightGray'),
        yaxis=dict(showgrid=True, gridcolor='LightGray'),
        height=750,
        hovermode="closest",
        hoverdistance=1000,
        spikedistance=1000,
        updatemenus=[
            dict(
                type="buttons",
                showactive=False,
                buttons=[
                    dict(
                        label="Reset",
                        method="relayout",
                        args=[{"updatemenus": [{"visible": False}]}]
                    )
                ]
            )
        ]
    )
    fig_main.update_traces(
        hovertemplate='<b>IP Address</b>: %{customdata}<br><b>Performance Ratio</b>: %{y:.2f}<br><b>Timestamp</b>: %{x}<extra></extra>',
        customdata=non_summary_df['IP Address']
    )

    # Generate the summary plot (unchanged)
    fig_summary = px.line(summary_df, x='Timestamp', y='Performance Ratio', color='IP Address', markers=True,
                          title="Summary Statistics (Min, Average, Median, Max)",
                          labels={'Performance Ratio': 'Performance Ratio', 'Timestamp': 'Timestamp'},
                          template='plotly_white')
    fig_summary.update_layout(
        font=dict(family="Montserrat", size=14, color="black"),
        title=dict(font=dict(size=20)),
        xaxis=dict(showgrid=True, gridcolor='LightGray'),
        yaxis=dict(showgrid=True, gridcolor='LightGray'),
        height=600
    )
    fig_summary.update_traces(
        hovertemplate='<b>Statistic</b>: %{customdata}<br><b>Performance Ratio</b>: %{y:.2f}<br><b>Timestamp</b>: %{x}<extra></extra>',
        customdata=summary_df['IP Address']
    )

    main_plot_html = fig_main.to_html(full_html=False, include_plotlyjs='cdn')
    summary_plot_html = fig_summary.to_html(full_html=False, include_plotlyjs=False)

    most_recent_df = non_summary_df.sort_values('Timestamp').groupby('IP Address').tail(1)
    table_html = most_recent_df.to_html(classes='display nowrap', index=False)

    html_content = f"""
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Supernode Inference Server Benchmark Performance</title>
        <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;500&display=swap" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.10.21/css/jquery.dataTables.css">
        <style>
            body {{
                font-family: 'Montserrat', sans-serif;
                background-color: #f5f5f5;
                color: #333;
                margin: 0;
                padding: 20px;
            }}
            h1 {{
                text-align: center;
                font-size: 2.5em;
                margin-bottom: 20px;
            }}
            .plot-container, .table-container {{
                width: 95%;
                margin: 0 auto;
                max-width: 1400px;
            }}
            hr {{
                margin: 40px 0;
            }}
            table.dataTable {{
                width: 100% !important;
                border-collapse: collapse;
            }}
            table.dataTable thead th {{
                background-color: #f2f2f2;
                text-align: left;
                padding: 8px;
            }}
            table.dataTable tbody tr:nth-child(even) {{
                background-color: #f9f9f9;
            }}
            table.dataTable tbody td {{
                padding: 8px;
            }}
            #toggleAllBtn {{
                margin: 10px 0;
                padding: 10px;
                background-color: #4CAF50;
                color: white;
                border: none;
                cursor: pointer;
            }}
        </style>
        <script src="https://code.jquery.com/jquery-3.5.1.js"></script>
        <script type="text/javascript" charset="utf8" src="https://cdn.datatables.net/1.10.21/js/jquery.dataTables.js"></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    </head>
    <body>
        <h1>Supernode Inference Server Benchmark Performance</h1>
        <div class="plot-container" id="main-plot">
            {main_plot_html}
        </div>
        <button id="toggleAllBtn">Toggle All Series</button>
        <hr>
        <div class="plot-container" id="summary-plot">
            {summary_plot_html}
        </div>
        <hr>
        <h2 style="text-align:center;">Benchmark Data Table</h2>
        <div class="table-container">
            {table_html}
        </div>
        <script>
            $(document).ready(function() {{
                $('table.display').DataTable({{
                    scrollX: true
                }});

                function setupPlotlyHoverEffects() {{
                    var mainPlotDiv = document.getElementById('main-plot');
                    if (!mainPlotDiv) {{
                        console.error('Main plot div not found');
                        return;
                    }}

                    var plotlyInstance = mainPlotDiv.getElementsByClassName('js-plotly-plot')[0];
                    if (!plotlyInstance) {{
                        console.error('Plotly instance not found');
                        return;
                    }}

                    var plotData = plotlyInstance.data;
                    if (!plotData || !Array.isArray(plotData)) {{
                        console.error('Plot data is not available or is not an array');
                        return;
                    }}

                    var originalColors = plotData.map(trace => trace.line ? trace.line.color : null);

                    plotlyInstance.on('plotly_hover', function(data) {{
                        if (!data.points || data.points.length === 0) return;
                        
                        var curveNumber = data.points[0].curveNumber;
                        var update = {{
                            'line.width': plotData.map((_, i) => i === curveNumber ? 4 : 1),
                            'line.color': plotData.map((_, i) => i === curveNumber ? 'black' : originalColors[i])
                        }};
                        Plotly.restyle(plotlyInstance, update);
                    }});

                    plotlyInstance.on('plotly_unhover', function() {{
                        var update = {{
                            'line.width': plotData.map(() => 1),
                            'line.color': originalColors
                        }};
                        Plotly.restyle(plotlyInstance, update);
                    }});

                    // Toggle all series visibility
                    $('#toggleAllBtn').click(function() {{
                        var newVisibility = plotData.map(trace => !trace.visible);
                        Plotly.restyle(plotlyInstance, {{'visible': newVisibility}});
                    }});
                }}

                // Wait for Plotly to be fully loaded
                var checkPlotlyReady = setInterval(function() {{
                    if (window.Plotly && document.getElementById('main-plot').getElementsByClassName('js-plotly-plot')[0]) {{
                        clearInterval(checkPlotlyReady);
                        setupPlotlyHoverEffects();
                    }}
                }}, 100);
            }});
        </script>
    </body>
    </html>
    """
    return HTMLResponse(content=html_content)

---
./service_functions.py
---
import asyncio
import base64
import decimal
import hashlib
import ipaddress
import json
import os
import io
import platform
import statistics
import time
import csv
import uuid
import socket
import subprocess
import random
import re
import sys
import traceback
import html
import tempfile
import warnings
import pickle
import pytz
from collections import defaultdict
from collections.abc import Iterable
from functools import wraps
from cachetools import TTLCache
from diskcache import Cache
from pathlib import Path
from urllib.parse import quote_plus, unquote_plus
from datetime import datetime, timedelta, date, timezone
import pandas as pd
import httpx
from httpx import Timeout
from urllib.parse import urlparse
from logger_config import logger
import zstandard as zstd
from sqlalchemy.exc import OperationalError, InvalidRequestError
from typing import List, Tuple, Dict, Union, Optional
from decouple import Config as DecoupleConfig, RepositoryEnv
from magika import Magika
import tiktoken
import anthropic
from groq import AsyncGroq
from mistralai.async_client import MistralAsyncClient
from mistralai.models.chat_completion import ChatMessage
from cryptography.fernet import Fernet
from fuzzywuzzy import process
from transformers import AutoTokenizer, GPT2TokenizerFast, WhisperTokenizer
import database_code as db_code
from sqlmodel import select, delete, func, SQLModel
from sqlalchemy.exc import IntegrityError
from sshtunnel import SSHTunnelForwarder, BaseSSHTunnelForwarderError
from mutagen import File as MutagenFile
from PIL import Image
import libpastelid

tracking_period_start = datetime.utcnow()
rpc_call_stats = defaultdict(lambda: {
    "count": 0,
    "cumulative_time": 0.0,
    "average_time": 0.0,
    "success_count": 0,
    "total_response_size": 0,
    "average_response_size": 0.0,
    "timeout_errors": 0,
    "connection_errors": 0,
    "other_errors": 0
})

pastel_keys_dir = os.path.expanduser("/home/ubuntu/.pastel/pastelkeys")
pastel_signer = libpastelid.PastelSigner(pastel_keys_dir)

encryption_key = None
magika = Magika()

SENSITIVE_ENV_FIELDS = ["LOCAL_PASTEL_ID_PASSPHRASE", "SWISS_ARMY_LLAMA_SECURITY_TOKEN", "OPENAI_API_KEY", "CLAUDE3_API_KEY", "GROQ_API_KEY", "MISTRAL_API_KEY", "STABILITY_API_KEY", "OPENROUTER_API_KEY"]
LOCAL_PASTEL_ID_PASSPHRASE = None
SWISS_ARMY_LLAMA_SECURITY_TOKEN = None
OPENAI_API_KEY = None
CLAUDE3_API_KEY = None
GROQ_API_KEY = None
MISTRAL_API_KEY = None
STABILITY_API_KEY = None
OPENROUTER_API_KEY = None

def get_local_ip():
    hostname = socket.gethostname()
    return socket.gethostbyname(hostname)

def get_env_value(key):
    current_dir = os.path.dirname(os.path.abspath(__file__))
    env_file_path = os.path.join(current_dir, '.env')    
    try:
        with open(env_file_path, 'r') as env_file:
            for line in env_file:
                if line.startswith(key + '='):
                    return line.split('=', 1)[1].strip() # Split on the first '=' to allow for '=' in the value
    except FileNotFoundError:
        print(f"Error: .env file at {env_file_path} not found.")
    return None

def generate_or_load_encryption_key_sync():
    key_file_path = os.path.expanduser('~/env_encryption_key_for_supernode_inference_app')
    key = None
    if os.path.exists(key_file_path): # Check if key file exists and load it
        with open(key_file_path, 'rb') as key_file:
            key = key_file.read()
        try:
            Fernet(key)  # Validate the key
            loaded_or_generated = "loaded"
        except ValueError:
            key = None
    if key is None: # If key is invalid or doesn't exist, generate a new one
        logger.info("Invalid or no encryption key found. Generating a new one.")
        loaded_or_generated = "generated"
        key = Fernet.generate_key()
        with open(key_file_path, 'wb') as key_file:
            key_file.write(key)
        print(f"Generated new encryption key for sensitive env fields: {key}")
        encrypt_sensitive_fields(key)  # Encrypt sensitive fields if generating key for the first time
    logger.info(f"Encryption key {loaded_or_generated} successfully.")        
    return key

def encrypt_sensitive_fields(key):
    current_dir = os.path.dirname(os.path.abspath(__file__))
    env_file_path = os.path.join(current_dir, '.env')    
    cipher_suite = Fernet(key)
    with open(env_file_path, 'r') as file: # Load existing .env file
        lines = file.readlines()
    updated_lines = [] # Encrypt and update sensitive fields
    for line in lines:
        if any(field in line for field in SENSITIVE_ENV_FIELDS):
            for field in SENSITIVE_ENV_FIELDS:
                if line.startswith(field):
                    value = line.strip().split('=')[1]
                    encrypted_data = cipher_suite.encrypt(value.encode()).decode()
                    url_encoded_encrypted_data = quote_plus(encrypted_data)
                    line = f"{field}={url_encoded_encrypted_data}\n"
                    print(f"Encrypted {field}: {url_encoded_encrypted_data}")
                    break
        updated_lines.append(line)
    with open(env_file_path, 'w') as file: # Write the updated lines back to the .env file
        file.writelines(updated_lines)
    logger.info(f"Updated {len(SENSITIVE_ENV_FIELDS)} sensitive fields in .env file with encrypted values!")

def decrypt_sensitive_data(url_encoded_encrypted_data, encryption_key):
    cipher_suite = Fernet(encryption_key)
    encrypted_data = unquote_plus(url_encoded_encrypted_data)  # URL-decode first
    decrypted_data = cipher_suite.decrypt(encrypted_data.encode()).decode()  # Ensure this is a bytes-like object
    return decrypted_data

def encrypt_sensitive_data(data, encryption_key):
    cipher_suite = Fernet(encryption_key)
    encrypted_data = cipher_suite.encrypt(data.encode()).decode()
    url_encoded_encrypted_data = quote_plus(encrypted_data)
    return url_encoded_encrypted_data

def decrypt_sensitive_fields():
    global LOCAL_PASTEL_ID_PASSPHRASE, SWISS_ARMY_LLAMA_SECURITY_TOKEN, OPENAI_API_KEY, CLAUDE3_API_KEY, GROQ_API_KEY, MISTRAL_API_KEY, STABILITY_API_KEY, OPENROUTER_API_KEY, encryption_key
    LOCAL_PASTEL_ID_PASSPHRASE = decrypt_sensitive_data(get_env_value("LOCAL_PASTEL_ID_PASSPHRASE"), encryption_key)
    SWISS_ARMY_LLAMA_SECURITY_TOKEN = decrypt_sensitive_data(get_env_value("SWISS_ARMY_LLAMA_SECURITY_TOKEN"), encryption_key)
    OPENAI_API_KEY = decrypt_sensitive_data(get_env_value("OPENAI_API_KEY"), encryption_key)
    CLAUDE3_API_KEY = decrypt_sensitive_data(get_env_value("CLAUDE3_API_KEY"), encryption_key)
    GROQ_API_KEY = decrypt_sensitive_data(get_env_value("GROQ_API_KEY"), encryption_key)
    MISTRAL_API_KEY = decrypt_sensitive_data(get_env_value("MISTRAL_API_KEY"), encryption_key)
    STABILITY_API_KEY = decrypt_sensitive_data(get_env_value("STABILITY_API_KEY"), encryption_key)
    OPENROUTER_API_KEY = decrypt_sensitive_data(get_env_value("OPENROUTER_API_KEY"), encryption_key)
        
number_of_cpus = os.cpu_count()
my_os = platform.system()
loop = asyncio.get_event_loop()
warnings.filterwarnings('ignore')
local_ip = get_local_ip()
benchmark_results_cache = [] # Global cache to store benchmark results in memory
performance_data_df = pd.DataFrame(columns=['IP Address', 'Performance Ratio', 'Actual Score', 'Seconds Since Last Updated'])
performance_data_history = {}
local_benchmark_csv_file_path = Path('local_sn_micro_benchmark_results.csv')
pickle_file_path = Path('performance_data_history.pkl')
use_libpastelid_for_pastelid_sign_verify = 1

# Configuration for diskcache
CACHE_DIR = './local_credit_pack_cache'
CREDIT_BALANCE_CACHE_INVALIDATION_PERIOD_IN_SECONDS = 5 * 60  # 5 minutes

# Initialize the cache
credit_pack_cache = Cache(CACHE_DIR)

use_purge_all_caches = 0
if use_purge_all_caches:
    logger.info("Purging all caches...")
    credit_pack_cache.clear()
    
config = DecoupleConfig(RepositoryEnv('.env'))
TEMP_OVERRIDE_LOCALHOST_ONLY = config.get("TEMP_OVERRIDE_LOCALHOST_ONLY", default=0, cast=int)
NUMBER_OF_DAYS_BEFORE_MESSAGES_ARE_CONSIDERED_OBSOLETE = config.get("NUMBER_OF_DAYS_BEFORE_MESSAGES_ARE_CONSIDERED_OBSOLETE", default=3, cast=int)
GITHUB_MODEL_MENU_URL = config.get("GITHUB_MODEL_MENU_URL")
CHALLENGE_EXPIRATION_TIME_IN_SECONDS = config.get("CHALLENGE_EXPIRATION_TIME_IN_SECONDS", default=300, cast=int)
SWISS_ARMY_LLAMA_PORT = config.get("SWISS_ARMY_LLAMA_PORT", default=8089, cast=int)
USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE = config.get("USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE", default=0, cast=int)
REMOTE_SWISS_ARMY_LLAMA_INSTANCE_SSH_KEY_PATH = config.get("REMOTE_SWISS_ARMY_LLAMA_INSTANCE_SSH_KEY_PATH", default="/home/ubuntu/vastai_privkey")
REMOTE_SWISS_ARMY_LLAMA_INSTANCE_IP_ADDRESS = config.get("REMOTE_SWISS_ARMY_LLAMA_INSTANCE_IP_ADDRESS", default="50.20.127.188")
REMOTE_SWISS_ARMY_LLAMA_INSTANCE_PORT = config.get("REMOTE_SWISS_ARMY_LLAMA_INSTANCE_PORT", default=40396, cast=int)
REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT = config.get("REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT", default=8087, cast=int)
REMOTE_SWISS_ARMY_LLAMA_EXPOSED_PORT = config.get("REMOTE_SWISS_ARMY_LLAMA_EXPOSED_PORT", default=8089, cast=int)
CREDIT_COST_MULTIPLIER_FACTOR = config.get("CREDIT_COST_MULTIPLIER_FACTOR", default=0.1, cast=float)
MESSAGING_TIMEOUT_IN_SECONDS = config.get("MESSAGING_TIMEOUT_IN_SECONDS", default=60, cast=int)
API_KEY_TESTS_FILE = "api_key_tests.json"
API_KEY_TEST_VALIDITY_HOURS = config.get("API_KEY_TEST_VALIDITY_HOURS", default=72, cast=int)
TARGET_VALUE_PER_CREDIT_IN_USD = config.get("TARGET_VALUE_PER_CREDIT_IN_USD", default=0.1, cast=float)
TARGET_PROFIT_MARGIN = config.get("TARGET_PROFIT_MARGIN", default=0.1, cast=float)
MINIMUM_COST_IN_CREDITS = config.get("MINIMUM_COST_IN_CREDITS", default=0.1, cast=float)
CREDIT_USAGE_TO_TRACKING_AMOUNT_MULTIPLIER = config.get("CREDIT_USAGE_TO_TRACKING_AMOUNT_MULTIPLIER", default=10, cast=int) # Since we always round inference credits to the nearest 0.1, this gives us enough resolution using Patoshis     
MAXIMUM_NUMBER_OF_PASTEL_BLOCKS_FOR_USER_TO_SEND_BURN_AMOUNT_FOR_CREDIT_TICKET = config.get("MAXIMUM_NUMBER_OF_PASTEL_BLOCKS_FOR_USER_TO_SEND_BURN_AMOUNT_FOR_CREDIT_TICKET", default=50, cast=int)
MAXIMUM_LOCAL_CREDIT_PRICE_DIFFERENCE_TO_ACCEPT_CREDIT_PRICING = config.get("MAXIMUM_LOCAL_CREDIT_PRICE_DIFFERENCE_TO_ACCEPT_CREDIT_PRICING", default=0.1, cast=float)
MAXIMUM_LOCAL_UTC_TIMESTAMP_DIFFERENCE_IN_SECONDS = config.get("MAXIMUM_LOCAL_UTC_TIMESTAMP_DIFFERENCE_IN_SECONDS", default=15.0, cast=float)
MAXIMUM_LOCAL_PASTEL_BLOCK_HEIGHT_DIFFERENCE_IN_BLOCKS = config.get("MAXIMUM_LOCAL_PASTEL_BLOCK_HEIGHT_DIFFERENCE_IN_BLOCKS", default=1, cast=int)
MINIMUM_NUMBER_OF_PASTEL_BLOCKS_BEFORE_TICKET_STORAGE_RETRY_ALLOWED = config.get("MINIMUM_NUMBER_OF_PASTEL_BLOCKS_BEFORE_TICKET_STORAGE_RETRY_ALLOWED", default=10, cast=int)
MINIMUM_CONFIRMATION_BLOCKS_FOR_CREDIT_PACK_BURN_TRANSACTION = config.get("MINIMUM_CONFIRMATION_BLOCKS_FOR_CREDIT_PACK_BURN_TRANSACTION", default=3, cast=int)
MINIMUM_NUMBER_OF_POTENTIALLY_AGREEING_SUPERNODES = config.get("MINIMUM_NUMBER_OF_POTENTIALLY_AGREEING_SUPERNODES", default=10, cast=int)
MAXIMUM_NUMBER_OF_CONCURRENT_RPC_REQUESTS = config.get("MAXIMUM_NUMBER_OF_CONCURRENT_RPC_REQUESTS", default=30, cast=int)
MICRO_BENCHMARK_PERFORMANCE_RATIO_THRESHOLD = config.get("MICRO_BENCHMARK_PERFORMANCE_RATIO_THRESHOLD", default=0.55, cast=float)
INDIVIDUAL_SUPERNODE_PRICE_AGREEMENT_REQUEST_TIMEOUT_PERIOD_IN_SECONDS = config.get("INDIVIDUAL_SUPERNODE_PRICE_AGREEMENT_REQUEST_TIMEOUT_PERIOD_IN_SECONDS", default=12, cast=int)
INDIVIDUAL_SUPERNODE_MODEL_MENU_REQUEST_TIMEOUT_PERIOD_IN_SECONDS = config.get("INDIVIDUAL_SUPERNODE_MODEL_MENU_REQUEST_TIMEOUT_PERIOD_IN_SECONDS", default=3, cast=int)
BURN_TRANSACTION_MAXIMUM_AGE_IN_DAYS = config.get("BURN_TRANSACTION_MAXIMUM_AGE_IN_DAYS", default=3, cast=float)
SUPERNODE_CREDIT_PRICE_AGREEMENT_QUORUM_PERCENTAGE = config.get("SUPERNODE_CREDIT_PRICE_AGREEMENT_QUORUM_PERCENTAGE", default=0.51, cast=float)
SUPERNODE_CREDIT_PRICE_AGREEMENT_MAJORITY_PERCENTAGE = config.get("SUPERNODE_CREDIT_PRICE_AGREEMENT_MAJORITY_PERCENTAGE", default=0.65, cast=float)
MINIMUM_CREDITS_PER_CREDIT_PACK = config.get("MINIMUM_CREDITS_PER_CREDIT_PACK", default=10, cast=int)
MAXIMUM_CREDITS_PER_CREDIT_PACK = config.get("MAXIMUM_CREDITS_PER_CREDIT_PACK", default=1000000, cast=int)
SUPERNODE_DATA_CACHE_EVICTION_TIME_IN_MINUTES = config.get("SUPERNODE_DATA_CACHE_EVICTION_TIME_IN_MINUTES", default=60, cast=int)
SKIP_BURN_TRANSACTION_BLOCK_CONFIRMATION_CHECK = 1
UVICORN_PORT = config.get("UVICORN_PORT", default=7123, cast=int)
COIN = 100000 # patoshis in 1 PSL
SUPERNODE_DATA_CACHE = TTLCache(maxsize=1, ttl=SUPERNODE_DATA_CACHE_EVICTION_TIME_IN_MINUTES * 60) # Define the cache with a TTL (time to live) in seconds
challenge_store = {}
file_store = {} # In-memory store for files with expiration times

def async_disk_cached(cache, ttl=None):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # Create a unique cache key based on the function name and arguments
            cache_key = f"{func.__name__}:{str(args)}:{str(kwargs)}"
            cached_result = cache.get(cache_key, default=None, expire_time=True)
            # Check if cached_result is a valid, non-None value
            if cached_result is not None and not (isinstance(cached_result, tuple) and all(v is None for v in cached_result)):
                if isinstance(cached_result, tuple) and len(cached_result) == 2:
                    value, expire_time = cached_result
                    if value is not None and (ttl is None or expire_time is None or expire_time > 0):
                        return value
                elif cached_result is not None:
                    return cached_result
            try:
                value = await func(*args, **kwargs)
            except Exception as e:
                logger.error(f"Exception in {func.__name__}: {str(e)}")
                logger.error(f"Exception type: {type(e).__name__}")
                logger.error(f"Exception details: {traceback.format_exc()}")
                raise
            if value is not None:
                cache.set(cache_key, value, expire=ttl)
            else:
                logger.warning(f"Not caching None value for {func.__name__}")
            return value
        return wrapper
    return decorator

# Initialize PastelSigner
pastel_keys_dir = os.path.expanduser("~/.pastel/pastelkeys")
pastel_signer = libpastelid.PastelSigner(pastel_keys_dir)

def parse_timestamp(timestamp_str):
    try:
        # Attempt to parse with fractional seconds
        return pd.to_datetime(timestamp_str, format='%Y-%m-%dT%H:%M:%S.%f')
    except ValueError:
        # Fall back to parsing without fractional seconds
        return pd.to_datetime(timestamp_str, format='%Y-%m-%dT%H:%M:%S')

def parse_and_format(value):
    try:
        # Check if the JSON string is already formatted
        if isinstance(value, str) and "\n" in value:
            return value
        # Unescape the JSON string if it's a string
        if isinstance(value, str):
            unescaped_value = json.loads(json.dumps(value))
            parsed_value = json.loads(unescaped_value)
        else:
            parsed_value = value
        return json.dumps(parsed_value, indent=4)
    except (json.JSONDecodeError, TypeError):
        return value

def normalize_data(data):
    if isinstance(data, dict):
        return {key: normalize_data(value) for key, value in data.items()}
    elif isinstance(data, list):
        return [normalize_data(item) for item in data]
    elif isinstance(data, uuid.UUID):
        return str(data)
    elif isinstance(data, datetime):
        if data.tzinfo is None:
            # Make the datetime offset-aware with UTC timezone
            return data.replace(tzinfo=pytz.UTC)
        else:
            # Convert to UTC timezone
            return data.astimezone(pytz.UTC)
    else:
        return data
    
def format_list(input_list):
    def json_serialize(item):
        if isinstance(item, uuid.UUID):
            return json.dumps(str(item), indent=4)
        elif isinstance(item, dict):
            return json.dumps(pretty_json_func(item), indent=4)
        elif isinstance(item, list):
            return format_list(item)
        else:
            return json.dumps(item, indent=4)
    formatted_list = "[\n" + ",\n".join("    " + json_serialize(item).replace("\n", "\n    ") for item in input_list) + "\n]"
    return formatted_list

def pretty_json_func(data):
    if isinstance(data, SQLModel):
        data = data.dict()  # Convert SQLModel instance to dictionary
    if isinstance(data, dict):
        formatted_data = {}
        for key, value in data.items():
            if isinstance(value, uuid.UUID):  # Convert UUIDs to string
                formatted_data[key] = str(value)
            elif isinstance(value, dict):  # Recursively handle dictionary values
                formatted_data[key] = pretty_json_func(value)
            elif isinstance(value, list):  # Special handling for lists
                formatted_data[key] = format_list(value)
            elif key.endswith("_json"):  # Handle keys that end with '_json'
                formatted_data[key] = parse_and_format(value)
            else:  # Handle other types of values
                formatted_data[key] = value
        return json.dumps(formatted_data, indent=4)
    elif isinstance(data, list):  # Top-level list handling
        return format_list(data)
    elif isinstance(data, str):  # Handle string type data separately
        return parse_and_format(data)
    else:
        return data  # Return data as is if not a dictionary or string
    
def abbreviated_pretty_json_func(data):
    max_payload_length_in_characters = 10000
    formatted_payload = pretty_json_func(data)
    if len(formatted_payload) > max_payload_length_in_characters:
        abbreviated_payload = formatted_payload[:max_payload_length_in_characters] + "..."
        closing_brackets = "]" * (formatted_payload.count("[") - formatted_payload[:max_payload_length_in_characters].count("["))
        closing_brackets += "}" * (formatted_payload.count("{") - formatted_payload[:max_payload_length_in_characters].count("{"))
        abbreviated_payload += closing_brackets
        formatted_payload = abbreviated_payload
    return formatted_payload    
    
def log_action_with_payload(action_string, payload_name, json_payload):
    formatted_payload = abbreviated_pretty_json_func(json_payload)
    logger.info(f"Now {action_string} {payload_name} with payload:\n{formatted_payload}")
    
def get_local_rpc_settings_func(directory_with_pastel_conf=os.path.expanduser("~/.pastel/")):
    with open(os.path.join(directory_with_pastel_conf, "pastel.conf"), 'r') as f:
        lines = f.readlines()
    other_flags = {}
    rpchost = '127.0.0.1'
    rpcport = '19932'
    rpcuser = None
    rpcpassword = None
    for line in lines:
        line = line.strip()
        if not line or line.startswith('#'):  # Ignore blank lines and comments
            continue
        if '=' in line:
            key, value = line.split('=', 1)  # Split only on the first '='
            key = key.strip()
            value = value.strip()
            if key == 'rpcport':
                rpcport = value
            elif key == 'rpcuser':
                rpcuser = value
            elif key == 'rpcpassword':
                rpcpassword = value
            elif key == 'rpchost':
                rpchost = value
            else:
                other_flags[key] = value
    return rpchost, rpcport, rpcuser, rpcpassword, other_flags

def get_network_info(rpc_port):
    if rpc_port == '9932':
        network = 'mainnet'
        burn_address = 'PtpasteLBurnAddressXXXXXXXXXXbJ5ndd'
    elif rpc_port == '19932':
        network = 'testnet'
        burn_address = 'tPpasteLBurnAddressXXXXXXXXXXX3wy7u'
    elif rpc_port == '29932':
        network = 'devnet'
        burn_address = '44oUgmZSL997veFEQDq569wv5tsT6KXf9QY7'
    else:
        raise ValueError(f"Unknown RPC port: {rpc_port}")
    return network, burn_address

def required_collateral(network):
    if network == 'mainnet':
        return 5000000  # 5 million PSL for mainnet
    else:
        return 1000000  # 1 million PSL for testnet/devnet

def write_rpc_settings_to_env_file_func(rpc_host, rpc_port, rpc_user, rpc_password, other_flags):
    with open('.env', 'w') as f:
        f.write(f"RPC_HOST={rpc_host}\n")
        f.write(f"RPC_PORT={rpc_port}\n")
        f.write(f"RPC_USER={rpc_user}\n")
        f.write(f"RPC_PASSWORD={rpc_password}\n")
        for current_flag in other_flags:
            current_value = other_flags[current_flag]
            try:
                f.write(f"{current_flag}={current_value}\n")
            except Exception as e:
                logger.error(f"Error writing to .env file: {e}")
                pass
    return

def is_base64_encoded(data):
    if not isinstance(data, str):
        return False
    if len(data) % 4 != 0:
        return False
    base64_pattern = re.compile(r'^[A-Za-z0-9+/]+={0,2}$')
    if not base64_pattern.match(data):
        return False
    try:
        base64.b64decode(data, validate=True)
        return True
    except Exception:
        return False

def kill_open_ssh_tunnels(local_port):
    try:
        lsof_command = [
            "lsof", "-i", f"TCP:{local_port}", "-sTCP:LISTEN"
        ]
        result = subprocess.run(lsof_command, capture_output=True, text=True)
        if result.stdout:
            lines = result.stdout.strip().split('\n')
            pids = []
            for line in lines[1:]:
                columns = line.split()
                if len(columns) > 1:
                    pids.append(columns[1])
            if pids:
                for pid in pids:
                    kill_command = ["kill", "-9", pid]
                    subprocess.run(kill_command)
                logger.info("Killed SSH tunnels with PIDs: {}".format(', '.join(pids)))
            else:
                logger.info("No SSH tunnels found to kill.")
        else:
            logger.info("No SSH tunnels found.")
    except Exception as e:
        logger.error("Error while killing SSH tunnels: {}".format(e))
        
def establish_ssh_tunnel():
    if USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
        kill_open_ssh_tunnels(REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT)
        key_path = REMOTE_SWISS_ARMY_LLAMA_INSTANCE_SSH_KEY_PATH
        if not os.access(key_path, os.R_OK):
            raise PermissionError("SSH key file at {} is not readable.".format(key_path))
        current_permissions = os.stat(key_path).st_mode & 0o777
        if current_permissions != 0o600:
            os.chmod(key_path, 0o600)
            logger.info("Permissions for SSH key file set to 600.")
        try:
            tunnel = SSHTunnelForwarder(
                (REMOTE_SWISS_ARMY_LLAMA_INSTANCE_IP_ADDRESS, REMOTE_SWISS_ARMY_LLAMA_INSTANCE_PORT),
                ssh_username="root",
                ssh_pkey=key_path,
                remote_bind_address=("localhost", REMOTE_SWISS_ARMY_LLAMA_EXPOSED_PORT),
                local_bind_address=("localhost", REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT),
                host_pkey_directories=[]  # Disable host key checking
            )
            tunnel.start()
            logger.info("SSH tunnel established: {}".format(tunnel.local_bind_address))
            while True:
                time.sleep(10)
        except BaseSSHTunnelForwarderError as e:
            logger.error("SSH tunnel error: {}".format(e))
        except Exception as e:
            logger.error("Error establishing SSH tunnel: {}".format(e))
                
def get_audio_length(audio_input) -> float:
    if isinstance(audio_input, bytes):
        audio_file = io.BytesIO(audio_input)
        audio = MutagenFile(audio_file)
    elif isinstance(audio_input, str):
        audio = MutagenFile(audio_input)
    else:
        raise ValueError("audio_input must be either bytes or a file path string.")
    if audio is None or not hasattr(audio.info, 'length'):
        raise ValueError("Could not determine the length of the audio file.")
    return audio.info.length
        
def convert_uuids_to_strings(data):
    if isinstance(data, dict):
        return {key: convert_uuids_to_strings(value) for key, value in data.items()}
    elif isinstance(data, list):
        return [convert_uuids_to_strings(item) for item in data]
    elif isinstance(data, uuid.UUID):
        return str(data)
    else:
        return data

def compute_sha3_256_hexdigest(input_str: str):
    """Compute the SHA3-256 hash of the input string and return the hexadecimal digest."""
    return hashlib.sha3_256(input_str.encode('utf-8')).hexdigest()

def compute_sha3_256_hexdigest_of_file(file_data: bytes):
    return hashlib.sha3_256(file_data).hexdigest()

def remove_file(path: str):
    if os.path.exists(path):
        os.remove(path)
    if path in file_store:
        del file_store[path]

async def save_file(file_content: bytes, filename: str):
    file_location = os.path.join(tempfile.gettempdir(), filename)
    with open(file_location, "wb") as buffer:
        buffer.write(file_content)
    file_hash = compute_sha3_256_hexdigest_of_file(file_content)
    file_size = os.path.getsize(file_location) # Calculate file size
    expire_at = datetime.utcnow() + timedelta(hours=24) # Set expiration time (24 hours)
    file_store[file_location] = expire_at
    return file_location, file_hash, file_size

async def upload_and_get_file_metadata(file_content: bytes, file_prefix: str = "document") -> Dict:
    file_name = f"{file_prefix}_{compute_sha3_256_hexdigest_of_file(file_content)[:8]}.{magika.identify_bytes(file_content).output.ct_label}"
    file_location, file_hash, file_size = await save_file(file_content, file_name)
    external_ip = get_external_ip_func()
    file_url = f"http://{external_ip}:{UVICORN_PORT}/download/{file_name}"    
    return {
        "file_location": file_location,
        "file_hash": file_hash,
        "file_size": file_size,
        "file_url": file_url
    }
    
async def calculate_xor_distance(pastelid1: str, pastelid2: str) -> int:
    hash1 = compute_sha3_256_hexdigest(pastelid1)
    hash2 = compute_sha3_256_hexdigest(pastelid2)
    xor_result = int(hash1, 16) ^ int(hash2, 16)
    return xor_result

async def get_supernode_url_from_pastelid_func(pastelid: str, supernode_list_df: pd.DataFrame) -> str:
    supernode_row = supernode_list_df[supernode_list_df['extKey'] == pastelid]
    if not supernode_row.empty:
        supernode_ipaddress_port = supernode_row['ipaddress:port'].values[0]
        ipaddress = supernode_ipaddress_port.split(':')[0]
        supernode_url = f"http://{ipaddress}:7123"
        return supernode_url
    else:
        raise ValueError(f"Supernode with PastelID {pastelid} not found in the supernode list")

async def get_closest_supernode_pastelid_from_list(local_pastelid: str, supernode_pastelids: List[str]) -> str:
    xor_distances = [(supernode_pastelid, await calculate_xor_distance(local_pastelid, supernode_pastelid)) for supernode_pastelid in supernode_pastelids]
    closest_supernode = min(xor_distances, key=lambda x: x[1])
    return closest_supernode[0]

async def get_closest_supernode_to_pastelid_url(input_pastelid: str, supernode_list_df: pd.DataFrame) -> Tuple[Optional[str], Optional[str]]:
    if not supernode_list_df.empty:
        list_of_supernode_pastelids = supernode_list_df['extKey'].tolist()
        closest_supernode_pastelid = await get_closest_supernode_pastelid_from_list(input_pastelid, list_of_supernode_pastelids)
        supernode_url = await get_supernode_url_from_pastelid_func(closest_supernode_pastelid, supernode_list_df)
        return supernode_url, closest_supernode_pastelid
    return None, None

async def get_n_closest_supernodes_to_pastelid_urls(n: int, input_pastelid: str, supernode_list_df: pd.DataFrame) -> List[Tuple[str, str]]:
    if not supernode_list_df.empty:
        list_of_supernode_pastelids = supernode_list_df['extKey'].tolist()
        xor_distances = [(supernode_pastelid, await calculate_xor_distance(input_pastelid, supernode_pastelid)) for supernode_pastelid in list_of_supernode_pastelids]
        sorted_xor_distances = sorted(xor_distances, key=lambda x: x[1])
        closest_supernodes = sorted_xor_distances[:n]
        supernode_urls_and_pastelids = [(await get_supernode_url_from_pastelid_func(pastelid, supernode_list_df), pastelid) for pastelid, _ in closest_supernodes]
        return supernode_urls_and_pastelids
    return []

class JSONRPCException(Exception):
    def __init__(self, rpc_error):
        parent_args = []
        try:
            parent_args.append(rpc_error['message'])
        except Exception as e:
            logger.error(f"Error occurred in JSONRPCException: {e}")
            pass
        Exception.__init__(self, *parent_args)
        self.error = rpc_error
        self.code = rpc_error['code'] if 'code' in rpc_error else None
        self.message = rpc_error['message'] if 'message' in rpc_error else None

    def __str__(self):
        return '%d: %s' % (self.code, self.message)

    def __repr__(self):
        return '<%s \'%s\'>' % (self.__class__.__name__, self)

def EncodeDecimal(o):
    if isinstance(o, decimal.Decimal):
        return float(round(o, 8))
    raise TypeError(repr(o) + " is not JSON serializable")

class AsyncAuthServiceProxy:
    _semaphore = asyncio.BoundedSemaphore(MAXIMUM_NUMBER_OF_CONCURRENT_RPC_REQUESTS)

    def __init__(self, service_url, service_name=None, reconnect_timeout=2, reconnect_amount=3, request_timeout=20):
        self.service_url = service_url
        self.service_name = service_name
        self.url = urlparse(service_url)
        self.id_count = 0
        user = self.url.username
        password = self.url.password
        authpair = f"{user}:{password}".encode('utf-8')
        self.auth_header = b'Basic ' + base64.b64encode(authpair)
        self.reconnect_timeout = reconnect_timeout
        self.reconnect_amount = reconnect_amount
        self.request_timeout = request_timeout
        self.client = httpx.AsyncClient(timeout=request_timeout, http2=True)

    def __getattr__(self, name):
        if name.startswith('__') and name.endswith('__'):
            raise AttributeError
        if self.service_name is not None:
            name = f"{self.service_name}.{name}"
        return AsyncAuthServiceProxy(self.service_url, name)

    async def __call__(self, *args):
        async with self._semaphore:
            self.id_count += 1
            postdata = json.dumps({
                'version': '2.0',
                'method': self.service_name,
                'params': args,
                'id': self.id_count
            })
            headers = {
                'Host': self.url.hostname,
                'User-Agent': "AuthServiceProxy/0.1",
                'Authorization': self.auth_header.decode(),
                'Content-type': 'application/json',
                'Connection': 'keep-alive'
            }
            for i in range(self.reconnect_amount):
                try:
                    if i > 0:
                        logger.warning(f"Reconnect try #{i+1}")
                        sleep_time = self.reconnect_timeout * (2 ** i)
                        logger.info(f"Waiting for {sleep_time} seconds before retrying.")
                        await asyncio.sleep(sleep_time)
                    response = await self.client.post(
                        self.service_url,
                        headers=headers,
                        content=postdata
                    )
                    response.raise_for_status()
                    response_json = response.json()
                    break
                except httpx.HTTPStatusError as e:
                    logger.error(f"HTTP error occurred in __call__: {e}")
                except httpx.RequestError as e:
                    logger.error(f"Request error occurred in __call__: {e}")
                except Exception as e:
                    logger.error(f"Unexpected error occurred in __call__: {e}")
            else:
                logger.error("Reconnect tries exceeded.")
                return
            if 'error' in response_json and response_json['error'] is not None:
                raise JSONRPCException(response_json['error'])
            elif 'result' not in response_json:
                raise JSONRPCException({
                    'code': -343, 'message': 'missing JSON-RPC result'})
            else:
                return response_json['result']
    async def close(self):
        await self.client.aclose()

async def save_stats_to_json():
    global rpc_call_stats, tracking_period_start  # Move both global declarations here
    while True:
        await asyncio.sleep(3600)  # Adjust this value for how often you want to save stats (e.g., every hour)
        tracking_period_end = datetime.utcnow()
        stats_snapshot = {
            "tracking_period_start": tracking_period_start.isoformat() + 'Z',
            "tracking_period_end": tracking_period_end.isoformat() + 'Z',
            "rpc_call_stats": dict(rpc_call_stats)
        }
        # Append the stats to the JSON file
        try:
            with open('rpc_call_stats.json', 'a') as f:
                f.write(json.dumps(stats_snapshot) + '\n')
        except Exception as e:
            print(f"Failed to save stats to JSON: {e}")
        # Reset tracking for the next period
        rpc_call_stats = defaultdict(lambda: {
            "count": 0,
            "cumulative_time": 0.0,
            "average_time": 0.0,
            "success_count": 0,
            "total_response_size": 0,
            "average_response_size": 0.0,
            "timeout_errors": 0,
            "connection_errors": 0,
            "other_errors": 0
        })
        tracking_period_start = tracking_period_end

def track_rpc_call(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        method_name = kwargs.get('method_name', func.__name__)
        
        def make_hashable(obj):
            if isinstance(obj, (list, tuple)):
                return tuple(make_hashable(e) for e in obj)
            elif isinstance(obj, dict):
                return tuple(sorted((k, make_hashable(v)) for k, v in obj.items()))
            elif isinstance(obj, set):
                return frozenset(make_hashable(e) for e in obj)
            return obj

        hashable_args = make_hashable(args)
        hashable_kwargs = make_hashable(kwargs)
        
        rpc_key = (method_name, hashable_args, hashable_kwargs)
        
        start_time = time.time()
        try:
            result = await func(*args, **kwargs)
            elapsed_time = time.time() - start_time
            response_size = len(str(result).encode('utf-8'))
            
            if rpc_key not in rpc_call_stats:
                rpc_call_stats[rpc_key] = {
                    "count": 0,
                    "cumulative_time": 0.0,
                    "average_time": 0.0,
                    "success_count": 0,
                    "total_response_size": 0,
                    "average_response_size": 0.0,
                    "timeout_errors": 0,
                    "connection_errors": 0,
                    "other_errors": 0
                }
            
            rpc_call_stats[rpc_key]["count"] += 1
            rpc_call_stats[rpc_key]["cumulative_time"] += elapsed_time
            rpc_call_stats[rpc_key]["average_time"] = (
                rpc_call_stats[rpc_key]["cumulative_time"] / rpc_call_stats[rpc_key]["count"]
            )
            rpc_call_stats[rpc_key]["total_response_size"] += response_size
            rpc_call_stats[rpc_key]["average_response_size"] = (
                rpc_call_stats[rpc_key]["total_response_size"] / rpc_call_stats[rpc_key]["count"]
            )
            rpc_call_stats[rpc_key]["success_count"] += 1
            return result
        except httpx.TimeoutException:
            if rpc_key not in rpc_call_stats:
                rpc_call_stats[rpc_key] = {"timeout_errors": 0}
            rpc_call_stats[rpc_key]["timeout_errors"] = rpc_call_stats[rpc_key].get("timeout_errors", 0) + 1
            raise
        except httpx.ConnectError:
            if rpc_key not in rpc_call_stats:
                rpc_call_stats[rpc_key] = {"connection_errors": 0}
            rpc_call_stats[rpc_key]["connection_errors"] = rpc_call_stats[rpc_key].get("connection_errors", 0) + 1
            raise
        except Exception as e:
            if rpc_key not in rpc_call_stats:
                rpc_call_stats[rpc_key] = {"other_errors": 0}
            rpc_call_stats[rpc_key]["other_errors"] = rpc_call_stats[rpc_key].get("other_errors", 0) + 1
            raise e
    return wrapper

#Wrapped RPC calls so we can track them and log their performance:
@track_rpc_call
async def getinfo(rpc_connection):
    return await rpc_connection.getinfo()

@track_rpc_call
async def getblockcount(rpc_connection):
    return await rpc_connection.getblockcount()

@track_rpc_call
async def getblockhash(rpc_connection, block_height):
    return await rpc_connection.getblockhash(block_height)

@track_rpc_call
async def getblock(rpc_connection, block_hash):
    return await rpc_connection.getblock(block_hash)

@track_rpc_call
async def listaddressamounts(rpc_connection):
    return await rpc_connection.listaddressamounts()

@track_rpc_call
async def z_getbalance(rpc_connection, address_to_check):
    return await rpc_connection.z_getbalance(address_to_check)

@track_rpc_call
async def getrawtransaction(rpc_connection, txid, verbose=1):
    return await rpc_connection.getrawtransaction(txid, verbose)

@track_rpc_call
async def masternode_top(rpc_connection):
    return await rpc_connection.masternode('top')

@track_rpc_call
async def masternodelist_full(rpc_connection):
    return await rpc_connection.masternodelist('full')

@track_rpc_call
async def masternodelist_rank(rpc_connection):
    return await rpc_connection.masternodelist('rank')

@track_rpc_call
async def masternodelist_pubkey(rpc_connection):
    return await rpc_connection.masternodelist('pubkey')

@track_rpc_call
async def masternodelist_extra(rpc_connection):
    return await rpc_connection.masternodelist('extra')

@track_rpc_call
async def masternode_message_list(rpc_connection):
    return await rpc_connection.masternode('message', 'list')

@track_rpc_call
async def pastelid_sign(rpc_connection, message_to_sign, pastelid, passphrase, algorithm='ed448'):
    return await rpc_connection.pastelid('sign', message_to_sign, pastelid, passphrase, algorithm)

@track_rpc_call
async def pastelid_verify(rpc_connection, message_to_verify, signature, pastelid, algorithm='ed448'):
    return await rpc_connection.pastelid('verify', message_to_verify, signature, pastelid, algorithm)

@track_rpc_call
async def masternode_message_send(rpc_connection, receiving_sn_pubkey, compressed_message_base64):
    return await rpc_connection.masternode('message', 'send', receiving_sn_pubkey, compressed_message_base64)

@track_rpc_call
async def tickets_register_contract(rpc_connection, ticket_json_b64, ticket_type_identifier, ticket_input_data_hash):
    return await rpc_connection.tickets('register', 'contract', ticket_json_b64, ticket_type_identifier, ticket_input_data_hash)

@track_rpc_call
async def tickets_get(rpc_connection, ticket_txid, verbose=1):
    return await rpc_connection.tickets('get', ticket_txid, verbose)

@track_rpc_call
async def generic_tickets_find(rpc_connection, credit_ticket_secondary_key):
    return await rpc_connection.tickets('find', 'contract', credit_ticket_secondary_key)

@track_rpc_call
async def tickets_list_contract(rpc_connection, ticket_type_identifier, starting_block_height):
    return await rpc_connection.tickets('list', 'contract', ticket_type_identifier, starting_block_height)

@track_rpc_call
async def listsinceblock(rpc_connection, start_block_hash, target_confirmations=1, include_watchonly=True):
    return await rpc_connection.listsinceblock(start_block_hash, target_confirmations, include_watchonly)

@track_rpc_call
async def tickets_list_id(rpc_connection, identifier):
    return await rpc_connection.tickets('list', 'id', identifier)

@track_rpc_call
async def getaddressutxosextra(rpc_connection, params):
    try:
        formatted_params = {
            "addresses": params.get('addresses', []),
            "simple": False,  # We want full info
            "minHeight": params.get('minHeight', 0),
            "sender": params.get('sender', ''),
            "mempool": params.get('mempool', True)
        }
        result = await rpc_connection.getaddressutxosextra(formatted_params)
        return result
    except Exception as e:
        logger.error(f"Error in getaddressutxosextra RPC call: {str(e)}")
        logger.error(f"Error type: {type(e)}")
        logger.error(f"Error details: {traceback.format_exc()}")
        return None  # Return None explicitly on error

@track_rpc_call
async def decoderawtransaction(rpc_connection, raw_tx_data):
    return await rpc_connection.decoderawtransaction(raw_tx_data)

@track_rpc_call
async def gettransaction(rpc_connection, txid, include_watchonly=False):
    return await rpc_connection.gettransaction(txid, include_watchonly)

async def micro_benchmarking_func():
    baseline_score = 16
    duration_of_benchmark_in_seconds = 4.0
    end_time = time.time() + duration_of_benchmark_in_seconds
    actual_score = 0
    while time.time() < end_time:
        try:
            info_results = await getinfo(rpc_connection)  # Await the coroutine
            if 'blocks' in info_results and isinstance(info_results['blocks'], int):
                actual_score += 1
        except Exception as e:
            logger.error(f"Error during benchmarking: {e}", exc_info=True)
            continue
    benchmark_performance_ratio = actual_score / baseline_score
    current_datetime_utc = datetime.utcnow().isoformat()
    logger.info(f"Benchmark performance ratio as of {current_datetime_utc}: {benchmark_performance_ratio}; Raw score: {actual_score}")
    benchmark_results_cache.append([current_datetime_utc, actual_score, benchmark_performance_ratio])
    cutoff_date = datetime.utcnow() - timedelta(weeks=2)
    benchmark_results_cache[:] = [row for row in benchmark_results_cache if datetime.fromisoformat(row[0]) >= cutoff_date]

async def write_benchmark_cache_to_csv():
    try:
        with open(local_benchmark_csv_file_path, mode='w', newline='') as file:
            writer = csv.writer(file)
            writer.writerows(benchmark_results_cache)
    except Exception as e:
        logger.error(f"Error writing to CSV: {e}", exc_info=True)

async def load_benchmark_cache_from_csv():
    global benchmark_results_cache
    try:
        if local_benchmark_csv_file_path.exists():
            with open(local_benchmark_csv_file_path, mode='r') as file:
                reader = csv.reader(file)
                benchmark_results_cache = list(reader)
    except Exception as e:
        logger.error(f"Error loading from CSV: {e}", exc_info=True)

async def schedule_micro_benchmark_periodically():
    await load_benchmark_cache_from_csv()
    while True:
        await micro_benchmarking_func()
        await asyncio.sleep(60)
        await write_benchmark_cache_to_csv()
        
async def get_current_pastel_block_height_func():
    curent_block_height = await getblockcount(rpc_connection)
    return curent_block_height

async def get_best_block_hash_and_merkle_root_func():
    best_block_height = await get_current_pastel_block_height_func()
    best_block_hash = await getblockhash(rpc_connection, best_block_height)
    best_block_details = await getblock(rpc_connection, best_block_hash)
    best_block_merkle_root = best_block_details['merkleroot']
    return best_block_hash, best_block_merkle_root, best_block_height

async def get_last_block_data_func():
    current_block_height = await get_current_pastel_block_height_func()
    block_data = await getblock(rpc_connection, str(current_block_height))
    return block_data

async def check_psl_address_balance_alternative_func(address_to_check):
    address_amounts_dict = await listaddressamounts(rpc_connection)
    # Convert the dictionary into a list of dictionaries, each representing a row
    data = [{'address': address, 'amount': amount} for address, amount in address_amounts_dict.items()]
    # Create the DataFrame from the list of dictionaries
    address_amounts_df = pd.DataFrame(data)
    # Filter the DataFrame for the specified address
    address_amounts_df_filtered = address_amounts_df[address_amounts_df['address'] == address_to_check]
    # Calculate the sum of the 'amount' column for the filtered DataFrame
    balance_at_address = address_amounts_df_filtered['amount'].sum()
    return balance_at_address

async def check_psl_address_balance_func(address_to_check):
    balance_at_address = await z_getbalance(rpc_connection, address_to_check)
    return balance_at_address

async def get_raw_transaction_func(txid):
    raw_transaction_data = await getrawtransaction(rpc_connection, txid, 1)
    return raw_transaction_data

async def generate_challenge(pastelid: str) -> Tuple[str, str]:
    """
    Generates a random challenge string and a unique challenge ID for a given PastelID.
    The challenge string is stored temporarily, associated with the challenge ID, and expires after a certain period of time.
    """
    challenge_string = str(uuid.uuid4())
    challenge_id = str(uuid.uuid4())
    expiration_time = time.time() + CHALLENGE_EXPIRATION_TIME_IN_SECONDS
    challenge_store[challenge_id] = {
        "pastelid": pastelid,
        "challenge_string": challenge_string,
        "expiration_time": expiration_time
    }
    return challenge_string, challenge_id

async def verify_challenge_signature(pastelid: str, signature: str, challenge_id: str) -> bool:
    """
    Verifies the signature of the PastelID on the challenge string associated with the provided challenge ID.
    If the signature is valid and the challenge ID exists and hasn't expired, it returns True. Otherwise, it returns False.
    """
    if challenge_id not in challenge_store:
        return False
    challenge_data = challenge_store[challenge_id]
    stored_pastelid = challenge_data["pastelid"]
    challenge_string = challenge_data["challenge_string"]
    expiration_time = challenge_data["expiration_time"]
    if pastelid != stored_pastelid:
        return False
    current_time = time.time()
    if current_time > expiration_time:
        del challenge_store[challenge_id]
        return False
    verification_result = await verify_message_with_pastelid_func(pastelid=pastelid, message_to_verify=challenge_string, pastelid_signature_on_message=signature) 
    is_valid_signature = verification_result == 'OK'
    if is_valid_signature:
        del challenge_store[challenge_id]
        return True
    else:
        return False

async def verify_challenge_signature_from_inference_request_id(inference_request_id: str, challenge_signature: str, challenge_id: str) -> bool:
    # Retrieve the inference API usage request from the database
    async with db_code.Session() as db:
        result = await db.exec(
            select(db_code.InferenceAPIUsageRequest).where(db_code.InferenceAPIUsageRequest.inference_request_id == inference_request_id)
        )
        inference_request = result.one_or_none()
    if inference_request:
        requesting_pastelid = inference_request.requesting_pastelid
        is_valid_signature = await verify_challenge_signature(requesting_pastelid, challenge_signature, challenge_id)
        return is_valid_signature
    else:
        return False

async def check_masternode_top_func():
    masternode_top_command_output = await masternode_top(rpc_connection)
    return masternode_top_command_output

async def check_inference_port(supernode, max_response_time_in_milliseconds, local_performance_data):
    ip_address_port = supernode.get('ipaddress:port')
    if not ip_address_port or ip_address_port.startswith(local_ip):
        return None
    ip_address = ip_address_port.split(":")[0]
    try:
        async with httpx.AsyncClient(timeout=max_response_time_in_milliseconds / 1000) as client:
            response = await client.get(f'http://{ip_address}:7123/liveness_ping')
            if response.status_code != 200:
                return None
            response_data = response.json()
            performance_ratio = response_data.get('performance_ratio_score')
            if performance_ratio is None:
                performance_ratio = float('nan')  # Assign NaN if the value is None
            actual_score = response_data.get('raw_benchmark_score', 'N/A')
            if not isinstance(performance_ratio, (int, float)) or performance_ratio < MICRO_BENCHMARK_PERFORMANCE_RATIO_THRESHOLD:
                performance_ratio = 'N/A'
                actual_score = 'N/A'
            timestamp_str = response_data.get('timestamp')
            timestamp = datetime.fromisoformat(timestamp_str)
            if timestamp.tzinfo is None:
                timestamp = timestamp.replace(tzinfo=timezone.utc)
            last_updated = (datetime.now(timezone.utc) - timestamp).total_seconds()
            local_performance_data.append({'IP Address': ip_address, 'Performance Ratio': performance_ratio, 'Actual Score': actual_score, 'Seconds Since Last Updated': last_updated})
            return supernode
    except (httpx.RequestError, httpx.ConnectTimeout, httpx.ConnectError, httpx.ReadTimeout, OSError) as e:  # noqa: F841
        return None

async def update_performance_data_df(local_performance_data):
    global performance_data_df
    local_performance_data_df = pd.DataFrame(local_performance_data)
    if not local_performance_data_df.empty and 'IP Address' in local_performance_data_df.columns:
        local_performance_data_df.sort_values(by='IP Address', inplace=True)
    # Replace "N/A" with NaN for numerical operations
    local_performance_data_df.replace("N/A", pd.NA, inplace=True)
    summary_statistics = {
        'IP Address': ['Min', 'Average', 'Median', 'Max'],
        'Performance Ratio': [
            pd.to_numeric(local_performance_data_df['Performance Ratio'], errors='coerce').min(),
            pd.to_numeric(local_performance_data_df['Performance Ratio'], errors='coerce').mean(),
            pd.to_numeric(local_performance_data_df['Performance Ratio'], errors='coerce').median(),
            pd.to_numeric(local_performance_data_df['Performance Ratio'], errors='coerce').max()
        ],
        'Actual Score': [
            pd.to_numeric(local_performance_data_df['Actual Score'], errors='coerce').min(),
            pd.to_numeric(local_performance_data_df['Actual Score'], errors='coerce').mean(),
            pd.to_numeric(local_performance_data_df['Actual Score'], errors='coerce').median(),
            pd.to_numeric(local_performance_data_df['Actual Score'], errors='coerce').max()
        ],
        'Seconds Since Last Updated': [
            pd.to_numeric(local_performance_data_df['Seconds Since Last Updated'], errors='coerce').min(),
            pd.to_numeric(local_performance_data_df['Seconds Since Last Updated'], errors='coerce').mean(),
            pd.to_numeric(local_performance_data_df['Seconds Since Last Updated'], errors='coerce').median(),
            pd.to_numeric(local_performance_data_df['Seconds Since Last Updated'], errors='coerce').max()
        ]
    }
    summary_df = pd.DataFrame(summary_statistics)
    local_performance_data_df = pd.concat([local_performance_data_df, summary_df], ignore_index=True)
    local_performance_data_df.to_csv('supernode_performance_data.csv', index=False)
    performance_data_df = pd.concat([performance_data_df, local_performance_data_df], ignore_index=True)
    return local_performance_data_df

async def save_performance_data_history(local_performance_data_df):
    global performance_data_history
    current_time_str = datetime.utcnow().isoformat()
    # Load existing data
    if pickle_file_path.exists():
        try:
            with open(pickle_file_path, 'rb') as f:
                existing_data = pickle.load(f)
        except (EOFError, pickle.UnpicklingError) as e:
            logger.error(f"Error reading existing pickle file: {e}", exc_info=True)
            existing_data = {}
    else:
        existing_data = {}
    # Update the global performance_data_history
    performance_data_history.update(existing_data)
    performance_data_history[current_time_str] = local_performance_data_df
    # Remove entries older than 3 days
    cutoff_date = datetime.utcnow() - timedelta(days=3)
    performance_data_history = {k: v for k, v in performance_data_history.items() if datetime.fromisoformat(k) >= cutoff_date}
    # Save updated data
    try:
        with open(pickle_file_path, 'wb') as f:
            pickle.dump(performance_data_history, f)
    except Exception as e:
        logger.error(f"Error saving pickle file: {e}", exc_info=True)

async def generate_supernode_inference_ip_blacklist(max_response_time_in_milliseconds=800):
    global performance_data_df, performance_data_history
    blacklist_path = Path('supernode_inference_ip_blacklist.txt')
    valid_supernode_list_path = Path('valid_supernode_list.txt')
    logger.info("Now compiling Supernode IP blacklist based on Supernode responses to port checks...")
    # Ensure the blacklist and valid supernode files exist
    if not blacklist_path.exists():
        blacklist_path.touch()
    if not valid_supernode_list_path.exists():
        valid_supernode_list_path.touch()
    # Perform the supernode checks
    full_supernode_list_df, _ = await check_supernode_list_func()
    local_performance_data = []
    check_results = await asyncio.gather(
        *(check_inference_port(supernode, max_response_time_in_milliseconds, local_performance_data) for supernode in full_supernode_list_df.to_dict(orient='records'))
    )
    logger.info(f"Gathered check results. Number of results: {len(check_results)}")
    logger.info(f"Local performance data entries: {len(local_performance_data)}")
    # Filter supernodes based on success/failure
    successful_nodes = {supernode['extKey'] for supernode in check_results if supernode is not None}
    successful_nodes_ip_addresses = {supernode['ipaddress:port']for supernode in check_results if supernode is not None}
    failed_nodes = {supernode['ipaddress:port'] for supernode in full_supernode_list_df.to_dict(orient='records') if supernode['extKey'] not in successful_nodes}
    logger.info(f"There were {len(failed_nodes)} failed Supernodes out of {len(full_supernode_list_df)} total Supernodes, a failure rate of {len(failed_nodes) / len(full_supernode_list_df) * 100:.2f}%")
    # Update performance data
    local_performance_data_df = await update_performance_data_df(local_performance_data)
    await save_performance_data_history(local_performance_data_df)
    # Write failed nodes to blacklist
    with blacklist_path.open('w') as blacklist_file:
        for failed_node in failed_nodes:
            ip_address = failed_node.split(':')[0]
            blacklist_file.write(f"{ip_address}\n")
    # Write successful nodes to the valid supernode list
    with valid_supernode_list_path.open('w') as valid_supernode_file:
        for successful_node in successful_nodes_ip_addresses:
            ip_address = successful_node.split(':')[0]
            valid_supernode_file.write(f"{ip_address}\n")
    return list(successful_nodes)

async def fetch_supernode_list_data():
    masternode_list_full_command_output = await masternodelist_full(rpc_connection)
    masternode_list_rank_command_output = await masternodelist_rank(rpc_connection)
    masternode_list_pubkey_command_output = await masternodelist_pubkey(rpc_connection)
    masternode_list_extra_command_output = await masternodelist_extra(rpc_connection)
    if masternode_list_full_command_output:
        masternode_list_full_df = pd.DataFrame([masternode_list_full_command_output[x].split() for x in masternode_list_full_command_output])
        masternode_list_full_df['txid_vout'] = [x for x in masternode_list_full_command_output]
        masternode_list_full_df.columns = ['supernode_status', 'protocol_version', 'supernode_psl_address', 'lastseentime', 'activeseconds', 'lastpaidtime', 'lastpaidblock', 'ipaddress:port', 'txid_vout']
        masternode_list_full_df.index = masternode_list_full_df['txid_vout']
        masternode_list_full_df.drop(columns=['txid_vout'], inplace=True)
        for txid_vout in masternode_list_full_df.index:
            rank = masternode_list_rank_command_output.get(txid_vout)
            pubkey = masternode_list_pubkey_command_output.get(txid_vout)
            extra = masternode_list_extra_command_output.get(txid_vout, {})
            masternode_list_full_df.at[txid_vout, 'rank'] = rank if rank is not None else 'Unknown'
            masternode_list_full_df.at[txid_vout, 'pubkey'] = pubkey if pubkey is not None else 'Unknown'
            masternode_list_full_df.at[txid_vout, 'extAddress'] = extra.get('extAddress', 'Unknown')
            masternode_list_full_df.at[txid_vout, 'extP2P'] = extra.get('extP2P', 'Unknown')
            masternode_list_full_df.at[txid_vout, 'extKey'] = extra.get('extKey', 'Unknown')
        masternode_list_full_df['lastseentime'] = pd.to_numeric(masternode_list_full_df['lastseentime'], downcast='integer')
        masternode_list_full_df['lastpaidtime'] = pd.to_numeric(masternode_list_full_df['lastpaidtime'], downcast='integer')
        masternode_list_full_df['lastseentime'] = pd.to_datetime(masternode_list_full_df['lastseentime'], unit='s')
        masternode_list_full_df['lastpaidtime'] = pd.to_datetime(masternode_list_full_df['lastpaidtime'], unit='s')
        masternode_list_full_df['activeseconds'] = masternode_list_full_df['activeseconds'].astype(int)
        masternode_list_full_df['lastpaidblock'] = masternode_list_full_df['lastpaidblock'].astype(int)
        masternode_list_full_df['activedays'] = masternode_list_full_df['activeseconds'].apply(lambda x: float(x) / 86400.0)
        masternode_list_full_df['rank'] = masternode_list_full_df['rank'].astype(int, errors='ignore')
        masternode_list_full_df = masternode_list_full_df[masternode_list_full_df['supernode_status'].isin(['ENABLED', 'PRE_ENABLED'])]
        masternode_list_full_df__json = masternode_list_full_df.to_json(orient='index')
        return masternode_list_full_df, masternode_list_full_df__json
    else:
        error_message = "Masternode list command returning nothing-- pasteld probably just started and hasn't yet finished the mnsync process!"
        logger.error(error_message)
        raise ValueError(error_message)

async def check_supernode_list_func():
    if 'supernode_data' not in SUPERNODE_DATA_CACHE:
        SUPERNODE_DATA_CACHE['supernode_data'] = await fetch_supernode_list_data()
    return SUPERNODE_DATA_CACHE['supernode_data']

async def get_local_machine_supernode_data_func():
    local_machine_ip = get_external_ip_func()
    supernode_list_full_df, _ = await check_supernode_list_func()
    proper_port_number = statistics.mode([x.split(':')[1] for x in supernode_list_full_df['ipaddress:port'].values.tolist()])
    local_machine_ip_with_proper_port = local_machine_ip + ':' + proper_port_number
    local_machine_supernode_data = supernode_list_full_df[supernode_list_full_df['ipaddress:port'] == local_machine_ip_with_proper_port]
    if len(local_machine_supernode_data) == 0:
        logger.error('Local machine is not a supernode!')
        return 0, 0, 0, 0
    else:
        local_sn_rank = local_machine_supernode_data['rank'].values[0]
        local_sn_pastelid = local_machine_supernode_data['extKey'].values[0]
    return local_machine_supernode_data, local_sn_rank, local_sn_pastelid, local_machine_ip_with_proper_port

async def get_my_local_pastelid_func():
    local_machine_supernode_data, _, _, _ = await get_local_machine_supernode_data_func()
    my_local_pastelid = local_machine_supernode_data['extKey'].values.tolist()[0]    
    return my_local_pastelid

async def get_sn_data_from_pastelid_func(specified_pastelid):
    supernode_list_full_df, _ = await check_supernode_list_func()
    specified_machine_supernode_data = supernode_list_full_df[supernode_list_full_df['extKey'] == specified_pastelid]
    if len(specified_machine_supernode_data) == 0:
        logger.error('Specified machine is not a supernode!')
        return pd.DataFrame()
    else:
        return specified_machine_supernode_data

async def get_sn_data_from_sn_pubkey_func(specified_sn_pubkey):
    supernode_list_full_df, _ = await check_supernode_list_func()
    specified_machine_supernode_data = supernode_list_full_df[supernode_list_full_df['pubkey'] == specified_sn_pubkey]
    if len(specified_machine_supernode_data) == 0:
        logger.error('Specified machine is not a supernode!')
        return pd.DataFrame()
    else:
        return specified_machine_supernode_data

async def compress_data_with_zstd_func(input_data):
    if isinstance(input_data, str):
        input_data = input_data.encode('utf-8')
    zstd_compression_level = 22
    zstandard_compressor = zstd.ZstdCompressor(level=zstd_compression_level, write_content_size=True, write_checksum=True)
    zstd_compressed_data = zstandard_compressor.compress(input_data)
    zstd_compressed_data__base64_encoded = base64.b64encode(zstd_compressed_data).decode('utf-8')
    return zstd_compressed_data, zstd_compressed_data__base64_encoded

async def decompress_data_with_zstd_func(compressed_input_data):
    zstd_decompressor = zstd.ZstdDecompressor()
    zstd_decompressed_data = zstd_decompressor.decompress(compressed_input_data)
    return zstd_decompressed_data

async def list_sn_messages_func():
    datetime_cutoff_to_ignore_obsolete_messages = pd.to_datetime(datetime.now(timezone.utc) - timedelta(days=NUMBER_OF_DAYS_BEFORE_MESSAGES_ARE_CONSIDERED_OBSOLETE)).isoformat()
    try:
        supernode_list_df, _ = await check_supernode_list_func()
    except Exception as e:  # noqa: F841
        return None
    txid_vout_to_pastelid_dict = dict(zip(supernode_list_df.index, supernode_list_df['extKey']))
    async with db_code.Session() as db:
        # Retrieve messages from the database that meet the timestamp criteria
        query = await db.exec(
            select(db_code.Message)
            .where(db_code.Message.timestamp >= datetime_cutoff_to_ignore_obsolete_messages)
            .order_by(db_code.Message.timestamp.desc())
        )
        db_messages = query.all()
        existing_messages = {(message.sending_sn_pastelid, message.receiving_sn_pastelid, message.timestamp) for message in db_messages}
    # Retrieve new messages from the RPC interface
    new_messages = await masternode_message_list(rpc_connection)
    new_messages_data = []
    for message in new_messages:
        message_key = list(message.keys())[0]
        message = message[message_key]
        sending_sn_txid_vout = message['From']
        receiving_sn_txid_vout = message['To']
        sending_pastelid = txid_vout_to_pastelid_dict.get(sending_sn_txid_vout)
        receiving_pastelid = txid_vout_to_pastelid_dict.get(receiving_sn_txid_vout)
        if sending_pastelid is None or receiving_pastelid is None:
            # logger.warning(f"Skipping message due to missing PastelID for txid_vout: {sending_sn_txid_vout} or {receiving_sn_txid_vout}")
            continue
        message_timestamp = pd.to_datetime(datetime.fromtimestamp(message['Timestamp']).isoformat(), utc=True)
        # Check if the message already exists in the database
        if (sending_pastelid, receiving_pastelid, message_timestamp) in existing_messages:
            logger.info("Message already exists in the database. Skipping...")
            continue
        message_body = base64.b64decode(message['Message'].encode('utf-8'))
        verification_status = await verify_received_message_using_pastelid_func(message_body, sending_pastelid)
        decompressed_message = await decompress_data_with_zstd_func(message_body)
        decompressed_message = decompressed_message.decode('utf-8')
        try:
            message_dict = json.loads(decompressed_message)
        except json.JSONDecodeError as e:
            logger.error(f"Error parsing JSON: {e}")
            logger.error(f"Decompressed message: {decompressed_message}")
            continue
        if verification_status == 'OK':
            new_message = {
                'sending_sn_pastelid': sending_pastelid,
                'receiving_sn_pastelid': receiving_pastelid,
                'message_type': message_dict['message_type'],
                'message_body': decompressed_message,
                'signature': message_dict['signature'],
                'timestamp': message_timestamp,
                'sending_sn_txid_vout': sending_sn_txid_vout,
                'receiving_sn_txid_vout': receiving_sn_txid_vout
            }
            new_messages_data.append(new_message)
    combined_messages_df = pd.DataFrame(new_messages_data)
    if not combined_messages_df.empty:
        combined_messages_df['timestamp'] = pd.to_datetime(combined_messages_df['timestamp'], utc=True)
        combined_messages_df = combined_messages_df[combined_messages_df['timestamp'] >= datetime_cutoff_to_ignore_obsolete_messages]
        combined_messages_df = combined_messages_df.sort_values('timestamp', ascending=False)
    return combined_messages_df

def get_oldest_pastelid_file_pubkey(pastel_keys_dir, pastelid_override=None):
    if pastelid_override:
        return pastelid_override
    pastel_key_files = [f for f in os.listdir(pastel_keys_dir) if os.path.isfile(os.path.join(pastel_keys_dir, f))]
    if not pastel_key_files:
        raise ValueError("No PastelID files found in the specified directory.")
    return min(pastel_key_files, key=lambda f: os.path.getctime(os.path.join(pastel_keys_dir, f)))

async def sign_message_with_libpastelid(message: str, pastelid: str, passphrase: str):
    signed_message = pastel_signer.sign_with_pastel_id(message, pastelid, passphrase)
    return signed_message

async def verify_message_with_libpastelid(pastelid: str, message: str, signature: str):
    is_valid = pastel_signer.verify_with_pastel_id(message, signature, pastelid)
    return "OK" if is_valid else "Failed"

async def sign_message_with_pastelid_func(pastelid: str, message_to_sign: str, passphrase: str) -> str:
    if use_libpastelid_for_pastelid_sign_verify:
        return await sign_message_with_libpastelid(message_to_sign, pastelid, passphrase)
    else:
        results_dict = await pastelid_sign(rpc_connection, message_to_sign, pastelid, passphrase, 'ed448')
        return results_dict['signature']

async def verify_message_with_pastelid_func(pastelid: str, message_to_verify: str, pastelid_signature_on_message: str) -> str:
    if use_libpastelid_for_pastelid_sign_verify:
        return await verify_message_with_libpastelid(pastelid, message_to_verify, pastelid_signature_on_message)
    else:
        verification_result = await pastelid_verify(rpc_connection, message_to_verify, pastelid_signature_on_message, pastelid, 'ed448')
        return verification_result['verification']
    
async def parse_sn_messages_from_last_k_minutes_func(k=10, message_type='all'):
    messages_list_df = await list_sn_messages_func()
    messages_list_df__recent = messages_list_df[messages_list_df['timestamp'] > (datetime.now(timezone.utc) - timedelta(minutes=k))]
    if message_type == 'all':
        list_of_message_dicts = messages_list_df__recent[['message_body', 'message_type', 'sending_sn_pastelid', 'timestamp']].to_dict(orient='records')
    else:
        list_of_message_dicts = messages_list_df__recent[messages_list_df__recent['message_type'] == message_type][['message_body', 'message_type', 'sending_sn_pastelid', 'timestamp']].to_dict(orient='records')
    if len(list_of_message_dicts) > 0:
        return [
            {
                'message': json.loads(msg['message_body'])['message'],  # Extract the 'message' field as a string
                'message_type': msg['message_type'],
                'sending_sn_pastelid': msg['sending_sn_pastelid'],
                'timestamp': msg['timestamp'].isoformat()  # Convert timestamp to ISO format
            }
            for msg in list_of_message_dicts
        ]
    else:
        return []

async def verify_received_message_using_pastelid_func(message_received, sending_sn_pastelid):
    try:
        decompressed_message = await decompress_data_with_zstd_func(message_received)
        message_received_dict = json.loads(decompressed_message)
        raw_message = message_received_dict['message']
        signature = message_received_dict['signature']
        verification_status = await verify_message_with_pastelid_func(sending_sn_pastelid, raw_message, signature)
    except Exception as e:
        logger.error(f"Error verifying message: {e}")
        verification_status = f"Message verification failed: {str(e)}"
    return verification_status

async def send_message_to_sn_using_pastelid_func(message_to_send, message_type, receiving_sn_pastelid, pastelid_passphrase):
    local_machine_supernode_data, _, _, _ = await get_local_machine_supernode_data_func()
    sending_sn_pastelid = local_machine_supernode_data['extKey'].values.tolist()[0]
    sending_sn_pubkey = local_machine_supernode_data['pubkey'].values.tolist()[0]
    pastelid_signature_on_message = await sign_message_with_pastelid_func(sending_sn_pastelid, message_to_send, pastelid_passphrase)
    signed_message_to_send = json.dumps({
        'message': message_to_send,
        'message_type': message_type,
        'signature': pastelid_signature_on_message,
        'sending_sn_pubkey': sending_sn_pubkey
    }, ensure_ascii=False)    
    compressed_message, _ = await compress_data_with_zstd_func(signed_message_to_send.encode('utf-8'))
    compressed_message_base64 = base64.b64encode(compressed_message).decode('utf-8')
    specified_machine_supernode_data = await get_sn_data_from_pastelid_func(receiving_sn_pastelid)
    receiving_sn_pubkey = specified_machine_supernode_data['pubkey'].values.tolist()[0]
    logger.info(f"Now sending message to SN with PastelID: {receiving_sn_pastelid} and SN pubkey: {receiving_sn_pubkey}: {message_to_send}")
    await masternode_message_send(rpc_connection, receiving_sn_pubkey, compressed_message_base64)
    return signed_message_to_send, pastelid_signature_on_message

async def broadcast_message_to_list_of_sns_using_pastelid_func(message_to_send, message_type, list_of_receiving_sn_pastelids, pastelid_passphrase, verbose=0):
    local_machine_supernode_data, _, _, _ = await get_local_machine_supernode_data_func()
    sending_sn_pastelid = local_machine_supernode_data['extKey'].values.tolist()[0]
    sending_sn_pubkey = local_machine_supernode_data['pubkey'].values.tolist()[0]
    pastelid_signature_on_message = await sign_message_with_pastelid_func(sending_sn_pastelid, message_to_send, pastelid_passphrase)
    signed_message_to_send = json.dumps({
        'message': message_to_send,
        'message_type': message_type,
        'signature': pastelid_signature_on_message,
        'sending_sn_pubkey': sending_sn_pubkey
    }, ensure_ascii=False)
    compressed_message, _ = await compress_data_with_zstd_func(signed_message_to_send.encode('utf-8'))
    compressed_message_base64 = base64.b64encode(compressed_message).decode('utf-8')
    if verbose:
        logger.info(f"Now sending message to list of {len(list_of_receiving_sn_pastelids)} SNs: `{message_to_send}`")        
    async def send_message(receiving_sn_pastelid):
        current_receiving_sn_pubkey = (await get_sn_data_from_pastelid_func(receiving_sn_pastelid))['pubkey'].values.tolist()[0]
        await masternode_message_send(rpc_connection, current_receiving_sn_pubkey, compressed_message_base64)
    await asyncio.gather(*[send_message(pastelid) for pastelid in list_of_receiving_sn_pastelids])
    return signed_message_to_send

async def broadcast_message_to_all_sns_using_pastelid_func(message_to_send, message_type, pastelid_passphrase, verbose=0):
    local_machine_supernode_data, _, _, _ = await get_local_machine_supernode_data_func()
    sending_sn_pastelid = local_machine_supernode_data['extKey'].values.tolist()[0]
    sending_sn_pubkey = local_machine_supernode_data['pubkey'].values.tolist()[0]
    pastelid_signature_on_message = await sign_message_with_pastelid_func(sending_sn_pastelid, message_to_send, pastelid_passphrase)
    signed_message_to_send = json.dumps({
        'message': message_to_send,
        'message_type': message_type,
        'signature': pastelid_signature_on_message,
        'sending_sn_pubkey': sending_sn_pubkey
    }, ensure_ascii=False)
    compressed_message, _ = await compress_data_with_zstd_func(signed_message_to_send.encode('utf-8'))
    compressed_message_base64 = base64.b64encode(compressed_message).decode('utf-8')
    list_of_receiving_sn_pastelids = (await check_supernode_list_func())[0]['extKey'].values.tolist()
    if verbose:
        logger.info(f"Now sending message to ALL {len(list_of_receiving_sn_pastelids)} SNs: `{message_to_send}`")        
    async def send_message(receiving_sn_pastelid):
        current_receiving_sn_pubkey = (await get_sn_data_from_pastelid_func(receiving_sn_pastelid))['pubkey'].values.tolist()[0]
        await masternode_message_send(rpc_connection, current_receiving_sn_pubkey, compressed_message_base64)
    await asyncio.gather(*[send_message(pastelid) for pastelid in list_of_receiving_sn_pastelids])
    return signed_message_to_send

async def get_supernode_model_menu(supernode_url):
    try:
        async with httpx.AsyncClient(timeout=Timeout(INDIVIDUAL_SUPERNODE_MODEL_MENU_REQUEST_TIMEOUT_PERIOD_IN_SECONDS)) as client:
            response = await client.get(f"{supernode_url}/get_inference_model_menu")
            response.raise_for_status()
            model_menu = response.json()
            return model_menu
    except Exception as e:
        logger.error(f"Error retrieving model menu from Supernode URL: {supernode_url}: {e}")
        return None

def is_model_supported(model_menu, desired_model_canonical_string, desired_model_inference_type_string, desired_model_parameters_json):
    if model_menu:
        desired_parameters = json.loads(desired_model_parameters_json)
        model_names = [model["model_name"] for model in model_menu["models"]]
        best_match = process.extractOne(desired_model_canonical_string, model_names)
        if best_match is not None and best_match[1] >= 95:
            matched_model = next(model for model in model_menu["models"] if model["model_name"] == best_match[0])
            if "supported_inference_type_strings" in matched_model and "model_parameters" in matched_model:
                if desired_model_inference_type_string in matched_model["supported_inference_type_strings"]:
                    for desired_param, desired_value in desired_parameters.items():
                        param_found = False
                        for param in matched_model["model_parameters"]:
                            if param["name"] == desired_param:
                                if "type" in param:
                                    if param["type"] == "int" and isinstance(int(desired_value), int):
                                        param_found = True
                                    elif param["type"] == "float" and isinstance(float(desired_value), float):
                                        param_found = True
                                    elif param["type"] == "string" and isinstance(str(desired_value), str):
                                        if "options" in param and desired_value in param["options"]:
                                            param_found = True
                                        elif "options" not in param:
                                            param_found = True
                                else:
                                    param_found = True
                                break
                        if not param_found:
                            return False
                    return True
            return False
    return False
        
async def broadcast_message_to_n_closest_supernodes_to_given_pastelid(input_pastelid, message_body, message_type):
    blacklist_path = Path('supernode_inference_ip_blacklist.txt')
    blacklisted_ips = set()
    if blacklist_path.exists():
        with blacklist_path.open('r') as blacklist_file:
            blacklisted_ips = {line.strip() for line in blacklist_file if line.strip()}
    else:
        logger.info("Blacklist file not found. Proceeding without blacklist filtering.")
    supernode_list_df, _ = await check_supernode_list_func()
    n = 4
    local_machine_supernode_data, _, _, _ = await get_local_machine_supernode_data_func()
    local_sn_pastelid = local_machine_supernode_data['extKey'].values.tolist()[0]
    message_body_dict = json.loads(message_body)
    desired_model_canonical_string = message_body_dict.get('requested_model_canonical_string')
    desired_model_inference_type_string = message_body_dict.get('model_inference_type_string')
    desired_model_parameters_json = base64.b64decode(message_body_dict.get('model_parameters_json_b64')).decode('utf-8')
    async def is_model_supported_async(supernode_ip_and_port, model_canonical_string, model_inference_type_string, model_parameters_json):
        supernode_ip = supernode_ip_and_port.split(':')[0]
        supernode_url = f"http://{supernode_ip}:7123"
        if supernode_ip in blacklisted_ips:
            return False
        model_menu = await get_supernode_model_menu(supernode_url)
        return is_model_supported(model_menu, model_canonical_string, model_inference_type_string, model_parameters_json)
    supported_supernodes_coroutines = [
        is_model_supported_async(row['ipaddress:port'], desired_model_canonical_string, desired_model_inference_type_string, desired_model_parameters_json)
        for _, row in supernode_list_df.iterrows()
    ]
    supported_supernodes_mask = await asyncio.gather(*supported_supernodes_coroutines)
    supported_supernodes = supernode_list_df[supported_supernodes_mask]
    supported_supernodes_minus_this_supernode = supported_supernodes[supported_supernodes['extKey'] != local_sn_pastelid]
    if len(supported_supernodes_minus_this_supernode) == 0:
        logger.error(f"No other supported supernodes found for the desired model: {desired_model_canonical_string} with inference type: {desired_model_inference_type_string} and parameters: {desired_model_parameters_json}")
        supported_supernodes_minus_this_supernode = supernode_list_df[supernode_list_df['extKey'] != local_sn_pastelid]
        logger.info("We had to choose audit supernodes which cannot process the request themselves if needed!")
    closest_supernodes = await get_n_closest_supernodes_to_pastelid_urls(n, input_pastelid, supported_supernodes_minus_this_supernode)
    list_of_supernode_pastelids = [x[1] for x in closest_supernodes]
    list_of_supernode_urls = [x[0] for x in closest_supernodes]
    list_of_supernode_ips = [x.split('//')[1].split(':')[0] for x in list_of_supernode_urls]
    signed_message = await broadcast_message_to_list_of_sns_using_pastelid_func(message_body, message_type, list_of_supernode_pastelids, LOCAL_PASTEL_ID_PASSPHRASE)
    logger.info(f"Broadcasted a {message_type} to {len(list_of_supernode_pastelids)} closest supernodes to PastelID: {input_pastelid} (with Supernode IPs of {list_of_supernode_ips}): {message_body}")
    return signed_message

async def retry_on_database_locked(func, *args, max_retries=5, initial_delay=1, backoff_factor=2, jitter_factor=0.1, **kwargs):
    delay = initial_delay
    for attempt in range(max_retries):
        try:
            result = func(*args, **kwargs)
            if asyncio.iscoroutine(result):
                return await result
            return result
        except OperationalError as e:
            if "database is locked" in str(e) and attempt < max_retries - 1:
                jitter = random.uniform(1 - jitter_factor, 1 + jitter_factor)
                delay *= backoff_factor * jitter
                logger.warning(f"Database locked. Retrying in {delay:.2f} second(s)...")
                await asyncio.sleep(delay)
            else:
                raise
        except InvalidRequestError as e:
            if "This Session's transaction has been rolled back due to a previous exception during flush" in str(e):
                logger.warning("Session transaction has been rolled back. Retrying...")
                db_session = args[0]  # Assuming the first argument is the db_session
                await db_session.rollback()
            else:
                raise
            
async def check_if_record_exists(db_session, model, **kwargs):
    existing_record = await db_session.execute(
        select(model).filter_by(**kwargs)
    )
    return existing_record.scalars().first()

async def process_broadcast_messages(message, db_session):
    try:
        message_body = json.loads(message.message_body)
        if message.message_type == 'inference_request_response_announcement_message':
            response_data = json.loads(message_body['message'])
            existing_request = await check_if_record_exists(
                db_session, db_code.InferenceAPIUsageRequest,
                sha3_256_hash_of_inference_request_fields=response_data['sha3_256_hash_of_inference_request_fields']
            )
            existing_response = await check_if_record_exists(
                db_session, db_code.InferenceAPIUsageResponse,
                sha3_256_hash_of_inference_request_response_fields=response_data['sha3_256_hash_of_inference_request_response_fields']
            )
            if not existing_request and not existing_response:
                usage_request = db_code.InferenceAPIUsageRequest(**response_data)
                usage_response = db_code.InferenceAPIUsageResponse(**response_data)
                await asyncio.sleep(random.uniform(0.1, 0.5))  # Random sleep before DB operations
                await retry_on_database_locked(db_session.add, usage_request)
                await retry_on_database_locked(db_session.add, usage_response)
                await retry_on_database_locked(db_session.commit)
                await retry_on_database_locked(db_session.refresh, usage_request)
                await retry_on_database_locked(db_session.refresh, usage_response)
            else:
                logger.info("Skipping insertion as the record already exists.")
        elif message.message_type == 'inference_request_result_announcement_message':
            result_data = json.loads(message_body['message'])
            existing_result = await check_if_record_exists(
                db_session, db_code.InferenceAPIOutputResult,
                sha3_256_hash_of_inference_result_fields=result_data['sha3_256_hash_of_inference_result_fields']
            )
            if not existing_result:
                output_result = db_code.InferenceAPIOutputResult(**result_data)
                await asyncio.sleep(random.uniform(0.1, 0.5))  # Random sleep before DB operations
                await retry_on_database_locked(db_session.add, output_result)
                await retry_on_database_locked(db_session.commit)
                await retry_on_database_locked(db_session.refresh, output_result)
            else:
                logger.info("Skipping insertion as the result record already exists.")
    except Exception as e:   # noqa: F841
        traceback.print_exc()
        
async def monitor_new_messages():
    last_processed_timestamp = None
    while True:
        try:
            async with db_code.Session() as db:
                if last_processed_timestamp is None:
                    query = await db.exec(select(db_code.Message.timestamp).order_by(db_code.Message.timestamp.desc()).limit(1))
                    last_processed_timestamp_raw = query.one_or_none()
                    if last_processed_timestamp_raw is None:
                        last_processed_timestamp = pd.Timestamp.min.tz_localize('UTC')
                    else:
                        last_processed_timestamp = pd.Timestamp(last_processed_timestamp_raw).tz_localize('UTC').tz_convert('UTC')
                try:
                    new_messages_df = await list_sn_messages_func()
                except Exception as e:   # noqa: F841
                    new_messages_df = None
                if new_messages_df is not None and not new_messages_df.empty:
                    new_messages_df['timestamp'] = pd.to_datetime(new_messages_df['timestamp'], utc=True)
                    new_messages_df = new_messages_df[new_messages_df['timestamp'] > last_processed_timestamp]
                    if not new_messages_df.empty:
                        for _, message in new_messages_df.iterrows():
                            query = await db.exec(
                                select(db_code.Message).where(
                                    db_code.Message.sending_sn_pastelid == message['sending_sn_pastelid'],
                                    db_code.Message.receiving_sn_pastelid == message['receiving_sn_pastelid'],
                                    db_code.Message.timestamp == message['timestamp']
                                )
                            )
                            existing_messages = query.all()
                            if len(existing_messages) > 1:
                                logger.error(f"Multiple rows found for message: {message}")
                                continue
                            elif len(existing_messages) == 1:
                                continue
                            log_action_with_payload("received new", "message", message)
                            last_processed_timestamp = message['timestamp']
                            sending_sn_pastelid = message['sending_sn_pastelid']
                            receiving_sn_pastelid = message['receiving_sn_pastelid']
                            message_size_bytes = len(message['message_body'].encode('utf-8'))
                            await asyncio.sleep(random.uniform(0.1, 0.5))
                            query = await db.exec(
                                select(db_code.MessageSenderMetadata).where(db_code.MessageSenderMetadata.sending_sn_pastelid == sending_sn_pastelid)
                            )
                            sender_metadata = query.one_or_none()
                            if sender_metadata:
                                sender_metadata.total_messages_sent += 1
                                sender_metadata.total_data_sent_bytes += message_size_bytes
                                sender_metadata.sending_sn_txid_vout = message['sending_sn_txid_vout']
                                sender_metadata.sending_sn_pubkey = message['signature']
                            else:
                                sender_metadata = db_code.MessageSenderMetadata(
                                    sending_sn_pastelid=sending_sn_pastelid,
                                    total_messages_sent=1,
                                    total_data_sent_bytes=message_size_bytes,
                                    sending_sn_txid_vout=message['sending_sn_txid_vout'],
                                    sending_sn_pubkey=message['signature']
                                )
                                db.add(sender_metadata)
                            query = await db.exec(
                                select(db_code.MessageReceiverMetadata).where(db_code.MessageReceiverMetadata.receiving_sn_pastelid == receiving_sn_pastelid)
                            )
                            receiver_metadata = query.one_or_none()
                            if receiver_metadata:
                                receiver_metadata.total_messages_received += 1
                                receiver_metadata.total_data_received_bytes += message_size_bytes
                                receiver_metadata.receiving_sn_txid_vout = message['receiving_sn_txid_vout']
                            else:
                                receiver_metadata = db_code.MessageReceiverMetadata(
                                    receiving_sn_pastelid=receiving_sn_pastelid,
                                    total_messages_received=1,
                                    total_data_received_bytes=message_size_bytes,
                                    receiving_sn_txid_vout=message['receiving_sn_txid_vout']
                                )
                                db.add(receiver_metadata)
                            query = await db.exec(
                                select(db_code.MessageSenderReceiverMetadata).where(
                                    db_code.MessageSenderReceiverMetadata.sending_sn_pastelid == sending_sn_pastelid,
                                    db_code.MessageSenderReceiverMetadata.receiving_sn_pastelid == receiving_sn_pastelid
                                )
                            )
                            sender_receiver_metadata = query.one_or_none()
                            if sender_receiver_metadata:
                                sender_receiver_metadata.total_messages += 1
                                sender_receiver_metadata.total_data_bytes += message_size_bytes
                            else:
                                sender_receiver_metadata = db_code.MessageSenderReceiverMetadata(
                                    sending_sn_pastelid=sending_sn_pastelid,
                                    receiving_sn_pastelid=receiving_sn_pastelid,
                                    total_messages=1,
                                    total_data_bytes=message_size_bytes
                                )
                                db.add(sender_receiver_metadata)
                            new_messages = [
                                db_code.Message(
                                    sending_sn_pastelid=row['sending_sn_pastelid'],
                                    receiving_sn_pastelid=row['receiving_sn_pastelid'],
                                    message_type=row['message_type'],
                                    message_body=row['message_body'],
                                    signature=row['signature'],
                                    timestamp=row['timestamp'],
                                    sending_sn_txid_vout=row['sending_sn_txid_vout'],
                                    receiving_sn_txid_vout=row['receiving_sn_txid_vout']
                                )
                                for _, row in new_messages_df.iterrows()
                            ]
                            await asyncio.sleep(random.uniform(0.1, 0.5))
                            await retry_on_database_locked(db.add_all, new_messages)
                            await retry_on_database_locked(db.commit)
                            query = await db.exec(
                                select(
                                    func.count(db_code.Message.id),
                                    func.count(func.distinct(db_code.Message.sending_sn_pastelid)),
                                    func.count(func.distinct(db_code.Message.receiving_sn_pastelid))
                                )
                            )
                            total_messages, total_senders, total_receivers = query.one()
                            query = await db.exec(select(db_code.MessageMetadata).order_by(db_code.MessageMetadata.timestamp.desc()).limit(1))
                            message_metadata = query.one_or_none()
                            if message_metadata:
                                message_metadata.total_messages = total_messages
                                message_metadata.total_senders = total_senders
                                message_metadata.total_receivers = total_receivers
                            else:
                                message_metadata = db_code.MessageMetadata(
                                    total_messages=total_messages,
                                    total_senders=total_senders,
                                    total_receivers=total_receivers
                                )
                                db.add(message_metadata)
                            await asyncio.sleep(random.uniform(0.1, 0.5))
                            await retry_on_database_locked(db.commit)
                            processing_tasks = [
                                process_broadcast_messages(message, db)
                                for message in new_messages
                            ]
                            await asyncio.gather(*processing_tasks)
                await asyncio.sleep(5)
        except Exception as e:
            logger.error(f"Error while monitoring new messages: {str(e)}")
            traceback.print_exc()
            await asyncio.sleep(5)
        finally:
            await asyncio.sleep(5)
            
async def get_list_of_credit_pack_ticket_txids_from_pastelid(pastelid: str) -> list:
    ticket_type_identifier = "INFERENCE_API_CREDIT_PACK_TICKET"
    starting_block_height = 700000
    list_of_ticket_data_dicts = await tickets_list_contract(rpc_connection, ticket_type_identifier, starting_block_height)
    list_of_ticket_internal_data_dicts = [x['ticket']['contract_ticket'] for x in list_of_ticket_data_dicts]
    list_of_ticket_input_data_json_strings_for_tickets_where_given_pastelid_is_in_list_of_allowed_users = [json.dumps(x['ticket_input_data_dict']) for x in list_of_ticket_internal_data_dicts if pastelid in x['ticket_input_data_dict']['credit_pack_purchase_request_dict']['list_of_authorized_pastelids_allowed_to_use_credit_pack']] 
    list_of_ticket_input_data_fully_parsed_sha3_256_hashes = [compute_fully_parsed_json_sha3_256_hash(x)[0] for x in list_of_ticket_input_data_json_strings_for_tickets_where_given_pastelid_is_in_list_of_allowed_users]
    list_of_credit_pack_ticket_data = [None] * len(list_of_ticket_input_data_fully_parsed_sha3_256_hashes)
    async def fetch_and_store_ticket_data(index, hash):
        list_of_credit_pack_ticket_data[index] = await generic_tickets_find(rpc_connection, hash)
    await asyncio.gather( *[fetch_and_store_ticket_data(i, hash) for i, hash in enumerate(list_of_ticket_input_data_fully_parsed_sha3_256_hashes)])
    list_of_credit_pack_ticket_registration_txids = [x['txid'] for x in list_of_credit_pack_ticket_data if x is not None]
    return list_of_credit_pack_ticket_registration_txids, list_of_credit_pack_ticket_data

async def create_user_message(from_pastelid: str, to_pastelid: str, message_body: str, message_signature: str) -> dict:
    async with db_code.Session() as db:
        user_message = db_code.UserMessage(from_pastelid=from_pastelid, to_pastelid=to_pastelid, message_body=message_body, message_signature=message_signature)
        db.add(user_message)
        db.commit()
        db.refresh(user_message)
        user_message_dict = user_message.model_dump()
        user_message_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in user_message_dict.items()}
        return user_message.model_dump()

async def create_supernode_user_message(sending_sn_pastelid: str, receiving_sn_pastelid: str, user_message_data: dict) -> dict:
    async with db_code.Session() as db:
        supernode_user_message = db_code.SupernodeUserMessage(
            message_body=user_message_data['message_body'],
            message_type="user_message",
            sending_sn_pastelid=sending_sn_pastelid,
            receiving_sn_pastelid=receiving_sn_pastelid,
            signature=user_message_data['message_signature'],
            timestamp=datetime.utcnow(),
            user_message_id=user_message_data['id']
        )
        db.add(supernode_user_message)
        db.commit()
        db.refresh(supernode_user_message)
        supernode_user_message_dict = supernode_user_message.model_dump()
        supernode_user_message_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in supernode_user_message_dict.items()}
        return supernode_user_message_dict

async def send_user_message_via_supernodes(from_pastelid: str, to_pastelid: str, message_body: str, message_signature: str) -> dict:
    user_message_data = await create_user_message(from_pastelid, to_pastelid, message_body, message_signature)
    local_machine_supernode_data, _, _, _ = await get_local_machine_supernode_data_func()
    sending_sn_pastelid = local_machine_supernode_data['extKey'][0]  # Assuming this is a list
    # Find the 3 closest Supernodes to the receiving end user's PastelID
    supernode_list_df, _ = await check_supernode_list_func()
    closest_supernodes = await get_n_closest_supernodes_to_pastelid_urls(3, to_pastelid, supernode_list_df)
    if not closest_supernodes:
        raise ValueError(f"No Supernodes found for PastelID: {to_pastelid}.")
    # Create a list to store the message_dicts for each Supernode
    message_dicts = []
    # Send the message to the 3 closest Supernodes in parallel using asyncio.gather
    send_tasks = []
    for closest_supernode_url, receiving_sn_pastelid in closest_supernodes:
        # Now that we have user_message_data and the receiving Supernode PastelID, let's create the supernode user message.
        supernode_user_message_data = await create_supernode_user_message(sending_sn_pastelid, receiving_sn_pastelid, user_message_data)
        # Preparing the message to be sent to the supernode.
        signed_message_to_send = json.dumps({
            'message': user_message_data['message_body'],
            'message_type': 'user_message',
            'signature': user_message_data['message_signature'],
            'from_pastelid': user_message_data['from_pastelid'],
            'to_pastelid': user_message_data['to_pastelid']
        }, ensure_ascii=False)
        # Send the message to the receiving Supernode.
        send_task = asyncio.create_task(send_message_to_sn_using_pastelid_func(signed_message_to_send, 'user_message', receiving_sn_pastelid, LOCAL_PASTEL_ID_PASSPHRASE))
        send_tasks.append(send_task)
    # Wait for all send tasks to complete
    send_results = await asyncio.gather(*send_tasks)
    # Create the message_dict for each Supernode
    for send_result in send_results:
        signed_message, pastelid_signature_on_message = send_result
        message_dict = {
            "message": user_message_data['message_body'],  # The content of the message
            "message_type": "user_message",  # Static type as per your design
            "sending_sn_pastelid": sending_sn_pastelid,  # From local machine supernode data
            "timestamp": datetime.now(timezone.utc).isoformat(),  # Current UTC timestamp
            "id": supernode_user_message_data['id'],  # ID from the supernode user message record
            "signature": pastelid_signature_on_message,
            "user_message": {
                # Details from the user_message_data
                "from_pastelid": from_pastelid,
                "to_pastelid": to_pastelid,
                "message_body": message_body,
                "message_signature": user_message_data['message_signature'],
                "id": user_message_data['id'],  # Assuming these fields are included in your dictionary
                "timestamp": user_message_data['timestamp']
            }
        }
        message_dicts.append(message_dict)
    return message_dicts

async def process_received_user_message(supernode_user_message: db_code.SupernodeUserMessage):
    async with db_code.Session() as db:
        query = await db.exec(select(db_code.UserMessage).where(db_code.UserMessage.id == supernode_user_message.user_message_id))
        user_message = query.one_or_none()
        if user_message:
            verification_status = await verify_message_with_pastelid_func(user_message.from_pastelid, user_message.message_body, user_message.message_signature)
            if verification_status == 'OK':
                # Process the user message (e.g., store it, forward it to the recipient, etc.)
                logger.info(f"Received and verified user message from {user_message.from_pastelid} to {user_message.to_pastelid}")
            else:
                logger.warning(f"Received user message from {user_message.from_pastelid} to {user_message.to_pastelid}, but verification failed")
        else:
            logger.warning(f"Received SupernodeUserMessage (id: {supernode_user_message.id}), but the associated UserMessage was not found")

async def get_user_messages_for_pastelid(pastelid: str) -> List[db_code.UserMessage]:
    async with db_code.Session() as db:
        query = await db.exec(select(db_code.UserMessage).where((db_code.UserMessage.from_pastelid == pastelid) | (db_code.UserMessage.to_pastelid == pastelid)))
        user_messages = query.all()
        return user_messages
            
#________________________________________________________________________________________________________________            
# Credit pack related service functions:

async def get_credit_pack_purchase_request(sha3_256_hash_of_credit_pack_purchase_request_fields: str) -> db_code.CreditPackPurchaseRequest:
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequest).where(db_code.CreditPackPurchaseRequest.sha3_256_hash_of_credit_pack_purchase_request_fields == sha3_256_hash_of_credit_pack_purchase_request_fields)
            )
            return result.one_or_none()
    except Exception as e:
        logger.error(f"Error getting credit pack purchase request: {str(e)}")
        raise

async def save_credit_pack_purchase_request(credit_pack_purchase_request: db_code.CreditPackPurchaseRequest) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_request)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase request: {str(e)}")
        raise

async def get_credit_pack_purchase_request_response(sha3_256_hash_of_credit_pack_purchase_request_response_fields: str) -> db_code.CreditPackPurchaseRequestResponse:
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestResponse).where(db_code.CreditPackPurchaseRequestResponse.sha3_256_hash_of_credit_pack_purchase_request_response_fields == sha3_256_hash_of_credit_pack_purchase_request_response_fields)
            )
            return result.one_or_none()
    except Exception as e:
        logger.error(f"Error getting credit pack purchase request response: {str(e)}")
        raise
    
async def get_credit_pack_purchase_request_response_from_request_hash(sha3_256_hash_of_credit_pack_purchase_request_fields: str) -> db_code.CreditPackPurchaseRequestResponse:
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestResponse).where(db_code.CreditPackPurchaseRequestResponse.sha3_256_hash_of_credit_pack_purchase_request_fields == sha3_256_hash_of_credit_pack_purchase_request_fields)
            )
            return result.one_or_none()
    except Exception as e:
        logger.error(f"Error getting credit pack purchase request response: {str(e)}")
        raise
    
async def get_credit_pack_purchase_request_from_response(response: db_code.CreditPackPurchaseRequestResponse) -> Optional[db_code.CreditPackPurchaseRequest]:
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequest).where(db_code.CreditPackPurchaseRequest.sha3_256_hash_of_credit_pack_purchase_request_fields == response.sha3_256_hash_of_credit_pack_purchase_request_fields)
            )
            return result.one_or_none()
    except Exception as e:
        logger.error(f"Error getting credit pack purchase request response: {str(e)}")
        raise    
    
async def save_credit_pack_purchase_request_response(credit_pack_purchase_request_response: db_code.CreditPackPurchaseRequestResponse) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_request_response)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase request response: {str(e)}")
        raise

async def get_credit_pack_purchase_request_rejection(sha3_256_hash_of_credit_pack_purchase_request_fields: str) -> db_code.CreditPackPurchaseRequestRejection:
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestRejection).where(db_code.CreditPackPurchaseRequestRejection.sha3_256_hash_of_credit_pack_purchase_request_fields == sha3_256_hash_of_credit_pack_purchase_request_fields)
            )
            return result.one_or_none()
    except Exception as e:
        logger.error(f"Error getting credit pack purchase request rejection: {str(e)}")
        raise

async def save_credit_pack_purchase_request_rejection(credit_pack_purchase_request_rejection: db_code.CreditPackPurchaseRequestRejection) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_request_rejection)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase request rejection: {str(e)}")
        raise

async def get_credit_pack_purchase_request_response_termination(sha3_256_hash_of_credit_pack_purchase_request_fields: str) -> db_code.CreditPackPurchaseRequestResponseTermination:
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestResponseTermination).where(db_code.CreditPackPurchaseRequestResponseTermination.sha3_256_hash_of_credit_pack_purchase_request_fields == sha3_256_hash_of_credit_pack_purchase_request_fields)
            )
            return result.one_or_none()
    except Exception as e:
        logger.error(f"Error getting credit pack purchase request response termination: {str(e)}")
        raise

async def save_credit_pack_purchase_request_response_termination(credit_pack_purchase_request_response_termination: db_code.CreditPackPurchaseRequestResponseTermination) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_request_response_termination)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase request response termination: {str(e)}")
        raise

async def get_credit_pack_purchase_request_confirmation(sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str) -> db_code.CreditPackPurchaseRequestConfirmation:
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestConfirmation).where(db_code.CreditPackPurchaseRequestConfirmation.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields == sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields)
            )
            return result.one_or_none()
    except Exception as e:
        logger.error(f"Error getting credit pack purchase request confirmation: {str(e)}")
        raise

async def get_credit_pack_purchase_request_confirmation_from_request_hash(sha3_256_hash_of_credit_pack_purchase_request_fields: str) -> db_code.CreditPackPurchaseRequestConfirmation:
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestConfirmation).where(db_code.CreditPackPurchaseRequestConfirmation.sha3_256_hash_of_credit_pack_purchase_request_fields == sha3_256_hash_of_credit_pack_purchase_request_fields)
            )
            return result.one_or_none()
    except Exception as e:
        logger.error(f"Error getting credit pack purchase request confirmation: {str(e)}")
        raise

async def save_credit_pack_purchase_request_confirmation(credit_pack_purchase_request_confirmation: db_code.CreditPackPurchaseRequestConfirmation) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_request_confirmation)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase request confirmation: {str(e)}")
        raise

async def get_credit_pack_purchase_request_confirmation_response(sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields: str) -> db_code.CreditPackPurchaseRequestConfirmationResponse:
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestConfirmationResponse).where(db_code.CreditPackPurchaseRequestConfirmationResponse.sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields == sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields)
            )
            return result.one_or_none()
    except Exception as e:
        logger.error(f"Error getting credit pack purchase request confirmation response: {str(e)}")
        raise
            
async def save_credit_pack_purchase_request_confirmation_response(credit_pack_purchase_request_confirmation_response: db_code.CreditPackPurchaseRequestConfirmationResponse) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_request_confirmation_response)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase request confirmation response: {str(e)}")
        raise
    
async def save_credit_pack_purchase_preliminary_price_quote(credit_pack_purchase_preliminary_price_quote: db_code.CreditPackPurchaseRequestPreliminaryPriceQuote) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_preliminary_price_quote)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase preliminary price quote: {str(e)}")
        raise

async def save_credit_pack_purchase_price_agreement_request(credit_pack_purchase_price_agreement_request: db_code.CreditPackPurchasePriceAgreementRequest) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_price_agreement_request)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase price agreement request: {str(e)}")
        raise

async def save_credit_pack_purchase_price_agreement_request_response(credit_pack_purchase_price_agreement_request_response: db_code.CreditPackPurchasePriceAgreementRequestResponse) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_price_agreement_request_response)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase price agreement request response: {str(e)}")
        raise    
    
async def save_credit_pack_purchase_request_status_check(credit_pack_purchase_request_status_check: db_code.CreditPackPurchaseRequestStatus) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_request_status_check)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase request status check: {str(e)}")
        raise    

async def save_credit_pack_purchase_request_final_response(response: db_code.CreditPackPurchaseRequestResponse) -> None:
    try:
        async with db_code.Session() as db:
            db.add(response)
            await db.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase request final response: {str(e)}")
        raise

async def save_credit_pack_storage_completion_announcement(response: db_code.CreditPackPurchaseRequestConfirmationResponse) -> None:
    try:
        async with db_code.Session() as db:
            db.add(response)
            await db.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack storage completion announcement: {str(e)}")
        raise
        
async def save_credit_pack_purchase_completion_announcement(confirmation: db_code.CreditPackPurchaseRequestConfirmation) -> None:
    try:
        async with db_code.Session() as db:
            db.add(confirmation)
            await db.commit()
    except Exception as e:
        logger.error(f"Error storing credit pack purchase completion announcement: {str(e)}")
        raise
    
async def save_credit_pack_storage_retry_request(response: db_code.CreditPackPurchaseRequestConfirmationResponse) -> None:
    try:
        async with db_code.Session() as db:
            db.add(response)
            await db.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack storage retry request: {str(e)}")
        raise
    
async def save_credit_pack_storage_retry_request_response(response: db_code.CreditPackPurchaseRequestConfirmationResponse) -> None:
    try:
        async with db_code.Session() as db:
            db.add(response)
            await db.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack storage retry request response: {str(e)}")
        raise

async def save_credit_pack_storage_retry_completion_announcement(response: db_code.CreditPackPurchaseRequestConfirmationResponse) -> None:
    try:
        async with db_code.Session() as db:
            db.add(response)
            await db.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack storage retry completion announcement: {str(e)}")
        raise
            
async def check_credit_pack_purchase_request_status(credit_pack_purchase_request: db_code.CreditPackPurchaseRequest) -> str:
    async with db_code.Session() as db:
        response = await db.exec(
            select(db_code.CreditPackPurchaseRequestResponse).where(db_code.CreditPackPurchaseRequestResponse.sha3_256_hash_of_credit_pack_purchase_request_fields == credit_pack_purchase_request.sha3_256_hash_of_credit_pack_purchase_request_fields)
        )
        response = response.one_or_none()
        if response is None:
            return "pending"
        else:
            confirmation = await db.exec(
                select(db_code.CreditPackPurchaseRequestConfirmation).where(db_code.CreditPackPurchaseRequestConfirmation.sha3_256_hash_of_credit_pack_purchase_request_response_fields == response.sha3_256_hash_of_credit_pack_purchase_request_response_fields)
            )
            confirmation = confirmation.one_or_none()
            if confirmation is None:
                return "approved"
            else:
                confirmation_response = await db.exec(
                    select(db_code.CreditPackPurchaseRequestConfirmationResponse).where(db_code.CreditPackPurchaseRequestConfirmationResponse.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields == confirmation.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields)
                )
                confirmation_response = confirmation_response.one_or_none()
                if confirmation_response is None:
                    return "confirmed"
                else:
                    if confirmation_response.credit_pack_confirmation_outcome_string == "success":
                        return "completed"
                    else:
                        return "failed"
                    
async def calculate_preliminary_psl_price_per_credit():
    try:
        # Get the current PSL market price in USD
        psl_price_usd = await fetch_current_psl_market_price()
        # Calculate the cost per credit in USD, considering the profit margin
        cost_per_credit_usd = TARGET_VALUE_PER_CREDIT_IN_USD / (1 - TARGET_PROFIT_MARGIN)
        # Convert the cost per credit from USD to PSL
        cost_per_credit_psl = cost_per_credit_usd / psl_price_usd
        # Round the cost per credit to the nearest 0.1
        rounded_cost_per_credit_psl = round(cost_per_credit_psl, 1)
        logger.info(f"Calculated preliminary price per credit: {rounded_cost_per_credit_psl:,.1f} PSL")
        return rounded_cost_per_credit_psl
    except (ValueError, ZeroDivisionError) as e:
        logger.error(f"Error calculating preliminary price per credit: {str(e)}")
        traceback.print_exc()
        raise

async def determine_agreement_with_proposed_price(proposed_psl_price_per_credit: float) -> bool:
    try:
        # Calculate the local preliminary price per credit
        local_price_per_credit = await calculate_preliminary_psl_price_per_credit()
        # Calculate the acceptable price range (within 10% of the local price)
        min_acceptable_price = local_price_per_credit * (1.0 - MAXIMUM_LOCAL_CREDIT_PRICE_DIFFERENCE_TO_ACCEPT_CREDIT_PRICING)
        max_acceptable_price = local_price_per_credit * (1.0 + MAXIMUM_LOCAL_CREDIT_PRICE_DIFFERENCE_TO_ACCEPT_CREDIT_PRICING)
        # Determine if the proposed price is within the acceptable range
        agree_with_proposed_price = min_acceptable_price <= proposed_psl_price_per_credit <= max_acceptable_price
        logger.info(f"Proposed price per credit: {proposed_psl_price_per_credit:,.1f} PSL")
        logger.info(f"Local price per credit: {local_price_per_credit:,.1f} PSL")
        logger.info(f"Acceptable price range: [{min_acceptable_price:,.1f}, {max_acceptable_price:,.1f}] PSL")
        logger.info(f"Agreement with proposed price: {agree_with_proposed_price}")
        return agree_with_proposed_price
    except Exception as e:
        logger.error(f"Error determining agreement with proposed price: {str(e)}")
        traceback.print_exc()        
        raise
    
async def check_burn_transaction(txid: str, credit_usage_tracking_psl_address: str, total_cost_in_psl: float, request_response_pastel_block_height: int) -> Tuple[bool, int]:
    try:
        max_block_height = request_response_pastel_block_height + MAXIMUM_NUMBER_OF_PASTEL_BLOCKS_FOR_USER_TO_SEND_BURN_AMOUNT_FOR_CREDIT_TICKET
        max_retries = 30
        initial_retry_delay_in_seconds = 30
        total_cost_in_psl = round(total_cost_in_psl,5)
        matching_transaction_found, exceeding_transaction_found, transaction_block_height, num_confirmations, amount_received_at_burn_address = await check_burn_address_for_tracking_transaction(
            credit_usage_tracking_psl_address, 
            total_cost_in_psl,
            txid,
            max_block_height,
            max_retries,
            initial_retry_delay_in_seconds
        )
        return matching_transaction_found, exceeding_transaction_found, transaction_block_height, num_confirmations, amount_received_at_burn_address
    except Exception as e:
        logger.error(f"Error checking burn transaction: {str(e)}")
        traceback.print_exc()
        raise

def turn_lists_and_dicts_into_strings_func(data: bytes) -> str:
    # Decode the bytes to a string
    data_str = data.decode('utf-8')
    # Parse the string as JSON
    data_dict = json.loads(data_str)
    def replace_lists_and_dicts_with_strings(obj):
        if isinstance(obj, dict):
            # Convert the dictionary to a JSON string if it's not the top-level object
            return json.dumps({k: replace_lists_and_dicts_with_strings(v) for k, v in obj.items()})
        elif isinstance(obj, list):
            # Convert the list to a JSON string
            return json.dumps(obj)
        else:
            return obj
    # Process the input data
    processed_data = replace_lists_and_dicts_with_strings(data_dict)
    # Return the processed data
    return processed_data

def transform_sqlmodel_list_and_dict_fields_into_strings_func(model_instance: SQLModel):
    for field_name, value in model_instance.__dict__.items():
        if isinstance(value, (list, dict)):
            # Convert the list or dictionary to a JSON string
            setattr(model_instance, field_name, json.dumps(value))
    return model_instance    
        
def parse_sqlmodel_strings_into_lists_and_dicts_func(model_instance: SQLModel) -> SQLModel:
    for field_name, value in model_instance.__dict__.items():
        if isinstance(value, str):  # Check if the field value is a string
            try:
                parsed_value = json.loads(value)  # Try to parse the string as JSON
                # Ensure the parsed value is a list or dictionary before setting it
                if isinstance(parsed_value, (list, dict)):
                    setattr(model_instance, field_name, parsed_value)  # Replace the string with a list or dictionary
            except json.JSONDecodeError:
                # If parsing fails, skip this field and continue
                continue
    return model_instance
        
async def retrieve_credit_pack_ticket_from_blockchain_using_txid(txid: str) -> db_code.CreditPackPurchaseRequestResponse:
    try:
        if not txid:
            logger.error("Error retrieving credit pack ticket from blockchain: No TXID provided")
            return None, None, None
        credit_pack_combined_blockchain_ticket_data_json = await retrieve_generic_ticket_data_from_blockchain(txid)
        if credit_pack_combined_blockchain_ticket_data_json:
            credit_pack_combined_blockchain_ticket_data_dict = json.loads(credit_pack_combined_blockchain_ticket_data_json)
        else:
            logger.error(f"Error retrieving credit pack ticket from blockchain for txid {txid}: No data found")
            return None, None, None
        if credit_pack_combined_blockchain_ticket_data_dict is None:
            return None, None, None
        credit_pack_purchase_request_dict = credit_pack_combined_blockchain_ticket_data_dict['credit_pack_purchase_request_dict']
        credit_pack_purchase_request_response_dict = credit_pack_combined_blockchain_ticket_data_dict['credit_pack_purchase_request_response_dict']
        credit_pack_purchase_request_confirmation_dict = credit_pack_combined_blockchain_ticket_data_dict['credit_pack_purchase_request_confirmation_dict']
        credit_pack_purchase_request = db_code.CreditPackPurchaseRequest(**credit_pack_purchase_request_dict)
        credit_pack_purchase_request_response = db_code.CreditPackPurchaseRequestResponse(**credit_pack_purchase_request_response_dict)
        credit_pack_purchase_request_confirmation = db_code.CreditPackPurchaseRequestConfirmation(**credit_pack_purchase_request_confirmation_dict)
        return credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation
    except Exception as e:
        logger.error(f"Error retrieving credit pack ticket from blockchain: {str(e)}")
        raise

async def retrieve_credit_pack_ticket_from_purchase_burn_txid(purchase_burn_txid: str):
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestConfirmation).where(db_code.CreditPackPurchaseRequestConfirmation.txid_of_credit_purchase_burn_transaction == purchase_burn_txid)
            )
            credit_pack_request_confirmation = result.one_or_none()
            if credit_pack_request_confirmation is None:
                return None, None, None
            sha3_256_hash_of_credit_pack_purchase_request_fields = credit_pack_request_confirmation.sha3_256_hash_of_credit_pack_purchase_request_fields
        # Try to retrieve the credit pack ticket from the local database using the txid mapping
        async with db_code.Session() as db_session:
            mapping = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestResponseTxidMapping).where(db_code.CreditPackPurchaseRequestResponseTxidMapping.sha3_256_hash_of_credit_pack_purchase_request_fields == sha3_256_hash_of_credit_pack_purchase_request_fields)
            )
            mapping_result = mapping.one_or_none()
            if mapping_result is not None:
                logger.info(f"Found credit pack data for purchase burn TXID {purchase_burn_txid} in the local database, so returning it immediately!")
                credit_pack_purchase_request = await get_credit_pack_purchase_request(mapping_result.sha3_256_hash_of_credit_pack_purchase_request_fields)
                credit_pack_purchase_request_response = await get_credit_pack_purchase_request_response_from_request_hash(mapping_result.sha3_256_hash_of_credit_pack_purchase_request_fields)
                credit_pack_purchase_request_confirmation = await get_credit_pack_purchase_request_confirmation_from_request_hash(mapping_result.sha3_256_hash_of_credit_pack_purchase_request_fields)
                return credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation            
    except Exception as e:
        logger.error(f"Error occurred while retrieving credit pack ticket data: {e}")
        traceback.print_exc()
        return None, None, None          
    
async def retrieve_credit_pack_ticket_using_txid(txid: str) -> Tuple[Optional[db_code.CreditPackPurchaseRequest], Optional[db_code.CreditPackPurchaseRequestResponse], Optional[db_code.CreditPackPurchaseRequestConfirmation]]:
    logger.info(f"Attempting to retrieve credit pack ticket for TXID: {txid}")
    max_retries = 10  # Maximum retries with backoff
    base_delay = 1  # Starting delay in seconds
    max_delay = 60  # Maximum delay in seconds for backoff
    total_wait_time = 0  # Total wait time across retries
    jitter_max = 1  # Max jitter in seconds
    if txid is None:
        logger.error("Error: TXID is None")
        return None, None, None
    try:
        async with db_code.Session() as db_session:
            mapping = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestResponseTxidMapping).where(db_code.CreditPackPurchaseRequestResponseTxidMapping.pastel_api_credit_pack_ticket_registration_txid == txid)
            )
            mapping_result = mapping.one_or_none()
            if mapping_result is not None:
                logger.info(f"Found mapping in local database for ticket registration TXID: {txid}")
                credit_pack_purchase_request = await get_credit_pack_purchase_request(mapping_result.sha3_256_hash_of_credit_pack_purchase_request_fields)
                credit_pack_purchase_request_response = await get_credit_pack_purchase_request_response_from_request_hash(mapping_result.sha3_256_hash_of_credit_pack_purchase_request_fields)
                credit_pack_purchase_request_confirmation = await get_credit_pack_purchase_request_confirmation_from_request_hash(mapping_result.sha3_256_hash_of_credit_pack_purchase_request_fields)
                logger.info(f"Successfully retrieved credit pack ticket data from local database for TXID: {txid}")
                return credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation
        logger.info(f"Ticket not found in local database for TXID {txid}, attempting retrieval from blockchain")
        credit_pack_purchase_request = credit_pack_purchase_request_response = credit_pack_purchase_request_confirmation = None
        for attempt in range(max_retries):
            try:
                logger.info(f"Attempting to retrieve ticket from blockchain using registration TXID: {txid}")
                credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation = await retrieve_credit_pack_ticket_from_blockchain_using_txid(txid)
                if all((credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation)):
                    logger.info(f"Successfully retrieved credit pack ticket from blockchain using registration TXID: {txid}")
                    break
            except Exception as e:
                logger.error(f"Error retrieving ticket using registration TXID {txid} on attempt {attempt + 1}: {str(e)}")
                delay = min(base_delay * 2 ** attempt + random.uniform(0, jitter_max), max_delay)
                total_wait_time += delay
                if total_wait_time >= 300:  # 5 minutes
                    logger.error(f"Failed to retrieve credit pack ticket from blockchain after {attempt + 1} attempts in 5 minutes")
                    break
                logger.info(f"Waiting for {delay:.2f} seconds before retrying...")
                await asyncio.sleep(delay)
        # If not found, try to retrieve the ticket assuming txid is a purchase burn transaction TXID
        if not all((credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation)):
            try:
                logger.info(f"Attempting to retrieve ticket from blockchain using burn transaction TXID: {txid}")
                credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation = await retrieve_credit_pack_ticket_from_purchase_burn_txid(txid)
                if all((credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation)):
                    logger.info(f"Successfully retrieved credit pack ticket from blockchain using burn transaction TXID: {txid}")
                else:
                    logger.error(f"Failed to retrieve credit pack ticket from blockchain for burn transaction TXID: {txid}")
            except Exception as e:
                logger.error(f"Error retrieving ticket using burn transaction TXID {txid}: {str(e)}")
        if all((credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation)):
            logger.info(f"Saving retrieved credit pack ticket to local database for TXID: {txid}")
            await save_or_update_credit_pack_ticket(credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation, txid)
        else:
            logger.error(f"Failed to retrieve credit pack ticket from blockchain for TXID: {txid}")
            return None, None, None
        return credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation
    except Exception as e:
        logger.error(f"Error retrieving credit pack ticket for TXID {txid}: {str(e)}")
        traceback.print_exc()
        raise

async def save_or_update_credit_pack_ticket(credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation, txid):
    logger.info(f"Attempting to save or update credit pack ticket for TXID: {txid}")
    async with db_code.Session() as db_session:
        try:
            async with db_session.begin():
                logger.info(f"Checking for existing credit pack purchase request response for TXID: {txid}")
                existing_response = await db_session.exec(
                    select(db_code.CreditPackPurchaseRequestResponse).where(
                        db_code.CreditPackPurchaseRequestResponse.sha3_256_hash_of_credit_pack_purchase_request_response_fields == credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_response_fields
                    )
                )
                existing_response = existing_response.one_or_none()
                if existing_response:
                    logger.info(f"Updating existing credit pack purchase request response for TXID: {txid}")
                    for key, value in credit_pack_purchase_request_response.dict().items():
                        setattr(existing_response, key, value)
                else:
                    logger.info(f"Adding new credit pack purchase request response for TXID: {txid}")
                    db_session.add(credit_pack_purchase_request_response)
                logger.info(f"Checking for existing credit pack purchase request confirmation for TXID: {txid}")
                existing_confirmation = await db_session.exec(
                    select(db_code.CreditPackPurchaseRequestConfirmation).where(
                        db_code.CreditPackPurchaseRequestConfirmation.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields == credit_pack_purchase_request_confirmation.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields
                    )
                )
                existing_confirmation = existing_confirmation.one_or_none()
                if existing_confirmation:
                    logger.info(f"Updating existing credit pack purchase request confirmation for TXID: {txid}")
                    for key, value in credit_pack_purchase_request_confirmation.dict().items():
                        setattr(existing_confirmation, key, value)
                else:
                    logger.info(f"Adding new credit pack purchase request confirmation for TXID: {txid}")
                    db_session.add(credit_pack_purchase_request_confirmation)
                logger.info(f"Updating TXID mapping for credit pack ticket with TXID: {txid}")
                await save_credit_pack_purchase_request_response_txid_mapping(credit_pack_purchase_request_response, txid)
            logger.info(f"Successfully saved or updated credit pack ticket data for TXID: {txid}")
        except Exception as e:
            logger.error(f"Error saving or updating credit pack ticket data for TXID {txid}: {str(e)}")
            raise
    logger.info(f"Consolidating WAL data for credit pack ticket with TXID: {txid}")
    await db_code.consolidate_wal_data()

async def save_credit_pack_purchase_request_response_txid_mapping(credit_pack_purchase_request_response: db_code.CreditPackPurchaseRequestResponse, txid: str) -> None:
    logger.info(f"Attempting to save credit pack purchase request response TXID mapping for TXID: {txid}")
    try:
        async with db_code.Session() as db_session:        
            existing_mapping = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestResponseTxidMapping).where(
                    db_code.CreditPackPurchaseRequestResponseTxidMapping.sha3_256_hash_of_credit_pack_purchase_request_fields == credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_fields
                )
            )
            existing_mapping = existing_mapping.one_or_none()
            if existing_mapping is None:
                logger.info(f"Creating new TXID mapping for credit pack ticket with TXID: {txid}")
                mapping = db_code.CreditPackPurchaseRequestResponseTxidMapping(
                    sha3_256_hash_of_credit_pack_purchase_request_fields=credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_fields,
                    pastel_api_credit_pack_ticket_registration_txid=txid
                )
                db_session.add(mapping)
            else:
                logger.info(f"Updating existing TXID mapping for credit pack ticket with TXID: {txid}")
                existing_mapping.pastel_api_credit_pack_ticket_registration_txid = txid
            logger.info(f"Successfully saved credit pack purchase request response TXID mapping for TXID: {txid}")
    except Exception as e:
        logger.error(f"Error saving credit pack purchase request response TXID mapping for TXID {txid}: {str(e)}")
        raise
    
async def get_final_credit_pack_registration_txid_from_credit_purchase_burn_txid(purchase_burn_txid: str):
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestConfirmation).where(db_code.CreditPackPurchaseRequestConfirmation.txid_of_credit_purchase_burn_transaction == purchase_burn_txid)
            )
            credit_pack_request_confirmation = result.one_or_none()
            sha3_256_hash_of_credit_pack_purchase_request_fields = credit_pack_request_confirmation.sha3_256_hash_of_credit_pack_purchase_request_fields    
            return sha3_256_hash_of_credit_pack_purchase_request_fields
    except Exception as e:
        logger.error(f"Error occurred while retrieving credit pack ticket data: {e}")
        traceback.print_exc()
        return None      
    
async def get_credit_pack_ticket_registration_txid_from_corresponding_burn_transaction_txid(purchase_burn_txid: str) -> str:
    try:
        async with db_code.Session() as db_session:
            # Retrieve the corresponding confirmation from the purchase burn txid
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestConfirmation).where(
                    db_code.CreditPackPurchaseRequestConfirmation.txid_of_credit_purchase_burn_transaction == purchase_burn_txid
                )
            )
            credit_pack_request_confirmation = result.one_or_none()
            if credit_pack_request_confirmation is None:
                raise ValueError(f"No confirmation found for purchase burn txid: {purchase_burn_txid}")
            # Retrieve the mapping for the sha3_256 hash from the confirmation
            sha3_256_hash_of_credit_pack_purchase_request_fields = credit_pack_request_confirmation.sha3_256_hash_of_credit_pack_purchase_request_fields
            mapping_result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestResponseTxidMapping).where(
                    db_code.CreditPackPurchaseRequestResponseTxidMapping.sha3_256_hash_of_credit_pack_purchase_request_fields == sha3_256_hash_of_credit_pack_purchase_request_fields
                )
            )
            mapping = mapping_result.one_or_none()
            if mapping is None:
                raise ValueError(f"No TXID mapping found for purchase burn txid: {purchase_burn_txid}")
            # Return the TXID of the credit pack ticket registration
            return mapping.pastel_api_credit_pack_ticket_registration_txid
    except Exception as e:
        logger.error(f"Error occurred while retrieving credit pack ticket registration TXID: {e}")
        traceback.print_exc()
        return None

def recursively_parse_json(data):
    # Helper function to handle recursive parsing
    if isinstance(data, str):
        try:
            parsed_data = json.loads(data)
            # After parsing, check if parsed_data is a dict or list, otherwise return the original string
            if isinstance(parsed_data, (dict, list)):
                return recursively_parse_json(parsed_data)
            else:
                return data  # Do not convert numbers, booleans, or nulls
        except json.JSONDecodeError:
            return data  # Return the original string if it's not JSON
    elif isinstance(data, dict):
        # Recursively process each key-value pair in the dictionary with sorted keys
        return {k: recursively_parse_json(data[k]) for k in sorted(data.keys())}
    elif isinstance(data, list):
        # Recursively process each element in the list
        return [recursively_parse_json(item) for item in data]
    else:
        # Return the data as is if it's neither a string, dict, nor list
        return data

def collect_leaf_nodes(data, parent_key=''):
    # Helper function to collect leaf nodes, concatenating key-value pairs
    if isinstance(data, dict):
        return '|'.join(f"{k} : {collect_leaf_nodes(v, k)}" for k, v in sorted(data.items()))
    elif isinstance(data, list):
        # Use parent key for elements of a list to keep the context
        return '|'.join(f"{parent_key} : {collect_leaf_nodes(item, parent_key)}" for item in data)
    elif isinstance(data, str) or not isinstance(data, Iterable):
        return str(data)
    return ''

def compute_fully_parsed_json_sha3_256_hash(input_json):
    # Parse the input JSON string
    parsed_data = json.loads(input_json)
    # Recursively parse nested JSON and other data structures
    fully_parsed_data = recursively_parse_json(parsed_data)
    # Collect all leaf nodes and concatenate them with key names
    concatenated_data = collect_leaf_nodes(fully_parsed_data)
    # Compute SHA3-256 hash
    sha3_hash = hashlib.sha3_256(concatenated_data.encode('utf-8')).hexdigest()
    return sha3_hash, concatenated_data

async def store_generic_ticket_data_in_blockchain(ticket_input_data_json: str, ticket_type_identifier: str):
    try:    
        if not isinstance(ticket_input_data_json, str):
            error_message = "Ticket data must be a valid string!"
            logger.error(error_message)
            raise ValueError(error_message)
        if not isinstance(ticket_type_identifier, str):
            error_message = "Ticket type identifier data must be a valid string!"
            logger.error(error_message)
            raise ValueError(error_message)
        ticket_type_identifier = ticket_type_identifier.upper()
        ticket_input_data_fully_parsed_sha3_256_hash, concatenated_data = compute_fully_parsed_json_sha3_256_hash(ticket_input_data_json)
        ticket_input_data_dict = recursively_parse_json(ticket_input_data_json)
        ticket_uncompressed_size_in_bytes = len(ticket_input_data_json.encode('utf-8'))
        ticket_dict = {"ticket_identifier_string": ticket_type_identifier,
                        "ticket_input_data_fully_parsed_sha3_256_hash": ticket_input_data_fully_parsed_sha3_256_hash,
                        "ticket_uncompressed_size_in_bytes": ticket_uncompressed_size_in_bytes,
                        "ticket_input_data_dict": ticket_input_data_dict}
        ticket_json = json.dumps(ticket_dict, ensure_ascii=False)
        ticket_json_b64 = base64.b64encode(ticket_json.encode('utf-8')).decode('utf-8')
        ticket_txid = ""
        logger.info("Now attempting to write data to blockchain using 'tickets register contract' command...")
        ticket_register_command_response = await tickets_register_contract(
            rpc_connection, ticket_json_b64, ticket_type_identifier, ticket_input_data_fully_parsed_sha3_256_hash)
        logger.info("Done with 'tickets register contract' command!")
        asyncio.sleep(2)
        if len(ticket_register_command_response) > 0:
            if 'txid' in ticket_register_command_response.keys():
                ticket_txid = ticket_register_command_response['txid']
                ticket_get_command_response = await tickets_get(rpc_connection, ticket_txid , 1)
                retrieved_ticket_data = ticket_get_command_response['ticket']['contract_ticket']
                ticket_tx_info = ticket_get_command_response['tx_info']
                if ticket_tx_info is None:
                    logger.error(f"Ticket was not processed correctly for registration txid {ticket_txid}")
                    return None
                uncompressed_ticket_size_in_bytes = ticket_tx_info['uncompressed_size']
                compressed_ticket_size_in_bytes = ticket_tx_info['compressed_size']
                retrieved_ticket_input_data_fully_parsed_sha3_256_hash = retrieved_ticket_data['ticket_input_data_fully_parsed_sha3_256_hash']
                retrieved_ticket_input_data_dict = retrieved_ticket_data['ticket_input_data_dict']
                retrieved_ticket_input_data_dict_json = json.dumps(retrieved_ticket_input_data_dict)
                computed_fully_parsed_json_sha3_256_hash, concatenated_data = compute_fully_parsed_json_sha3_256_hash(retrieved_ticket_input_data_dict_json)
                assert(computed_fully_parsed_json_sha3_256_hash==retrieved_ticket_input_data_fully_parsed_sha3_256_hash)
                assert(computed_fully_parsed_json_sha3_256_hash==ticket_input_data_fully_parsed_sha3_256_hash)
                logger.info(f"Generic blockchain ticket of sub-type {ticket_type_identifier} was successfully stored in the blockchain with TXID {ticket_txid}")
                logger.info(f"Original Data length: {uncompressed_ticket_size_in_bytes:,} bytes; Compressed data length: {compressed_ticket_size_in_bytes:,} bytes;") #Number of multisig outputs: {len(txouts):,}; Total size of multisig outputs in bytes: {sum(len(txout[1]) for txout in txouts):,}") 
        else:
            logger.error("Error storing ticket data in the blockchain! Could not retrieve and perfectly reconstruct original ticket data.")
        return ticket_txid, ticket_dict, ticket_json_b64
    except Exception as e:
        logger.error(f"Error occurred while storing ticket data in the blockchain: {e}")
        traceback.print_exc()
        return None

async def retrieve_generic_ticket_data_from_blockchain(ticket_txid: str):
    try:
        if ticket_txid is None:
            logger.error("No ticket TXID provided!")
            return None
        ticket_get_command_response = await tickets_get(rpc_connection, ticket_txid, 1)
        retrieved_ticket_data = ticket_get_command_response['ticket']['contract_ticket']
        if retrieved_ticket_data is None:
            logger.error(f"Error: no ticket data returned for TXID {ticket_txid}")
            return None
        retrieved_ticket_input_data_fully_parsed_sha3_256_hash = retrieved_ticket_data['ticket_input_data_fully_parsed_sha3_256_hash']
        retrieved_ticket_input_data_dict = retrieved_ticket_data['ticket_input_data_dict']
        retrieved_ticket_input_data_dict_json = json.dumps(retrieved_ticket_input_data_dict)
        computed_fully_parsed_json_sha3_256_hash, concatenated_data = compute_fully_parsed_json_sha3_256_hash(retrieved_ticket_input_data_dict_json)
        assert(computed_fully_parsed_json_sha3_256_hash == retrieved_ticket_input_data_fully_parsed_sha3_256_hash)
        credit_pack_combined_blockchain_ticket_data_json = json.dumps(retrieved_ticket_input_data_dict)
        return credit_pack_combined_blockchain_ticket_data_json
    except Exception as e:
        logger.error(f"Error occurred while retrieving ticket data in the blockchain: {e}")
        traceback.print_exc()
        return None
    
async def get_list_of_credit_pack_ticket_txids_already_in_db():
    async with db_code.Session() as db_session:
        mappings = await db_session.exec(select(db_code.CreditPackPurchaseRequestResponseTxidMapping))
        mapping_results = mappings.all()
        if mapping_results is not None:
            logger.info(f"Found {len(mapping_results):,} credit pack TXIDs already stored in database...")
            list_of_already_stored_credit_pack_txids = list(set([x.pastel_api_credit_pack_ticket_registration_txid for x in mapping_results]))          
            return list_of_already_stored_credit_pack_txids
        else:
            return []

async def list_generic_tickets_in_blockchain_and_parse_and_validate_and_store_them(ticket_type_identifier: str = "INFERENCE_API_CREDIT_PACK_TICKET", starting_block_height: int = 700000, force_revalidate_all_tickets: int = 0):
    try:
        if not isinstance(ticket_type_identifier, str):
            error_message = "Ticket type identifier data must be a valid string!"
            logger.error(error_message)
            raise ValueError(error_message)
        ticket_type_identifier = ticket_type_identifier.upper()
        logger.info(f"Getting all blockchain tickets of type {ticket_type_identifier} starting from block height {starting_block_height:,}...")
        list_of_ticket_data_dicts = await tickets_list_contract(rpc_connection, ticket_type_identifier, starting_block_height)
        list_of_ticket_internal_data_dicts = [x['ticket']['contract_ticket'] for x in list_of_ticket_data_dicts]
        logger.info(f"Found {len(list_of_ticket_internal_data_dicts):,} total tickets in the blockchain since block height {starting_block_height:,} of type {ticket_type_identifier}; Now checking if they are internally consistent...")
        list_of_retrieved_ticket_input_data_fully_parsed_sha3_256_hashes = [x['ticket_input_data_fully_parsed_sha3_256_hash'] for x in list_of_ticket_internal_data_dicts]
        list_of_retrieved_ticket_input_data_dicts = [x['ticket_input_data_dict'] for x in list_of_ticket_internal_data_dicts]
        list_of_retrieved_ticket_input_data_dicts_json = [json.dumps(x) for x in list_of_retrieved_ticket_input_data_dicts]
        list_of_computed_fully_parsed_json_sha3_256_hashes = [compute_fully_parsed_json_sha3_256_hash(x)[0] for x in list_of_retrieved_ticket_input_data_dicts_json]
        list_of_computed_fully_parsed_json_sha3_256_hashes_sorted = sorted(list_of_computed_fully_parsed_json_sha3_256_hashes)
        list_of_retrieved_ticket_input_data_fully_parsed_sha3_256_hashes_sorted = sorted(list_of_retrieved_ticket_input_data_fully_parsed_sha3_256_hashes)
        mismatches = [(i, x) for i, x in enumerate(list_of_retrieved_ticket_input_data_fully_parsed_sha3_256_hashes_sorted) if list_of_computed_fully_parsed_json_sha3_256_hashes_sorted[i] != x]
        if mismatches:
            logger.error(f"Mismatches found in hashes: {mismatches}")
        else:
            logger.info("All retrieved tickets were internally consistent and match the computed SHA3-256 hash of all fields!")
        list_of_already_stored_credit_pack_txids = await get_list_of_credit_pack_ticket_txids_already_in_db()
        list_of_known_bad_credit_pack_txids = await get_list_of_all_known_bad_credit_pack_ticket_txids_from_db()
        list_of_ticket_txids = [x['txid'] for x in list_of_ticket_data_dicts]
        if force_revalidate_all_tickets:
            list_of_ticket_txids_not_already_stored = list_of_ticket_txids
            if len(list_of_ticket_txids) > 0:
                logger.info(f"Now attempting to perform in-depth validation of all aspects of the {len(list_of_ticket_txids):,} retrieved {ticket_type_identifier} tickets (Forcing full revalidation of ALL tickets, even those already stored in the local database or which are in the known bad list of TXIDs!)...")
        else:
            list_of_ticket_txids_not_already_stored = [x for x in list_of_ticket_txids if x not in list_of_already_stored_credit_pack_txids and x not in list_of_known_bad_credit_pack_txids]
            if len(list_of_ticket_txids_not_already_stored) > 0:
                logger.info(f"Now attempting to perform in-depth validation of all aspects of the {len(list_of_ticket_txids_not_already_stored):,} retrieved {ticket_type_identifier} tickets that are not already in the database or in the known bad list of TXIDs...")
        list_of_fully_validated_ticket_txids = []
        for idx, current_txid in enumerate(list_of_ticket_txids_not_already_stored):
            logger.info(f"Validating ticket {idx+1}/{len(list_of_ticket_txids_not_already_stored)}: {current_txid}")
            try:
                validation_results = await validate_existing_credit_pack_ticket(current_txid)
                if validation_results is None:
                    logger.error(f"[Ticket {idx+1}/{len(list_of_ticket_txids_not_already_stored)}] Validation for TXID {current_txid} returned None. Skipping this ticket.")
                    continue
                asyncio.sleep(0.5)
                if not validation_results["credit_pack_ticket_is_valid"]:
                    logger.warning(f"[Ticket {idx+1}/{len(list_of_ticket_txids_not_already_stored)}] Blockchain ticket of type {ticket_type_identifier} and TXID {current_txid} was unable to be fully validated, so skipping it! Reasons for validation failure:\n{validation_results['validation_failure_reasons_list']}")
                else:
                    logger.info(f"[Ticket {idx+1}/{len(list_of_ticket_txids_not_already_stored)}] Blockchain ticket of type {ticket_type_identifier} and TXID {current_txid} was fully validated! Now saving to database...")
                    list_of_fully_validated_ticket_txids.append(current_txid)
                    credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation = await retrieve_credit_pack_ticket_from_blockchain_using_txid(current_txid)
                    if all((credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation)):
                        await save_credit_pack_ticket_to_database(credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation, current_txid)
                        logger.info(f"Successfully saved credit pack ticket with TXID {current_txid} to database.")
                    else:
                        logger.error(f"Failed to retrieve complete credit pack ticket data for TXID {current_txid} from blockchain.")
            except Exception as e:
                logger.error(f"Exception occurred while validating TXID {current_txid}: {str(e)}")
                logger.error(f"Exception type: {type(e).__name__}")
                logger.error(f"Exception details: {traceback.format_exc()}")
                continue                    
        if len(list_of_ticket_txids_not_already_stored) > 0:
            logger.info(f"We were able to fully validate {len(list_of_fully_validated_ticket_txids):,} of the {len(list_of_ticket_txids_not_already_stored):,} retrieved {ticket_type_identifier} tickets!")
        return list_of_retrieved_ticket_input_data_dicts_json, list_of_fully_validated_ticket_txids
    except Exception as e:
        logger.error(f"Error occurred while listing generic blockchain tickets of type {ticket_type_identifier}: {e}")
        traceback.print_exc()
        return None
    
async def periodic_ticket_listing_and_validation():
    while True:
        try:
            starting_block_height = 700000
            await list_generic_tickets_in_blockchain_and_parse_and_validate_and_store_them(starting_block_height=starting_block_height)
            await asyncio.sleep(5*60)  # Sleep for 5 minutes
        except Exception as e:
            logger.error(f"Error in periodic ticket listing and validation: {str(e)}")
            traceback.print_exc()

async def store_credit_pack_ticket_in_blockchain(credit_pack_combined_blockchain_ticket_data_json: str) -> str:
    try:
        logger.info("Now attempting to write the ticket data to the blockchain...")
        ticket_type_identifier = "INFERENCE_API_CREDIT_PACK_TICKET"
        credit_pack_ticket_txid, ticket_dict, ticket_json_b64 = await store_generic_ticket_data_in_blockchain(credit_pack_combined_blockchain_ticket_data_json, ticket_type_identifier)
        total_bytes_used = 0
        logger.info(f"Received back pastel txid of {credit_pack_ticket_txid} for the stored blockchain ticket data; total bytes used to store the data in the blockchain was {total_bytes_used:,}; now waiting for the transaction to be confirmed...")
        max_retries = 20
        retry_delay = 20
        try_count = 0
        num_confirmations = 0
        storage_validation_error_string = ""
        while try_count < max_retries and num_confirmations == 0:
            # Retrieve the transaction details using the wrapped gettransaction function
            tx_info = await gettransaction(rpc_connection, credit_pack_ticket_txid)
            if tx_info:
                num_confirmations = tx_info.get("confirmations", 0)
                if (num_confirmations > 0) or SKIP_BURN_TRANSACTION_BLOCK_CONFIRMATION_CHECK:
                    if num_confirmations > 0:
                        logger.info(f"Transaction {credit_pack_ticket_txid} has been confirmed with {num_confirmations} confirmations.")
                    else:
                        logger.info(f"Transaction {credit_pack_ticket_txid} has not yet been confirmed, but we are skipping confirmation check to speed things up.")
                    break
                else:
                    logger.info(f"Transaction {credit_pack_ticket_txid} is not yet confirmed. Waiting for {retry_delay:.2f} seconds before checking again.")
                    await asyncio.sleep(retry_delay)
                    try_count += 1
                    retry_delay *= 1.15  # Optional: increase delay between retries
            else:
                logger.warning(f"Transaction {credit_pack_ticket_txid} not found. Waiting for {retry_delay} seconds before checking again.")
                await asyncio.sleep(retry_delay)
                try_count += 1
                retry_delay *= 1.15  # Optional: increase delay between retries
        if (num_confirmations > 0) or SKIP_BURN_TRANSACTION_BLOCK_CONFIRMATION_CHECK:
            logger.info("Now verifying that we can reconstruct the original file written exactly...")
            reconstructed_file_data = await retrieve_generic_ticket_data_from_blockchain(credit_pack_ticket_txid)
            retrieved_data_fully_parsed_sha3_256_hash, _ = compute_fully_parsed_json_sha3_256_hash(reconstructed_file_data)
            original_data_fully_parsed_sha3_256_hash, _ = compute_fully_parsed_json_sha3_256_hash(credit_pack_combined_blockchain_ticket_data_json)
            if retrieved_data_fully_parsed_sha3_256_hash == original_data_fully_parsed_sha3_256_hash:
                logger.info("Successfully verified that the stored blockchain ticket data can be reconstructed exactly!")
                use_test_reconstruction_of_object_from_json = 1
                if use_test_reconstruction_of_object_from_json:
                    credit_pack_combined_blockchain_ticket_data_dict = json.loads(reconstructed_file_data)
                    credit_pack_purchase_request_dict = credit_pack_combined_blockchain_ticket_data_dict['credit_pack_purchase_request_dict']
                    credit_pack_purchase_request_response_dict = credit_pack_combined_blockchain_ticket_data_dict['credit_pack_purchase_request_response_dict']
                    credit_pack_purchase_request_confirmation_dict = credit_pack_combined_blockchain_ticket_data_dict['credit_pack_purchase_request_confirmation_dict']
                    credit_pack_purchase_request = db_code.CreditPackPurchaseRequest(**credit_pack_purchase_request_dict)  # noqa: F841
                    credit_pack_purchase_request_response = db_code.CreditPackPurchaseRequestResponse(**credit_pack_purchase_request_response_dict)  # noqa: F841
                    credit_pack_purchase_request_confirmation = db_code.CreditPackPurchaseRequestConfirmation(**credit_pack_purchase_request_confirmation_dict)  # noqa: F841
                    logger.info(f"Reconstructed credit pack ticket data:\n Purchase Request: {abbreviated_pretty_json_func(credit_pack_purchase_request_dict)}\nPurchase Request Response: {abbreviated_pretty_json_func(credit_pack_purchase_request_response_dict)}\nPurchase Request Confirmation: {abbreviated_pretty_json_func(credit_pack_purchase_request_confirmation_dict)}")
            else:
                logger.error("Failed to verify that the stored blockchain ticket data can be reconstructed exactly!")
                storage_validation_error_string = "Failed to verify that the stored blockchain ticket data can be reconstructed exactly! Difference: " + str(set(reconstructed_file_data).symmetric_difference(set(credit_pack_combined_blockchain_ticket_data_json)))
                logger.error(storage_validation_error_string)
                return credit_pack_ticket_txid, storage_validation_error_string
        else:
            storage_validation_error_string = f"Transaction {credit_pack_ticket_txid} was not confirmed after {max_retries} attempts."
            logger.error(storage_validation_error_string)
            raise TimeoutError(f"Transaction {credit_pack_ticket_txid} was not confirmed after {max_retries} attempts.")
        return credit_pack_ticket_txid, storage_validation_error_string
    except Exception as e:
        storage_validation_error_string = f"Error storing credit pack ticket: {str(e)}"
        logger.error(storage_validation_error_string)
        traceback.print_exc()
        return credit_pack_ticket_txid, storage_validation_error_string
    
async def check_original_supernode_storage_confirmation(sha3_256_hash_of_credit_pack_purchase_request_response_fields: str) -> bool:
    async with db_code.Session() as db:
        result = await db.exec(
            select(db_code.CreditPackPurchaseRequestConfirmationResponse).where(db_code.CreditPackPurchaseRequestConfirmationResponse.sha3_256_hash_of_credit_pack_purchase_request_response_fields == sha3_256_hash_of_credit_pack_purchase_request_response_fields)
        )
        return result.one_or_none() is not None
    
async def check_if_credit_usage_tracking_psl_address_has_already_been_used_for_a_credit_pack(credit_usage_tracking_psl_address: str):
    async with db_code.Session() as db:
        result = await db.exec(
            select(db_code.CreditPackPurchaseRequestResponse).where(db_code.CreditPackPurchaseRequestResponse.credit_usage_tracking_psl_address == credit_usage_tracking_psl_address)
        )
        credit_pack_purchase_request_response = result.one_or_none()
        if credit_pack_purchase_request_response is not None:
            credit_tracking_address_already_used = True
        else:
            credit_tracking_address_already_used = False
        return credit_tracking_address_already_used, credit_pack_purchase_request_response

async def process_credit_purchase_initial_request(credit_pack_purchase_request: db_code.CreditPackPurchaseRequest) -> db_code.CreditPackPurchaseRequestPreliminaryPriceQuote:
    try:
        # Validate the request fields
        request_validation_errors = await validate_credit_pack_ticket_message_data_func(credit_pack_purchase_request)
        if request_validation_errors:
            rejection_message = await generate_credit_pack_request_rejection_message(credit_pack_purchase_request, request_validation_errors)
            logger.error(f"Invalid credit purchase request: {', '.join(request_validation_errors)}")
            return rejection_message
        if credit_pack_purchase_request.requested_initial_credits_in_credit_pack > MAXIMUM_CREDITS_PER_CREDIT_PACK:
            rejection_message = f"Requested initial credits in credit pack exceeds the maximum of {MAXIMUM_CREDITS_PER_CREDIT_PACK} credits allowed in a single credit pack!"
            logger.error(rejection_message)
            return rejection_message
        if credit_pack_purchase_request.requested_initial_credits_in_credit_pack < MINIMUM_CREDITS_PER_CREDIT_PACK:
            rejection_message = f"Requested initial credits in credit pack must be greater than or equal to {MINIMUM_CREDITS_PER_CREDIT_PACK} credits!"
            logger.error(rejection_message)
            return rejection_message
        # Check if credit_usage_tracking_psl_address has already been used for an existing credit pack at any time:
        credit_tracking_address_already_used, credit_pack_purchase_request_response = await check_if_credit_usage_tracking_psl_address_has_already_been_used_for_a_credit_pack(credit_pack_purchase_request.credit_usage_tracking_psl_address)
        if credit_tracking_address_already_used:
            rejection_message = f"The specified credit tracking address of {credit_pack_purchase_request.credit_usage_tracking_psl_address} has already been used for a credit pack purchase (with sha3-256 hash of credit request fields of {credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_fields})"
            logger.error(rejection_message)
            return rejection_message
        # Determine the preliminary price quote
        preliminary_quoted_price_per_credit_in_psl = await calculate_preliminary_psl_price_per_credit()
        preliminary_total_cost_of_credit_pack_in_psl = round(preliminary_quoted_price_per_credit_in_psl * credit_pack_purchase_request.requested_initial_credits_in_credit_pack, 5)
        # Create the response without the hash and signature fields
        credit_pack_purchase_request_fields_json = await extract_response_fields_from_credit_pack_ticket_message_data_as_json_func(credit_pack_purchase_request)
        credit_pack_purchase_request_response = db_code.CreditPackPurchaseRequestPreliminaryPriceQuote(
            sha3_256_hash_of_credit_pack_purchase_request_fields=credit_pack_purchase_request.sha3_256_hash_of_credit_pack_purchase_request_fields,
            credit_usage_tracking_psl_address=credit_pack_purchase_request.credit_usage_tracking_psl_address,
            credit_pack_purchase_request_fields_json_b64=base64.b64encode(credit_pack_purchase_request_fields_json.encode('utf-8')).decode('utf-8'),
            preliminary_quoted_price_per_credit_in_psl=preliminary_quoted_price_per_credit_in_psl,
            preliminary_total_cost_of_credit_pack_in_psl=preliminary_total_cost_of_credit_pack_in_psl,
            preliminary_price_quote_timestamp_utc_iso_string = datetime.now(timezone.utc).isoformat(),
            preliminary_price_quote_pastel_block_height=await get_current_pastel_block_height_func(),
            preliminary_price_quote_message_version_string="1.0",
            responding_supernode_pastelid=MY_PASTELID,
            sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields="",
            responding_supernode_signature_on_credit_pack_purchase_request_preliminary_price_quote_hash=""
        )
        # Generate the hash and signature fields
        credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(credit_pack_purchase_request_response)
        credit_pack_purchase_request_response.responding_supernode_signature_on_credit_pack_purchase_request_preliminary_price_quote_hash = await sign_message_with_pastelid_func(
            MY_PASTELID,
            credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields,
            LOCAL_PASTEL_ID_PASSPHRASE
        )
        # Validate the response
        response_validation_errors = await validate_credit_pack_ticket_message_data_func(credit_pack_purchase_request_response)
        if response_validation_errors:
            logger.error(f"Invalid credit purchase request preliminary price quote: {', '.join(response_validation_errors)}")
            raise ValueError(f"Invalid credit purchase request preliminary price quote: {', '.join(response_validation_errors)}")
        await save_credit_pack_purchase_request(credit_pack_purchase_request)
        await save_credit_pack_purchase_request_response(credit_pack_purchase_request_response)        
        return credit_pack_purchase_request_response
    except Exception as e:
        logger.error(f"Error processing credit purchase initial request: {str(e)}")
        traceback.print_exc()
        raise
    
async def generate_credit_pack_request_rejection_message(credit_pack_request: db_code.CreditPackPurchaseRequest, validation_errors: List[str]) -> db_code.CreditPackPurchaseRequestRejection:
    credit_pack_purchase_request_fields_json = await extract_response_fields_from_credit_pack_ticket_message_data_as_json_func(credit_pack_request)
    rejection_message = db_code.CreditPackPurchaseRequestRejection(
        sha3_256_hash_of_credit_pack_purchase_request_fields=credit_pack_request.sha3_256_hash_of_credit_pack_purchase_request_fields,
        credit_pack_purchase_request_fields_json_b64=base64.b64encode(credit_pack_purchase_request_fields_json.encode('utf-8')).decode('utf-8'),
        rejection_reason_string=", ".join(validation_errors),
        rejection_timestamp_utc_iso_string=datetime.now(timezone.utc).isoformat(),
        rejection_pastel_block_height=await get_current_pastel_block_height_func(),
        credit_purchase_request_rejection_message_version_string="1.0",
        responding_supernode_pastelid=MY_PASTELID,
        sha3_256_hash_of_credit_pack_purchase_request_rejection_fields="",
        responding_supernode_signature_on_credit_pack_purchase_request_rejection_hash=""
    )
    rejection_message.sha3_256_hash_of_credit_pack_purchase_request_rejection_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(rejection_message)
    rejection_message.responding_supernode_signature_on_credit_pack_purchase_request_rejection_hash = await sign_message_with_pastelid_func(
        MY_PASTELID,
        rejection_message.sha3_256_hash_of_credit_pack_purchase_request_rejection_fields,
        LOCAL_PASTEL_ID_PASSPHRASE
    )
    rejection_validation_errors = await validate_credit_pack_ticket_message_data_func(rejection_message)
    if rejection_validation_errors:
        logger.error(f"Invalid credit purchase request rejection message: {', '.join(rejection_validation_errors)}")
        raise ValueError(f"Invalid credit purchase request rejection message: {', '.join(rejection_validation_errors)}")
    await save_credit_pack_purchase_request_rejection(rejection_message)
    return rejection_message

async def select_potentially_agreeing_supernodes() -> List[str]:
    try:
        blacklist_path = Path('supernode_inference_ip_blacklist.txt')
        blacklisted_ips = set()
        if blacklist_path.exists(): # Read the blacklist file if it exists
            with blacklist_path.open('r') as blacklist_file:
                blacklisted_ips = {line.strip() for line in blacklist_file if line.strip()}
        else:
            logger.info("Blacklist file not found. Proceeding without blacklist filtering.")
        total_number_of_blacklisted_sns = len(blacklisted_ips)
        best_block_hash, best_block_merkle_root, _ = await get_best_block_hash_and_merkle_root_func() # Get the best block hash and merkle root
        supernode_list_df, _ = await check_supernode_list_func() # Get the list of all supernodes
        number_of_supernodes_found = len(supernode_list_df) - 1
        if number_of_supernodes_found < MINIMUM_NUMBER_OF_POTENTIALLY_AGREEING_SUPERNODES:
            logger.warning(f"Fewer than {MINIMUM_NUMBER_OF_POTENTIALLY_AGREEING_SUPERNODES} supernodes available. Using all {number_of_supernodes_found} available supernodes.")
            all_other_supernode_pastelids = [x for x in supernode_list_df['extKey'].tolist() if x != MY_PASTELID]
            return all_other_supernode_pastelids
        # Compute the XOR distance between each supernode's hash(pastelid) and the best block's merkle root
        xor_distances = []
        list_of_blacklisted_supernode_pastelids = []
        for _, row in supernode_list_df.iterrows():
            ip_address_port = row['ipaddress:port']
            ip_address = ip_address_port.split(":")[0]
            if ip_address in blacklisted_ips:
                current_supernode_pastelid = row['extKey']
                list_of_blacklisted_supernode_pastelids.append(current_supernode_pastelid)
                continue
            supernode_pastelid = row['extKey']
            supernode_pastelid_hash = get_sha256_hash_of_input_data_func(supernode_pastelid)
            supernode_pastelid_int = int(supernode_pastelid_hash, 16)
            merkle_root_int = int(best_block_merkle_root, 16)
            xor_distance = supernode_pastelid_int ^ merkle_root_int
            xor_distances.append((supernode_pastelid, xor_distance))
        # Sort the supernodes based on their XOR distances in ascending order
        sorted_supernodes = sorted(xor_distances, key=lambda x: x[1])
        # Select the supernodes with the closest XOR distances
        potentially_agreeing_supernodes = [supernode[0] for supernode in sorted_supernodes if supernode[0] != MY_PASTELID]
        return potentially_agreeing_supernodes, total_number_of_blacklisted_sns, list_of_blacklisted_supernode_pastelids
    except Exception as e:
        logger.error(f"Error selecting potentially agreeing supernodes: {str(e)}")
        traceback.print_exc()
        raise    

async def select_top_n_closest_supernodes_to_best_block_merkle_root(supernode_pastelids: List[str], n: int = 10, best_block_merkle_root: Optional[str] = None) -> List[str]:
    try:
        if best_block_merkle_root is None:
            _, best_block_merkle_root, _ = await get_best_block_hash_and_merkle_root_func()  # Get the best block hash and merkle root if not supplied
        best_block_merkle_root_hash = get_sha256_hash_of_input_data_func(best_block_merkle_root)
        merkle_root_hash_int = int(best_block_merkle_root_hash, 16)
        # Compute the XOR distance between each supernode's hash(pastelid) and the best block's merkle root
        xor_distances = []
        for pastelid in supernode_pastelids:
            supernode_pastelid_hash = get_sha256_hash_of_input_data_func(pastelid)
            supernode_pastelid_int = int(supernode_pastelid_hash, 16)
            xor_distance = supernode_pastelid_int ^ merkle_root_hash_int
            xor_distances.append((pastelid, xor_distance))
        # Sort the supernodes based on their XOR distances in ascending order
        sorted_supernodes = sorted(xor_distances, key=lambda x: x[1])
        # Select the top N supernodes with the closest XOR distances
        top_n_supernodes = [supernode[0] for supernode in sorted_supernodes[:n]]
        return top_n_supernodes
    except Exception as e:
        logger.error(f"Error selecting top N closest supernodes to best block merkle root: {str(e)}")
        traceback.print_exc()
        raise

async def check_liveness(supernode_base_url: str) -> bool:
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{supernode_base_url}/liveness_ping", timeout=2)
            return response.status_code == 200
    except httpx.RequestError:
        return False
    
async def get_supernode_url_and_check_liveness(supernode_pastelid, supernode_list_df, client):
    try:
        supernode_base_url = await get_supernode_url_from_pastelid_func(supernode_pastelid, supernode_list_df)
        is_alive = await check_liveness(supernode_base_url)
        return supernode_base_url, is_alive
    except Exception as e:
        logger.warning(f"Error getting supernode URL or checking liveness for supernode {supernode_pastelid}: {str(e)}")
        return None, False
    
async def send_price_agreement_request_to_supernodes(request: db_code.CreditPackPurchasePriceAgreementRequest, supernodes: List[str]) -> List[db_code.CreditPackPurchasePriceAgreementRequestResponse]:
    blacklist_path = Path('supernode_inference_ip_blacklist.txt')
    blacklisted_ips = set()
    if blacklist_path.exists():
        with blacklist_path.open('r') as blacklist_file:
            blacklisted_ips = {line.strip() for line in blacklist_file if line.strip()}
    else:
        logger.info("Blacklist file not found. Proceeding without blacklist filtering.")
    async with httpx.AsyncClient(timeout=Timeout(None)) as client:
        supernode_list_df, _ = await check_supernode_list_func()
        # Prepare tasks for supernode liveness check
        supernode_liveness_tasks = [
            asyncio.create_task(get_supernode_url_and_check_liveness(supernode_pastelid, supernode_list_df, client))
            for supernode_pastelid in supernodes
        ]
        supernode_urls_and_statuses = await asyncio.gather(*supernode_liveness_tasks)
        price_agreement_semaphore = asyncio.Semaphore(MAXIMUM_NUMBER_OF_CONCURRENT_RPC_REQUESTS*5)
        async def send_request(supernode_base_url, payload, timeout):
            async with price_agreement_semaphore:
                try:
                    response = await client.post(supernode_base_url, json=payload, timeout=timeout)
                    return response
                except httpx.RequestError as e:
                    logger.error(f"Error sending request to {supernode_base_url}: {str(e)}")
                    return None
        async def process_supernode(supernode_base_url, is_alive):
            if is_alive:
                async with price_agreement_semaphore:
                    challenge_dict = await request_and_sign_challenge(supernode_base_url)
                challenge = challenge_dict["challenge"]
                challenge_id = challenge_dict["challenge_id"]
                challenge_signature = challenge_dict["challenge_signature"]
                request_dict = request.model_dump()
                request_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in request_dict.items()}
                payload = {
                    "credit_pack_price_agreement_request": request_dict,
                    "challenge": challenge,
                    "challenge_id": challenge_id,
                    "challenge_signature": challenge_signature
                }
                url = f"{supernode_base_url}/credit_pack_price_agreement_request"
                timeout = Timeout(INDIVIDUAL_SUPERNODE_PRICE_AGREEMENT_REQUEST_TIMEOUT_PERIOD_IN_SECONDS)  # Timeout for individual supernode request
                return await send_request(url, payload, timeout)
            else:
                logger.warning(f"Supernode {supernode_base_url} is not responding on port 7123")
                return None
        request_tasks = [
            process_supernode(supernode_base_url, is_alive)
            for supernode_base_url, is_alive in supernode_urls_and_statuses
            if supernode_base_url.split(":")[1].replace("//", "").split("/")[0] not in blacklisted_ips
        ]
        logger.info(f"Now sending out {len(request_tasks):,} price agreement requests to potentially agreeing supernodes...")
        datetime_start = datetime.now(timezone.utc)
        responses = await asyncio.gather(*request_tasks, return_exceptions=True)
        datetime_end = datetime.now(timezone.utc)
        duration = datetime_end - datetime_start
        logger.info(f"Finished sending price agreement requests to supernodes in {duration.total_seconds():.2f} seconds!")
        price_agreement_request_responses = [
            db_code.CreditPackPurchasePriceAgreementRequestResponse(**response.json())
            for response in responses
            if isinstance(response, httpx.Response) and response.status_code == 200
        ]
        logger.info(f"Received a total of {len(price_agreement_request_responses):,} valid price agreement responses from supernodes out of {len(request_tasks):,} total requests sent, a success rate of {len(price_agreement_request_responses)/len(request_tasks):.2%}")
        return price_agreement_request_responses

async def request_and_sign_challenge(supernode_url: str) -> Dict[str, str]:
    async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS/5)) as client:
        response = await client.get(f"{supernode_url}/request_challenge/{MY_PASTELID}")
        response.raise_for_status()
        result = response.json()
        challenge = result["challenge"]
        challenge_id = result["challenge_id"]
        # Sign the challenge string using the local RPC client
        challenge_signature = await sign_message_with_pastelid_func(MY_PASTELID, challenge, LOCAL_PASTEL_ID_PASSPHRASE)
        return {
            "challenge": challenge,
            "challenge_id": challenge_id,
            "challenge_signature": challenge_signature
        }    

async def send_credit_pack_purchase_request_final_response_to_supernodes(response: db_code.CreditPackPurchaseRequestResponse, supernodes: List[str]) -> List[httpx.Response]:
    try:
        # Read the blacklist file if it exists
        blacklist_path = Path('supernode_inference_ip_blacklist.txt')
        blacklisted_ips = set()
        if blacklist_path.exists():
            with blacklist_path.open('r') as blacklist_file:
                blacklisted_ips = {line.strip() for line in blacklist_file if line.strip()}
        else:
            logger.info("Blacklist file not found. Proceeding without blacklist filtering.")
        async with httpx.AsyncClient() as client:
            tasks = []
            supernode_list_df, _ = await check_supernode_list_func()
            for supernode_pastelid in supernodes:
                payload = {}
                try:
                    supernode_base_url = await get_supernode_url_from_pastelid_func(supernode_pastelid, supernode_list_df)
                    ip_address_port = supernode_base_url.split("//")[1].split(":")[0]
                    if ip_address_port in blacklisted_ips:
                        logger.info(f"Skipping blacklisted supernode {supernode_pastelid}")
                        continue
                    url = f"{supernode_base_url}/credit_pack_purchase_request_final_response_announcement"
                    challenge_dict = await request_and_sign_challenge(supernode_base_url)
                    challenge = challenge_dict["challenge"]
                    challenge_id = challenge_dict["challenge_id"]
                    challenge_signature = challenge_dict["challenge_signature"]
                    response_dict = response.model_dump()
                    response_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in response_dict.items()}
                    payload = {
                        "response": response_dict,
                        "challenge": challenge,
                        "challenge_id": challenge_id,
                        "challenge_signature": challenge_signature
                    }
                except Exception as e:
                    logger.warning(f"Error getting challenge from supernode {supernode_pastelid}: {str(e)}")
                if len(payload) > 0:
                    task = asyncio.create_task(client.post(url, json=payload))
                    tasks.append(task)
            responses = await asyncio.gather(*tasks, return_exceptions=True)
            valid_responses = []
            for response in responses:
                if isinstance(response, httpx.Response):
                    if response.status_code == 200:
                        valid_responses.append(response)
                    else:
                        logger.warning(f"Error sending final response announcement to supernode {response.url}: {response.text}")
                else:
                    logger.error(f"Error sending final response announcement to supernode: {str(response)}")
            return valid_responses
    except Exception as e:
        logger.error(f"Error sending final response announcement to supernodes: {str(e)}")
        traceback.print_exc()
        raise    

def transform_json(input_string):
    # Parse the JSON string into a Python dictionary
    data = json.loads(input_string)
    # Serialize the dictionary back to a JSON string
    # Ensure that special characters are escaped correctly
    transformed_string = json.dumps(data, ensure_ascii=False)
    return transformed_string

async def process_credit_pack_price_agreement_request(price_agreement_request: db_code.CreditPackPurchasePriceAgreementRequest) -> Union[db_code.CreditPackPurchasePriceAgreementRequestResponse, str]:
    try:
        # Validate the request fields
        request_validation_errors = await validate_credit_pack_ticket_message_data_func(price_agreement_request)
        if request_validation_errors:
            logger.error(f"Invalid price agreement request: {', '.join(request_validation_errors)}")
            return f"Invalid price agreement request: {', '.join(request_validation_errors)}"
        # Determine if the supernode agrees with the proposed price
        agree_with_proposed_price = await determine_agreement_with_proposed_price(price_agreement_request.proposed_psl_price_per_credit)
        # Create the response without the hash and signature fields
        response = db_code.CreditPackPurchasePriceAgreementRequestResponse(
            sha3_256_hash_of_price_agreement_request_fields=price_agreement_request.sha3_256_hash_of_price_agreement_request_fields,
            credit_pack_purchase_request_fields_json_b64=price_agreement_request.credit_pack_purchase_request_fields_json_b64,
            agree_with_proposed_price=agree_with_proposed_price,
            credit_usage_tracking_psl_address=price_agreement_request.credit_usage_tracking_psl_address,
            proposed_psl_price_per_credit=price_agreement_request.proposed_psl_price_per_credit,
            proposed_price_agreement_response_timestamp_utc_iso_string=datetime.now(timezone.utc).isoformat(),
            proposed_price_agreement_response_pastel_block_height=await get_current_pastel_block_height_func(),
            proposed_price_agreement_response_message_version_string="1.0",
            responding_supernode_signature_on_credit_pack_purchase_request_fields_json_b64=await sign_message_with_pastelid_func(MY_PASTELID, price_agreement_request.credit_pack_purchase_request_fields_json_b64, LOCAL_PASTEL_ID_PASSPHRASE),
            responding_supernode_pastelid=MY_PASTELID,
            sha3_256_hash_of_price_agreement_request_response_fields="",
            responding_supernode_signature_on_price_agreement_request_response_hash=""
        )
        # Generate the hash and signature fields
        response.sha3_256_hash_of_price_agreement_request_response_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(response)
        response.responding_supernode_signature_on_price_agreement_request_response_hash = await sign_message_with_pastelid_func(
            MY_PASTELID,
            response.sha3_256_hash_of_price_agreement_request_response_fields,
            LOCAL_PASTEL_ID_PASSPHRASE
        )
        # Validate the response
        response_validation_errors = await validate_credit_pack_ticket_message_data_func(response)
        if response_validation_errors:
            logger.error(f"Invalid price agreement request response: {', '.join(response_validation_errors)}")
            return f"Invalid price agreement request response: {', '.join(response_validation_errors)}"
        await save_credit_pack_purchase_price_agreement_request_response(response)        
        return response
    except Exception as e:
        logger.error(f"Error processing credit pack price agreement request: {str(e)}")
        traceback.print_exc()
        raise
            
async def process_credit_purchase_preliminary_price_quote_response(preliminary_price_quote_response: db_code.CreditPackPurchaseRequestPreliminaryPriceQuoteResponse) -> Union[db_code.CreditPackPurchaseRequestResponse, db_code.CreditPackPurchaseRequestResponseTermination]:
    try:
        # Validate the response fields
        if not preliminary_price_quote_response.agree_with_preliminary_price_quote:
            logger.warning("End user does not agree with preliminary price quote! Unable to proceed with credit pack purchase.")
            raise ValueError("End user does not agree with preliminary price quote")
        response_validation_errors = await validate_credit_pack_ticket_message_data_func(preliminary_price_quote_response)
        if response_validation_errors:
            logger.error(f"Invalid preliminary price quote response: {', '.join(response_validation_errors)}")
            raise ValueError(f"Invalid preliminary price quote response: {', '.join(response_validation_errors)}")
        # Select the potentially agreeing supernodes
        logger.info("Now selecting potentially agreeing supernodes to sign off on the proposed credit pricing for the credit pack purchase request...")
        potentially_agreeing_supernodes, total_number_of_blacklisted_sns, list_of_blacklisted_supernode_pastelids = await select_potentially_agreeing_supernodes()
        logger.info(f"Selected {len(potentially_agreeing_supernodes)} potentially agreeing supernodes: {potentially_agreeing_supernodes}")
        # Create the price agreement request without the hash and signature fields
        price_agreement_request = db_code.CreditPackPurchasePriceAgreementRequest(
            sha3_256_hash_of_credit_pack_purchase_request_response_fields=preliminary_price_quote_response.sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields,
            supernode_requesting_price_agreement_pastelid=MY_PASTELID,
            credit_pack_purchase_request_fields_json_b64=preliminary_price_quote_response.credit_pack_purchase_request_fields_json_b64,
            credit_usage_tracking_psl_address=preliminary_price_quote_response.credit_usage_tracking_psl_address,
            proposed_psl_price_per_credit=preliminary_price_quote_response.preliminary_quoted_price_per_credit_in_psl,
            price_agreement_request_timestamp_utc_iso_string=datetime.now(timezone.utc).isoformat(),
            price_agreement_request_pastel_block_height=await get_current_pastel_block_height_func(),
            price_agreement_request_message_version_string="1.0",
            sha3_256_hash_of_price_agreement_request_fields="",
            supernode_requesting_price_agreement_pastelid_signature_on_request_hash=""
        )
        # Generate the hash and signature fields
        price_agreement_request.sha3_256_hash_of_price_agreement_request_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(price_agreement_request)
        price_agreement_request.supernode_requesting_price_agreement_pastelid_signature_on_request_hash = await sign_message_with_pastelid_func(
            MY_PASTELID,
            price_agreement_request.sha3_256_hash_of_price_agreement_request_fields,
            LOCAL_PASTEL_ID_PASSPHRASE
        )
        # Validate the price_agreement_request
        request_validation_errors = await validate_credit_pack_ticket_message_data_func(price_agreement_request)
        if request_validation_errors:
            logger.error(f"Invalid price agreement request: {', '.join(request_validation_errors)}")
            raise ValueError(f"Invalid price agreement request: {', '.join(request_validation_errors)}") 
        await save_credit_pack_purchase_price_agreement_request(price_agreement_request)
        # Send the price agreement request to the potentially agreeing supernodes
        logger.info(f"Now sending price agreement request to {len(potentially_agreeing_supernodes)} potentially agreeing supernodes...")
        price_agreement_request_responses = await send_price_agreement_request_to_supernodes(price_agreement_request, potentially_agreeing_supernodes)
        # Process the price agreement request responses
        valid_price_agreement_request_responses = []
        use_manual_signature_validation = 0
        for current_price_agreement_response in price_agreement_request_responses:
            if use_manual_signature_validation:
                agreeing_supernode_pastelid = current_price_agreement_response.responding_supernode_pastelid
                sha3_256_hash_of_price_agreement_request_response_fields = current_price_agreement_response.sha3_256_hash_of_price_agreement_request_response_fields
                responding_supernode_signature_on_price_agreement_request_response_hash = current_price_agreement_response.responding_supernode_signature_on_price_agreement_request_response_hash
                is_signature_valid = await verify_message_with_pastelid_func(agreeing_supernode_pastelid, sha3_256_hash_of_price_agreement_request_response_fields, responding_supernode_signature_on_price_agreement_request_response_hash)
                if not is_signature_valid:
                    logger.warning(f"Error validating price agreement request response signature from supernode {current_price_agreement_response.responding_supernode_pastelid}")
                    continue
            response_validation_errors = await validate_credit_pack_ticket_message_data_func(current_price_agreement_response)
            if not response_validation_errors:
                valid_price_agreement_request_responses.append(current_price_agreement_response)
        supernode_price_agreement_response_percentage_achieved = len(valid_price_agreement_request_responses) / len(potentially_agreeing_supernodes)
        logger.info(f"Received {len(valid_price_agreement_request_responses)} valid price agreement responses from potentially agreeing supernodes out of {len(potentially_agreeing_supernodes)} asked (excluding {total_number_of_blacklisted_sns} blacklisted supernodes), for a quorum percentage of {supernode_price_agreement_response_percentage_achieved:.2%} (Required minimum quorum percentage is {SUPERNODE_CREDIT_PRICE_AGREEMENT_QUORUM_PERCENTAGE:.2%})")              
        # Check if enough supernodes responded with valid responses
        if supernode_price_agreement_response_percentage_achieved <= SUPERNODE_CREDIT_PRICE_AGREEMENT_QUORUM_PERCENTAGE:
            logger.warning(f"Not enough supernodes responded with valid price agreement responses; only {supernode_price_agreement_response_percentage_achieved:.2%} of the supernodes responded, less than the required quorum percentage of {SUPERNODE_CREDIT_PRICE_AGREEMENT_QUORUM_PERCENTAGE:.2%}")
            logger.info("Responding to end user with termination message...")
            termination_message = db_code.CreditPackPurchaseRequestResponseTermination(
                sha3_256_hash_of_credit_pack_purchase_request_fields=preliminary_price_quote_response.sha3_256_hash_of_credit_pack_purchase_request_fields,
                credit_pack_purchase_request_fields_json_b64=preliminary_price_quote_response.credit_pack_purchase_request_fields_json_b64,
                termination_reason_string="Not enough supernodes responded with valid price agreement responses",
                termination_timestamp_utc_iso_string=datetime.now(timezone.utc).isoformat(),
                termination_pastel_block_height=await get_current_pastel_block_height_func(),
                credit_purchase_request_termination_message_version_string="1.0",
                responding_supernode_pastelid=MY_PASTELID,
                sha3_256_hash_of_credit_pack_purchase_request_termination_fields="",
                responding_supernode_signature_on_credit_pack_purchase_request_termination_hash="",
            )
            termination_message.sha3_256_hash_of_credit_pack_purchase_request_termination_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(termination_message)
            termination_message.responding_supernode_signature_on_credit_pack_purchase_request_termination_hash = await sign_message_with_pastelid_func(
                MY_PASTELID,
                termination_message.sha3_256_hash_of_credit_pack_purchase_request_termination_fields,
                LOCAL_PASTEL_ID_PASSPHRASE
            )
            termination_validation_errors = await validate_credit_pack_ticket_message_data_func(termination_message)
            if termination_validation_errors:
                logger.error(f"Invalid credit purchase request termination message: {', '.join(termination_validation_errors)}")
                raise ValueError(f"Invalid credit purchase request termination message: {', '.join(termination_validation_errors)}")
            return termination_message
        # Tally the agreeing supernodes
        list_of_agreeing_supernodes = [response.responding_supernode_pastelid for response in valid_price_agreement_request_responses if response.agree_with_proposed_price]
        supernode_price_agreement_voting_percentage = len(list_of_agreeing_supernodes) / len(valid_price_agreement_request_responses)
        logger.info(f"Of the {len(valid_price_agreement_request_responses)} valid price agreement responses, {len(list_of_agreeing_supernodes)} supernodes agreed to the proposed pricing, achieving a voting percentage of {supernode_price_agreement_voting_percentage:.2%}")
        # Check if enough supernodes agreed to the proposed pricing
        if supernode_price_agreement_voting_percentage <= SUPERNODE_CREDIT_PRICE_AGREEMENT_MAJORITY_PERCENTAGE:
            logger.warning(f"Not enough supernodes agreed to the proposed pricing; only {supernode_price_agreement_voting_percentage:.2%} of the supernodes agreed, less than the required majority percentage of {SUPERNODE_CREDIT_PRICE_AGREEMENT_MAJORITY_PERCENTAGE:.2%}")
            logger.info("Responding to end user with termination message...")
            termination_message = db_code.CreditPackPurchaseRequestResponseTermination(
                sha3_256_hash_of_credit_pack_purchase_request_fields=preliminary_price_quote_response.sha3_256_hash_of_credit_pack_purchase_request_fields,
                credit_pack_purchase_request_fields_json_b64=preliminary_price_quote_response.credit_pack_purchase_request_fields_json_b64,
                termination_reason_string="Not enough supernodes agreed to the proposed pricing",
                termination_timestamp_utc_iso_string=datetime.now(timezone.utc).isoformat(),
                termination_pastel_block_height=await get_current_pastel_block_height_func(),
                credit_purchase_request_termination_message_version_string="1.0",
                responding_supernode_pastelid=MY_PASTELID,
                sha3_256_hash_of_credit_pack_purchase_request_termination_fields="",
                responding_supernode_signature_on_credit_pack_purchase_request_termination_hash=""
            )
            termination_message.sha3_256_hash_of_credit_pack_purchase_request_termination_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(termination_message)
            termination_message.responding_supernode_signature_on_credit_pack_purchase_request_termination_hash = await sign_message_with_pastelid_func(
                MY_PASTELID,
                termination_message.sha3_256_hash_of_credit_pack_purchase_request_termination_fields,
                LOCAL_PASTEL_ID_PASSPHRASE
            )
            termination_validation_errors = await validate_credit_pack_ticket_message_data_func(termination_message)
            if termination_validation_errors:
                logger.error(f"Invalid credit purchase request termination message: {', '.join(termination_validation_errors)}")
                raise ValueError(f"Invalid credit purchase request termination message: {', '.join(termination_validation_errors)}")            
            return termination_message
        logger.info(f"Enough supernodes agreed to the proposed pricing; {len(list_of_agreeing_supernodes)} supernodes agreed to the proposed pricing, achieving a voting percentage of {supernode_price_agreement_voting_percentage:.2%}, more than the required minimum percentage of {SUPERNODE_CREDIT_PRICE_AGREEMENT_MAJORITY_PERCENTAGE:.2%}")
        # Select top N closest supernodes to the best block merkle root for inclusion in the response
        _, best_block_merkle_root, best_block_height = await get_best_block_hash_and_merkle_root_func()
        list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion = await select_top_n_closest_supernodes_to_best_block_merkle_root(list_of_agreeing_supernodes, n=10, best_block_merkle_root=best_block_merkle_root)
        # Aggregate the signatures from the selected agreeing supernodes
        selected_agreeing_supernodes_signatures_dict = {}
        for response in valid_price_agreement_request_responses:
            if response.responding_supernode_pastelid in list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion:
                selected_agreeing_supernodes_signatures_dict[response.responding_supernode_pastelid] = {
                    "price_agreement_request_response_hash_signature": response.responding_supernode_signature_on_price_agreement_request_response_hash,
                    "credit_pack_purchase_request_fields_json_b64_signature": response.responding_supernode_signature_on_credit_pack_purchase_request_fields_json_b64
                }
        if isinstance(preliminary_price_quote_response.credit_pack_purchase_request_fields_json_b64, str):
            credit_pack_purchase_request_fields_json = base64.b64decode(preliminary_price_quote_response.credit_pack_purchase_request_fields_json_b64).decode('utf-8')
            credit_request_response_dict = json.loads(credit_pack_purchase_request_fields_json)
        requested_initial_credits_in_credit_pack = credit_request_response_dict['requested_initial_credits_in_credit_pack']
        # Create the credit pack purchase request response
        credit_pack_purchase_request_response = db_code.CreditPackPurchaseRequestResponse(
            sha3_256_hash_of_credit_pack_purchase_request_fields=preliminary_price_quote_response.sha3_256_hash_of_credit_pack_purchase_request_fields,
            credit_pack_purchase_request_fields_json_b64=preliminary_price_quote_response.credit_pack_purchase_request_fields_json_b64,
            psl_cost_per_credit=preliminary_price_quote_response.preliminary_quoted_price_per_credit_in_psl,
            proposed_total_cost_of_credit_pack_in_psl=requested_initial_credits_in_credit_pack*preliminary_price_quote_response.preliminary_quoted_price_per_credit_in_psl,
            credit_usage_tracking_psl_address=preliminary_price_quote_response.credit_usage_tracking_psl_address,
            request_response_timestamp_utc_iso_string=datetime.now(timezone.utc).isoformat(),
            request_response_pastel_block_height=await get_current_pastel_block_height_func(),
            best_block_merkle_root=best_block_merkle_root,
            best_block_height=best_block_height,
            credit_purchase_request_response_message_version_string="1.0",
            responding_supernode_pastelid=MY_PASTELID,
            list_of_blacklisted_supernode_pastelids=list_of_blacklisted_supernode_pastelids,
            list_of_potentially_agreeing_supernodes=potentially_agreeing_supernodes,
            list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms=list_of_agreeing_supernodes,
            list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion=list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion,
            selected_agreeing_supernodes_signatures_dict=selected_agreeing_supernodes_signatures_dict,
            sha3_256_hash_of_credit_pack_purchase_request_response_fields="",
            responding_supernode_signature_on_credit_pack_purchase_request_response_hash=""
        )
        logger.info(f"Now generating the final credit pack purchase request response and assembling the {len(list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion)} selected agreeing supernode signatures...")
        # Generate the hash and signature fields
        response_fields_that_are_hashed = await extract_response_fields_from_credit_pack_ticket_message_data_as_json_func(credit_pack_purchase_request_response)  # noqa: F841
        credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_response_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(credit_pack_purchase_request_response)
        credit_pack_purchase_request_response.responding_supernode_signature_on_credit_pack_purchase_request_response_hash = await sign_message_with_pastelid_func(
            MY_PASTELID,
            credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_response_fields,
            LOCAL_PASTEL_ID_PASSPHRASE
        )
        # Validate the credit_pack_purchase_request_response
        request_response_validation_errors = await validate_credit_pack_ticket_message_data_func(credit_pack_purchase_request_response)
        if request_response_validation_errors:
            logger.error(f"Invalid credit pack purchase request response: {', '.join(request_response_validation_errors)}")
            raise ValueError(f"Invalid credit pack purchase request response: {', '.join(request_response_validation_errors)}")          
        await save_credit_pack_purchase_request_response(credit_pack_purchase_request_response)
        # Send the final credit pack purchase request response to the agreeing supernodes
        logger.info(f"Now sending the final credit pack purchase request response to the list of {len(list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion)} selected agreeing supernodes: {list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion}")
        announcement_responses = await send_credit_pack_purchase_request_final_response_to_supernodes(credit_pack_purchase_request_response, list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion)
        logger.info(f"Received {len(announcement_responses)} responses to the final credit pack purchase request response announcement, of which {len([response for response in announcement_responses if response.status_code == 200])} were successful")
        return credit_pack_purchase_request_response
    except Exception as e:
        logger.error(f"Error processing credit purchase preliminary price quote response: {str(e)}")
        traceback.print_exc()
        raise

async def get_credit_purchase_request_status(status_request: db_code.CreditPackRequestStatusCheck) -> db_code.CreditPackPurchaseRequestStatus:
    try:
        # Validate the request fields
        if not status_request.sha3_256_hash_of_credit_pack_purchase_request_fields or not status_request.requesting_end_user_pastelid:
            raise ValueError("Invalid status check request")
        await save_credit_pack_purchase_request_status_check(status_request)
        # Retrieve the credit pack purchase request
        credit_pack_purchase_request = await get_credit_pack_purchase_request(status_request.sha3_256_hash_of_credit_pack_purchase_request_fields)
        # Retrieve the credit pack purchase request response
        credit_pack_purchase_request_response = await get_credit_pack_purchase_request_response_from_request_hash(status_request.sha3_256_hash_of_credit_pack_purchase_request_fields)
        # Check the status of the credit pack purchase request
        status = await check_credit_pack_purchase_request_status(credit_pack_purchase_request)
        # Determine the status details based on the status
        if status == "pending":
            status_details = "Waiting for the credit pack purchase request to be processed"
        elif status == "approved":
            status_details = "Credit pack purchase request has been approved, waiting for confirmation"
        elif status == "confirmed":
            status_details = "Credit pack purchase has been confirmed, waiting for completion"
        elif status == "completed":
            status_details = "Credit pack purchase has been completed successfully"
        elif status == "failed":
            status_details = "Credit pack purchase has failed"
        else:
            status_details = "Unknown status"
        # Create the response
        response = db_code.CreditPackPurchaseRequestStatus(
            sha3_256_hash_of_credit_pack_purchase_request_fields=status_request.sha3_256_hash_of_credit_pack_purchase_request_fields,
            sha3_256_hash_of_credit_pack_purchase_request_response_fields=credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_response_fields if credit_pack_purchase_request_response else "",
            status=status,
            status_details=status_details,
            status_update_timestamp_utc_iso_string=datetime.now(timezone.utc).isoformat(),
            status_update_pastel_block_height=await get_current_pastel_block_height_func(),
            credit_purchase_request_status_message_version_string="1.0",
            responding_supernode_pastelid=credit_pack_purchase_request_response.responding_supernode_pastelid if credit_pack_purchase_request_response else MY_PASTELID,
            sha3_256_hash_of_credit_pack_purchase_request_status_fields="",
            responding_supernode_signature_on_credit_pack_purchase_request_status_hash=""
        )
        # Generate the hash and signature fields
        response.sha3_256_hash_of_credit_pack_purchase_request_status_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(response)
        response.responding_supernode_signature_on_credit_pack_purchase_request_status_hash = await sign_message_with_pastelid_func(
            MY_PASTELID,
            response.sha3_256_hash_of_credit_pack_purchase_request_status_fields,
            LOCAL_PASTEL_ID_PASSPHRASE
        )
        await save_credit_pack_purchase_request_status_check(response)
        return response
    except Exception as e:
        logger.error(f"Error getting credit purchase request status: {str(e)}")
        traceback.print_exc()
        raise
    
async def process_credit_pack_purchase_request_final_response_announcement(response: db_code.CreditPackPurchaseRequestResponse) -> None:
    try:
        # Validate the response fields
        if not response.sha3_256_hash_of_credit_pack_purchase_request_fields or not response.credit_pack_purchase_request_fields_json_b64:
            raise ValueError("Invalid final response announcement")
        # Save the final response to the db
        await save_credit_pack_purchase_request_final_response(response)
    except Exception as e:
        logger.error(f"Error processing credit pack purchase request final response announcement: {str(e)}")
        raise
    
async def get_block_height_for_credit_pack_purchase_request_confirmation(sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str) -> int:
    try:
        # Retrieve the CreditPackPurchaseRequestConfirmation from the database using the hash
        confirmation = await db_code.CreditPackPurchaseRequestConfirmation.get(sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields=sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields)
        return confirmation.credit_purchase_request_confirmation_pastel_block_height
    except Exception as e:
        logger.error(f"Error retrieving block height for CreditPackPurchaseRequestConfirmation: {str(e)}")
        raise
    
async def get_block_height_from_block_hash(pastel_block_hash: str):
    if not pastel_block_hash:
        raise ValueError("Invalid block hash provided.")
    try:
        block_details = await getblock(rpc_connection, pastel_block_hash)
        block_height = block_details.get('height')
        if block_height is not None:
            return block_height
        else:
            raise ValueError("Block height could not be retrieved.")
    except Exception as e:
        raise Exception(f"Error retrieving block height: {str(e)}")

async def get_closest_agreeing_supernode_pastelid(end_user_pastelid: str, agreeing_supernode_pastelids: List[str]) -> str:
    try:
        end_user_pastelid_hash = compute_sha3_256_hexdigest(end_user_pastelid)
        end_user_pastelid_int = int(end_user_pastelid_hash, 16)
        xor_distances = []
        for supernode_pastelid in agreeing_supernode_pastelids:
            supernode_pastelid_hash = compute_sha3_256_hexdigest(supernode_pastelid)
            supernode_pastelid_int = int(supernode_pastelid_hash, 16)
            distance = end_user_pastelid_int ^ supernode_pastelid_int
            xor_distances.append((supernode_pastelid, distance))
        closest_supernode_pastelid = min(xor_distances, key=lambda x: x[1])[0]
        return closest_supernode_pastelid
    except Exception as e:
        logger.error(f"Error getting closest agreeing supernode pastelid: {str(e)}")
        traceback.print_exc()
        raise    
    
async def send_credit_pack_storage_completion_announcement_to_supernodes(response: db_code.CreditPackPurchaseRequestConfirmationResponse, agreeing_supernode_pastelids: List[str]) -> List[httpx.Response]:
    try:
        # Read the blacklist file if it exists
        blacklist_path = Path('supernode_inference_ip_blacklist.txt')
        blacklisted_ips = set()
        if blacklist_path.exists():
            with blacklist_path.open('r') as blacklist_file:
                blacklisted_ips = {line.strip() for line in blacklist_file if line.strip()}
        else:
            logger.info("Blacklist file not found. Proceeding without blacklist filtering.")
        
        async with httpx.AsyncClient() as client:
            tasks = []
            supernode_list_df, _ = await check_supernode_list_func()
            for supernode_pastelid in agreeing_supernode_pastelids:
                payload = {}
                try:
                    supernode_base_url = await get_supernode_url_from_pastelid_func(supernode_pastelid, supernode_list_df)
                    ip_address_port = supernode_base_url.split("//")[1].split(":")[0]
                    if ip_address_port in blacklisted_ips:
                        logger.info(f"Skipping blacklisted supernode {supernode_pastelid}")
                        continue
                    url = f"{supernode_base_url}/credit_pack_storage_completion_announcement"
                    challenge_dict = await request_and_sign_challenge(supernode_base_url)
                    challenge = challenge_dict["challenge"]
                    challenge_id = challenge_dict["challenge_id"]
                    challenge_signature = challenge_dict["challenge_signature"]
                    response_dict = response.model_dump()
                    response_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in response_dict.items()}
                    payload = {
                        "storage_completion_announcement": response_dict,
                        "challenge": challenge,
                        "challenge_id": challenge_id,
                        "challenge_signature": challenge_signature
                    }
                except Exception as e:
                    logger.warning(f"Error getting challenge from supernode {supernode_pastelid}: {str(e)}")
                if len(payload) > 0:
                    task = asyncio.create_task(client.post(url, json=payload))
                    tasks.append(task)
            responses = await asyncio.gather(*tasks, return_exceptions=True)
            valid_responses = []
            for response in responses:
                if isinstance(response, httpx.Response):
                    if response.status_code == 200:
                        valid_responses.append(response)
                    else:
                        logger.warning(f"Error sending storage completion announcement to supernode {response.url}: {response.text}")
                else:
                    logger.error(f"Error sending storage completion announcement to supernode: {str(response)}")
            return valid_responses
    except Exception as e:
        logger.error(f"Error sending storage completion announcement to supernodes: {str(e)}")
        traceback.print_exc()
        raise
    
async def process_credit_purchase_request_confirmation(confirmation: db_code.CreditPackPurchaseRequestConfirmation) -> db_code.CreditPackPurchaseRequestConfirmationResponse:
    try:
        # Validate the confirmation fields
        validation_errors = await validate_credit_pack_ticket_message_data_func(confirmation)
        if validation_errors:
            logger.error(f"Invalid credit purchase request confirmation: {', '.join(validation_errors)}")
            raise ValueError(f"Invalid credit purchase request confirmation: {', '.join(validation_errors)}")
        await save_credit_pack_purchase_request_confirmation(confirmation)
        # Retrieve the credit pack purchase request response
        credit_pack_purchase_request_response = await get_credit_pack_purchase_request_response(confirmation.sha3_256_hash_of_credit_pack_purchase_request_response_fields)
        # Check the burn transaction
        logger.info("Now checking the burn transaction for the credit pack purchase request...")
        matching_transaction_found = False
        exceeding_transaction_found = False
        num_confirmations = 0
        current_block_height = await get_current_pastel_block_height_func()
        initial_transaction_check_sleep_time_in_seconds = 30 
        current_transaction_check_sleep_time_in_seconds = initial_transaction_check_sleep_time_in_seconds
        while current_block_height <= credit_pack_purchase_request_response.request_response_pastel_block_height + MAXIMUM_NUMBER_OF_PASTEL_BLOCKS_FOR_USER_TO_SEND_BURN_AMOUNT_FOR_CREDIT_TICKET:
            matching_transaction_found, exceeding_transaction_found, transaction_block_height, num_confirmations, amount_received_at_burn_address = await check_burn_transaction(
                confirmation.txid_of_credit_purchase_burn_transaction,
                credit_pack_purchase_request_response.credit_usage_tracking_psl_address,
                credit_pack_purchase_request_response.proposed_total_cost_of_credit_pack_in_psl,
                credit_pack_purchase_request_response.request_response_pastel_block_height
            )
            if matching_transaction_found or exceeding_transaction_found:
                if (num_confirmations >= MINIMUM_CONFIRMATION_BLOCKS_FOR_CREDIT_PACK_BURN_TRANSACTION) or SKIP_BURN_TRANSACTION_BLOCK_CONFIRMATION_CHECK:
                    break
                else:
                    logger.info(f"Waiting for {current_transaction_check_sleep_time_in_seconds} seconds to check again if burn transaction is confirmed...")
                    await asyncio.sleep(current_transaction_check_sleep_time_in_seconds) 
            else:
                logger.info(f"Waiting for {current_transaction_check_sleep_time_in_seconds} seconds to check again if burn transaction is confirmed...")
                await asyncio.sleep(current_transaction_check_sleep_time_in_seconds)
            current_transaction_check_sleep_time_in_seconds = round(current_transaction_check_sleep_time_in_seconds*1.05, 2)
            current_block_height = await get_current_pastel_block_height_func()
        if matching_transaction_found or exceeding_transaction_found:
            # Store the credit pack ticket on the blockchain
            credit_pack_purchase_request_json = base64.b64decode(credit_pack_purchase_request_response.credit_pack_purchase_request_fields_json_b64).decode('utf-8')
            credit_pack_purchase_request_dict = json.loads(credit_pack_purchase_request_json)
            credit_pack_purchase_request_response_dict = credit_pack_purchase_request_response.model_dump()
            credit_pack_purchase_request_response_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in credit_pack_purchase_request_response_dict.items()}
            credit_pack_purchase_request_confirmation_dict = confirmation.model_dump()
            credit_pack_purchase_request_confirmation_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in credit_pack_purchase_request_confirmation_dict.items()}
            credit_pack_combined_blockchain_ticket_data = {"credit_pack_purchase_request_dict": credit_pack_purchase_request_dict, "credit_pack_purchase_request_response_dict": credit_pack_purchase_request_response_dict, "credit_pack_purchase_request_confirmation_dict": credit_pack_purchase_request_confirmation_dict}
            credit_pack_combined_blockchain_ticket_data_json = json.dumps(credit_pack_combined_blockchain_ticket_data)
            credit_pack_ticket_bytes_before_compression = sys.getsizeof(credit_pack_combined_blockchain_ticket_data_json)
            compressed_credit_pack_ticket, _ = await compress_data_with_zstd_func(credit_pack_combined_blockchain_ticket_data_json)
            credit_pack_ticket_bytes_after_compression = sys.getsizeof(compressed_credit_pack_ticket)
            compression_ratio = credit_pack_ticket_bytes_before_compression / credit_pack_ticket_bytes_after_compression
            logger.info(f"Achieved a compression ratio of {compression_ratio:.2f} on credit pack ticket data!")
            logger.info(f"Required burn transaction confirmed with {num_confirmations} confirmations; now attempting to write the credit pack ticket to the blockchain (a total of {credit_pack_ticket_bytes_before_compression:,} bytes before compression and {credit_pack_ticket_bytes_after_compression:,} bytes after compression)...")
            log_action_with_payload("Writing", "the credit pack ticket to the blockchain", credit_pack_combined_blockchain_ticket_data_json)
            pastel_api_credit_pack_ticket_registration_txid, storage_validation_error_string = await store_credit_pack_ticket_in_blockchain(credit_pack_combined_blockchain_ticket_data_json)
            if storage_validation_error_string=="":
                credit_pack_confirmation_outcome_string = "success"
                await save_credit_pack_purchase_request_response_txid_mapping(credit_pack_purchase_request_response, pastel_api_credit_pack_ticket_registration_txid)
            else:
                credit_pack_confirmation_outcome_string = "failed"
            # Create the confirmation response without the hash and signature fields
            confirmation_response = db_code.CreditPackPurchaseRequestConfirmationResponse(
                sha3_256_hash_of_credit_pack_purchase_request_fields=confirmation.sha3_256_hash_of_credit_pack_purchase_request_fields,
                sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields=confirmation.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields,
                credit_pack_confirmation_outcome_string=credit_pack_confirmation_outcome_string,
                pastel_api_credit_pack_ticket_registration_txid=pastel_api_credit_pack_ticket_registration_txid,
                credit_pack_confirmation_failure_reason_if_applicable="",
                credit_purchase_request_confirmation_response_utc_iso_string=datetime.now(timezone.utc).isoformat(),
                credit_purchase_request_confirmation_response_pastel_block_height=await get_current_pastel_block_height_func(),
                credit_purchase_request_confirmation_response_message_version_string="1.0",
                responding_supernode_pastelid=MY_PASTELID,
                sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields="",
                responding_supernode_signature_on_credit_pack_purchase_request_confirmation_response_hash=""
            )
            # Generate the hash and signature fields
            confirmation_response.sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(confirmation_response)
            confirmation_response.responding_supernode_signature_on_credit_pack_purchase_request_confirmation_response_hash = await sign_message_with_pastelid_func(
                MY_PASTELID,
                confirmation_response.sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields,
                LOCAL_PASTEL_ID_PASSPHRASE
            )
            confirmation_response_validation_errors = await validate_credit_pack_ticket_message_data_func(confirmation_response)
            # Send the CreditPackPurchaseRequestConfirmationResponse to the agreeing supernodes
            logger.info(f"Now attempting to send the credit pack storage completion announcement to the {len(credit_pack_purchase_request_response.list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms)} agreeing Supernodes...")
            announcement_responses = await send_credit_pack_storage_completion_announcement_to_supernodes(confirmation_response, credit_pack_purchase_request_response.list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms)
            logger.info(f"Received {len(announcement_responses)} responses to the credit pack storage completion announcement, of which {len([response for response in announcement_responses if response.status_code == 200])} were successful")
        else:
            # Create the confirmation response with failure without the hash and signature fields
            confirmation_response = db_code.CreditPackPurchaseRequestConfirmationResponse(
                sha3_256_hash_of_credit_pack_purchase_request_fields=confirmation.sha3_256_hash_of_credit_pack_purchase_request_fields,
                sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields=confirmation.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields,
                credit_pack_confirmation_outcome_string="failure",
                pastel_api_credit_pack_ticket_registration_txid="",
                credit_pack_confirmation_failure_reason_if_applicable="Burn transaction not confirmed within the required number of blocks",
                credit_purchase_request_confirmation_response_utc_iso_string=datetime.now(timezone.utc).isoformat(),
                credit_purchase_request_confirmation_response_pastel_block_height=await get_current_pastel_block_height_func(),
                credit_purchase_request_confirmation_response_message_version_string="1.0",
                responding_supernode_pastelid=MY_PASTELID,
                sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields="",
                responding_supernode_signature_on_credit_pack_purchase_request_confirmation_response_hash=""
            )
            # Generate the hash and signature fields
            confirmation_response.sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(confirmation_response)
            confirmation_response.responding_supernode_signature_on_credit_pack_purchase_request_confirmation_response_hash = await sign_message_with_pastelid_func(
                MY_PASTELID,
                confirmation_response.sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields,
                LOCAL_PASTEL_ID_PASSPHRASE
            )
        confirmation_response_validation_errors = await validate_credit_pack_ticket_message_data_func(confirmation_response)
        if confirmation_response_validation_errors:
            logger.error(f"Invalid credit purchase request confirmation response: {', '.join(confirmation_response_validation_errors)}")
            raise ValueError(f"Invalid credit purchase request confirmation response: {', '.join(confirmation_response_validation_errors)}")
        await save_credit_pack_purchase_request_confirmation_response(confirmation_response)            
        # Validate the response
        confirmation_response_validation_errors = await validate_credit_pack_ticket_message_data_func(confirmation_response)
        if confirmation_response_validation_errors:
            logger.error(f"Invalid credit pack purchase completion announcement: {', '.join(confirmation_response_validation_errors)}")
            raise ValueError(f"Invalid credit purchase request confirmation response: {', '.join(validation_errors)}")
        await save_credit_pack_purchase_request_confirmation_response(confirmation_response)
        return confirmation_response
    except Exception as e:
        logger.error(f"Error processing credit purchase request confirmation: {str(e)}")
        traceback.print_exc()
        raise    
    
async def process_credit_pack_purchase_completion_announcement(confirmation: db_code.CreditPackPurchaseRequestConfirmation) -> None:
    try:
        # Validate the confirmation fields
        if not confirmation.sha3_256_hash_of_credit_pack_purchase_request_fields or not confirmation.sha3_256_hash_of_credit_pack_purchase_request_response_fields:
            raise ValueError("Invalid credit pack purchase completion announcement")
        confirmation_validation_errors = await validate_credit_pack_ticket_message_data_func(confirmation)
        if confirmation_validation_errors:
            logger.error(f"Invalid credit pack purchase completion announcement: {', '.join(confirmation_validation_errors)}")
            raise ValueError(f"Invalid credit pack purchase completion announcement: {', '.join(confirmation_validation_errors)}")
        # Store the completion announcement
        await save_credit_pack_purchase_completion_announcement(confirmation)
    except Exception as e:
        logger.error(f"Error processing credit pack purchase completion announcement: {str(e)}")
        traceback.print_exc()
        raise

async def process_credit_pack_storage_completion_announcement(completion_response: db_code.CreditPackPurchaseRequestConfirmationResponse) -> None:
    try:
        # Validate the response fields
        if not completion_response.sha3_256_hash_of_credit_pack_purchase_request_fields or not completion_response.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields:
            raise ValueError("Invalid credit pack storage completion announcement")
        completion_response_validation_errors = await validate_credit_pack_ticket_message_data_func(completion_response)
        if completion_response_validation_errors:
            logger.error(f"Invalid credit pack storage completion announcement: {', '.join(completion_response_validation_errors)}")
            raise ValueError(f"Invalid credit pack storage completion announcement: {', '.join(completion_response_validation_errors)}")        
        # Store the storage completion announcement
        await save_credit_pack_storage_completion_announcement(completion_response)
    except Exception as e:
        logger.error(f"Error processing credit pack storage completion announcement: {str(e)}")
        traceback.print_exc()
        raise

async def process_credit_pack_storage_retry_request(storage_retry_request: db_code.CreditPackStorageRetryRequest) -> db_code.CreditPackStorageRetryRequestResponse:
    try:
        # Validate the request fields
        storage_retry_request_validation_errors = await validate_credit_pack_ticket_message_data_func(storage_retry_request)
        if storage_retry_request_validation_errors:
            logger.error(f"Invalid credit pack storage retry request: {', '.join(storage_retry_request_validation_errors)}")
            raise ValueError(f"Invalid credit pack storage retry request: {', '.join(storage_retry_request_validation_errors)}")
        await save_credit_pack_storage_retry_request(storage_retry_request)
        # Check if the supernode receiving the retry request is the closest agreeing supernode to the end user's pastelid
        closest_agreeing_supernode_pastelid = await get_closest_agreeing_supernode_pastelid(storage_retry_request.requesting_end_user_pastelid, json.loads(storage_retry_request.credit_pack_purchase_request_response_json)["list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms"])
        if closest_agreeing_supernode_pastelid != MY_PASTELID:
            error_message = f"The supernode receiving the retry request is not the closest agreeing supernode to the end user's pastelid! Closest agreeing supernode to end user's pastelid ({storage_retry_request.requesting_end_user_pastelid}) is {closest_agreeing_supernode_pastelid}, whereas our local pastelid is {MY_PASTELID}"
            logger.error(error_message)
            raise ValueError(error_message)
        # Check if more than MINIMUM_NUMBER_OF_PASTEL_BLOCKS_BEFORE_TICKET_STORAGE_RETRY_ALLOWED Pastel blocks have elapsed since the CreditPackPurchaseRequestConfirmation was sent
        confirmation_block_height = await get_block_height_for_credit_pack_purchase_request_confirmation(storage_retry_request.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields)
        current_block_height = await get_current_pastel_block_height_func()
        if current_block_height - confirmation_block_height <= MINIMUM_NUMBER_OF_PASTEL_BLOCKS_BEFORE_TICKET_STORAGE_RETRY_ALLOWED:
            error_message = f"Insufficient time has elapsed since the CreditPackPurchaseRequestConfirmation was sent (current block height: {current_block_height}, confirmation block height: {confirmation_block_height}, so the elapsed block count of {current_block_height - confirmation_block_height} is less than the minimum required block count of {MINIMUM_NUMBER_OF_PASTEL_BLOCKS_BEFORE_TICKET_STORAGE_RETRY_ALLOWED})"
            logger.error(error_message)
            raise ValueError(error_message)
        # Check if the original responding supernode has confirmed the storage
        original_supernode_confirmed_storage = await check_original_supernode_storage_confirmation(storage_retry_request.sha3_256_hash_of_credit_pack_purchase_request_response_fields)
        if not original_supernode_confirmed_storage:
            # Store the credit pack ticket on the blockchain
            credit_pack_purchase_request_response = await get_credit_pack_purchase_request_response(storage_retry_request.sha3_256_hash_of_credit_pack_purchase_request_response_fields)
            credit_pack_purchase_request_response_dict = credit_pack_purchase_request_response.model_dump()
            credit_pack_purchase_request_json = base64.b64decode(credit_pack_purchase_request_response_dict['credit_pack_purchase_request_fields_json_b64']).decode('utf-8')
            credit_pack_purchase_request_dict = json.loads(credit_pack_purchase_request_json)
            credit_pack_purchase_request_confirmation = await get_credit_pack_purchase_request_confirmation_from_request_hash(credit_pack_purchase_request_response_dict['sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields'])
            credit_pack_purchase_request_confirmation_dict = credit_pack_purchase_request_confirmation.model_dump()
            credit_pack_purchase_request_confirmation_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in credit_pack_purchase_request_confirmation_dict.items()}
            credit_pack_combined_blockchain_ticket_data = {"credit_pack_purchase_request_dict": credit_pack_purchase_request_dict, "credit_pack_purchase_request_response_dict": credit_pack_purchase_request_response_dict, "credit_pack_purchase_request_confirmation_dict": credit_pack_purchase_request_confirmation_dict}
            credit_pack_combined_blockchain_ticket_data_json = json.dumps(credit_pack_combined_blockchain_ticket_data)
            pastel_api_credit_pack_ticket_registration_txid, storage_validation_error_string = await store_credit_pack_ticket_in_blockchain(credit_pack_combined_blockchain_ticket_data_json)
            if storage_validation_error_string=="":
                credit_pack_confirmation_outcome_string = "success"
            else:
                credit_pack_confirmation_outcome_string = "failed"            
            # Create the retry request response without the hash and signature fields
            retry_request_response = db_code.CreditPackStorageRetryRequestResponse(
                sha3_256_hash_of_credit_pack_purchase_request_fields=storage_retry_request.sha3_256_hash_of_credit_pack_purchase_request_response_fields,
                sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields=storage_retry_request.sha3_256_hash_of_credit_pack_purchase_request_response_fields,
                credit_pack_storage_retry_confirmation_outcome_string=credit_pack_confirmation_outcome_string,
                pastel_api_credit_pack_ticket_registration_txid=pastel_api_credit_pack_ticket_registration_txid,
                credit_pack_storage_retry_confirmation_failure_reason_if_applicable="",
                credit_pack_storage_retry_confirmation_response_utc_iso_string=datetime.now(timezone.utc).isoformat(),
                credit_pack_storage_retry_confirmation_response_pastel_block_height=await get_current_pastel_block_height_func(),
                credit_pack_storage_retry_confirmation_response_message_version_string="1.0",
                closest_agreeing_supernode_to_retry_storage_pastelid=MY_PASTELID,
                sha3_256_hash_of_credit_pack_storage_retry_confirmation_response_fields="",
                closest_agreeing_supernode_to_retry_storage_pastelid_signature_on_credit_pack_storage_retry_confirmation_response_hash=""
            )
            # Generate the hash and signature fields
            retry_request_response.sha3_256_hash_of_credit_pack_storage_retry_confirmation_response_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(retry_request_response)
            retry_request_response.closest_agreeing_supernode_to_retry_storage_pastelid_signature_on_credit_pack_storage_retry_confirmation_response_hash = await sign_message_with_pastelid_func(
                MY_PASTELID,
                retry_request_response.sha3_256_hash_of_credit_pack_storage_retry_confirmation_response_fields,
                LOCAL_PASTEL_ID_PASSPHRASE
            )
        else:
            # Create the retry request response with failure without the hash and signature fields
            retry_request_response = db_code.CreditPackStorageRetryRequestResponse(
                sha3_256_hash_of_credit_pack_purchase_request_fields=storage_retry_request.sha3_256_hash_of_credit_pack_purchase_request_response_fields,
                sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields=storage_retry_request.sha3_256_hash_of_credit_pack_purchase_request_response_fields,
                credit_pack_storage_retry_confirmation_outcome_string="failure",
                pastel_api_credit_pack_ticket_registration_txid="",
                credit_pack_storage_retry_confirmation_failure_reason_if_applicable="Original responding supernode already confirmed storage",
                credit_pack_storage_retry_confirmation_response_utc_iso_string=datetime.now(timezone.utc).isoformat(),
                credit_pack_storage_retry_confirmation_response_pastel_block_height=await get_current_pastel_block_height_func(),
                credit_pack_storage_retry_confirmation_response_message_version_string="1.0",
                closest_agreeing_supernode_to_retry_storage_pastelid=MY_PASTELID,
                sha3_256_hash_of_credit_pack_storage_retry_confirmation_response_fields="",
                closest_agreeing_supernode_to_retry_storage_pastelid_signature_on_credit_pack_storage_retry_confirmation_response_hash=""
            )
            # Generate the hash and signature fields
            retry_request_response.sha3_256_hash_of_credit_pack_storage_retry_confirmation_response_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(retry_request_response)
            retry_request_response.closest_agreeing_supernode_to_retry_storage_pastelid_signature_on_credit_pack_storage_retry_confirmation_response_hash = await sign_message_with_pastelid_func(
                MY_PASTELID,
                retry_request_response.sha3_256_hash_of_credit_pack_storage_retry_confirmation_response_fields,
                LOCAL_PASTEL_ID_PASSPHRASE
            )
        # Validate the response
        retry_request_response_validation_errors = await validate_credit_pack_ticket_message_data_func(retry_request_response)
        if retry_request_response_validation_errors:
            logger.error(f"Invalid credit pack storage retry request response: {', '.join(retry_request_response_validation_errors)}")
            raise ValueError(f"Invalid credit pack storage retry request response: {', '.join(retry_request_response_validation_errors)}")
        await save_credit_pack_storage_retry_request_response(retry_request_response)
        return retry_request_response_validation_errors
    except Exception as e:
        logger.error(f"Error processing credit pack storage retry request: {str(e)}")
        traceback.print_exc()
        raise
                    
async def process_credit_pack_storage_retry_completion_announcement(retry_completion_response: db_code.CreditPackStorageRetryRequestResponse) -> None:
    try:
        # Validate the response fields
        if not retry_completion_response.sha3_256_hash_of_credit_pack_purchase_request_fields or not retry_completion_response.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields:
            raise ValueError("Invalid credit pack storage retry completion announcement")
        retry_completion_response_validation_errors = await validate_credit_pack_ticket_message_data_func(retry_completion_response)
        if retry_completion_response_validation_errors:
            logger.error(f"Invalid credit pack storage retry completion announcement: {', '.join(retry_completion_response_validation_errors)}")
            raise ValueError(f"Invalid credit pack storage retry completion announcement: {', '.join(retry_completion_response_validation_errors)}")        
        # Store the storage retry completion announcement
        await save_credit_pack_storage_retry_completion_announcement(retry_completion_response)
    except Exception as e:
        logger.error(f"Error processing credit pack storage retry completion announcement: {str(e)}")
        traceback.print_exc()
        raise
    
async def insert_credit_pack_ticket_txid_into_known_bad_table_in_db(credit_pack_ticket_txid: str, list_of_reasons_it_is_known_bad: str):
    known_bad_txid_object = db_code.CreditPackKnownBadTXID(credit_pack_ticket_txid=credit_pack_ticket_txid, list_of_reasons_it_is_known_bad=list_of_reasons_it_is_known_bad)
    async with db_code.Session() as db_session:
        db_session.add(known_bad_txid_object)
        await db_session.commit()
        await db_session.refresh(known_bad_txid_object)
    return known_bad_txid_object

async def clear_out_all_credit_packs_from_known_bad_table():
    try:
        async with db_code.Session() as db_session:
            # Delete all entries from the CreditPackKnownBadTXID table
            await db_session.exec(delete(db_code.CreditPackKnownBadTXID))
            await db_session.commit()
            logger.info("Successfully cleared all credit packs from the known bad table.")
    except Exception as e:
        logger.error(f"Error clearing credit packs from known bad table: {str(e)}")
        traceback.print_exc()
        raise

async def get_list_of_all_known_bad_credit_pack_ticket_txids_from_db():
    async with db_code.Session() as db_session:
        known_bad_txids = await db_session.exec(select(db_code.CreditPackKnownBadTXID))
        known_bad_txid_results = known_bad_txids.all()
        if known_bad_txid_results is not None:
            list_of_all_known_bad_credit_pack_txids = list(set([x.credit_pack_ticket_txid for x in known_bad_txid_results]))          
            return list_of_all_known_bad_credit_pack_txids
        else:
            return []        

async def check_if_credit_pack_ticket_txid_in_list_of_known_bad_txids_in_db(credit_pack_ticket_txid: str):
    async with db_code.Session() as db_session:
        # Query to check if the TXID exists in the known bad TXIDs table
        result = await db_session.execute(
            select(db_code.CreditPackKnownBadTXID)
            .where(db_code.CreditPackKnownBadTXID.credit_pack_ticket_txid == credit_pack_ticket_txid)
        )
        known_bad_txid = result.scalar_one_or_none()
        # Return True if the TXID is found in the known bad list, False otherwise
        return known_bad_txid is not None
    
async def validate_merkle_root_at_block_height(merkle_root_to_check: str, block_height: int) -> bool:
    try:
        if block_height is None:
            logger.error("Block height is None")
            return False
        block_hash = await getblockhash(rpc_connection, block_height)
        if not block_hash:
            logger.error(f"Failed to get block hash for height {block_height}")
            return False
        block_details = await getblock(rpc_connection, block_hash)
        if not block_details:
            logger.error(f"Failed to get block details for hash {block_hash}")
            return False
        if 'merkleroot' not in block_details:
            logger.error(f"Merkle root not found in block details for hash {block_hash}")
            return False
        return block_details['merkleroot'] == merkle_root_to_check
    except Exception as e:
        logger.error(f"Error validating merkle root: {str(e)}")
        return False

@async_disk_cached(credit_pack_cache)
async def get_credit_pack_ticket_data(credit_pack_ticket_registration_txid: str, list_of_credit_pack_ticket_data: list, idx: int) -> dict:
    try:
        current_credit_pack_ticket_data = list_of_credit_pack_ticket_data[idx]
        current_credit_pack_ticket_data_json = base64.b64decode(current_credit_pack_ticket_data['ticket']['contract_ticket']).decode('utf-8')
        current_credit_pack_ticket_data_dict = json.loads(current_credit_pack_ticket_data_json)
        return {
            'credit_pack_registration_txid': credit_pack_ticket_registration_txid,
            'credit_purchase_request_confirmation_pastel_block_height': current_credit_pack_ticket_data_dict['ticket_input_data_dict']['credit_pack_purchase_request_confirmation_dict']['credit_purchase_request_confirmation_pastel_block_height'],
            'requesting_end_user_pastelid': current_credit_pack_ticket_data_dict['ticket_input_data_dict']['credit_pack_purchase_request_confirmation_dict']['requesting_end_user_pastelid'],
            'ticket_input_data_fully_parsed_sha3_256_hash': current_credit_pack_ticket_data_dict['ticket_input_data_fully_parsed_sha3_256_hash'],
            'txid_of_credit_purchase_burn_transaction': current_credit_pack_ticket_data_dict['ticket_input_data_dict']['credit_pack_purchase_request_confirmation_dict']['txid_of_credit_purchase_burn_transaction'],
            'credit_usage_tracking_psl_address': current_credit_pack_ticket_data_dict['ticket_input_data_dict']['credit_pack_purchase_request_response_dict']['credit_usage_tracking_psl_address'],
            'psl_cost_per_credit': current_credit_pack_ticket_data_dict['ticket_input_data_dict']['credit_pack_purchase_request_response_dict']['psl_cost_per_credit'],
            'requested_initial_credits_in_credit_pack': current_credit_pack_ticket_data_dict['ticket_input_data_dict']['credit_pack_purchase_request_dict']['requested_initial_credits_in_credit_pack'],
            'complete_credit_pack_data_json': current_credit_pack_ticket_data_json,
        }
    except Exception as e:
        logger.error(f"Error retrieving credit pack ticket data for TXID {credit_pack_ticket_registration_txid}: {str(e)}")
        traceback.print_exc()
        raise
    
@async_disk_cached(credit_pack_cache)
async def validate_existing_credit_pack_ticket(credit_pack_ticket_txid: str) -> dict:
    try:
        use_verbose_validation = 0
        logger.info(f"Validating credit pack ticket with TXID: {credit_pack_ticket_txid}")
        # Retrieve the credit pack ticket data from the blockchain
        credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation = await retrieve_credit_pack_ticket_from_blockchain_using_txid(credit_pack_ticket_txid)
        if use_verbose_validation:
            logger.info(f"Credit pack ticket data for credit pack with TXID {credit_pack_ticket_txid}:\n\nTicket Request Response:\n\n {abbreviated_pretty_json_func(credit_pack_purchase_request_response.model_dump())} \n\nTicket Request Confirmation:\n\n {abbreviated_pretty_json_func(credit_pack_purchase_request_confirmation.model_dump())}")
        validation_results = {
            "credit_pack_ticket_is_valid": True,
            "validation_checks": [],
            "validation_failure_reasons_list": []
        }
        # Define validation tasks
        validation_tasks = [
            validate_payment(credit_pack_ticket_txid, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation, validation_results),
            validate_supernode_lists(credit_pack_ticket_txid, credit_pack_purchase_request_response, validation_results),
            validate_hashes(credit_pack_ticket_txid, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation, validation_results),
            validate_signatures(credit_pack_ticket_txid, credit_pack_purchase_request_response, use_verbose_validation, validation_results),
            validate_selected_agreeing_supernodes(credit_pack_ticket_txid, credit_pack_purchase_request_response, validation_results)
        ]
        # Run validation tasks in parallel
        await asyncio.gather(*validation_tasks)
        if validation_results["credit_pack_ticket_is_valid"] and not validation_results["validation_failure_reasons_list"]:
            logger.info(f"All validation checks passed for credit pack ticket with TXID {credit_pack_ticket_txid}")
        else:
            logger.info(f"Validation failures for credit pack ticket with TXID {credit_pack_ticket_txid}: {validation_results['validation_failure_reasons_list']}")
            list_of_reasons_it_is_known_bad = json.dumps(validation_results['validation_failure_reasons_list'])
            known_bad_txid_object = await insert_credit_pack_ticket_txid_into_known_bad_table_in_db(credit_pack_ticket_txid, list_of_reasons_it_is_known_bad)
            if known_bad_txid_object:
                logger.info(f"Added invalid credit pack ticket TXID {credit_pack_ticket_txid} to known bad list in database!")
        # Sort validation_failure_reasons_list alphabetically
        validation_results["validation_failure_reasons_list"].sort()
        logger.info(f"Validation completed successfully for TXID: {credit_pack_ticket_txid}")
        return validation_results
    except Exception as e:
        logger.error(f"Error validating credit pack ticket with TXID {credit_pack_ticket_txid}: {str(e)}")
        logger.error(f"Exception type: {type(e).__name__}")
        logger.error(f"Exception details: {traceback.format_exc()}")
        return {
            "credit_pack_ticket_is_valid": False,
            "validation_checks": [],
            "validation_failure_reasons_list": [f"Exception during validation: {str(e)}"]
        }
        
async def validate_payment(credit_pack_ticket_txid, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation, validation_results):
    matching_transaction_found, exceeding_transaction_found, transaction_block_height, num_confirmations, amount_received_at_burn_address = await check_burn_transaction(
        credit_pack_purchase_request_confirmation.txid_of_credit_purchase_burn_transaction,
        credit_pack_purchase_request_response.credit_usage_tracking_psl_address,
        credit_pack_purchase_request_response.proposed_total_cost_of_credit_pack_in_psl,
        credit_pack_purchase_request_response.request_response_pastel_block_height
    )
    validation_results["validation_checks"].append({
        "check_name": f"Ticket Payment Burn Transaction validation (Burn payment TXID: {credit_pack_purchase_request_confirmation.txid_of_credit_purchase_burn_transaction} sent with {amount_received_at_burn_address} PSL",
        "is_valid": matching_transaction_found or exceeding_transaction_found
    })
    if not (matching_transaction_found or exceeding_transaction_found):
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append(f"Invalid burn transaction for credit pack ticket with TXID: {credit_pack_ticket_txid}")

async def validate_supernode_lists(credit_pack_ticket_txid, credit_pack_purchase_request_response, validation_results):
    active_supernodes_count_at_the_time, active_supernodes_at_the_time = await fetch_active_supernodes_count_and_details(credit_pack_purchase_request_response.request_response_pastel_block_height)
    list_of_active_supernode_pastelids_at_the_time = [x["pastel_id"] for x in active_supernodes_at_the_time]
    list_of_potentially_agreeing_supernodes = credit_pack_purchase_request_response.list_of_potentially_agreeing_supernodes
    list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms = credit_pack_purchase_request_response.list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms
    list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion = credit_pack_purchase_request_response.list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion
    if list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion is None:
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append("Required field 'list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion' is missing in credit pack purchase request response")
        return
    for potentially_agreeing_supernode_pastelid in list_of_potentially_agreeing_supernodes:
        potentially_agreeing_supernode_pastelid_in_list_of_active_supernodes_at_block_height = potentially_agreeing_supernode_pastelid in list_of_active_supernode_pastelids_at_the_time
        validation_results["validation_checks"].append({
            "check_name": f"Potentially agreeing supernode with pastelid {potentially_agreeing_supernode_pastelid} was in the list of active supernodes as of block height {credit_pack_purchase_request_response.request_response_pastel_block_height:,}",
            "is_valid": potentially_agreeing_supernode_pastelid_in_list_of_active_supernodes_at_block_height
        })
        if not potentially_agreeing_supernode_pastelid_in_list_of_active_supernodes_at_block_height:
            validation_results["credit_pack_ticket_is_valid"] = False
            validation_results["validation_failure_reasons_list"].append(f"Potentially agreeing supernode with pastelid {potentially_agreeing_supernode_pastelid} was NOT in the list of active supernodes as of block height {credit_pack_purchase_request_response.request_response_pastel_block_height:,}")
    for agreeing_supernode_pastelid in list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms:
        agreeing_supernode_pastelid_in_list_of_active_supernodes_at_block_height = agreeing_supernode_pastelid in list_of_active_supernode_pastelids_at_the_time
        validation_results["validation_checks"].append({
            "check_name": f"Agreeing supernode with pastelid {agreeing_supernode_pastelid} was in the list of active supernodes as of block height {credit_pack_purchase_request_response.request_response_pastel_block_height:,}",
            "is_valid": agreeing_supernode_pastelid_in_list_of_active_supernodes_at_block_height
        })
        if not agreeing_supernode_pastelid_in_list_of_active_supernodes_at_block_height:
            validation_results["credit_pack_ticket_is_valid"] = False
            validation_results["validation_failure_reasons_list"].append(f"Agreeing supernode with pastelid {agreeing_supernode_pastelid} was NOT in the list of active supernodes as of block height {credit_pack_purchase_request_response.request_response_pastel_block_height:,}")

async def validate_hashes(credit_pack_ticket_txid, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation, validation_results):
    credit_pack_purchase_request_response_transformed = parse_sqlmodel_strings_into_lists_and_dicts_func(credit_pack_purchase_request_response)
    credit_pack_purchase_request_confirmation_transformed = parse_sqlmodel_strings_into_lists_and_dicts_func(credit_pack_purchase_request_confirmation)
    validation_errors_in_credit_pack_purchase_request_response = await validate_credit_pack_blockchain_ticket_data_field_hashes(credit_pack_purchase_request_response_transformed)
    validation_errors_in_credit_pack_purchase_request_confirmation = await validate_credit_pack_blockchain_ticket_data_field_hashes(credit_pack_purchase_request_confirmation_transformed)
    if len(validation_errors_in_credit_pack_purchase_request_response) > 0:
        logger.warning(f"Warning! Computed hash does not match for ticket request response object for credit pack ticket with txid {credit_pack_ticket_txid}; Validation errors detected:\n{validation_errors_in_credit_pack_purchase_request_response}")
        validation_results["validation_checks"].append({
            "check_name": f"Computed hash does not match for ticket request response object for credit pack ticket with txid: {validation_errors_in_credit_pack_purchase_request_response}",
            "is_valid": False
        })
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append("Hash of credit pack request response object stored in blockchain does not match the hash included in the object.")
    if len(validation_errors_in_credit_pack_purchase_request_confirmation) > 0:
        logger.warning(f"Warning! Computed hash does not match for ticket request confirmation object for credit pack ticket with txid {credit_pack_ticket_txid}; Validation errors detected:\n{validation_errors_in_credit_pack_purchase_request_confirmation}")
        validation_results["validation_checks"].append({
            "check_name": f"Computed hash does not match for ticket request confirmation object for credit pack ticket with txid: {validation_errors_in_credit_pack_purchase_request_confirmation}",
            "is_valid": False
        })
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append("Hash of credit pack request confirmation object stored in blockchain does not match the hash included in the object.")

async def validate_signatures(credit_pack_ticket_txid, credit_pack_purchase_request_response, use_verbose_validation, validation_results):
    list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion = credit_pack_purchase_request_response.list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion
    if list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion:
        selected_agreeing_supernodes_signatures_dict = credit_pack_purchase_request_response.selected_agreeing_supernodes_signatures_dict
        for agreeing_supernode_pastelid in list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion:
            signatures = selected_agreeing_supernodes_signatures_dict[agreeing_supernode_pastelid]
            if use_verbose_validation:
                logger.info(f"Verifying signature for selected agreeing supernode {agreeing_supernode_pastelid}")
                logger.info(f"Message to verify (decoded from b64): {credit_pack_purchase_request_response.credit_pack_purchase_request_fields_json_b64}")
                logger.info(f"Signature: {signatures['credit_pack_purchase_request_fields_json_b64_signature']}")
            is_fields_json_b64_signature_valid = await verify_message_with_pastelid_func(
                agreeing_supernode_pastelid,
                credit_pack_purchase_request_response.credit_pack_purchase_request_fields_json_b64,
                signatures['credit_pack_purchase_request_fields_json_b64_signature']
            )
            if not is_fields_json_b64_signature_valid:
                logger.warning(f"Warning! Signature failed for SN {agreeing_supernode_pastelid} for credit pack with txid {credit_pack_ticket_txid}")
            if use_verbose_validation:
                logger.info(f"Signature validation result: {is_fields_json_b64_signature_valid}")
            validation_results["validation_checks"].append({
                "check_name": f"Signature validation for selected agreeing supernode {agreeing_supernode_pastelid} on credit pack purchase request fields json",
                "is_valid": is_fields_json_b64_signature_valid
            })
            if not is_fields_json_b64_signature_valid:
                validation_results["credit_pack_ticket_is_valid"] = False
                validation_results["validation_failure_reasons_list"].append(f"Signature failed for SN {agreeing_supernode_pastelid} for credit pack with txid {credit_pack_ticket_txid}")
    else:
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append("Required field 'list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion' is missing in credit pack purchase request response")

async def validate_selected_agreeing_supernodes(credit_pack_ticket_txid, credit_pack_purchase_request_response, validation_results):
    list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms = credit_pack_purchase_request_response.list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms
    best_block_merkle_root = credit_pack_purchase_request_response.best_block_merkle_root
    best_block_height = credit_pack_purchase_request_response.best_block_height
    if not best_block_merkle_root or not best_block_height:
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append(f"Missing best block merkle root or height for credit pack with txid {credit_pack_ticket_txid}")
        return
    selected_agreeing_supernodes = await select_top_n_closest_supernodes_to_best_block_merkle_root(
        list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms,
        n=10,
        best_block_merkle_root=best_block_merkle_root
    )
    best_block_merkle_root_matches = await validate_merkle_root_at_block_height(best_block_merkle_root, best_block_height)
    if not best_block_merkle_root_matches:
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append(f"Best block merkle root {best_block_merkle_root} does not match actual merkle root as of block height {best_block_height} for credit pack with txid {credit_pack_ticket_txid}")
    max_block_height_difference_between_best_block_height_and_request_response_pastel_block_height = 10
    actual_block_height_difference_between_best_block_height_and_request_response_pastel_block_height = abs(credit_pack_purchase_request_response.request_response_pastel_block_height - best_block_height)
    if actual_block_height_difference_between_best_block_height_and_request_response_pastel_block_height > max_block_height_difference_between_best_block_height_and_request_response_pastel_block_height:
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append(f"Block height difference between specified best block height {best_block_height} and request response pastel block height {credit_pack_purchase_request_response.request_response_pastel_block_height} exceeds the maximum allowed difference of {max_block_height_difference_between_best_block_height_and_request_response_pastel_block_height} for credit pack with txid {credit_pack_ticket_txid}")
    selected_agreeing_supernodes_set = set(credit_pack_purchase_request_response.list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion)
    computed_selected_agreeing_supernodes_set = set(selected_agreeing_supernodes)
    is_selected_agreeing_supernodes_valid = selected_agreeing_supernodes_set == computed_selected_agreeing_supernodes_set
    validation_results["validation_checks"].append({
        "check_name": "Validation of selected agreeing supernodes for signature inclusion",
        "is_valid": is_selected_agreeing_supernodes_valid,
        "expected_selected_agreeing_supernodes": list(computed_selected_agreeing_supernodes_set),
        "actual_selected_agreeing_supernodes": list(selected_agreeing_supernodes_set)
    })
    if not is_selected_agreeing_supernodes_valid:
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append("Selected agreeing supernodes for signature inclusion do not match the expected set based on XOR distance to the best block merkle root.")
    num_potentially_agreeing_supernodes = len(credit_pack_purchase_request_response.list_of_potentially_agreeing_supernodes)
    num_agreeing_supernodes = len(list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms)
    number_of_blacklisted_supernodes_at_the_time = len(credit_pack_purchase_request_response.list_of_blacklisted_supernode_pastelids)
    quorum_percentage = num_potentially_agreeing_supernodes / (num_potentially_agreeing_supernodes + number_of_blacklisted_supernodes_at_the_time)
    agreeing_percentage = num_agreeing_supernodes / num_potentially_agreeing_supernodes
    is_quorum_valid = quorum_percentage >= SUPERNODE_CREDIT_PRICE_AGREEMENT_QUORUM_PERCENTAGE
    is_agreeing_percentage_valid = agreeing_percentage >= SUPERNODE_CREDIT_PRICE_AGREEMENT_MAJORITY_PERCENTAGE
    validation_results["validation_checks"].append({
        "check_name": "Agreeing supernodes percentage validation",
        "is_valid": is_agreeing_percentage_valid and is_quorum_valid,
        "quorum_percentage": round(quorum_percentage, 5),
        "agreeing_percentage": round(agreeing_percentage, 5)
    })
    if not is_quorum_valid:
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append(f"Agreeing supernodes quorum percentage validation failed for credit pack with txid {credit_pack_ticket_txid}; quorum % was only {quorum_percentage:.3f} (i.e., {num_potentially_agreeing_supernodes} out of {(num_potentially_agreeing_supernodes + number_of_blacklisted_supernodes_at_the_time)}) and the minimum required % is {SUPERNODE_CREDIT_PRICE_AGREEMENT_QUORUM_PERCENTAGE:.3f}")
    if not is_agreeing_percentage_valid:
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append(f"Agreeing supernodes agreement percentage validation failed for credit pack with txid {credit_pack_ticket_txid}; agreement % was only {agreeing_percentage:.3f} (i.e., {num_agreeing_supernodes} out of {num_potentially_agreeing_supernodes}) and the minimum required % is {SUPERNODE_CREDIT_PRICE_AGREEMENT_MAJORITY_PERCENTAGE:.3f}")

async def get_valid_credit_pack_tickets_for_pastelid_old(pastelid: str) -> List[dict]:
    async def process_request_confirmation(request_confirmation):
        txid = request_confirmation.txid_of_credit_purchase_burn_transaction
        logger.info(f'Getting credit pack registration txid for ticket with burn txid of {txid}...')
        registration_txid = await get_credit_pack_ticket_registration_txid_from_corresponding_burn_transaction_txid(txid)
        logger.info(f'Got credit pack registration txid for ticket with burn txid of {txid}: {registration_txid}')        
        credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation = await retrieve_credit_pack_ticket_from_blockchain_using_txid(registration_txid)
        if not all((credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation)):
            logger.warning(f"Incomplete credit pack ticket data in database for TXID {registration_txid}. Fetching from blockchain.")
            credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation = await retrieve_credit_pack_ticket_from_blockchain_using_txid(registration_txid)
            if all((credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation)):
                await save_credit_pack_ticket_to_database(credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation, registration_txid)
            else:
                logger.error(f"Failed to retrieve complete credit pack ticket data for TXID {txid} from blockchain")
                return None
        current_credit_balance, number_of_confirmation_transactions = await determine_current_credit_pack_balance_based_on_tracking_transactions(txid)
        complete_ticket = {
            "credit_pack_purchase_request": json.loads(base64.b64decode(credit_pack_purchase_request_response.credit_pack_purchase_request_fields_json_b64).decode('utf-8')),
            "credit_pack_purchase_request_response": credit_pack_purchase_request_response.model_dump(),
            "credit_pack_purchase_request_confirmation": credit_pack_purchase_request_confirmation.model_dump(),
            "credit_pack_registration_txid": registration_txid,
            "credit_pack_current_credit_balance": current_credit_balance,
            "balance_as_of_datetime": datetime.now(timezone.utc).isoformat()
        }
        complete_ticket = convert_uuids_to_strings(complete_ticket)
        complete_ticket = normalize_data(complete_ticket)
        async with db_code.Session() as db_session:
            existing_data = await db_session.exec(
                select(db_code.CreditPackCompleteTicketWithBalance)
                .where(db_code.CreditPackCompleteTicketWithBalance.credit_pack_ticket_registration_txid == registration_txid)
            )
            existing_data = existing_data.first()
            if existing_data:
                existing_data.complete_credit_pack_data_json = json.dumps(complete_ticket)
                existing_data.datetime_last_updated = datetime.now(timezone.utc)
                db_session.add(existing_data)
            else:
                new_complete_ticket = db_code.CreditPackCompleteTicketWithBalance(
                    credit_pack_ticket_registration_txid=registration_txid,
                    complete_credit_pack_data_json=json.dumps(complete_ticket),
                    datetime_last_updated=datetime.now(timezone.utc)
                )
                db_session.add(new_complete_ticket)
            await db_session.commit()
        return complete_ticket
    try:
        async with db_code.Session() as db_session:
            credit_pack_request_confirmations_results = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestConfirmation)
                .where(db_code.CreditPackPurchaseRequestConfirmation.requesting_end_user_pastelid == pastelid)
            )
            credit_pack_request_confirmations = credit_pack_request_confirmations_results.all()
        tasks = [process_request_confirmation(request_confirmation) for request_confirmation in credit_pack_request_confirmations]
        complete_tickets = await asyncio.gather(*tasks)
        complete_tickets = [ticket for ticket in complete_tickets if ticket]
        return complete_tickets
    except Exception as e:
        logger.error(f"Error retrieving credit pack tickets for PastelID {pastelid}: {str(e)}")
        traceback.print_exc()
        raise

@async_disk_cached(credit_pack_cache, ttl=CREDIT_BALANCE_CACHE_INVALIDATION_PERIOD_IN_SECONDS)
async def get_valid_credit_pack_tickets_for_pastelid_cacheable(pastelid: str) -> List[dict]:
    try:
        list_of_credit_pack_ticket_registration_txids, list_of_credit_pack_ticket_data = await get_list_of_credit_pack_ticket_txids_from_pastelid(pastelid)
        complete_validated_tickets_list = [None] * len(list_of_credit_pack_ticket_registration_txids)
        async def process_ticket(idx, current_credit_pack_ticket_registration_txid):
            logger.info(f"Attempting to validate credit pack ticket {idx+1} of {len(list_of_credit_pack_ticket_registration_txids)} for PastelID {pastelid}...")
            validation_results = await validate_existing_credit_pack_ticket(current_credit_pack_ticket_registration_txid)
            if validation_results and len(validation_results['validation_failure_reasons_list']) == 0:
                try:
                    ticket_data = await get_credit_pack_ticket_data(current_credit_pack_ticket_registration_txid, list_of_credit_pack_ticket_data, idx)
                    logger.info(f"Finished validating and parsing credit pack ticket data for PastelID {pastelid} with txid {current_credit_pack_ticket_registration_txid}!")
                    return idx, ticket_data
                except Exception as e:
                    logger.error(f"Error processing credit pack ticket data for PastelID {pastelid}: {str(e)}")
                    traceback.print_exc()
            return idx, None
        tasks = [process_ticket(idx, txid) for idx, txid in enumerate(list_of_credit_pack_ticket_registration_txids)]
        results = await asyncio.gather(*tasks)
        for idx, ticket_data in results:
            if ticket_data is not None:
                complete_validated_tickets_list[idx] = ticket_data
        # Remove None values while preserving order
        complete_validated_tickets_list = [ticket for ticket in complete_validated_tickets_list if ticket is not None]
        return complete_validated_tickets_list
    except Exception as e:
        logger.error(f"Error retrieving credit pack tickets for PastelID {pastelid}: {str(e)}")
        traceback.print_exc()
        raise

@async_disk_cached(credit_pack_cache, ttl=CREDIT_BALANCE_CACHE_INVALIDATION_PERIOD_IN_SECONDS)
async def get_credit_pack_current_balance(credit_pack_ticket_registration_txid: str) -> Tuple[float, int]:
    async def get_balance():
        return await determine_current_credit_pack_balance_based_on_tracking_transactions(credit_pack_ticket_registration_txid)
    try:
        # Pass the function `get_balance` directly without any arguments
        current_credit_balance, number_of_confirmation_transactions = await retry_on_database_locked(get_balance)
        return current_credit_balance, number_of_confirmation_transactions
    except Exception as e:
        logger.error(f"Error getting current balance for credit pack ticket with TXID {credit_pack_ticket_registration_txid}: {str(e)}")
        traceback.print_exc()
        raise

@async_disk_cached(credit_pack_cache, ttl=CREDIT_BALANCE_CACHE_INVALIDATION_PERIOD_IN_SECONDS)
async def get_valid_credit_pack_tickets_for_pastelid(pastelid: str) -> List[dict]:
    try:
        complete_validated_tickets_list = await get_valid_credit_pack_tickets_for_pastelid_cacheable(pastelid)
        async def update_ticket_balance(current_credit_pack_ticket):
            current_credit_pack_ticket_registration_txid = current_credit_pack_ticket['credit_pack_registration_txid']
            logger.info(f"Determining current credit balance for credit pack ticket with txid {current_credit_pack_ticket_registration_txid} for PastelID {pastelid} (results will be cached for {round(CREDIT_BALANCE_CACHE_INVALIDATION_PERIOD_IN_SECONDS/60.0,2)} minutes)...")
            try:
                current_credit_balance, number_of_confirmation_transactions = await get_credit_pack_current_balance(current_credit_pack_ticket_registration_txid)
                current_credit_pack_ticket['credit_pack_current_credit_balance'] = current_credit_balance
                current_credit_pack_ticket['number_of_confirmation_transactions'] = number_of_confirmation_transactions
                current_credit_pack_ticket['balance_as_of_datetime'] = datetime.now(timezone.utc).isoformat()
            except Exception as e:
                logger.error(f"Error updating balance for ticket {current_credit_pack_ticket_registration_txid}: {str(e)}")
                current_credit_pack_ticket['credit_pack_current_credit_balance'] = None
                current_credit_pack_ticket['number_of_confirmation_transactions'] = None
                current_credit_pack_ticket['balance_as_of_datetime'] = None
            return current_credit_pack_ticket
        updated_tickets = await asyncio.gather(*[update_ticket_balance(ticket) for ticket in complete_validated_tickets_list])
        return updated_tickets
    except Exception as e:
        logger.error(f"Error determining credit balance for credit pack tickets for PastelID {pastelid}: {str(e)}")
        traceback.print_exc()
        raise
        
async def save_credit_pack_ticket_to_database(credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation, txid):
    async with db_code.Session() as db_session:
        try:
            # First, check if we already have this ticket in our database
            existing_mapping = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestResponseTxidMapping)
                .where(db_code.CreditPackPurchaseRequestResponseTxidMapping.pastel_api_credit_pack_ticket_registration_txid == txid)
            )
            existing_mapping = existing_mapping.one_or_none()

            if existing_mapping:
                # If we have this ticket, update the existing data
                existing_data = await db_session.exec(
                    select(db_code.CreditPackCompleteTicketWithBalance)
                    .where(db_code.CreditPackCompleteTicketWithBalance.credit_pack_ticket_registration_txid == txid)
                )
                existing_data = existing_data.one_or_none()

                if existing_data:
                    complete_ticket = json.loads(existing_data.complete_credit_pack_data_json)
                    current_credit_balance, number_of_confirmation_transactions = await determine_current_credit_pack_balance_based_on_tracking_transactions(txid)
                    complete_ticket['credit_pack_current_credit_balance'] = current_credit_balance
                    complete_ticket['balance_as_of_datetime'] = datetime.now(timezone.utc).isoformat()
                    complete_ticket_json = json.dumps(complete_ticket)

                    existing_data.complete_credit_pack_data_json = complete_ticket_json
                    existing_data.datetime_last_updated = datetime.now(timezone.utc)
                    db_session.add(existing_data)
            else:
                # If we don't have this ticket, create new entries
                current_credit_balance, number_of_confirmation_transactions = await determine_current_credit_pack_balance_based_on_tracking_transactions(txid)
                
                complete_ticket = {
                    "credit_pack_purchase_request": json.loads(base64.b64decode(credit_pack_purchase_request_response.credit_pack_purchase_request_fields_json_b64).decode('utf-8')),
                    "credit_pack_purchase_request_response": credit_pack_purchase_request_response.model_dump(),
                    "credit_pack_purchase_request_confirmation": credit_pack_purchase_request_confirmation.model_dump(),
                    "credit_pack_registration_txid": txid,
                    "credit_pack_current_credit_balance": current_credit_balance,
                    "balance_as_of_datetime": datetime.now(timezone.utc).isoformat()
                }
                complete_ticket = convert_uuids_to_strings(complete_ticket)
                complete_ticket = normalize_data(complete_ticket)
                complete_ticket_json = json.dumps(complete_ticket)

                new_complete_ticket = db_code.CreditPackCompleteTicketWithBalance(
                    credit_pack_ticket_registration_txid=txid,
                    complete_credit_pack_data_json=complete_ticket_json,
                    datetime_last_updated=datetime.now(timezone.utc)
                )
                db_session.add(new_complete_ticket)

                # Create new mapping
                new_mapping = db_code.CreditPackPurchaseRequestResponseTxidMapping(
                    sha3_256_hash_of_credit_pack_purchase_request_fields=credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_fields,
                    pastel_api_credit_pack_ticket_registration_txid=txid
                )
                db_session.add(new_mapping)

            # Commit the changes
            await db_session.commit()
            logger.info(f"Successfully saved/updated credit pack ticket data for TXID {txid} to database.")

        except Exception as e:
            logger.error(f"Error saving/updating retrieved credit pack ticket to the local database: {str(e)}")
            await db_session.rollback()
            raise

    await db_code.consolidate_wal_data()

        
#________________________________________________________________________________________________________________            
                
# Inference request related service functions:
                
async def get_inference_model_menu(use_verbose=0):
    try:
        # Load the API key test results from the file
        api_key_tests = load_api_key_tests()
        # Fetch the latest model menu from GitHub
        async with httpx.AsyncClient() as client:
            response = await client.get(GITHUB_MODEL_MENU_URL)
            response.raise_for_status()
            github_model_menu = response.json()
        # Test API keys and filter out models based on the availability of valid API keys
        filtered_model_menu = {"models": []}
        for model in github_model_menu["models"]:
            if model["model_name"].startswith("stability-"):
                if STABILITY_API_KEY and await is_api_key_valid("stability", api_key_tests):
                    filtered_model_menu["models"].append(model)
                    if use_verbose:
                        logger.info(f"Added Stability model: {model['model_name']} to the filtered model menu.")
            elif model["model_name"].startswith("openai-"):
                if OPENAI_API_KEY and await is_api_key_valid("openai", api_key_tests):
                    filtered_model_menu["models"].append(model)
                    if use_verbose:
                        logger.info(f"Added OpenAImodel: {model['model_name']} to the filtered model menu.")
            elif model["model_name"].startswith("mistralapi-"):
                if MISTRAL_API_KEY and await is_api_key_valid("mistral", api_key_tests):
                    filtered_model_menu["models"].append(model)
                    if use_verbose:
                        logger.info(f"Added MistralAPI model: {model['model_name']} to the filtered model menu.")
            elif model["model_name"].startswith("groq-"):
                if GROQ_API_KEY and await is_api_key_valid("groq", api_key_tests):
                    filtered_model_menu["models"].append(model)
                    if use_verbose:
                        logger.info(f"Added Groq API model: {model['model_name']} to the filtered model menu.")
            elif "claude" in model["model_name"].lower():
                if CLAUDE3_API_KEY and await is_api_key_valid("claude", api_key_tests):
                    filtered_model_menu["models"].append(model)
                    if use_verbose:
                        logger.info(f"Added Anthropic API model: {model['model_name']} to the filtered model menu.")
            elif "openrouter" in model["model_name"].lower():
                if OPENROUTER_API_KEY and await is_api_key_valid("openrouter", api_key_tests):
                    filtered_model_menu["models"].append(model)
                    if use_verbose:
                        logger.info(f"Added OpenRouter model: {model['model_name']} to the filtered model menu.")
            else:
                # Models that don't require API keys can be automatically included
                filtered_model_menu["models"].append(model)
        # Save the filtered model menu locally
        with open("model_menu.json", "w") as file:
            json.dump(filtered_model_menu, file, indent=2)
        save_api_key_tests(api_key_tests) # Save the updated API key test results to file
        return filtered_model_menu
    except Exception as e:
        logger.error(f"Error retrieving inference model menu: {str(e)}")
        traceback.print_exc()
        raise

def load_api_key_tests():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    api_key_test_json_file_path = os.path.join(current_dir, API_KEY_TESTS_FILE) 
    try:
        with open(api_key_test_json_file_path, "r") as file:
            return json.load(file)
    except FileNotFoundError:
        return {}

def save_api_key_tests(api_key_tests):
    current_dir = os.path.dirname(os.path.abspath(__file__))
    api_key_test_json_file_path = os.path.join(current_dir, API_KEY_TESTS_FILE)     
    with open(api_key_test_json_file_path, "w") as file:
        json.dump(api_key_tests, file, indent=2)

async def is_api_key_valid(api_name, api_key_tests):
    if api_name not in api_key_tests or not is_test_result_valid(api_key_tests[api_name]["timestamp"]):
        test_passed = await run_api_key_test(api_name)
        api_key_tests[api_name] = {
            "passed": test_passed,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        return test_passed
    else:
        return api_key_tests[api_name]["passed"]

def is_test_result_valid(test_timestamp):
    test_datetime = datetime.fromisoformat(test_timestamp).replace(tzinfo=timezone.utc)
    return (datetime.now(timezone.utc) - test_datetime) < timedelta(hours=API_KEY_TEST_VALIDITY_HOURS)

async def run_api_key_test(api_name):
    if api_name == "stability":
        return await test_stability_api_key()
    elif api_name == "openai":
        return await test_openai_api_key()
    elif api_name == "mistral":
        return await test_mistral_api_key()
    elif api_name == "groq":
        return await test_groq_api_key()
    elif api_name == "claude":
        return await test_claude_api_key()
    elif api_name == "openrouter":
        return await test_openrouter_api_key()    
    else:
        return False

async def test_openai_api_key():
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {OPENAI_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "gpt-3.5-turbo",
                    "messages": [{"role": "user", "content": "Test; just reply with the word yes if you're working!"}]
                }
            )
            return response.status_code == 200
    except Exception as e:
        logger.warning(f"OpenAI API key test failed: {str(e)}")
        return False
    
async def test_stability_api_key():
    try:
        engine_id = "stable-diffusion-v1-6"
        api_host = "https://api.stability.ai"
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{api_host}/v1/generation/{engine_id}/text-to-image",
                headers={
                    "Content-Type": "application/json",
                    "Accept": "application/json",
                    "Authorization": f"Bearer {STABILITY_API_KEY}"
                },
                json={
                    "text_prompts": [
                        {
                            "text": "A lighthouse on a cliff"
                        }
                    ],
                    "cfg_scale": 7,
                    "height": 512,
                    "width": 512,
                    "samples": 1,
                    "steps": 10,
                },
            )
            if response.status_code != 200:
                raise Exception("Non-200 response: " + str(response.text))
            data = response.json()
            if "artifacts" in data and len(data["artifacts"]) > 0:
                for artifact in data["artifacts"]:  # Directly iterating over the list of dictionaries
                    if artifact.get("finishReason") == "SUCCESS":
                        logger.info("Stability API test passed.")
                        return True
            logger.info("Stability API test failed!")                    
            return False
    except Exception as e:
        logger.warning(f"Stability API key test failed: {str(e)}")
        return False
    
async def test_openrouter_api_key():
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "openai/gpt-3.5-turbo",
                    "messages": [{"role": "user", "content": "Test; just reply with the word yes if you're working!"}]
                }
            )
            return response.status_code == 200
    except Exception as e:
        logger.warning(f"OpenRouter API key test failed: {str(e)}")
        return False    

async def test_mistral_api_key():
    try:
        client = MistralAsyncClient(api_key=MISTRAL_API_KEY)
        async_response = client.chat_stream(
            model="mistral-small-latest",
            messages=[ChatMessage(role="user", content="Test; just reply with the word yes if you're working!")],
            max_tokens=10,
            temperature=0.7,
        )
        completion_text = ""
        async for chunk in async_response:
            if chunk.choices[0].delta.content:
                completion_text += chunk.choices[0].delta.content
        logger.info(f"Mistral API test response: {completion_text}")                
        return len(completion_text) > 0
    except Exception as e:
        logger.warning(f"Mistral API key test failed: {str(e)}")
        return False

async def test_groq_api_key():
    try:
        client = AsyncGroq(api_key=GROQ_API_KEY)
        chat_completion = await client.chat.completions.create(
            messages=[{"role": "user", "content": "Test; just reply with the word yes if you're working!"}],
            model="mixtral-8x7b-32768",
            max_tokens=10,
            temperature=0.7,
        )
        response_string = chat_completion.choices[0].message.content.strip()
        logger.info(f"Groq API test response: {response_string}")
        if response_string is not None:
            test_passed = len(response_string) > 0
        else:
            test_passed = False
        return test_passed
    except Exception as e:
        logger.warning(f"Groq API key test failed: {str(e)}")
        return False

async def test_claude_api_key():
    try:
        client = anthropic.AsyncAnthropic(api_key=CLAUDE3_API_KEY)
        async with client.messages.stream(
            model="claude-3-haiku-20240307",
            max_tokens=10,
            temperature=0.7,
            messages=[{"role": "user", "content": "Test; just reply with the word yes if you're working!"}],
        ) as stream:
            message = await stream.get_final_message()
            response_string = message.content[0].text.strip()
            logger.info(f"Anthropic API test response: {response_string}")
            if response_string is not None:
                test_passed = len(response_string) > 0
            else:
                test_passed = False
            return test_passed
    except Exception as e:
        logger.warning(f"Claude API key test failed: {str(e)}")
        return False

async def save_inference_api_usage_request(inference_request_model: db_code.InferenceAPIUsageRequest) -> db_code.InferenceAPIUsageRequest:
    async with db_code.Session() as db_session:
        db_session.add(inference_request_model)
        await db_session.commit()
        await db_session.refresh(inference_request_model)
    return inference_request_model

def get_fallback_tokenizer():
    return "gpt2"  # Default to "gpt2" tokenizer as a fallback

def get_tokenizer(model_name: str):
    model_name = model_name.replace('swiss_army_llama-', '')
    model_to_tokenizer_mapping = {
        "claude3": "Xenova/claude-tokenizer",
        "phi": "TheBloke/phi-2-GGUF",
        "openai": "cl100k_base",
        "groq-llama3": "gradientai/Llama-3-70B-Instruct-Gradient-1048k",
        "groq-mixtral": "EleutherAI/gpt-neox-20b",
        "groq-gemma": "google/flan-ul2",
        "bge-m3": "Shitao/bge-m3",
        "mistralapi": "mistralai/Mistral-7B-Instruct-v0.2",
        "stability": "openai/clip-vit-large-patch14",
        "Lexi-Llama-3-8B-Uncensored_Q5_K_M": "gradientai/Llama-3-70B-Instruct-Gradient-1048k",
        "Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M": "gradientai/Llama-3-70B-Instruct-Gradient-1048k",
        "Meta-Llama-3-8B-Instruct.Q3_K_S": "gradientai/Llama-3-70B-Instruct-Gradient-1048k",
        "whisper": "openai/whisper-large-v2",
        "clip-interrogator": "openai/clip-vit-large-patch14",
        "videocap-transformer": "ArhanK005/videocap-transformer",
        "openrouter/google": "google/flan-t5-xl",
        "openrouter/anthropic": "Xenova/claude-tokenizer",
        "openrouter/meta-llama": "huggyllama/llama-7b",
        "openrouter/distilgpt2": "distilgpt2",
        "openrouter/bigscience/bloom": "bigscience/bloom-1b7",
        "openrouter/databricks/dolly-v2-12b": "databricks/dolly-v2-12b",
        "openrouter/EleutherAI/gpt-j-6b": "EleutherAI/gpt-j-6B",
        "openrouter/stabilityai/stablelm-tuned-alpha-7b": "stabilityai/stablelm-base-alpha-7b",
        "openrouter/togethercomputer/GPT-JT-6B-v1": "togethercomputer/GPT-JT-6B-v1",
        "openrouter/tiiuae/falcon-7b-instruct": "tiiuae/falcon-7b-instruct"
    }
    best_match = process.extractOne(model_name.lower(), model_to_tokenizer_mapping.keys())
    return model_to_tokenizer_mapping.get(best_match[0], get_fallback_tokenizer())  # Default to fallback tokenizer if no match found

def count_tokens(model_name: str, input_data: str) -> int:
    tokenizer_name = get_tokenizer(model_name)
    logger.info(f"Selected tokenizer {tokenizer_name} for model {model_name}")
    try:
        if 'claude' in model_name.lower():
            tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer_name)
        elif 'whisper' in model_name.lower():
            tokenizer = WhisperTokenizer.from_pretrained(tokenizer_name)
        elif 'clip-interrogator' in model_name.lower() or 'stability' in model_name.lower():
            return 0
        elif 'videocap-transformer' in model_name.lower():
            tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        elif 'openai' in model_name.lower():
            encoding = tiktoken.get_encoding(tokenizer_name)
            input_tokens = encoding.encode(input_data)
            return len(input_tokens)
        else:
            tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        if hasattr(tokenizer, "encode"):
            input_tokens = tokenizer.encode(input_data)
        else:
            input_tokens = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(input_data))
        return len(input_tokens)
    except Exception as e:
        logger.error(f"Failed to load tokenizer {tokenizer_name} for model {model_name}: {e}")
        fallback_tokenizer_name = get_fallback_tokenizer()
        logger.info(f"Falling back to tokenizer {fallback_tokenizer_name}")
        if 'openai' in fallback_tokenizer_name.lower():
            encoding = tiktoken.get_encoding(fallback_tokenizer_name)
            input_tokens = encoding.encode(input_data)
        else:
            tokenizer = AutoTokenizer.from_pretrained(fallback_tokenizer_name)
            if hasattr(tokenizer, "encode"):
                input_tokens = tokenizer.encode(input_data)
            else:
                input_tokens = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(input_data))
        return len(input_tokens)

def calculate_api_cost(model_name: str, input_data: str, model_parameters: Dict) -> float:
    # Define the pricing data for each API service and model
    logger.info(f"Evaluating API cost for model: {model_name}")
    pricing_data = {
        "claude-2.1": {"input_cost": 0.008, "output_cost": 0.024, "per_call_cost": 0.0128},
        "claude3-haiku": {"input_cost": 0.00025, "output_cost": 0.00125, "per_call_cost": 0.0006},
        "claude3.5-sonnet": {"input_cost": 0.0003, "output_cost": 0.0015, "per_call_cost": 0.0},
        "claude3-opus": {"input_cost": 0.0015, "output_cost": 0.0075, "per_call_cost": 0.0},
        "mistralapi-mistral-small-latest": {"input_cost": 0.002, "output_cost": 0.006, "per_call_cost": 0.0032},
        "mistralapi-mistral-medium-latest": {"input_cost": 0.0027, "output_cost": 0.0081, "per_call_cost": 0},
        "mistralapi-mistral-large-latest": {"input_cost": 0.008, "output_cost": 0.024, "per_call_cost": 0.0128},
        "mistralapi-mistral-7b": {"input_cost": 0.00025, "output_cost": 0.00025, "per_call_cost": 0},
        "mistralapi-open-mistral-7b": {"input_cost": 0.00025, "output_cost": 0.00025, "per_call_cost": 0},
        "mistralapi-open-mixtral-8x7b": {"input_cost": 0.0007, "output_cost": 0.0007, "per_call_cost": 0},
        "mistralapi-mistral-embed": {"input_cost": 0.0001, "output_cost": 0, "per_call_cost": 0},
        "openai-gpt-4o": {"input_cost": 0.005, "output_cost": 0.015, "per_call_cost": 0.0},
        "openai-gpt-4o-2024-05-13": {"input_cost": 0.005, "output_cost": 0.015, "per_call_cost": 0.0},
        "openai-gpt-4o-vision": {"input_cost": 0.005, "output_cost": 0.015, "per_call_cost": 0.0},
        "openai-gpt-4-turbo": {"input_cost": 0.01, "output_cost": 0.03, "per_call_cost": 0.0},
        "openai-gpt-4-turbo-2024-04-09": {"input_cost": 0.01, "output_cost": 0.03, "per_call_cost": 0.0},
        "openai-gpt-4": {"input_cost": 0.03, "output_cost": 0.06, "per_call_cost": 0.0},
        "openai-gpt-4-32k": {"input_cost": 0.06, "output_cost": 0.12, "per_call_cost": 0.0},
        "openai-gpt-3.5-turbo-0125": {"input_cost": 0.0005, "output_cost": 0.0015, "per_call_cost": 0.0},
        "openai-gpt-3.5-turbo-instruct": {"input_cost": 0.0015, "output_cost": 0.002, "per_call_cost": 0.0},
        "openai-gpt-3.5-turbo-16k": {"input_cost": 0.003, "output_cost": 0.004, "per_call_cost": 0.0},
        "openai-gpt-4-0125-preview": {"input_cost": 0.01, "output_cost": 0.03, "per_call_cost": 0.0},
        "openai-gpt-4-1106-preview": {"input_cost": 0.01, "output_cost": 0.03, "per_call_cost": 0.0},
        "openai-gpt-4-vision-preview": {"input_cost": 0.01, "output_cost": 0.03, "per_call_cost": 0.0},
        "openai-text-embedding-ada-002": {"input_cost": 0.0004, "output_cost": 0.0, "per_call_cost": 0.0},
        "groq-llama3-70b-8192": {"input_cost": 0.0007, "output_cost": 0.0008, "per_call_cost": 0},
        "groq-llama3-8b-8192": {"input_cost": 0.0001, "output_cost": 0.0001, "per_call_cost": 0},
        "groq-mixtral-8x7b-32768": {"input_cost": 0.00027, "output_cost": 0.00027, "per_call_cost": 0},
        "groq-gemma-7b-it": {"input_cost": 0.0001, "output_cost": 0.0001, "per_call_cost": 0},
        "stability-core": {"credits_per_call": 3},
        "stability-ultra": {"credits_per_call": 8},
        "stability-sd3-medium": {"credits_per_call": 3.5},
        "stability-sd3-large": {"credits_per_call": 6.5},
        "stability-sd3-large-turbo": {"credits_per_call": 4},
        "stability-sdxl-1.0": {"credits_per_call": 0.4},  # Average of 0.2-0.6
        "stability-sd-1.6": {"credits_per_call": 0.6},  # Average of 0.2-1.0
        "stability-creative-upscaler": {"credits_per_call": 25},
        "stability-conservative-upscaler": {"credits_per_call": 25},
        "stability-esrgan": {"credits_per_call": 0.2},
        "stability-search-and-replace": {"credits_per_call": 4},
        "stability-inpaint": {"credits_per_call": 3},
        "stability-erase": {"credits_per_call": 3},
        "stability-outpaint": {"credits_per_call": 4},
        "stability-remove-background": {"credits_per_call": 2},
        "stability-search-and-recolor": {"credits_per_call": 5},
        "stability-structure": {"credits_per_call": 3},
        "stability-sketch": {"credits_per_call": 3},
        "stability-style": {"credits_per_call": 4},
        "stability-video": {"credits_per_call": 20},
        "stability-fast-3d": {"credits_per_call": 2},
        "openrouter/auto": {"input_cost": 0.0005, "output_cost": 0.0015, "per_call_cost": 0},
        "openrouter/nousresearch/nous-capybara-7b:free": {"input_cost": 0, "output_cost": 0, "per_call_cost": 0},
        "openrouter/mistralai/mistral-7b-instruct:free": {"input_cost": 0, "output_cost": 0, "per_call_cost": 0},
        "openrouter/gryphe/mythomist-7b:free": {"input_cost": 0, "output_cost": 0, "per_call_cost": 0},
        "openrouter/undi95/toppy-m-7b:free": {"input_cost": 0, "output_cost": 0, "per_call_cost": 0},
        "openrouter/cinematika-7b:free": {"input_cost": 0, "output_cost": 0, "per_call_cost": 0},
        "openrouter/google/gemma-7b-it:free": {"input_cost": 0, "output_cost": 0, "per_call_cost": 0},
        "openrouter/jebcarter/psyfighter-13b": {"input_cost": 0.001, "output_cost": 0.001, "per_call_cost": 0},
        "openrouter/koboldai/psyfighter-13b-2": {"input_cost": 0.001, "output_cost": 0.001, "per_call_cost": 0},
        "openrouter/intel/neural-chat-7b": {"input_cost": 0.005, "output_cost": 0.005, "per_call_cost": 0},
        "openrouter/haotian-liu/llava-13b": {"input_cost": 0.005, "output_cost": 0.005, "per_call_cost": 0},
        "openrouter/nousresearch/nous-hermes-2-vision-7b": {"input_cost": 0.005, "output_cost": 0.005, "per_call_cost": 0},
        "openrouter/meta-llama/llama-2-13b-chat": {"input_cost": 0.0001474, "output_cost": 0.0001474, "per_call_cost": 0},
        "openrouter/migtissera/synthia-70b": {"input_cost": 0.00375, "output_cost": 0.00375, "per_call_cost": 0},
        "openrouter/pygmalionai/mythalion-13b": {"input_cost": 0.001125, "output_cost": 0.001125, "per_call_cost": 0},
        "openrouter/xwin-lm/xwin-lm-70b": {"input_cost": 0.00375, "output_cost": 0.00375, "per_call_cost": 0},
        "openrouter/alpindale/goliath-120b": {"input_cost": 0.009375, "output_cost": 0.009375, "per_call_cost": 0},
        "openrouter/neversleep/noromaid-20b": {"input_cost": 0.00225, "output_cost": 0.00225, "per_call_cost": 0},
        "openrouter/gryphe/mythomist-7b": {"input_cost": 0.000375, "output_cost": 0.000375, "per_call_cost": 0},
        "openrouter/sophosympatheia/midnight-rose-70b": {"input_cost": 0.009, "output_cost": 0.009, "per_call_cost": 0},
        "openrouter/undi95/remm-slerp-l2-13b:extended": {"input_cost": 0.001125, "output_cost": 0.001125, "per_call_cost": 0},
        "openrouter/gryphe/mythomax-l2-13b:extended": {"input_cost": 0.001125, "output_cost": 0.001125, "per_call_cost": 0},
        "openrouter/mancer/weaver": {"input_cost": 0.003375, "output_cost": 0.003375, "per_call_cost": 0},
        "openrouter/nousresearch/nous-hermes-llama2-13b": {"input_cost": 0.00027, "output_cost": 0.00027, "per_call_cost": 0},
        "openrouter/nousresearch/nous-capybara-7b": {"input_cost": 0.00018, "output_cost": 0.00018, "per_call_cost": 0},
        "openrouter/meta-llama/codellama-34b-instruct": {"input_cost": 0.00072, "output_cost": 0.00072, "per_call_cost": 0},
        "openrouter/codellama/codellama-70b-instruct": {"input_cost": 0.00081, "output_cost": 0.00081, "per_call_cost": 0},
        "openrouter/phind/phind-codellama-34b": {"input_cost": 0.00072, "output_cost": 0.00072, "per_call_cost": 0},
        "openrouter/teknium/openhermes-2-mistral-7b": {"input_cost": 0.00018, "output_cost": 0.00018, "per_call_cost": 0},
        "openrouter/teknium/openhermes-2.5-mistral-7b": {"input_cost": 0.00018, "output_cost": 0.00018, "per_call_cost": 0},
        "openrouter/undi95/remm-slerp-l2-13b": {"input_cost": 0.00027, "output_cost": 0.00027, "per_call_cost": 0},
        "openrouter/undi95/toppy-m-7b": {"input_cost": 0.00018, "output_cost": 0.00018, "per_call_cost": 0},
        "openrouter/cinematika-7b": {"input_cost": 0.00018, "output_cost": 0.00018, "per_call_cost": 0},
        "openrouter/01-ai/yi-34b-chat": {"input_cost": 0.00072, "output_cost": 0.00072, "per_call_cost": 0},
        "openrouter/01-ai/yi-34b": {"input_cost": 0.00072, "output_cost": 0.00072, "per_call_cost": 0},
        "openrouter/01-ai/yi-6b": {"input_cost": 0.000126, "output_cost": 0.000126, "per_call_cost": 0},
        "openrouter/togethercomputer/stripedhyena-nous-7b": {"input_cost": 0.00018, "output_cost": 0.00018, "per_call_cost": 0},
        "openrouter/togethercomputer/stripedhyena-hessian-7b": {"input_cost": 0.00018, "output_cost": 0.00018, "per_call_cost": 0},
        "openrouter/mistralai/mixtral-8x7b": {"input_cost": 0.00054, "output_cost": 0.00054, "per_call_cost": 0},
        "openrouter/nousresearch/nous-hermes-yi-34b": {"input_cost": 0.00072, "output_cost": 0.00072, "per_call_cost": 0},
        "openrouter/nousresearch/nous-hermes-2-mixtral-8x7b-sft": {"input_cost": 0.00054, "output_cost": 0.00054, "per_call_cost": 0},
        "openrouter/nousresearch/nous-hermes-2-mistral-7b-dpo": {"input_cost": 0.00018, "output_cost": 0.00018, "per_call_cost": 0},
        "openrouter/mistralai/mixtral-8x22b": {"input_cost": 0.00108, "output_cost": 0.00108, "per_call_cost": 0},
        "openrouter/open-orca/mistral-7b-openorca": {"input_cost": 0.0001425, "output_cost": 0.0001425, "per_call_cost": 0},
        "openrouter/huggingfaceh4/zephyr-7b-beta": {"input_cost": 0.0001425, "output_cost": 0.0001425, "per_call_cost": 0},
        "openrouter/openai/gpt-3.5-turbo": {"input_cost": 0.0005, "output_cost": 0.0015, "per_call_cost": 0},
        "openrouter/openai/gpt-3.5-turbo-0125": {"input_cost": 0.0005, "output_cost": 0.0015, "per_call_cost": 0},
        "openrouter/openai/gpt-3.5-turbo-16k": {"input_cost": 0.003, "output_cost": 0.004, "per_call_cost": 0},
        "openrouter/openai/gpt-4-turbo": {"input_cost": 0.01, "output_cost": 0.03, "per_call_cost": 0},
        "openrouter/openai/gpt-4-turbo-preview": {"input_cost": 0.01, "output_cost": 0.03, "per_call_cost": 0},
        "openrouter/openai/gpt-4": {"input_cost": 0.03, "output_cost": 0.06, "per_call_cost": 0},
        "openrouter/openai/gpt-4-32k": {"input_cost": 0.06, "output_cost": 0.12, "per_call_cost": 0},
        "openrouter/openai/gpt-4-vision-preview": {"input_cost": 0.01, "output_cost": 0.03, "per_call_cost": 0},
        "openrouter/openai/gpt-3.5-turbo-instruct": {"input_cost": 0.0015, "output_cost": 0.002, "per_call_cost": 0},
        "openrouter/google/palm-2-chat-bison": {"input_cost": 0.00025, "output_cost": 0.0005, "per_call_cost": 0},
        "openrouter/google/palm-2-codechat-bison": {"input_cost": 0.00025, "output_cost": 0.0005, "per_call_cost": 0},
        "openrouter/google/palm-2-chat-bison-32k": {"input_cost": 0.00025, "output_cost": 0.0005, "per_call_cost": 0},
        "openrouter/google/palm-2-codechat-bison-32k": {"input_cost": 0.00025, "output_cost": 0.0005, "per_call_cost": 0},
        "openrouter/google/gemini-pro": {"input_cost": 0.000125, "output_cost": 0.000375, "per_call_cost": 0},
        "openrouter/google/gemini-pro-vision": {"input_cost": 0.000125, "output_cost": 0.000375, "per_call_cost": 0},
        "openrouter/google/gemini-pro-1.5": {"input_cost": 0.0025, "output_cost": 0.0075, "per_call_cost": 0},
        "openrouter/anthropic/claude-3-opus": {"input_cost": 0.015, "output_cost": 0.075, "per_call_cost": 0},
        "openrouter/anthropic/claude-3-sonnet": {"input_cost": 0.003, "output_cost": 0.015, "per_call_cost": 0},
        "openrouter/anthropic/claude-3-haiku": {"input_cost": 0.00025, "output_cost": 0.00125, "per_call_cost": 0},
        "openrouter/anthropic/claude-3-opus:beta": {"input_cost": 0.015, "output_cost": 0.075, "per_call_cost": 0},
        "openrouter/anthropic/claude-3-sonnet:beta": {"input_cost": 0.003, "output_cost": 0.015, "per_call_cost": 0},
        "openrouter/anthropic/claude-3-haiku:beta": {"input_cost": 0.00025, "output_cost": 0.00125, "per_call_cost": 0},
        "openrouter/meta-llama/llama-2-70b-chat": {"input_cost": 0.0007, "output_cost": 0.0009, "per_call_cost": 0},
        "openrouter/nousresearch/nous-capybara-34b": {"input_cost": 0.0009, "output_cost": 0.0009, "per_call_cost": 0},
        "openrouter/jondurbin/airoboros-l2-70b": {"input_cost": 0.0007, "output_cost": 0.0009, "per_call_cost": 0},
        "openrouter/jondurbin/bagel-34b": {"input_cost": 0.00575, "output_cost": 0.00575, "per_call_cost": 0},
        "openrouter/austism/chronos-hermes-13b": {"input_cost": 0.00022, "output_cost": 0.00022, "per_call_cost": 0},
        "openrouter/mistralai/mistral-7b-instruct": {"input_cost": 0.00013, "output_cost": 0.00013, "per_call_cost": 0},
        "openrouter/gryphe/mythomax-l2-13b": {"input_cost": 0.0002, "output_cost": 0.0002, "per_call_cost": 0},
        "openrouter/openchat/openchat-7b": {"input_cost": 0.00013, "output_cost": 0.00013, "per_call_cost": 0},
        "openrouter/lizpreciatior/lzlv-70b-fp16-hf": {"input_cost": 0.0007, "output_cost": 0.0008, "per_call_cost": 0},
        "openrouter/mistralai/mixtral-8x7b-instruct": {"input_cost": 0.00027, "output_cost": 0.00027, "per_call_cost": 0},
        "openrouter/cognitivecomputations/dolphin-mixtral-8x7b": {"input_cost": 0.0005, "output_cost": 0.0005, "per_call_cost": 0},
        "openrouter/neversleep/noromaid-mixtral-8x7b-instruct": {"input_cost": 0.008, "output_cost": 0.008, "per_call_cost": 0},        
    }
    # Find the best match for the model name using fuzzy string matching
    best_match = process.extractOne(model_name.lower(), pricing_data.keys())
    if best_match is None or best_match[1] < 60:  # Adjust the threshold as needed
        logger.warning(f"No pricing data found for model: {model_name}")
        return 0.0
    model_pricing = pricing_data[best_match[0]]
    if model_name.startswith("stability-"):
        # For Stability models, calculate the cost based on the credits per call
        number_of_completions_to_generate = model_parameters.get("number_of_completions_to_generate", 1)
        credits_cost = model_pricing["credits_per_call"] * number_of_completions_to_generate
        estimated_cost = credits_cost * 10 / 1000  # Convert credits to dollars ($10 per 1,000 credits)
    elif model_name.endswith("4o-vision"):
        if input_data is None:
            logger.error("Input data is empty!")
        input_data_dict = json.loads(input_data)
        image_data_binary = base64.b64decode(input_data_dict["image"]) # Decode image data and question from input_data
        question = input_data_dict["question"]
        # Calculate image resolution
        image = Image.open(io.BytesIO(image_data_binary))
        width, height = image.size
        image_file_size_in_mb = len(image_data_binary) / (1024 * 1024)  # Calculate image file size in MB
        logger.info(f"Submitted image file is {image_file_size_in_mb:.2f}MB and has resolution of {width} x {height}")
        # Resize logic to fit the larger dimension to 1286 pixels while maintaining aspect ratio
        target_larger_dimension = 1286
        aspect_ratio = width / height
        if width > height:
            resized_width = target_larger_dimension
            resized_height = int(target_larger_dimension / aspect_ratio)
        else:
            resized_height = target_larger_dimension
            resized_width = int(target_larger_dimension * aspect_ratio)
        logger.info(f"Image will be resized automatically by OpenAI to a resolution of {resized_width} x {resized_height}")
        # Calculate number of tiles and tokens
        tiles_x = -(-resized_width // 512)
        tiles_y = -(-resized_height // 512)
        total_tiles = tiles_x * tiles_y
        base_tokens = 85
        tile_tokens = 170 * total_tiles
        total_tokens = base_tokens + tile_tokens
        # Count tokens in the question
        question_tokens = count_tokens(model_name, question)
        input_data_tokens = total_tokens + question_tokens
        logger.info(f"Total input data tokens: {input_data_tokens}")
        number_of_tokens_to_generate = model_parameters.get("number_of_tokens_to_generate", 1000)
        number_of_completions_to_generate = model_parameters.get("number_of_completions_to_generate", 1)
        input_cost = float(model_pricing["input_cost"]) * float(input_data_tokens) / 1000.0
        output_cost = float(model_pricing["output_cost"]) * float(number_of_tokens_to_generate) / 1000.0
        per_call_cost = float(model_pricing["per_call_cost"]) * float(number_of_completions_to_generate)
        estimated_cost = input_cost + output_cost + per_call_cost
    else:
        # For other models, calculate the cost based on input/output tokens and per-call cost
        input_data_tokens = count_tokens(model_name, input_data)
        logger.info(f"Total input data tokens: {input_data_tokens}")
        number_of_tokens_to_generate = model_parameters.get("number_of_tokens_to_generate", 1000)
        number_of_completions_to_generate = model_parameters.get("number_of_completions_to_generate", 1)
        input_cost = float(model_pricing["input_cost"]) * float(input_data_tokens) / 1000.0
        output_cost = float(model_pricing["output_cost"]) * float(number_of_tokens_to_generate) / 1000.0
        per_call_cost = float(model_pricing["per_call_cost"]) * float(number_of_completions_to_generate)
        estimated_cost = input_cost + output_cost + per_call_cost
    logger.info(f"Estimated cost: ${estimated_cost:.4f}")
    return estimated_cost

async def convert_document_to_sentences(file_content: bytes, tried_local=False) -> Dict:
    logger.info("Now calling Swiss Army Llama to convert document to sentences.")
    local_swiss_army_llama_responding = is_swiss_army_llama_responding(local=True)
    if USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
        remote_swiss_army_llama_responding = is_swiss_army_llama_responding(local=False)
    else:
        remote_swiss_army_llama_responding = 0
    if remote_swiss_army_llama_responding and USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
        port = REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT
    elif local_swiss_army_llama_responding:
        port = SWISS_ARMY_LLAMA_PORT
    else:
        logger.error(f"Neither the local Swiss Army Llama (supposed to be running on port {SWISS_ARMY_LLAMA_PORT}) nor the remote Swiss Army Llama (supposed to be running, if enabled, on mapped port {REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT}) is responding!")
        raise ValueError("Swiss Army Llama is not responding.")
    metadata = await upload_and_get_file_metadata(file_content, "document")
    file_url = metadata["file_url"]
    file_hash = metadata["file_hash"]
    file_size = metadata["file_size"]
    url = f"http://localhost:{port}/convert_document_to_sentences/"
    async with httpx.AsyncClient(timeout=60) as client:
        try:
            response = await client.post(url, data={
                "url": file_url,
                "hash": file_hash,
                "size": file_size,
                "token": SWISS_ARMY_LLAMA_SECURITY_TOKEN
            })
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"Failed to convert document to sentences: {e}")
            if port == REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT and not tried_local:
                logger.info("Falling back to local Swiss Army Llama.")
                return await convert_document_to_sentences(file_content, tried_local=True)
            raise ValueError("Error converting document to sentences")
            
async def calculate_proposed_inference_cost_in_credits(requested_model_data: Dict, model_parameters: Dict, model_inference_type_string: str, input_data: str) -> float:
    model_name = requested_model_data["model_name"]
    if 'swiss_army_llama-' not in model_name:
        api_cost = calculate_api_cost(model_name, input_data, model_parameters)
        if api_cost > 0.0:
            target_value_per_credit = TARGET_VALUE_PER_CREDIT_IN_USD
            target_profit_margin = TARGET_PROFIT_MARGIN
            proposed_cost_in_credits = api_cost / (target_value_per_credit * (1 - target_profit_margin))
            final_proposed_cost_in_credits = max([MINIMUM_COST_IN_CREDITS, round(proposed_cost_in_credits, 1)])
            logger.info(f"Proposed cost in credits (API-based): {final_proposed_cost_in_credits}")
            return final_proposed_cost_in_credits
    else:
        credit_costs = requested_model_data["credit_costs"][model_inference_type_string]
        if model_inference_type_string == "ask_question_about_an_image":
            input_tokens = 3000
        else:
            input_tokens = count_tokens(model_name, input_data) if model_inference_type_string != "embedding_document" else 0
        compute_cost = float(credit_costs["compute_cost"])
        memory_cost = float(credit_costs["memory_cost"])
        if model_inference_type_string == "text_completion" or model_inference_type_string == "ask_question_about_an_image":
            output_token_cost = float(credit_costs["output_tokens"])
            number_of_tokens_to_generate = int(model_parameters.get("number_of_tokens_to_generate", 1000))
            number_of_completions_to_generate = int(model_parameters.get("number_of_completions_to_generate", 1))
            estimated_output_tokens = number_of_tokens_to_generate
            proposed_cost_in_credits = number_of_completions_to_generate * (
                (input_tokens * float(credit_costs["input_tokens"])) +
                (estimated_output_tokens * output_token_cost) +
                compute_cost
            ) + memory_cost
        elif model_inference_type_string == "embedding_document":
            if is_base64_encoded(input_data):
                input_data = base64.b64decode(input_data)
                input_data = input_data.decode('utf-8')
            try:
                input_data_dict = json.loads(input_data)
                document_file_data = input_data_dict['document']
                if is_base64_encoded(document_file_data):
                    document_file_data = base64.b64decode(document_file_data)
                document_stats = await convert_document_to_sentences(document_file_data)
                sentences = document_stats["individual_sentences"]
                total_sentences = document_stats["total_number_of_sentences"]
                concatenated_sentences = " ".join(sentences)
                total_tokens = count_tokens(model_name, concatenated_sentences)
                proposed_cost_in_credits = (
                    (total_tokens * float(credit_costs["average_tokens_per_sentence"])) +
                    (total_sentences * float(credit_costs["total_sentences"])) +
                    (1 if model_parameters.get("query_string") else 0) * float(credit_costs["query_string_included"]) +
                    compute_cost +
                    memory_cost
                )
            except Exception as e:
                logger.error(f"Error parsing document data from input: {str(e)}")
                traceback.print_exc()
                raise
        elif model_inference_type_string == "embedding_audio":
            if is_base64_encoded(input_data):
                input_data = base64.b64decode(input_data)
                input_data = input_data.decode('utf-8')
            try:
                input_data_dict = json.loads(input_data)
                audio_file_data = input_data_dict['audio']
                if is_base64_encoded(audio_file_data):
                    audio_file_data = base64.b64decode(audio_file_data)            
                audio_length_seconds = get_audio_length(audio_file_data)
                proposed_cost_in_credits = (
                    (1 if model_parameters.get("query_string") else 0) * float(credit_costs["query_string_included"]) +
                    (audio_length_seconds * float(credit_costs["audio_file_length_in_seconds"])) +
                    compute_cost +
                    memory_cost
                )
            except Exception as e:
                logger.error(f"Error parsing document data from input: {str(e)}")
                traceback.print_exc()
                raise
        final_proposed_cost_in_credits = round(proposed_cost_in_credits * CREDIT_COST_MULTIPLIER_FACTOR, 1)
        final_proposed_cost_in_credits = max([MINIMUM_COST_IN_CREDITS, final_proposed_cost_in_credits])
        logger.info(f"Proposed cost in credits (local LLM): {final_proposed_cost_in_credits}")
        return final_proposed_cost_in_credits

async def fetch_current_psl_market_price():
    async def check_prices():
        async with httpx.AsyncClient() as client:
            try:
                # Fetch data from CoinMarketCap
                response_cmc = await client.get("https://coinmarketcap.com/currencies/pastel/")
                price_cmc = float(re.search(r'price today is \$([0-9\.]+) USD', response_cmc.text).group(1))
            except (httpx.RequestError, AttributeError, ValueError):
                price_cmc = None
            try:
                # Fetch data from CoinGecko
                response_cg = await client.get("https://api.coingecko.com/api/v3/simple/price?ids=pastel&vs_currencies=usd")
                if response_cg.status_code == 200:
                    data = response_cg.json()
                    price_cg = data.get("pastel", {}).get("usd")
                else:
                    price_cg = None
            except (httpx.RequestError, AttributeError, ValueError):
                price_cg = None
        return price_cmc, price_cg
    price_cmc, price_cg = await check_prices()
    if price_cmc is None and price_cg is None:
        #Sleep for a couple seconds and try again:
        await asyncio.sleep(2)
        price_cmc, price_cg = await check_prices()
    # Calculate the average price
    prices = [price for price in [price_cmc, price_cg] if price is not None]
    if not prices:
        raise ValueError("Could not retrieve PSL price from any source.")
    average_price = sum(prices) / len(prices)
    # Validate the price
    if not 0.0000001 < average_price < 0.02:
        raise ValueError(f"Invalid PSL price: {average_price}")
    logger.info(f"The current Average PSL price is: ${average_price:.8f} based on {len(prices)} sources")
    return average_price

async def estimated_market_price_of_inference_credits_in_psl_terms() -> float:
    try:
        psl_price_usd = await fetch_current_psl_market_price()
        target_value_per_credit_usd = TARGET_VALUE_PER_CREDIT_IN_USD
        target_profit_margin = TARGET_PROFIT_MARGIN
        # Calculate the cost per credit in USD, considering the profit margin
        cost_per_credit_usd = target_value_per_credit_usd / (1 - target_profit_margin)
        # Convert the cost per credit from USD to PSL
        cost_per_credit_psl = cost_per_credit_usd / psl_price_usd
        logger.info(f"Estimated market price of 1.0 inference credit: {cost_per_credit_psl:.4f} PSL")
        return cost_per_credit_psl
    except (ValueError, ZeroDivisionError) as e:
        logger.error(f"Error calculating estimated market price of inference credits: {str(e)}")
        raise

def normalize_string(s):
    """Remove non-alphanumeric characters and convert to lowercase."""
    return re.sub(r'\W+', '', s).lower()

def validate_pastel_txid_string(input_string: str):
    # Sample txid: 625694b632a05f5df8d70904b9b3ff03d144aec0352b2290a275379586daf8db
    return re.match(r'^[0-9a-fA-F]{64}$', input_string) is not None

def is_swiss_army_llama_responding(local=True):
    port = SWISS_ARMY_LLAMA_PORT if local else REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT
    try:
        url = f"http://localhost:{port}/get_list_of_available_model_names/"
        params = {'token': SWISS_ARMY_LLAMA_SECURITY_TOKEN}
        response = httpx.get(url, params=params)
        return response.status_code == 200
    except Exception as e:
        logger.error("Error: {}".format(e))
        return False

async def check_if_input_text_would_get_rejected_from_api_services(input_text: str) -> bool:
    inference_request_allowed = True
    local_swiss_army_llama_responding = is_swiss_army_llama_responding(local=True)
    if USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
        remote_swiss_army_llama_responding = is_swiss_army_llama_responding(local=False)
    else:
        remote_swiss_army_llama_responding = 0
    if remote_swiss_army_llama_responding and USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
        port = REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT
    elif local_swiss_army_llama_responding:
        port = SWISS_ARMY_LLAMA_PORT
    else:
        logger.error(f"Neither the local Swiss Army Llama (supposed to be running on port {SWISS_ARMY_LLAMA_PORT}) nor the remote Swiss Army Llama (supposed to be running, if enabled, on mapped port {REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT}) is responding! Unable to check if request is allowed, so just saying request status simply marked as 'ACCEPT'!")
        return None
    start_time = time.time()
    logger.info("Now checking suitability of requested inference for API services to avoid banning (using Swiss Army Llama with model TinyLlama-1.1B-Chat)...")
    input_prompt = "A user wants to do inference using the following prompt text using the OpenAI API; is the content or nature of the request problematic or offensive in any way that makes it likely that OpenAI will reject the request for safety reasons and potentially ban the API key for submitting it? Respond ONLY with a one word answer-- `REJECT` if the request will cause problems with OpenAI, or `ACCEPT` if it's unlikely to pose a problem. Here is the inference request submitted by the user:\n\n{}".format(input_text)
    url = f"http://localhost:{port}/get_text_completions_from_input_prompt/"
    try:
        async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS * 3)) as client:
            payload = {
                "input_prompt": input_prompt,
                "llm_model_name": "Meta-Llama-3-8B-Instruct.Q3_K_S",
                "temperature": 0.7,
                "number_of_tokens_to_generate": 3,
                "number_of_completions_to_generate": 1,
                "grammar_file_string": "accept_or_reject",
            }
            response = await client.post(
                url,
                json=payload,
                params={"token": SWISS_ARMY_LLAMA_SECURITY_TOKEN}
            )
            response.raise_for_status()
            output_results = response.json()
            output_text = output_results[0]["generated_text"]
            end_time = time.time()
            duration_in_seconds = (end_time - start_time)
            logger.info("Total time taken for inference request content check: {} seconds".format(round(duration_in_seconds, 2)))
            if output_text == "ACCEPT":
                logger.info("Inference request is not problematic, passing on to API service now...")
                return inference_request_allowed
            if output_text == "REJECT":
                logger.error("Error! Inference request was determined to be problematic and likely to result in a rejection or ban from the API service!")
                inference_request_allowed = False
                return inference_request_allowed
            logger.warning("Warning! Inference check was supposed to result in either 'ACCEPT' or 'REJECT', but it instead returned: '{}'".format(output_text))
            return inference_request_allowed
    except Exception as e:
        logger.error("Failed to execute inference check request with Swiss Army Llama: {}".format(e))
        if port == REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT:
            logger.info("Falling back to local Swiss Army Llama.")
            return await check_if_input_text_would_get_rejected_from_api_services(input_text)
    return inference_request_allowed

async def validate_inference_api_usage_request(inference_api_usage_request: db_code.InferenceAPIUsageRequest) -> Tuple[bool, float, float]:
    try:
        validation_errors = await validate_inference_request_message_data_func(inference_api_usage_request)
        if validation_errors:
            raise ValueError(f"Invalid inference request message: {', '.join(validation_errors)}")
        requesting_pastelid = inference_api_usage_request.requesting_pastelid
        credit_pack_ticket_pastel_txid = inference_api_usage_request.credit_pack_ticket_pastel_txid
        requested_model = inference_api_usage_request.requested_model_canonical_string
        model_inference_type_string = inference_api_usage_request.model_inference_type_string
        model_parameters = base64.b64decode(inference_api_usage_request.model_parameters_json_b64).decode('utf-8')
        input_data = inference_api_usage_request.model_input_data_json_b64
        if not validate_pastel_txid_string(credit_pack_ticket_pastel_txid):
            logger.error(f"Invalid Pastel TXID: {credit_pack_ticket_pastel_txid}")
            return False, 0, 0
        _, credit_pack_purchase_request_response, _ = await retrieve_credit_pack_ticket_using_txid(credit_pack_ticket_pastel_txid)
        credit_pack_purchase_request_object = await get_credit_pack_purchase_request_from_response(credit_pack_purchase_request_response)
        if credit_pack_purchase_request_object:
            list_of_authorized_pastelids_allowed_to_use_credit_pack = json.dumps(credit_pack_purchase_request_object.list_of_authorized_pastelids_allowed_to_use_credit_pack)
            if requesting_pastelid not in list_of_authorized_pastelids_allowed_to_use_credit_pack:
                logger.warning(f"Unauthorized PastelID: {requesting_pastelid}")
                return False, 0, 0
        model_menu = await get_inference_model_menu()
        requested_model_data = next((model for model in model_menu["models"] if normalize_string(model["model_name"]) == normalize_string(requested_model)), None)
        if requested_model_data is None:
            logger.warning(f"Invalid model requested: {requested_model}")
            return False, 0, 0
        if "api_based_pricing" in requested_model_data['credit_costs']:
            is_api_based_model = 1
        else:
            is_api_based_model = 0
        if model_inference_type_string not in requested_model_data["supported_inference_type_strings"]:
            logger.warning(f"Unsupported inference type '{model_inference_type_string}' for model '{requested_model}'")
            return False, 0, 0
        if not is_api_based_model:
            logger.info("Inference request is for Swiss Army Llama model, so checking if the service is available...")
            local_swiss_army_llama_responding = is_swiss_army_llama_responding(local=True)
            if USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
                remote_swiss_army_llama_responding = is_swiss_army_llama_responding(local=False)
            else:
                remote_swiss_army_llama_responding = 0
            if remote_swiss_army_llama_responding and USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
                port = REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT
            elif local_swiss_army_llama_responding:
                port = SWISS_ARMY_LLAMA_PORT
            else:
                logger.error(f"Neither the local Swiss Army Llama (supposed to be running on port {SWISS_ARMY_LLAMA_PORT}) nor the remote Swiss Army Llama (supposed to be running, if enabled, on mapped port {REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT}) is responding!")
                return False, 0, 0                        
            async with httpx.AsyncClient() as client:
                params = {"token": SWISS_ARMY_LLAMA_SECURITY_TOKEN}
                response = await client.get(f"http://localhost:{port}/get_list_of_available_model_names/", params=params)
                if response.status_code == 200:
                    available_models = response.json()["model_names"]
                    if requested_model not in available_models:
                        add_model_response = await client.post(f"http://localhost:{port}/add_new_model/", params={"token": SWISS_ARMY_LLAMA_SECURITY_TOKEN, "model_url": requested_model_data["model_url"]})
                        if add_model_response.status_code != 200:
                            logger.warning(f"Failed to add new model to Swiss Army Llama: {requested_model}")
                            return False, 0, 0
                else:
                    logger.warning(f"Failed to retrieve available models from Swiss Army Llama API on port {port}")
                    return False, 0, 0
        model_parameters_dict = json.loads(model_parameters)
        input_data_binary = base64.b64decode(input_data)
        result = magika.identify_bytes(input_data_binary)
        detected_data_type = result.output.ct_label
        use_check_inference_requests_locally_before_sending_to_api_service = 0
        if detected_data_type == "txt":
            input_data = input_data_binary.decode("utf-8")
            if is_api_based_model and use_check_inference_requests_locally_before_sending_to_api_service:
                inference_request_allowed = await check_if_input_text_would_get_rejected_from_api_services(input_data)
                if not inference_request_allowed:
                    logger.error(f"Cannot proceed with inference request to model {requested_model} because of risk that it will be rejected and lead to banning!")
                    return False, 0, 0
        if is_base64_encoded(input_data):
            input_data = base64.b64decode(input_data)
            input_data = input_data.decode('utf-8')     
        proposed_cost_in_credits = await calculate_proposed_inference_cost_in_credits(requested_model_data, model_parameters_dict, model_inference_type_string, input_data)
        validation_results = await validate_existing_credit_pack_ticket(credit_pack_ticket_pastel_txid)
        if not validation_results["credit_pack_ticket_is_valid"]:
            logger.warning(f"Invalid credit pack ticket: {validation_results['validation_failure_reasons_list']}")
            return False, 0, 0
        else:
            logger.info(f"Credit pack ticket with txid {credit_pack_ticket_pastel_txid} passed all validation checks: {abbreviated_pretty_json_func(validation_results['validation_checks'])}")
        current_credit_balance, number_of_confirmation_transactions_from_tracking_address_to_burn_address = await determine_current_credit_pack_balance_based_on_tracking_transactions(credit_pack_ticket_pastel_txid)
        if proposed_cost_in_credits >= current_credit_balance:
            logger.warning(f"Insufficient credits for the request. Required: {proposed_cost_in_credits:,}, Available: {current_credit_balance:,}")
            return False, proposed_cost_in_credits, current_credit_balance
        else:
            logger.info(f"Credit pack ticket has sufficient credits for the request. Required: {proposed_cost_in_credits:,}, Available: {current_credit_balance:,}")
        remaining_credits_after_request = current_credit_balance - proposed_cost_in_credits
        return True, proposed_cost_in_credits, remaining_credits_after_request
    except Exception as e:
        logger.error(f"Error validating inference API usage request: {str(e)}")
        traceback.print_exc()
        raise
    
async def process_inference_api_usage_request(inference_api_usage_request: db_code.InferenceAPIUsageRequest) -> db_code.InferenceAPIUsageResponse: 
    # Validate the inference API usage request
    is_valid_request, proposed_cost_in_credits, remaining_credits_after_request = await validate_inference_api_usage_request(inference_api_usage_request) 
    inference_api_usage_request_dict = inference_api_usage_request.model_dump()
    inference_api_usage_request_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in inference_api_usage_request_dict.items()}
    if not is_valid_request:
        logger.error("Invalid inference API usage request received!")
        raise ValueError(f"Error! Received invalid inference API usage request: {inference_api_usage_request_dict}")
    else:
        log_action_with_payload("Received", "inference API usage request", inference_api_usage_request_dict)
    # Save the inference API usage request
    saved_request = await save_inference_api_usage_request(inference_api_usage_request)
    credit_pack_ticket_pastel_txid = inference_api_usage_request.credit_pack_ticket_pastel_txid
    _, credit_pack_purchase_request_response, _ = await retrieve_credit_pack_ticket_using_txid(credit_pack_ticket_pastel_txid)
    credit_usage_tracking_psl_address = credit_pack_purchase_request_response.credit_usage_tracking_psl_address
    # Create and save the InferenceAPIUsageResponse
    inference_response = await create_and_save_inference_api_usage_response(saved_request, proposed_cost_in_credits, remaining_credits_after_request, credit_usage_tracking_psl_address)
    return inference_response

async def create_and_save_inference_api_usage_response(saved_request: db_code.InferenceAPIUsageRequest, proposed_cost_in_credits: float, remaining_credits_after_request: float, credit_usage_tracking_psl_address: str) -> db_code.InferenceAPIUsageResponse:
    # Generate a unique identifier for the inference response
    inference_response_id = str(uuid.uuid4())
    # Create an InferenceAPIUsageResponse instance without the hash and signature fields
    _, _, local_supernode_pastelid, _ = await get_local_machine_supernode_data_func()
    inference_response = db_code.InferenceAPIUsageResponse(
        inference_response_id=inference_response_id,
        inference_request_id=saved_request.inference_request_id,
        proposed_cost_of_request_in_inference_credits=proposed_cost_in_credits,
        remaining_credits_in_pack_after_request_processed=round(remaining_credits_after_request,1),
        credit_usage_tracking_psl_address=credit_usage_tracking_psl_address,
        request_confirmation_message_amount_in_patoshis=int(proposed_cost_in_credits * CREDIT_USAGE_TO_TRACKING_AMOUNT_MULTIPLIER),
        max_block_height_to_include_confirmation_transaction=await get_current_pastel_block_height_func() + 10,  # Adjust as needed
        inference_request_response_utc_iso_string=datetime.now(timezone.utc).isoformat(),
        inference_request_response_pastel_block_height=await get_current_pastel_block_height_func(),
        inference_request_response_message_version_string="1.0"
    )
    # Generate the hash and signature fields
    inference_response.sha3_256_hash_of_inference_request_response_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(inference_response)
    confirmation_signature = await sign_message_with_pastelid_func(local_supernode_pastelid, inference_response.sha3_256_hash_of_inference_request_response_fields, LOCAL_PASTEL_ID_PASSPHRASE)
    inference_response.supernode_pastelid_and_signature_on_inference_request_response_hash = json.dumps({'signing_sn_pastelid': local_supernode_pastelid, 'sn_signature_on_response_hash': confirmation_signature})
    # Save the InferenceAPIUsageResponse to the database
    async with db_code.Session() as db_session:
        db_session.add(inference_response)
        await db_session.commit()
        await db_session.refresh(inference_response)
    return inference_response

async def check_burn_address_for_tracking_transaction(
    tracking_address: str,
    expected_amount: float,
    txid: Optional[str],
    max_block_height: int,
    max_retries: int = 10,
    initial_retry_delay: int = 25
) -> Tuple[bool, int, int]:
    global burn_address
    try_count = 0
    retry_delay = initial_retry_delay
    total_amount_to_burn_address = 0.0
    while try_count < max_retries:
        if txid is None:  # If txid is not provided, search for transactions using listsinceblock RPC method
            start_block_hash = await getblockhash(rpc_connection, 0)
            listsinceblock_output = await listsinceblock(rpc_connection, start_block_hash, 1, True)
            all_transactions = listsinceblock_output["transactions"]
            all_burn_transactions = [
                tx for tx in all_transactions
                if tx.get("address") == burn_address and tx.get("category") == "receive"
            ]
            all_burn_transactions_df = pd.DataFrame.from_records(all_burn_transactions)
            all_burn_transactions_df_filtered = all_burn_transactions_df[all_burn_transactions_df['amount'] == expected_amount]
            if len(all_burn_transactions_df_filtered) == 1:
                txid = all_burn_transactions_df_filtered['txid'].values[0]
            else:
                latest_block_height = await get_current_pastel_block_height_func()
                min_confirmations = latest_block_height - max_block_height
                max_confirmations = latest_block_height - (max_block_height - MAXIMUM_NUMBER_OF_PASTEL_BLOCKS_FOR_USER_TO_SEND_BURN_AMOUNT_FOR_CREDIT_TICKET)
                all_burn_transactions_df_filtered2 = all_burn_transactions_df_filtered[all_burn_transactions_df_filtered['confirmations'] <= max_confirmations]
                all_burn_transactions_df_filtered3 = all_burn_transactions_df_filtered2[all_burn_transactions_df_filtered2['confirmations'] >= min_confirmations]
                if len(all_burn_transactions_df_filtered3) > 1:
                    logger.warning(f"Multiple transactions found with the same amount and confirmations, but the most recent one is {all_burn_transactions_df_filtered3['txid'].values[0]}")
                txid = all_burn_transactions_df_filtered3['txid'].values[0]
        decoded_tx_data = await get_and_decode_raw_transaction(txid)
        if decoded_tx_data:
            # Check if the transaction matches the specified criteria
            if any(vout["scriptPubKey"].get("addresses", [None])[0] == burn_address for vout in decoded_tx_data["vout"]):
                # Calculate the total amount sent to the burn address in the transaction
                total_amount_to_burn_address = sum(
                    vout["value"] for vout in decoded_tx_data["vout"]
                    if vout["scriptPubKey"].get("addresses", [None])[0] == burn_address
                )
                if total_amount_to_burn_address == expected_amount:
                    # Retrieve the transaction details using gettransaction RPC method
                    tx_info = await gettransaction(rpc_connection, txid)
                    if tx_info:
                        num_confirmations = tx_info.get("confirmations", 0)
                        transaction_block_hash = tx_info.get("blockhash", None)
                        if transaction_block_hash:
                            transaction_block_height = await get_block_height_from_block_hash(transaction_block_hash)
                        else:
                            transaction_block_height = 0
                        if ((num_confirmations >= MINIMUM_CONFIRMATION_BLOCKS_FOR_CREDIT_PACK_BURN_TRANSACTION) or SKIP_BURN_TRANSACTION_BLOCK_CONFIRMATION_CHECK) and transaction_block_height <= max_block_height:
                            if SKIP_BURN_TRANSACTION_BLOCK_CONFIRMATION_CHECK:
                                logger.info(f"Matching confirmed transaction found with {num_confirmations:,} confirmation blocks, which is acceptable because the 'SKIP_BURN_TRANSACTION_BLOCK_CONFIRMATION_CHECK' flag is set to TRUE...")
                            else:
                                logger.info(f"Matching confirmed transaction found with {num_confirmations:,} confirmation blocks, greater than or equal to the required confirmation blocks of {MINIMUM_CONFIRMATION_BLOCKS_FOR_CREDIT_PACK_BURN_TRANSACTION}!")
                            return True, False, transaction_block_height, num_confirmations, total_amount_to_burn_address
                        else:
                            logger.info(f"Matching unconfirmed transaction found! Waiting for it to be mined in a block with at least {MINIMUM_CONFIRMATION_BLOCKS_FOR_CREDIT_PACK_BURN_TRANSACTION} confirmation blocks! (Currently it has only {num_confirmations} confirmation blocks)")
                            return True, False, transaction_block_height, num_confirmations, total_amount_to_burn_address
                elif total_amount_to_burn_address >= expected_amount:
                    # Retrieve the transaction details using gettransaction RPC method
                    tx_info = await gettransaction(rpc_connection, txid)
                    if tx_info:
                        num_confirmations = tx_info.get("confirmations", 0)
                        transaction_block_hash = tx_info.get("blockhash", None)
                        if transaction_block_hash:
                            transaction_block_height = await get_block_height_from_block_hash(transaction_block_hash)
                        else:
                            transaction_block_height = 0
                        if ((num_confirmations >= MINIMUM_CONFIRMATION_BLOCKS_FOR_CREDIT_PACK_BURN_TRANSACTION) or SKIP_BURN_TRANSACTION_BLOCK_CONFIRMATION_CHECK) and transaction_block_height <= max_block_height:
                            logger.info(f"Matching confirmed transaction was not found, but we did find a confirmed (with {num_confirmations} confirmation blocks) burn transaction with more than the expected amount ({total_amount_to_burn_address} sent versus the expected amount of {expected_amount})")
                            return False, True, transaction_block_height, num_confirmations, total_amount_to_burn_address
                        else:
                            logger.info(f"Matching unconfirmed transaction was not found, but we did find an unconfirmed burn transaction with more than the expected amount ({total_amount_to_burn_address} sent versus the expected amount of {expected_amount})")
                            return False, True, transaction_block_height, num_confirmations, total_amount_to_burn_address
                else:
                    logger.warning(f"Transaction {txid} found, but the amount sent to the burn address ({total_amount_to_burn_address}) is less than the expected amount ({expected_amount})")
            else:
                logger.warning(f"Transaction {txid} does not send funds to the specified burn address")
        else:
            logger.warning(f"Transaction {txid} not found.")
        # If the transaction is not found or does not match the criteria, wait before retrying
        logger.info(f"WAITING {retry_delay} seconds before checking transaction status again...")
        await asyncio.sleep(retry_delay)
        try_count += 1
        retry_delay *= 1.15  # Optional: increase delay between retries
    logger.info(f"Transaction not found or did not match the criteria after {max_retries} attempts.")
    return False, False, None, None, total_amount_to_burn_address

async def process_inference_confirmation(inference_request_id: str, inference_confirmation: db_code.InferenceConfirmation) -> bool:
    try:
        # Retrieve the inference API usage request from the database
        async with db_code.Session() as db:
            inference_request = await db.exec(
                select(db_code.InferenceAPIUsageRequest).where(db_code.InferenceAPIUsageRequest.inference_request_id == inference_request_id)
            )
            inference_request = inference_request.one_or_none()
        if inference_request is None:
            logger.warning(f"Invalid inference request ID: {inference_request_id}")
            return False
        # Retrieve the inference API usage request response from the database
        async with db_code.Session() as db:
            inference_response = await db.exec(
                select(db_code.InferenceAPIUsageResponse).where(db_code.InferenceAPIUsageResponse.inference_request_id == inference_request_id)
            )
            inference_response = inference_response.one_or_none()
        # Ensure burn address is tracked by local wallet:
        burn_address_already_imported = await check_if_address_is_already_imported_in_local_wallet(burn_address)
        if not burn_address_already_imported:
            await import_address_func(burn_address, "burn_address", True)        
        # Check burn address for tracking transaction:
        confirmation_transaction_txid = inference_confirmation.confirmation_transaction['txid']
        credit_usage_tracking_amount_in_psl = float(inference_response.request_confirmation_message_amount_in_patoshis)/(10**5) # Divide by number of Patoshis per PSL
        matching_transaction_found, exceeding_transaction_found, transaction_block_height, num_confirmations, amount_received_at_burn_address = await check_burn_address_for_tracking_transaction(inference_response.credit_usage_tracking_psl_address, credit_usage_tracking_amount_in_psl, confirmation_transaction_txid, inference_response.max_block_height_to_include_confirmation_transaction)
        if matching_transaction_found:
            logger.info(f"Found correct inference request confirmation tracking transaction in burn address (with {num_confirmations} confirmation blocks so far)! TXID: {confirmation_transaction_txid}; Tracking Amount in PSL: {credit_usage_tracking_amount_in_psl};")
            computed_current_credit_pack_balance, number_of_confirmation_transactions_from_tracking_address_to_burn_address = await determine_current_credit_pack_balance_based_on_tracking_transactions(inference_request.credit_pack_ticket_pastel_txid)
            logger.info(f"Computed current credit pack balance: {computed_current_credit_pack_balance:,.1f} based on {number_of_confirmation_transactions_from_tracking_address_to_burn_address:,} tracking transactions from tracking address to burn address.")       
            # Update the inference request status to "confirmed"
            inference_request.status = "confirmed"
            async with db_code.Session() as db:
                db.add(inference_request)
                await db.commit()
                await db.refresh(inference_request)
            # Trigger the inference request processing
            asyncio.create_task(execute_inference_request(inference_request_id))
            return True
        else:
            logger.error(f"Did not find correct inference request confirmation tracking transaction in burn address! TXID: {confirmation_transaction_txid}; Tracking Amount in PSL: {credit_usage_tracking_amount_in_psl};") 
    except Exception as e:
        logger.error(f"Error processing inference confirmation: {str(e)}")
        traceback.print_exc()
        raise

async def save_inference_output_results(inference_request_id: str, inference_response_id: str, output_results: dict, output_results_file_type_strings: dict) -> None:
    try:
        _, _, local_supernode_pastelid, _ = await get_local_machine_supernode_data_func()
        # Generate a unique identifier for the inference result
        inference_result_id = str(uuid.uuid4())
        # Create an inference output result record without the hash and signature fields
        inference_output_result = db_code.InferenceAPIOutputResult(
            inference_result_id=inference_result_id,
            inference_request_id=inference_request_id,
            inference_response_id=inference_response_id,
            responding_supernode_pastelid=local_supernode_pastelid,
            inference_result_json_base64=base64.b64encode(json.dumps(output_results).encode("utf-8")).decode("utf-8"),
            inference_result_file_type_strings=json.dumps(output_results_file_type_strings),
            inference_result_utc_iso_string=datetime.now(timezone.utc).isoformat(),
            inference_result_pastel_block_height=await get_current_pastel_block_height_func(),
            inference_result_message_version_string="1.0"
        )
        # Generate the hash and signature fields
        inference_output_result.sha3_256_hash_of_inference_result_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(inference_output_result)
        result_id_signature = await sign_message_with_pastelid_func(local_supernode_pastelid, inference_output_result.sha3_256_hash_of_inference_result_fields, LOCAL_PASTEL_ID_PASSPHRASE)
        inference_output_result.responding_supernode_signature_on_inference_result_id = result_id_signature
        # Save the inference output result to the database
        async with db_code.Session() as db:
            db.add(inference_output_result)
            await db.commit()
            await db.refresh(inference_output_result)
    except Exception as e:
        logger.error(f"Error saving inference output results: {str(e)}")
        traceback.print_exc()
        raise

def get_claude3_model_name(model_name: str) -> str:
    model_mapping = {
        "claude3-haiku": "claude-3-haiku-20240307",
        "claude3-opus": "claude-3-opus-20240229",
        "claude3.5-sonnet": "claude-3-5-sonnet-20240620"
    }
    return model_mapping.get(model_name, "")

async def submit_inference_request_to_stability_api(inference_request):
    model_parameters = json.loads(base64.b64decode(inference_request.model_parameters_json_b64).decode("utf-8"))
    if inference_request.model_inference_type_string == "text_to_image":
        prompt = base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")
        if "stability-core" in inference_request.requested_model_canonical_string:
            return await _handle_stability_core_request(prompt, model_parameters)
        elif "sd3" in inference_request.requested_model_canonical_string:
            return await _handle_sd3_request(prompt, model_parameters, inference_request.requested_model_canonical_string)
        else:
            logger.error(f"Unsupported model for text-to-image: {inference_request.requested_model_canonical_string}")
            return None, None
    elif inference_request.model_inference_type_string in ["conservative_upscale", "creative_upscale"]:
        input_image = base64.b64decode(inference_request.model_input_data_json_b64)
        prompt = model_parameters.get("prompt", "")
        if inference_request.model_inference_type_string == "conservative_upscale":
            return await _handle_conservative_upscale_request(input_image, prompt, model_parameters)
        else:
            return await _handle_creative_upscale_request(input_image, prompt, model_parameters)
    else:
        logger.error(f"Unsupported inference type: {inference_request.model_inference_type_string}")
        return None, None

async def _handle_stability_core_request(prompt, model_parameters):
    async with httpx.AsyncClient(timeout=httpx.Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
        data = {
            "prompt": prompt,
            "aspect_ratio": model_parameters.get("aspect_ratio", "1:1"),
            "output_format": model_parameters.get("output_format", "png")
        }
        if model_parameters.get("negative_prompt"):
            data["negative_prompt"] = model_parameters["negative_prompt"]
        if model_parameters.get("seed") is not None:
            data["seed"] = model_parameters["seed"]
        if model_parameters.get("style_preset"):
            data["style_preset"] = model_parameters["style_preset"]
        response = await client.post(
            "https://api.stability.ai/v2beta/stable-image/generate/core",
            headers={
                "Authorization": f"Bearer {STABILITY_API_KEY}",
                "Accept": "application/json"
            },
            files={"none": ''},
            data=data,
        )
        if response.status_code == 200:
            output_results = response.json()
            output_results_file_type_strings = {
                "output_text": "base64_image",
                "output_files": ["NA"]
            }
            return output_results, output_results_file_type_strings
        else:
            logger.error(f"Error generating image from Stability Core API: {response.text}")
            return None, None

async def _handle_sd3_request(prompt, model_parameters, model_name):
    async with httpx.AsyncClient(timeout=httpx.Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
        data = {
            "prompt": prompt,
            "output_format": model_parameters.get("output_format", "png"),
            "model": model_name.replace("stability-", "")
        }
        if model_parameters.get("aspect_ratio"):
            data["aspect_ratio"] = model_parameters["aspect_ratio"]
        if model_parameters.get("seed") is not None:
            data["seed"] = model_parameters["seed"]
        if model_parameters.get("negative_prompt") and "turbo" not in model_name:
            data["negative_prompt"] = model_parameters["negative_prompt"]
        response = await client.post(
            "https://api.stability.ai/v2beta/stable-image/generate/sd3",
            headers={
                "Authorization": f"Bearer {STABILITY_API_KEY}",
                "Accept": "application/json"
            },
            files={"none": ''},
            data=data,
        )
        if response.status_code == 200:
            output_results = response.json()
            output_results_file_type_strings = {
                "output_text": "base64_image",
                "output_files": ["NA"]
            }
            return output_results, output_results_file_type_strings
        else:
            logger.error(f"Error generating image from SD3 API: {response.text}")
            return None, None

async def _handle_conservative_upscale_request(input_image, prompt, model_parameters):
    async with httpx.AsyncClient(timeout=httpx.Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
        data = {
            "prompt": prompt,
            "output_format": model_parameters.get("output_format", "png"),
        }
        if model_parameters.get("negative_prompt"):
            data["negative_prompt"] = model_parameters["negative_prompt"]
        if model_parameters.get("seed") is not None:
            data["seed"] = model_parameters["seed"]
        response = await client.post(
            "https://api.stability.ai/v2beta/stable-image/upscale/conservative",
            headers={
                "Authorization": f"Bearer {STABILITY_API_KEY}",
                "Accept": "application/json"
            },
            files={"image": input_image},
            data=data,
        )
        if response.status_code == 200:
            output_results = base64.b64encode(response.content).decode("utf-8")
            output_results_file_type_strings = {
                "output_text": "base64_image",
                "output_files": ["NA"]
            }
            return output_results, output_results_file_type_strings
        else:
            logger.error(f"Error upscaling image with conservative method: {response.text}")
            return None, None

async def _handle_creative_upscale_request(input_image, prompt, model_parameters):
    async with httpx.AsyncClient(timeout=httpx.Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
        data = {
            "prompt": prompt,
            "output_format": model_parameters.get("output_format", "png"),
            "creativity": model_parameters.get("creativity", 0.3),
        }
        if model_parameters.get("negative_prompt"):
            data["negative_prompt"] = model_parameters["negative_prompt"]
        if model_parameters.get("seed") is not None:
            data["seed"] = model_parameters["seed"]
        response = await client.post(
            "https://api.stability.ai/v2beta/stable-image/upscale/creative",
            headers={
                "Authorization": f"Bearer {STABILITY_API_KEY}",
                "Accept": "application/json"
            },
            files={"image": input_image},
            data=data,
        )
        if response.status_code == 200:
            generation_id = response.json().get("id")
            while True:
                await asyncio.sleep(10)  # Wait for 10 seconds before polling for result
                result_response = await client.get(
                    f"https://api.stability.ai/v2beta/stable-image/upscale/creative/result/{generation_id}",
                    headers={
                        "Authorization": f"Bearer {STABILITY_API_KEY}",
                        "Accept": "image/*"
                    },
                )
                if result_response.status_code == 200:
                    output_results = base64.b64encode(result_response.content).decode("utf-8")
                    output_results_file_type_strings = {
                        "output_text": "base64_image",
                        "output_files": ["NA"]
                    }
                    return output_results, output_results_file_type_strings
                elif result_response.status_code != 202:
                    logger.error(f"Error retrieving creatively upscaled image: {result_response.text}")
                    return None, None
        else:
            logger.error(f"Error initiating creative upscale request: {response.text}")
            return None, None

async def submit_inference_request_to_openai_api(inference_request):
    logger.info("Now accessing OpenAI API...")
    if inference_request.model_inference_type_string == "text_completion":
        model_parameters = json.loads(base64.b64decode(inference_request.model_parameters_json_b64).decode("utf-8"))
        input_prompt = base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")
        num_completions = int(model_parameters.get("number_of_completions_to_generate", 1))
        output_results = []
        total_input_tokens = 0
        total_output_tokens = 0
        openai_text_completion_response_timeout_seconds = 60
        async with httpx.AsyncClient(timeout=openai_text_completion_response_timeout_seconds) as client:
            for i in range(num_completions):
                response = await client.post(
                    "https://api.openai.com/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {OPENAI_API_KEY}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": inference_request.requested_model_canonical_string.replace("openai-", ""),
                        "messages": [{"role": "user", "content": input_prompt}],
                        "max_tokens": int(model_parameters.get("number_of_tokens_to_generate", 1000)),
                        "temperature": float(model_parameters.get("temperature", 0.7)),
                        "n": 1
                    }
                )
                if response.status_code == 200:
                    response_json = response.json()
                    output_results.append(response_json["choices"][0]["message"]["content"])
                    total_input_tokens += response_json["usage"]["prompt_tokens"]
                    total_output_tokens += response_json["usage"]["completion_tokens"]
                else:
                    logger.error(f"Error generating text from OpenAI API: {response.text}")
                    return None, None
        logger.info(f"Total input tokens used with {inference_request.requested_model_canonical_string} model: {total_input_tokens}")
        logger.info(f"Total output tokens used with {inference_request.requested_model_canonical_string} model: {total_output_tokens}")
        if num_completions == 1:
            output_text = output_results[0]
        else:
            output_text = json.dumps({f"completion_{i+1:02}": result for i, result in enumerate(output_results)})
        logger.info(f"Generated the following output text using {inference_request.requested_model_canonical_string}: {output_text[:100]} <abbreviated>...")
        result = magika.identify_bytes(output_text.encode("utf-8"))
        detected_data_type = result.output.ct_label
        output_results_file_type_strings = {
            "output_text": detected_data_type,
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings
    elif inference_request.model_inference_type_string == "embedding":
        input_text = base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://api.openai.com/v1/embeddings",
                headers={
                    "Authorization": f"Bearer {OPENAI_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": inference_request.requested_model_canonical_string.replace("openai-", ""),
                    "input": input_text
                }
            )
            if response.status_code == 200:
                output_results = response.json()["data"][0]["embedding"]
                output_results_file_type_strings = {
                    "output_text": "embedding",
                    "output_files": ["NA"]
                }
                return output_results, output_results_file_type_strings
            else:
                logger.error(f"Error generating embedding from OpenAI API: {response.text}")
                return None, None
    elif inference_request.model_inference_type_string == "ask_question_about_an_image":
        model_parameters = json.loads(base64.b64decode(inference_request.model_parameters_json_b64).decode("utf-8"))
        num_completions = int(model_parameters.get("number_of_completions_to_generate", 1))
        input_data = json.loads(base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8"))
        image_data_binary = base64.b64decode(input_data["image"])
        question = input_data["question"]
        base64_image = base64.b64encode(image_data_binary).decode('utf-8')
        output_results = []
        total_input_tokens = 0
        total_output_tokens = 0        
        openai_vision_timeout_seconds = 90
        async with httpx.AsyncClient(timeout=openai_vision_timeout_seconds) as client:
            for i in range(num_completions):
                response = await client.post(
                    "https://api.openai.com/v1/chat/completions",
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {OPENAI_API_KEY}"
                    },
                    json={
                        "model": inference_request.requested_model_canonical_string.replace("openai-", "").replace("-vision", ""),
                        "messages": [
                            {
                                "role": "user",
                                "content": [
                                    {
                                        "type": "text",
                                        "text": question
                                    },
                                    {
                                        "type": "image_url",
                                        "image_url": {
                                            "url": f"data:image/jpeg;base64,{base64_image}"
                                        }
                                    }
                                ]
                            }
                        ],
                        "max_tokens": int(json.loads(base64.b64decode(inference_request.model_parameters_json_b64).decode("utf-8")).get("number_of_tokens_to_generate", 300))
                    }
                )
                if response.status_code == 200:
                    response_json = response.json()
                    output_results.append(response_json["choices"][0]["message"]["content"])
                    total_input_tokens += response_json["usage"]["prompt_tokens"]
                    total_output_tokens += response_json["usage"]["completion_tokens"]                    
                else:
                    logger.error(f"Error generating response from OpenAI API: {response.text}")
                    return None, None
        logger.info(f"Total input tokens used with {inference_request.requested_model_canonical_string} model: {total_input_tokens}")
        logger.info(f"Total output tokens used with {inference_request.requested_model_canonical_string} model: {total_output_tokens}")
        if num_completions == 1:
            output_text = output_results[0]
        else:
            output_text = json.dumps({f"completion_{i+1:02}": result for i, result in enumerate(output_results)})             
        logger.info(f"Generated the following output text using {inference_request.requested_model_canonical_string}: {output_text[:100]} <abbreviated>...")
        result = magika.identify_bytes(output_text.encode("utf-8"))
        detected_data_type = result.output.ct_label
        output_results_file_type_strings = {
            "output_text": detected_data_type,
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings               
    else:
        logger.warning(f"Unsupported inference type for OpenAI model: {inference_request.model_inference_type_string}")
        return None, None
    
async def submit_inference_request_to_openrouter(inference_request):
    logger.info("Now accessing OpenRouter...")
    if inference_request.model_inference_type_string == "text_completion":
        model_parameters = json.loads(base64.b64decode(inference_request.model_parameters_json_b64).decode("utf-8"))       
        messages = [{"role": "user", "content": base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")}]
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": inference_request.requested_model_canonical_string,
                    "messages": messages,
                    "max_tokens": int(model_parameters.get("number_of_tokens_to_generate", 1000)),
                    "temperature": float(model_parameters.get("temperature", 0.7)),
                }
            )
            if response.status_code == 200:
                output_results = response.json()["choices"][0]["message"]["content"]
                result = magika.identify_bytes(output_results.encode("utf-8")) 
                detected_data_type = result.output.ct_label
                output_results_file_type_strings = {
                    "output_text": detected_data_type,
                    "output_files": ["NA"]
                }
                return output_results, output_results_file_type_strings
            else:
                logger.error(f"Error generating text from OpenRouter: {response.text}")
                return None, None
    else:
        logger.warning(f"Unsupported inference type for OpenRouter model: {inference_request.model_inference_type_string}")
        return None, None

async def submit_inference_request_to_mistral_api(inference_request):
    # Integrate with the Mistral API to perform the inference task
    logger.info("Now accessing Mistral API...")
    client = MistralAsyncClient(api_key=MISTRAL_API_KEY)
    if inference_request.model_inference_type_string == "text_completion":
        model_parameters = json.loads(base64.b64decode(inference_request.model_parameters_json_b64).decode("utf-8"))        
        input_prompt = base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")
        num_completions = int(model_parameters.get("number_of_completions_to_generate", 1))
        output_results = []
        total_input_tokens = 0
        total_output_tokens = 0
        for i in range(num_completions):
            messages = [ChatMessage(role="user", content=input_prompt)]
            async_response = client.chat_stream(
                model=inference_request.requested_model_canonical_string.replace("mistralapi-",""),
                messages=messages,
                max_tokens=int(model_parameters.get("number_of_tokens_to_generate", 1000)),
                temperature=float(model_parameters.get("temperature", 0.7)),
            )
            completion_text = ""
            prompt_tokens = 0
            completion_tokens = 0
            async for chunk in async_response:
                if chunk.choices[0].delta.content:
                    completion_text += chunk.choices[0].delta.content
                    completion_tokens += 1
                else:
                    prompt_tokens += 1
            output_results.append(completion_text)
            total_input_tokens += prompt_tokens
            total_output_tokens += completion_tokens
        logger.info(f"Total input tokens used with {inference_request.requested_model_canonical_string} model: {total_input_tokens}")
        logger.info(f"Total output tokens used with {inference_request.requested_model_canonical_string} model: {total_output_tokens}")
        if num_completions == 1:
            output_text = output_results[0]
        else:
            output_text = json.dumps({f"completion_{i+1:02}": result for i, result in enumerate(output_results)})
        logger.info(f"Generated the following output text using {inference_request.requested_model_canonical_string}: {output_text[:100]} <abbreviated>...")
        result = magika.identify_bytes(output_text.encode("utf-8"))
        detected_data_type = result.output.ct_label
        output_results_file_type_strings = {
            "output_text": detected_data_type,
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings
    elif inference_request.model_inference_type_string == "embedding":
        input_text = base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")
        embeddings_batch_response = client.embeddings(
            model=inference_request.requested_model_canonical_string,
            input=[input_text],
        )
        output_results = embeddings_batch_response.data[0].embedding
        output_results_file_type_strings = {
            "output_text": "embedding",
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings
    else:
        logger.warning(f"Unsupported inference type for Mistral model: {inference_request.model_inference_type_string}")
        return None, None
    
async def submit_inference_request_to_groq_api(inference_request):
    # Integrate with the Groq API to perform the inference task
    logger.info("Now accessing Groq API...")
    client = AsyncGroq(api_key=GROQ_API_KEY)
    if inference_request.model_inference_type_string == "text_completion":
        model_parameters = json.loads(base64.b64decode(inference_request.model_parameters_json_b64).decode("utf-8"))
        input_prompt = base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")
        num_completions = int(model_parameters.get("number_of_completions_to_generate", 1))
        output_results = []
        total_input_tokens = 0
        total_output_tokens = 0
        for i in range(num_completions):
            chat_completion = await client.chat.completions.create(
                messages=[{"role": "user", "content": input_prompt}],
                model=inference_request.requested_model_canonical_string.replace("groq-",""),
                max_tokens=int(model_parameters.get("number_of_tokens_to_generate", 1000)),
                temperature=float(model_parameters.get("temperature", 0.7)),
            )
            output_results.append(chat_completion.choices[0].message.content)
            total_input_tokens += chat_completion.usage.prompt_tokens
            total_output_tokens += chat_completion.usage.completion_tokens
        logger.info(f"Total input tokens used with {inference_request.requested_model_canonical_string} model: {total_input_tokens}")
        logger.info(f"Total output tokens used with {inference_request.requested_model_canonical_string} model: {total_output_tokens}")
        if num_completions == 1:
            output_text = output_results[0]
        else:
            output_text = json.dumps({f"completion_{i+1:02}": result for i, result in enumerate(output_results)})
        logger.info(f"Generated the following output text using {inference_request.requested_model_canonical_string}: {output_text[:100]} <abbreviated>...")
        result = magika.identify_bytes(output_text.encode("utf-8"))
        detected_data_type = result.output.ct_label
        output_results_file_type_strings = {
            "output_text": detected_data_type,
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings
    elif inference_request.model_inference_type_string == "embedding":
        input_text = base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")
        response = await client.embed(
            input=input_text,
            model=inference_request.requested_model_canonical_string[5:],
        )
        output_results = response.embedding
        output_results_file_type_strings = {
            "output_text": "embedding",
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings
    else:
        logger.warning(f"Unsupported inference type for Groq model: {inference_request.model_inference_type_string}")
        return None, None

async def submit_inference_request_to_claude_api(inference_request):
    # Integrate with the Claude API to perform the inference task
    logger.info("Now accessing Claude (Anthropic) API...")
    client = anthropic.AsyncAnthropic(api_key=CLAUDE3_API_KEY)
    claude3_model_id_string = get_claude3_model_name(inference_request.requested_model_canonical_string)
    if inference_request.model_inference_type_string == "text_completion":
        model_parameters = json.loads(base64.b64decode(inference_request.model_parameters_json_b64).decode("utf-8"))
        input_prompt = base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")
        num_completions = int(model_parameters.get("number_of_completions_to_generate", 1))
        output_results = []
        total_input_tokens = 0
        total_output_tokens = 0
        for i in range(num_completions):
            async with client.messages.stream(
                model=claude3_model_id_string,
                max_tokens=int(model_parameters.get("number_of_tokens_to_generate", 1000)),
                temperature=float(model_parameters.get("temperature", 0.7)),
                messages=[{"role": "user", "content": input_prompt}],
            ) as stream:
                message = await stream.get_final_message()
                output_results.append(message.content[0].text)
                total_input_tokens += message.usage.input_tokens
                total_output_tokens += message.usage.output_tokens
        logger.info(f"Total input tokens used with {claude3_model_id_string} model: {total_input_tokens}")
        logger.info(f"Total output tokens used with {claude3_model_id_string} model: {total_output_tokens}")
        if num_completions == 1:
            output_text = output_results[0]
        else:
            output_text = json.dumps({f"completion_{i+1:02}": result for i, result in enumerate(output_results)})
        logger.info(f"Generated the following output text using {claude3_model_id_string}: {output_text[:100]} <abbreviated>...")
        result = magika.identify_bytes(output_text.encode("utf-8"))
        detected_data_type = result.output.ct_label
        output_results_file_type_strings = {
            "output_text": detected_data_type,
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings
    elif inference_request.model_inference_type_string == "embedding":
        input_text = base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")
        response = await client.embed(
            text=input_text,
            model=claude3_model_id_string
        )
        output_results = response.embedding
        output_results_file_type_strings = {
            "output_text": "embedding",
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings
    else:
        logger.warning(f"Unsupported inference type for Claude3 Haiku: {inference_request.model_inference_type_string}")
        return None, None

# Swiss Army Llama related functions:

def determine_swiss_army_llama_port():
    local_swiss_army_llama_responding = is_swiss_army_llama_responding(local=True)
    if USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
        remote_swiss_army_llama_responding = is_swiss_army_llama_responding(local=False)
    else:
        remote_swiss_army_llama_responding = 0
    
    if remote_swiss_army_llama_responding and USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
        return REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT
    elif local_swiss_army_llama_responding:
        return SWISS_ARMY_LLAMA_PORT
    return None

async def handle_swiss_army_llama_exception(e, client, inference_request, model_parameters, port, is_fallback, handler_function):
    logger.error("Failed to execute inference request: {}".format(e))
    if port == REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT and not is_fallback:
        logger.info("Falling back to local Swiss Army Llama.")
        return await handler_function(client, inference_request, model_parameters, SWISS_ARMY_LLAMA_PORT, True)
    else:
        return None, None
    
async def handle_swiss_army_llama_text_completion(client, inference_request, model_parameters, port, is_fallback):
    payload = {
        "input_prompt": base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8"),
        "llm_model_name": inference_request.requested_model_canonical_string.replace("swiss_army_llama-", ""),
        "temperature": model_parameters.get("temperature", 0.7),
        "number_of_tokens_to_generate": model_parameters.get("number_of_tokens_to_generate", 1000),
        "number_of_completions_to_generate": model_parameters.get("number_of_completions_to_generate", 1),
        "grammar_file_string": model_parameters.get("grammar_file_string", ""),
    }
    try:
        response = await client.post(
            f"http://localhost:{port}/get_text_completions_from_input_prompt/",
            json=payload,
            params={"token": SWISS_ARMY_LLAMA_SECURITY_TOKEN}
        )
        response.raise_for_status()
        output_results = response.json()
        output_text = output_results[0]["generated_text"]
        result = magika.identify_bytes(output_text.encode("utf-8"))
        detected_data_type = result.output.ct_label
        output_results_file_type_strings = {
            "output_text": detected_data_type,
            "output_files": ["NA"]
        }
        return output_text, output_results_file_type_strings
    except Exception as e:
        return await handle_swiss_army_llama_exception(e, client, inference_request, model_parameters, port, is_fallback, handle_swiss_army_llama_text_completion)

async def handle_swiss_army_llama_image_question(client, inference_request, model_parameters, port, is_fallback):
    input_data = json.loads(base64.b64decode(inference_request.model_input_data_json_b64).decode())
    image_data_binary = base64.b64decode(input_data["image"])
    question = input_data["question"]
    payload = {
        "question": question,
        "llm_model_name": inference_request.requested_model_canonical_string.replace("swiss_army_llama-", ""),
        "temperature": model_parameters.get("temperature", 0.7),
        "number_of_tokens_to_generate": model_parameters.get("number_of_tokens_to_generate", 256),
        "number_of_completions_to_generate": model_parameters.get("number_of_completions_to_generate", 1)
    }
    files = {"image": ("image.png", image_data_binary, "image/png")}
    try:
        response = await client.post(
            f"http://localhost:{port}/ask_question_about_image/",
            data=payload,
            files=files,
            params={"token": SWISS_ARMY_LLAMA_SECURITY_TOKEN}
        )
        response.raise_for_status()
        output_results = response.json()
        output_text = output_results[0]["generated_text"]
        result = magika.identify_bytes(output_text.encode("utf-8"))
        detected_data_type = result.output.ct_label
        output_results_file_type_strings = {
            "output_text": detected_data_type,
            "output_files": ["NA"]
        }
        return output_text, output_results_file_type_strings
    except Exception as e:
        return await handle_swiss_army_llama_exception(e, client, inference_request, model_parameters, port, is_fallback, handle_swiss_army_llama_image_question)

async def handle_swiss_army_llama_embedding_document(client, inference_request, model_parameters, port, is_fallback):
    input_data = inference_request.model_input_data_json_b64
    if is_base64_encoded(input_data):
        input_data = base64.b64decode(input_data)
        input_data = input_data.decode('utf-8')
    try:
        input_data_dict = json.loads(input_data)
        document_file_data = input_data_dict['document']
        if is_base64_encoded(document_file_data):
            document_file_data = base64.b64decode(document_file_data)
    except Exception as e:
        logger.error(f"Error decoding audio data: {str(e)}")
        traceback.print_exc()
        raise("Error decoding audio data")
    try:
        file_metadata = await upload_and_get_file_metadata(document_file_data, file_prefix="document")
        file_url = file_metadata["file_url"]
        file_hash = file_metadata["file_hash"]
        file_size = file_metadata["file_size"]
        query_text = model_parameters.get("query_text", None)
    except Exception as e:
        logger.error(f"Error parsing document data from input: {str(e)}")
        traceback.print_exc()
        raise
    params = {
        "llm_model_name": inference_request.requested_model_canonical_string.replace("swiss_army_llama-", ""),
        "embedding_pooling_method": model_parameters.get("embedding_pooling_method", "svd"),
        "corpus_identifier_string": model_parameters.get("corpus_identifier_string", ""),
        "json_format": model_parameters.get("json_format", "records"),
        "send_back_json_or_zip_file": model_parameters.get("send_back_json_or_zip_file", "zip"),
        "query_text": query_text
    }
    files = {
        'url': (None, file_url),
        'hash': (None, file_hash),
        'size': (None, str(file_size))
    }
    try:
        response = await client.post(
            f"http://localhost:{port}/get_all_embedding_vectors_for_document/",
            params=params,
            files=files,
            headers={"accept": "application/json"}
        )
        response.raise_for_status()
        if model_parameters.get("send_back_json_or_zip_file", "zip") == "json":
            output_results = response.json()
            output_results_file_type_strings = {
                "output_text": "embedding_document",
                "output_files": ["NA"]
            }            
        else:
            zip_file_content = response.read()
            output_results = base64.b64encode(zip_file_content).decode('utf-8')
            output_results_file_type_strings = {
                "output_text": "NA",
                "output_files": ["zip"]
            }
        return output_results, output_results_file_type_strings
    except Exception as e:
        return await handle_swiss_army_llama_exception(e, client, inference_request, model_parameters, port, is_fallback, handle_swiss_army_llama_embedding_document)
    
async def handle_swiss_army_llama_embedding_audio(client, inference_request, model_parameters, port, is_fallback):
    input_data = inference_request.model_input_data_json_b64
    if is_base64_encoded(input_data):
        input_data = base64.b64decode(input_data)
        input_data = input_data.decode('utf-8')
    try:
        input_data_dict = json.loads(input_data)
        audio_file_data = input_data_dict['audio']
        if is_base64_encoded(audio_file_data):
            audio_file_data = base64.b64decode(audio_file_data) 
    except Exception as e:
        logger.error(f"Error decoding audio data: {str(e)}")
        traceback.print_exc()
        raise("Error decoding audio data")
    try:
        file_metadata = await upload_and_get_file_metadata(audio_file_data, file_prefix="audio")
        file_url = file_metadata["file_url"]
        file_hash = file_metadata["file_hash"]
        file_size = file_metadata["file_size"]
        query_text = model_parameters.get("query_text", "")
        corpus_identifier_string = model_parameters.get("corpus_identifier_string", "")
    except Exception as e:
        logger.error(f"Error parsing audio data from input: {str(e)}")
        traceback.print_exc()
        raise
    params = {
        "compute_embeddings_for_resulting_transcript_document": model_parameters.get("compute_embeddings_for_resulting_transcript_document", True),
        "llm_model_name": inference_request.requested_model_canonical_string.replace("swiss_army_llama-", ""),
        "embedding_pooling_method": model_parameters.get("embedding_pooling_method", "svd"),
        "corpus_identifier_string": corpus_identifier_string
    }
    files = {
        'url': (None, file_url),
        'hash': (None, file_hash),
        'size': (None, str(file_size))
    }
    try:
        response = await client.post(
            f"http://localhost:{port}/compute_transcript_with_whisper_from_audio/",
            params=params,
            files=files,
            headers={"accept": "application/json"}
        )
        response.raise_for_status()
        output_results = response.json()
        output_results_file_type_strings = {
            "output_text": "embedding_audio",
            "output_files": ["NA"]
        }
        if query_text:
            search_payload = {
                "query_text": query_text,
                "number_of_most_similar_strings_to_return": model_parameters.get("number_of_most_similar_strings_to_return", 10),
                "llm_model_name": inference_request.requested_model_canonical_string.replace("swiss_army_llama-", ""),
                "embedding_pooling_method": model_parameters.get("embedding_pooling_method", "svd"),
                "corpus_identifier_string": corpus_identifier_string
            }
            search_response = await client.post(
                f"http://localhost:{port}/search_stored_embeddings_with_query_string_for_semantic_similarity/",
                json=search_payload,
                headers={"accept": "application/json", "Content-Type": "application/json"}
            )
            search_response.raise_for_status()
            search_results = search_response.json()
            output_results["search_results"] = search_results
        return output_results, output_results_file_type_strings
    except Exception as e:
        return await handle_swiss_army_llama_exception(e, client, inference_request, model_parameters, port, is_fallback, handle_swiss_army_llama_embedding_audio)

async def handle_swiss_army_llama_semantic_search(client, inference_request, model_parameters, port, is_fallback):
    payload = {
        "query_text": model_parameters.get("query_text", ""),
        "number_of_most_similar_strings_to_return": model_parameters.get("number_of_most_similar_strings_to_return", 10),
        "llm_model_name": inference_request.requested_model_canonical_string.replace("swiss_army_llama-", ""),
        "embedding_pooling_method": model_parameters.get("embedding_pooling_method", "svd_first_four"),
        "corpus_identifier_string": model_parameters.get("corpus_identifier_string", "")
    }
    try:
        response = await client.post(
            f"http://localhost:{port}/search_stored_embeddings_with_query_string_for_semantic_similarity/",
            json=payload,
            params={"token": SWISS_ARMY_LLAMA_SECURITY_TOKEN}
        )
        response.raise_for_status()
        output_results = response.json()
        output_results_file_type_strings = {
            "output_text": "semantic_search",
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings
    except Exception as e:
        return await handle_swiss_army_llama_exception(e, client, inference_request, model_parameters, port, is_fallback, handle_swiss_army_llama_semantic_search)

async def handle_swiss_army_llama_advanced_semantic_search(client, inference_request, model_parameters, port, is_fallback):
    payload = {
        "query_text": model_parameters.get("query_text", ""),
        "llm_model_name": inference_request.requested_model_canonical_string.replace("swiss_army_llama-", ""),
        "embedding_pooling_method": model_parameters.get("embedding_pooling_method", "svd"),
        "corpus_identifier_string": model_parameters.get("corpus_identifier_string", "string"),
        "similarity_filter_percentage": model_parameters.get("similarity_filter_percentage", 0.01),
        "number_of_most_similar_strings_to_return": model_parameters.get("number_of_most_similar_strings_to_return", 0),
        "result_sorting_metric": model_parameters.get("result_sorting_metric", "hoeffding_d")
    }
    try:
        response = await client.post(
            f"http://localhost:{port}/advanced_search_stored_embeddings_with_query_string_for_semantic_similarity/",
            json=payload,
            params={"token": SWISS_ARMY_LLAMA_SECURITY_TOKEN}
        )
        response.raise_for_status()
        output_results = response.json()
        output_results_file_type_strings = {
            "output_text": "advanced_semantic_search",
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings
    except Exception as e:
        return await handle_swiss_army_llama_exception(e, client, inference_request, model_parameters, port, is_fallback, handle_swiss_army_llama_advanced_semantic_search)

async def submit_inference_request_to_swiss_army_llama(inference_request, is_fallback=False):
    logger.info("Now calling Swiss Army Llama with model {}".format(inference_request.requested_model_canonical_string))
    model_parameters = json.loads(base64.b64decode(inference_request.model_parameters_json_b64).decode("utf-8"))
    port = determine_swiss_army_llama_port()
    if not port:
        logger.error(f"Neither the local (port {SWISS_ARMY_LLAMA_PORT}) nor the remote (port {REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT}) Swiss Army Llama is responding!")
        return None, None
    async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS * 12)) as client:
        if inference_request.model_inference_type_string == "text_completion":
            return await handle_swiss_army_llama_text_completion(client, inference_request, model_parameters, port, is_fallback)
        elif inference_request.model_inference_type_string == "embedding_document":
            return await handle_swiss_army_llama_embedding_document(client, inference_request, model_parameters, port, is_fallback)
        elif inference_request.model_inference_type_string == "embedding_audio":
            return await handle_swiss_army_llama_embedding_audio(client, inference_request, model_parameters, port, is_fallback)
        elif inference_request.model_inference_type_string == "ask_question_about_an_image":
            return await handle_swiss_army_llama_image_question(client, inference_request, model_parameters, port, is_fallback)
        elif inference_request.model_inference_type_string == "semantic_search":
            return await handle_swiss_army_llama_semantic_search(client, inference_request, model_parameters, port, is_fallback)
        elif inference_request.model_inference_type_string == "advanced_semantic_search":
            return await handle_swiss_army_llama_advanced_semantic_search(client, inference_request, model_parameters, port, is_fallback)
        else:
            logger.warning("Unsupported inference type: {}".format(inference_request.model_inference_type_string))
            return None, None

async def execute_inference_request(inference_request_id: str) -> None:
    try:
        # Retrieve the inference API usage request from the database
        async with db_code.Session() as db:
            query = await db.exec(
                select(db_code.InferenceAPIUsageRequest).where(db_code.InferenceAPIUsageRequest.inference_request_id == inference_request_id)
            )
            inference_request = query.one_or_none()
        if inference_request is None:
            logger.warning(f"Invalid inference request ID: {inference_request_id}")
            return
        # Retrieve the inference API usage request response from the database
        async with db_code.Session() as db:
            query = await db.exec(
                select(db_code.InferenceAPIUsageResponse).where(db_code.InferenceAPIUsageResponse.inference_request_id == inference_request_id)
            )
            inference_response = query.one_or_none()
        if inference_request.requested_model_canonical_string.startswith("stability-"):
            output_results, output_results_file_type_strings = await submit_inference_request_to_stability_api(inference_request)
        elif inference_request.requested_model_canonical_string.startswith("openai-"):
            output_results, output_results_file_type_strings = await submit_inference_request_to_openai_api(inference_request)
        elif inference_request.requested_model_canonical_string.startswith("mistralapi-"):
            output_results, output_results_file_type_strings = await submit_inference_request_to_mistral_api(inference_request)
        elif inference_request.requested_model_canonical_string.startswith("groq-"):
            output_results, output_results_file_type_strings = await submit_inference_request_to_groq_api(inference_request)
        elif "claude" in inference_request.requested_model_canonical_string.lower():
            output_results, output_results_file_type_strings = await submit_inference_request_to_claude_api(inference_request)
        elif inference_request.requested_model_canonical_string.startswith("openrouter/"):
            output_results, output_results_file_type_strings = await submit_inference_request_to_openrouter(inference_request)
        elif inference_request.requested_model_canonical_string.startswith("swiss_army_llama-"):
            output_results, output_results_file_type_strings = await submit_inference_request_to_swiss_army_llama(inference_request)
        else:
            error_message = f"Unsupported provider or model selected for {inference_request.requested_model_canonical_string}: {inference_request.model_inference_type_string}"
            logger.error(error_message)
            raise ValueError(error_message)
        if output_results is not None and output_results_file_type_strings is not None:
            # Save the inference output results to the database
            await save_inference_output_results(inference_request_id, inference_response.inference_response_id, output_results, output_results_file_type_strings)
    except Exception as e:
        logger.error(f"Error executing inference request: {str(e)}")
        traceback.print_exc()
        raise

async def check_status_of_inference_request_results(inference_response_id: str) -> bool:
    try:
        async with db_code.Session() as db_session:
            # Retrieve the inference output result
            result = await db_session.exec(
                select(db_code.InferenceAPIOutputResult).where(db_code.InferenceAPIOutputResult.inference_response_id == inference_response_id)
            )
            inference_output_result = result.one_or_none()
            if inference_output_result is None:
                return False
            else:
                return True
    except Exception as e:
        logger.error(f"Error checking status of inference request results: {str(e)}")
        raise

async def get_inference_output_results_and_verify_authorization(inference_response_id: str, requesting_pastelid: str) -> db_code.InferenceAPIOutputResult:
    async with db_code.Session() as db_session:
        # Retrieve the inference output result
        query = await db_session.exec(
            select(db_code.InferenceAPIOutputResult).where(db_code.InferenceAPIOutputResult.inference_response_id == inference_response_id)
        )
        inference_output_result = query.one_or_none()
        if inference_output_result is None:
            raise ValueError("Inference output results not found")
        # Retrieve the inference request to verify requesting PastelID
        query = await db_session.exec(
            select(db_code.InferenceAPIUsageRequest).where(db_code.InferenceAPIUsageRequest.inference_request_id == inference_output_result.inference_request_id)
        )
        inference_request = query.one_or_none()
        if inference_request is None or inference_request.requesting_pastelid != requesting_pastelid:
            raise ValueError("Unauthorized access to inference output results")
        return inference_output_result

async def fetch_all_mnid_tickets_details():
    mnid_response = await tickets_list_id(rpc_connection, 'mn')
    if mnid_response is None or len(mnid_response) == 0:
        return []
    tickets_data = {ticket['txid']: ticket for ticket in mnid_response}
    async with db_code.Session() as session:
        async with session.begin():
            result = await session.execute(select(db_code.MNIDTicketDetails.txid).where(db_code.MNIDTicketDetails.txid.in_(tickets_data.keys())))
            existing_txids = result.scalars().all()  # Correct method to fetch scalar results directly
            existing_txids_set = set(existing_txids)
            new_tickets_to_insert = []
            for txid, ticket in tickets_data.items():
                if txid not in existing_txids_set:
                    new_ticket = db_code.MNIDTicketDetails(
                        txid=txid,
                        pastel_id=ticket['ticket']['pastelID'],
                        address=ticket['ticket']['address'],
                        pq_key=ticket['ticket']['pq_key'],
                        outpoint=ticket['ticket']['outpoint'],
                        block_height=ticket['height'],
                        timestamp=datetime.utcfromtimestamp(int(ticket['ticket']['timeStamp']))
                    )
                    new_tickets_to_insert.append(new_ticket)
            if new_tickets_to_insert:
                try:
                    session.add_all(new_tickets_to_insert)
                    await session.commit()
                except IntegrityError as e:
                    await session.rollback()
                    logger.error(f"Error inserting new tickets due to a unique constraint failure: {e}")
    return new_tickets_to_insert

async def fetch_active_supernodes_count_and_details(block_height: int):
    async with db_code.Session() as session:
        async with session.begin():  # Fetch all mnid tickets created up to the specified block height
            result = await session.execute(
                select(db_code.MNIDTicketDetails)
                .where(db_code.MNIDTicketDetails.block_height <= block_height)
            )
            mnid_tickets = result.scalars().all()
            active_supernodes = []
            for ticket in mnid_tickets:
                try:
                    # Check if outpoint is valid
                    if ticket.outpoint and isinstance(ticket.outpoint, str) and '-' in ticket.outpoint:
                        txid, vout_str = ticket.outpoint.split('-')
                        # Check if vout is a valid integer
                        if not vout_str.isdigit():
                            continue
                        vout = int(vout_str)
                        tx_info = await getrawtransaction(rpc_connection, txid, 1)
                        # Ensure tx_info is a dictionary and contains 'vout'
                        if tx_info and isinstance(tx_info, dict) and 'vout' in tx_info:
                            # Ensure vout is within the valid range of tx_info['vout']
                            if vout >= 0 and vout < len(tx_info['vout']):
                                vout_data = tx_info['vout'][vout]
                                # Ensure vout_data is a dictionary and contains necessary keys
                                if vout_data and isinstance(vout_data, dict) and 'n' in vout_data and 'value' in vout_data:
                                    # Check if the outpoint is still unspent and meets collateral requirements
                                    if vout_data['n'] == vout and vout_data['value'] >= masternode_collateral_amount:
                                        # Ensure timestamp is valid
                                        if isinstance(ticket.timestamp, (datetime, int)):
                                            timestamp = datetime.utcfromtimestamp(ticket.timestamp.timestamp() if isinstance(ticket.timestamp, datetime) else int(ticket.timestamp))
                                            supernode_details = {
                                                "txid": ticket.txid,
                                                "pastel_id": ticket.pastel_id,
                                                "address": ticket.address,
                                                "pq_key": ticket.pq_key,
                                                "outpoint": ticket.outpoint,
                                                "block_height": ticket.block_height,
                                                "timestamp": timestamp
                                            }
                                            active_supernodes.append(supernode_details)
                except (KeyError, ValueError, TypeError) as e:  # noqa: F841
                    pass
    active_supernodes_count = len(active_supernodes)
    return active_supernodes_count, active_supernodes

async def determine_current_credit_pack_balance_based_on_tracking_transactions(credit_pack_ticket_txid: str):
    logger.info(f"Retrieving credit pack ticket data for txid: {credit_pack_ticket_txid}")
    _, credit_pack_purchase_request_response, _ = await retrieve_credit_pack_ticket_using_txid(credit_pack_ticket_txid)
    credit_pack_purchase_request_fields_json = base64.b64decode(
        credit_pack_purchase_request_response.credit_pack_purchase_request_fields_json_b64
    ).decode('utf-8')
    credit_pack_purchase_request_dict = json.loads(credit_pack_purchase_request_fields_json)
    initial_credit_balance = credit_pack_purchase_request_dict['requested_initial_credits_in_credit_pack']
    credit_usage_tracking_psl_address = credit_pack_purchase_request_response.credit_usage_tracking_psl_address
    logger.info(f"Credit pack ticket data retrieved. Initial credit balance: {initial_credit_balance:,.1f}, Tracking address: {credit_usage_tracking_psl_address}")
    min_height_for_credit_pack_tickets = 700000
    try:
        logger.info(f"Now scanning blockchain for burn transactions sent from address {credit_usage_tracking_psl_address}...")
        params = {
            "addresses": [burn_address],
            "mempool": True,
            "minHeight": min_height_for_credit_pack_tickets,
            "sender": credit_usage_tracking_psl_address
        }
        burn_transactions = await getaddressutxosextra(rpc_connection, params)
        # Check if the result is valid
        if not burn_transactions:
            logger.info(f"No transactions found for address {credit_usage_tracking_psl_address}. Returning initial balance.")
            return initial_credit_balance, 0
        # Filter out transactions where 'patoshis' exceeds 100000
        burn_transactions = [tx for tx in burn_transactions if tx.get('patoshis', 0) <= 100000]
        # Calculate the total credits consumed
        total_credits_consumed = sum(tx.get('patoshis', 0) for tx in burn_transactions) / CREDIT_USAGE_TO_TRACKING_AMOUNT_MULTIPLIER
        current_credit_balance = initial_credit_balance - total_credits_consumed
        number_of_confirmation_transactions = len(burn_transactions)
        logger.info(f"Calculation completed. Initial credit balance: {initial_credit_balance:,.1f}; "
                    f"Total credits consumed: {total_credits_consumed:,.1f} across {number_of_confirmation_transactions:,} transactions; "
                    f"Current credit balance: {current_credit_balance:,.1f}")
        return current_credit_balance, number_of_confirmation_transactions
    except Exception as e:
        logger.error(f"Error while determining current credit pack balance: {str(e)}")
        return initial_credit_balance, 0
        
async def update_inference_sn_reputation_score(supernode_pastelid: str, reputation_score: float) -> bool:
    try:
        # TODO: Implement the logic to update the inference SN reputation score
        # This could involve storing the reputation score in a database or broadcasting it to other supernodes
        # For now, let's assume the reputation score is updated successfully
        return True
    except Exception as e:
        logger.error(f"Error updating inference SN reputation score: {str(e)}")
        raise

async def get_inference_api_usage_request_for_audit(inference_request_id: str) -> db_code.InferenceAPIUsageRequest:
    async with db_code.Session() as db_session:
        query = await db_session.exec(
            select(db_code.InferenceAPIUsageRequest).where(db_code.InferenceAPIUsageRequest.inference_request_id == inference_request_id)
        )
        result = query.one_or_none()
        return result
        
async def get_inference_api_usage_response_for_audit(inference_request_or_response_id: str) -> db_code.InferenceAPIUsageResponse:
    async with db_code.Session() as db_session:
        query1 = await db_session.exec(
            select(db_code.InferenceAPIUsageResponse).where(db_code.InferenceAPIUsageResponse.inference_request_id == inference_request_or_response_id)
        )
        result1 = query1.one_or_none()
        query2 = await db_session.exec(
            select(db_code.InferenceAPIUsageResponse).where(db_code.InferenceAPIUsageResponse.inference_response_id == inference_request_or_response_id)
        )
        result2 = query2.one_or_none()        
        if result1 is not None:
            result = result1
        elif result2 is not None:
            result = result2
        return result

async def get_inference_api_usage_result_for_audit(inference_request_or_response_id: str) -> db_code.InferenceAPIOutputResult:
    async with db_code.Session() as db_session:
        query1 = await db_session.exec(
            select(db_code.InferenceAPIOutputResult).where(db_code.InferenceAPIOutputResult.inference_request_id == inference_request_or_response_id)
        )
        result1 = query1.one_or_none()
        query2 = await db_session.exec(
            select(db_code.InferenceAPIOutputResult).where(db_code.InferenceAPIOutputResult.inference_response_id == inference_request_or_response_id)
        )
        result2 = query2.one_or_none()        
        if result1 is not None:
            result = result1
        elif result2 is not None:
            result = result2        
        return result
    
# ________________________________________________________________________________________________________________________________

# Blockchain ticket related functions:

def check_if_transparent_psl_address_is_valid_func(pastel_address_string):
    if len(pastel_address_string) == 35 and (pastel_address_string[0:2] == 'Pt'):
        pastel_address_is_valid = 1
    else:
        pastel_address_is_valid = 0
    return pastel_address_is_valid

def check_if_transparent_lsp_address_is_valid_func(pastel_address_string):
    if len(pastel_address_string) == 35 and (pastel_address_string[0:2] == 'tP'):
        pastel_address_is_valid = 1
    else:
        pastel_address_is_valid = 0
    return pastel_address_is_valid

def check_if_transparent_address_is_valid_func(pastel_address_string):
    pastel_address_is_valid = 0
    if rpc_port == '9932':
        if len(pastel_address_string) == 35 and (pastel_address_string[0:2] == 'Pt'):
            pastel_address_is_valid = 1
    elif rpc_port == '19932':
        if len(pastel_address_string) == 35 and (pastel_address_string[0:2] == 'tP'):
            pastel_address_is_valid = 1
    elif rpc_port == '29932':
        if len(pastel_address_string) == 36 and (pastel_address_string[0:2] == '44'):
            pastel_address_is_valid = 1
    return pastel_address_is_valid             

async def get_df_json_from_tickets_list_rpc_response_func(rpc_response):
    tickets_df = pd.DataFrame.from_records([rpc_response[idx]['ticket'] for idx, x in enumerate(rpc_response)])
    tickets_df['txid'] = [rpc_response[idx]['txid'] for idx, x in enumerate(rpc_response)]
    tickets_df['height'] = [rpc_response[idx]['height'] for idx, x in enumerate(rpc_response)]
    tickets_df_json = tickets_df.to_json(orient='index')
    return tickets_df_json

async def import_address_func(address: str, label: str = "", rescan: bool = False) -> None:
    try:
        await rpc_connection.importaddress(address, label, rescan)
        logger.info(f"Imported address: {address}")
    except Exception as e:
        logger.error(f"Error importing address: {address}. Error: {e}")

async def check_if_address_is_already_imported_in_local_wallet(address_to_check):
    address_amounts_dict = await listaddressamounts(rpc_connection)
    # Convert the dictionary into a list of dictionaries, each representing a row
    data = [{'address': address, 'amount': amount} for address, amount in address_amounts_dict.items()]
    # Create the DataFrame from the list of dictionaries
    address_amounts_df = pd.DataFrame(data)        
    # Filter the DataFrame for the specified address
    address_amounts_df_filtered = address_amounts_df[address_amounts_df['address'] == address_to_check]
    if address_amounts_df_filtered.empty:
        return False
    return True

async def get_and_decode_raw_transaction(txid: str) -> dict:
    try:
        # Retrieve the raw transaction data
        raw_tx_data = await getrawtransaction(rpc_connection, txid)
        if not raw_tx_data:
            logger.error(f"Failed to retrieve raw transaction data for {txid}")
            return {}
        return raw_tx_data
    except Exception as e:
        logger.error(f"Error in get_and_decode_transaction for {txid}: {e}")
        return {}

async def get_transaction_details(txid: str, include_watchonly: bool = False) -> dict:
    try:
        # Call the 'gettransaction' RPC method with the provided txid and includeWatchonly flag
        transaction_details = await gettransaction(rpc_connection, txid, include_watchonly)
        # Log the retrieved transaction details
        logger.info(f"Retrieved transaction details for {txid}: {transaction_details}")
        return transaction_details
    except Exception as e:
        logger.error(f"Error retrieving transaction details for {txid}: {e}")
        return {}

#Misc helper functions:
class MyTimer():
    def __init__(self):
        self.start = time.time()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        end = time.time()
        runtime = end - self.start
        msg = '({time} seconds to complete)'
        logger.info(msg.format(time=round(runtime, 2)))

def compute_elapsed_time_in_minutes_between_two_datetimes_func(start_datetime, end_datetime):
    time_delta = (end_datetime - start_datetime)
    total_seconds_elapsed = time_delta.total_seconds()
    total_minutes_elapsed = total_seconds_elapsed / 60
    return total_minutes_elapsed

def compute_elapsed_time_in_minutes_since_start_datetime_func(start_datetime):
    end_datetime = datetime.utcnow()
    total_minutes_elapsed = compute_elapsed_time_in_minutes_between_two_datetimes_func(start_datetime, end_datetime)
    return total_minutes_elapsed

def check_if_ip_address_is_valid_func(ip_address_string):
    try:
        _ = ipaddress.ip_address(ip_address_string)
        ip_address_is_valid = 1
    except Exception as e:
        logger.error('Validation Error: ' + str(e))
        ip_address_is_valid = 0
    return ip_address_is_valid

def compare_datetimes(datetime_input1, datetime_input2):
    # Check if the inputs are datetime objects, otherwise parse them
    if not isinstance(datetime_input1, datetime):
        datetime_input1 = pd.to_datetime(datetime_input1)
    if not isinstance(datetime_input2, datetime):
        datetime_input2 = pd.to_datetime(datetime_input2)
    # Ensure both datetime objects are timezone-aware
    if datetime_input1.tzinfo is None:
        datetime_input1 = datetime_input1.replace(tzinfo=pytz.UTC)
    if datetime_input2.tzinfo is None:
        datetime_input2 = datetime_input2.replace(tzinfo=pytz.UTC)
    # Calculate the difference in seconds
    difference_in_seconds = abs((datetime_input2 - datetime_input1).total_seconds())
    # Check if the difference is within the acceptable range
    datetimes_are_close_enough_to_consider_them_matching = (
        difference_in_seconds <= MAXIMUM_LOCAL_UTC_TIMESTAMP_DIFFERENCE_IN_SECONDS
    )
    if not datetimes_are_close_enough_to_consider_them_matching:
        logger.warning(f"Timestamps are too far apart: {difference_in_seconds} seconds")
    return difference_in_seconds, datetimes_are_close_enough_to_consider_them_matching

def get_sha256_hash_of_input_data_func(input_data_or_string):
    if isinstance(input_data_or_string, str):
        input_data_or_string = input_data_or_string.encode('utf-8')
    sha256_hash_of_input_data = hashlib.sha3_256(input_data_or_string).hexdigest()
    return sha256_hash_of_input_data

def sort_dict_by_keys(input_dict):
    # Sort the main dictionary by keys and then sort each nested dictionary by keys
    sorted_dict = {key: dict(sorted(value.items())) for key, value in sorted(input_dict.items())}
    return json.dumps(sorted_dict, indent=4)  # Convert the dictionary to a JSON string for output

async def extract_response_fields_from_credit_pack_ticket_message_data_as_json_func(model_instance: SQLModel) -> str:
    response_fields = {}
    last_hash_field_name = None
    last_signature_field_names = []
    for field_name in model_instance.__fields__.keys():
        if field_name.startswith("sha3_256_hash_of"):
            last_hash_field_name = field_name
        elif "_signature_on_" in field_name:
            last_signature_field_names.append(field_name)
    if isinstance(model_instance, db_code.CreditPackPurchasePriceAgreementRequestResponse):
        fields_to_exclude = [last_hash_field_name, 'id'] + last_signature_field_names
    else:
        fields_to_exclude = [last_hash_field_name, last_signature_field_names[-1], 'id']
    for field_name, field_value in model_instance.__dict__.items():
        if field_name in fields_to_exclude or '_sa_instance_state' in field_name:
            continue
        if field_value is not None:
            if isinstance(field_value, (datetime, date)):
                response_fields[field_name] = field_value.isoformat()
            elif isinstance(field_value, list):
                response_fields[field_name] = json.dumps(field_value, ensure_ascii=False, sort_keys=True)
            elif isinstance(field_value, dict):
                response_fields[field_name] = sort_dict_by_keys(field_value)
            elif isinstance(field_value, decimal.Decimal):
                response_fields[field_name] = str(field_value)
            elif isinstance(field_value, bool):
                response_fields[field_name] = int(field_value)
            else:
                response_fields[field_name] = field_value
    sorted_response_fields = dict(sorted(response_fields.items()))
    return json.dumps(sorted_response_fields, ensure_ascii=False, sort_keys=True)

async def compute_sha3_256_hash_of_sqlmodel_response_fields(model_instance: SQLModel) -> str:
    response_fields_json = await extract_response_fields_from_credit_pack_ticket_message_data_as_json_func(model_instance)
    sha256_hash_of_response_fields = get_sha256_hash_of_input_data_func(response_fields_json)
    return sha256_hash_of_response_fields

async def validate_credit_pack_blockchain_ticket_data_field_hashes(model_instance: SQLModel):
    validation_errors = []
    # model_instance.credit_purchase_request_response_message_version_string = str(model_instance.credit_purchase_request_response_message_version_string)
    response_fields_json = await extract_response_fields_from_credit_pack_ticket_message_data_as_json_func(model_instance)
    expected_hash = get_sha256_hash_of_input_data_func(response_fields_json)
    last_hash_field_name = None
    for field_name in model_instance.__fields__:
        if field_name.startswith("sha3_256_hash_of") and field_name.endswith("_fields"):
            last_hash_field_name = field_name
    if last_hash_field_name:
        actual_hash = getattr(model_instance, last_hash_field_name)
        if actual_hash != expected_hash:
            # print('Skipping hash validation check for now...') # TODO: Fix this!
            validation_errors.append(f"SHA3-256 hash in field {last_hash_field_name} does not match the computed hash of the response fields")    
    return validation_errors

async def validate_credit_pack_ticket_message_data_func(model_instance: SQLModel):
    validation_errors = []
    # Validate timestamp fields
    for field_name, field_value in model_instance.__dict__.items():
        if field_name.endswith("_timestamp_utc_iso_string"):
            try:
                pd.to_datetime(field_value)
            except ValueError:
                validation_errors.append(f"Invalid timestamp format for field {field_name}")
            # Check if the timestamp is within an acceptable range of the current time
            current_timestamp = pd.to_datetime(datetime.utcnow().replace(tzinfo=pytz.UTC))
            timestamp_diff, timestamps_match = compare_datetimes(field_value, current_timestamp)
            if not timestamps_match:
                validation_errors.append(f"Timestamp in field {field_name} is too far from the current time")
    # Validate pastel block height fields
    best_block_hash, best_block_merkle_root, best_block_height = await get_best_block_hash_and_merkle_root_func()
    for field_name, field_value in model_instance.__dict__.items():
        if field_name.endswith("_pastel_block_height"):
            if abs(field_value - best_block_height) > MAXIMUM_LOCAL_PASTEL_BLOCK_HEIGHT_DIFFERENCE_IN_BLOCKS:
                validation_errors.append(f"Pastel block height in field {field_name} does not match the current block height; difference is {abs(field_value - best_block_height)} blocks (local: {field_value}, remote: {best_block_height})")
    # Validate hash fields
    expected_hash = await compute_sha3_256_hash_of_sqlmodel_response_fields(model_instance)
    hash_field_name = None
    last_hash_field_name = None
    for field_name in model_instance.__fields__:
        if field_name.startswith("sha3_256_hash_of") and field_name.endswith("_fields"):
            last_hash_field_name = field_name
    if last_hash_field_name:
        actual_hash = getattr(model_instance, last_hash_field_name)
        if actual_hash != expected_hash:
            validation_errors.append(f"SHA3-256 hash in field {last_hash_field_name} does not match the computed hash of the response fields")
    # Validate pastelid signature fields
    if isinstance(model_instance, db_code.CreditPackPurchasePriceAgreementRequestResponse):
        signature_field_names = []
        hash_field_name = None
        for field_name in model_instance.__fields__:
            if "_pastelid" in field_name:
                first_pastelid = field_name
                break
        for field_name in model_instance.__fields__:
            if "_signature_on_" in field_name:
                signature_field_names.append(field_name)
            elif "sha3_256_hash_of_" in field_name and field_name.endswith("_fields"):
                hash_field_name = field_name
        if signature_field_names and hash_field_name:
            if hasattr(model_instance, first_pastelid):
                pastelid = getattr(model_instance, first_pastelid)
                for signature_field_name in signature_field_names:
                    if signature_field_name == "responding_supernode_signature_on_price_agreement_request_response_hash":
                        message_to_verify = getattr(model_instance, "sha3_256_hash_of_price_agreement_request_response_fields")
                    elif signature_field_name == "responding_supernode_signature_on_credit_pack_purchase_request_fields_json_b64":
                        message_to_verify = getattr(model_instance, "credit_pack_purchase_request_fields_json_b64")
                    else:
                        continue
                    signature = getattr(model_instance, signature_field_name)
                    verification_result = await verify_message_with_pastelid_func(pastelid, message_to_verify, signature)
                    if verification_result != 'OK':
                        validation_errors.append(f"Pastelid signature in field {signature_field_name} failed verification")
            else:
                validation_errors.append(f"Corresponding pastelid field {first_pastelid} not found for signature fields {signature_field_names}")
    else:
        # Validate pastelid signature fields
        last_signature_field_name = None
        last_hash_field_name = None
        for field_name in model_instance.__fields__:
            if "_pastelid" in field_name:
                first_pastelid = field_name
                break
        for field_name in model_instance.__fields__:
            if "_signature_on_" in field_name:
                last_signature_field_name = field_name
            elif "sha3_256_hash_of_" in field_name and field_name.endswith("_fields"):
                last_hash_field_name = field_name
        if last_signature_field_name and last_hash_field_name:
            signature_field_name = last_signature_field_name
            hash_field_name = last_hash_field_name
            if first_pastelid == last_signature_field_name:
                first_pastelid = "NA"
            if hasattr(model_instance, first_pastelid) or first_pastelid == "NA":
                if first_pastelid == "NA":
                    pastelid_and_signature_combined_field_name = last_signature_field_name
                    pastelid_and_signature_combined_field_json = getattr(model_instance, pastelid_and_signature_combined_field_name)
                    pastelid_and_signature_combined_field_dict = json.loads(pastelid_and_signature_combined_field_json)
                    pastelid_and_signature_combined_field_dict_keys = pastelid_and_signature_combined_field_dict.keys()
                    for current_key in pastelid_and_signature_combined_field_dict_keys:
                        if "pastelid" in current_key:
                            pastelid = pastelid_and_signature_combined_field_dict[current_key]
                        if "signature" in current_key:
                            signature = pastelid_and_signature_combined_field_dict[current_key]
                    message_to_verify = getattr(model_instance, hash_field_name)
                else:
                    pastelid = getattr(model_instance, first_pastelid)
                    message_to_verify = getattr(model_instance, hash_field_name)
                    signature = getattr(model_instance, signature_field_name)
                verification_result = await verify_message_with_pastelid_func(pastelid, message_to_verify, signature)
                if verification_result != 'OK':
                    validation_errors.append(f"Pastelid signature in field {signature_field_name} failed verification")
            else:
                validation_errors.append(f"Corresponding pastelid field {first_pastelid} not found for signature field {signature_field_name}")
        return validation_errors

async def validate_inference_request_message_data_func(model_instance: SQLModel):
    validation_errors = await validate_credit_pack_ticket_message_data_func(model_instance)
    return validation_errors

def get_external_ip_func() -> str:
    urls = [
        "https://ipinfo.io/ip",
        "https://api.ipify.org",
        "https://ifconfig.me"
    ]
    for url in urls:
        try:
            response = httpx.get(url)
            response.raise_for_status()
            return response.text.strip()
        except Exception as e:
            logger.error(f"Failed to get external IP from {url}: {e}")
    raise RuntimeError("Unable to get external IP address from all fallback options.")

def safe_highlight_func(text, pattern, replacement):
    try:
        return re.sub(pattern, replacement, text)
    except Exception as e:
        logger.warning(f"Failed to apply highlight rule: {e}")
        return text

def highlight_rules_func(text):
    rules = [
        (re.compile(r"\b(success\w*)\b", re.IGNORECASE), '#COLOR1_OPEN#', '#COLOR1_CLOSE#'),
        (re.compile(r"\b(error|fail\w*)\b", re.IGNORECASE), '#COLOR2_OPEN#', '#COLOR2_CLOSE#'),
        (re.compile(r"\b(pending)\b", re.IGNORECASE), '#COLOR3_OPEN#', '#COLOR3_CLOSE#'),
        (re.compile(r"\b(response)\b", re.IGNORECASE), '#COLOR4_OPEN#', '#COLOR4_CLOSE#'),
        (re.compile(r'\"(.*?)\"', re.IGNORECASE), '#COLOR5_OPEN#', '#COLOR5_CLOSE#'),
        (re.compile(r"\'(.*?)\'", re.IGNORECASE), "#COLOR6_OPEN#", '#COLOR6_CLOSE#'),
        (re.compile(r"\`(.*?)\`", re.IGNORECASE), '#COLOR7_OPEN#', '#COLOR7_CLOSE#'),
        (re.compile(r"\b(https?://\S+)\b", re.IGNORECASE), '#COLOR8_OPEN#', '#COLOR8_CLOSE#'),
        (re.compile(r"\b(\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2},\d{3})\b", re.IGNORECASE), '#COLOR9_OPEN#', '#COLOR9_CLOSE#'),
        (re.compile(r"\b(_{100,})\b", re.IGNORECASE), '#COLOR10_OPEN#', '#COLOR10_CLOSE#'),
        (re.compile(r"\b(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d+)\b", re.IGNORECASE), '#COLOR11_OPEN#', '#COLOR11_CLOSE#'),
        (re.compile(r"\b([a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12})\b", re.IGNORECASE), '#COLOR12_OPEN#', '#COLOR12_CLOSE#'),
        (re.compile(r"\b([a-f0-9]{64})\b", re.IGNORECASE), '#COLOR13_OPEN#', '#COLOR13_CLOSE#')                                
    ]
    for pattern, replacement_open, replacement_close in rules:
        text = pattern.sub(f"{replacement_open}\\1{replacement_close}", text)
    text = html.escape(text)
    text = text.replace('#COLOR1_OPEN#', '<span style="color: #baffc9;">').replace('#COLOR1_CLOSE#', '</span>')
    text = text.replace('#COLOR2_OPEN#', '<span style="color: #ffb3ba;">').replace('#COLOR2_CLOSE#', '</span>')
    text = text.replace('#COLOR3_OPEN#', '<span style="color: #ffdfba;">').replace('#COLOR3_CLOSE#', '</span>')
    text = text.replace('#COLOR4_OPEN#', '<span style="color: #ffffba;">').replace('#COLOR4_CLOSE#', '</span>')
    text = text.replace('#COLOR5_OPEN#', '<span style="color: #bdc7e7;">').replace('#COLOR5_CLOSE#', '</span>')
    text = text.replace('#COLOR6_OPEN#', "<span style='color: #d5db9c;'>").replace('#COLOR6_CLOSE#', '</span>')
    text = text.replace('#COLOR7_OPEN#', '<span style="color: #a8d8ea;">').replace('#COLOR7_CLOSE#', '</span>')
    text = text.replace('#COLOR8_OPEN#', '<span style="color: #e2a8a8;">').replace('#COLOR8_CLOSE#', '</span>')
    text = text.replace('#COLOR9_OPEN#', '<span style="color: #ece2d0;">').replace('#COLOR9_CLOSE#', '</span>')
    text = text.replace('#COLOR10_OPEN#', '<span style="color: #d6e0f0;">').replace('#COLOR10_CLOSE#', '</span>')
    text = text.replace('#COLOR11_OPEN#', '<span style="color: #f2d2e2;">').replace('#COLOR11_CLOSE#', '</span>')
    text = text.replace('#COLOR12_OPEN#', '<span style="color: #d5f2ea;">').replace('#COLOR12_CLOSE#', '</span>')
    text = text.replace('#COLOR13_OPEN#', '<span style="color: #f2ebd3;">').replace('#COLOR13_CLOSE#', '</span>')
    return text

async def ensure_burn_address_imported_as_watch_address_in_local_wallet():
    burn_address_already_imported = await check_if_address_is_already_imported_in_local_wallet(burn_address)
    if not burn_address_already_imported:
        logger.info(f"Burn address is NOT yet imported! Now attempting to import burn address {burn_address} as a watch address in the local wallet...")
        await import_address_func(burn_address, "burn_address", True)
    else:
        logger.info(f"Burn address {burn_address} is already imported as a watch address in the local wallet!")
    

#_______________________________________________________________


rpc_host, rpc_port, rpc_user, rpc_password, other_flags = get_local_rpc_settings_func()
network, burn_address = get_network_info(rpc_port)
masternode_collateral_amount = required_collateral(network)
rpc_connection = AsyncAuthServiceProxy(f"http://{rpc_user}:{rpc_password}@{rpc_host}:{rpc_port}")

if rpc_port == '9932':
    burn_address = 'PtpasteLBurnAddressXXXXXXXXXXbJ5ndd'
elif rpc_port == '19932':
    burn_address = 'tPpasteLBurnAddressXXXXXXXXXXX3wy7u'
elif rpc_port == '29932':
    burn_address = '44oUgmZSL997veFEQDq569wv5tsT6KXf9QY7' # https://blockchain-devel.slack.com/archives/C03Q2MCQG9K/p1705896449986459

encryption_key = generate_or_load_encryption_key_sync()  # Generate or load the encryption key synchronously    
decrypt_sensitive_fields()
MY_PASTELID = asyncio.run(get_my_local_pastelid_func())
logger.info(f"Using local PastelID: {MY_PASTELID}")
if 'genpassphrase' in other_flags.keys():
    LOCAL_PASTEL_ID_PASSPHRASE = other_flags['genpassphrase']

use_encrypt_new_secrets = 0
if use_encrypt_new_secrets:
    encrypted_openai_key = encrypt_sensitive_data("abc123", encryption_key)
    print(f"Encrypted OpenAI key: {encrypted_openai_key}")
    
    encrypted_groq_key = encrypt_sensitive_data("abc123", encryption_key)
    print(f"Encrypted groq key: {encrypted_groq_key}")

    encrypted_mistral_key = encrypt_sensitive_data("abc123", encryption_key)
    print(f"Encrypted mistral key: {encrypted_mistral_key}")
    
    encrypted_stability_key = encrypt_sensitive_data("abc123", encryption_key)
    print(f"Encrypted stability key: {encrypted_stability_key}")    
    
    encrypted_openrouter_key = encrypt_sensitive_data("abc123", encryption_key)
    print(f"Encrypted openrouter key: {encrypted_openrouter_key}")
    
use_test_market_price_data = 0
if use_test_market_price_data:
    current_psl_price = asyncio.run(fetch_current_psl_market_price())
    
use_get_inference_model_menu_on_start = 0
if use_get_inference_model_menu_on_start:
    random_async_wait_duration_in_seconds = round(random.random()*10.0, 3)
    logger.info(f"Checking API keys and getting inference model menu (but first waiting for a random period of {random_async_wait_duration_in_seconds} seconds to not overwhelm the APIs)...")
    asyncio.run(asyncio.sleep(random_async_wait_duration_in_seconds))
    use_verbose=1
    asyncio.run(get_inference_model_menu(use_verbose))


---
./model_menu.json
---
{
  "models": [
    {
      "model_name": "swiss_army_llama-Llama-3.1-8B-Lexi-Uncensored_V2_Q5",
      "model_url": "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/blob/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf",
      "description": "The Llama-3.1-8B-Lexi-Uncensored V2, based on the powerful Meta Llama-3.1-8B model, is designed for uncensored text generation with a focus on creative and innovative content.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Grammar File String: The grammar file used to restrict text generation [Optional] (Default is to not use any grammar file. Examples: `json`, `list`)"
        },
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "text_completion": {
          "input_tokens": 1.6,
          "output_tokens": 1.2,
          "compute_cost": 0.9,
          "memory_cost": 0.8
        },
        "embedding": {
          "input_tokens": 0.6,
          "compute_cost": 0.7,
          "memory_cost": 0.5
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.15,
          "total_sentences": 0.08,
          "query_string_included": 0.35,
          "compute_cost": 1.0,
          "memory_cost": 0.7
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.03,
          "query_string_included": 0.35,
          "compute_cost": 1.1,
          "memory_cost": 0.9
        }
      }
    },
    {
      "model_name": "swiss_army_llama-Hermes-3-Llama-3.1-8B.Q4_K_M",
      "model_url": "https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/blob/main/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf",
      "description": "Hermes 2 is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house. This version excels at Function Calling, JSON Structured Outputs, and has improved on several other metrics, scoring 90% on function calling evaluation and 84% on structured JSON Output evaluation.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Grammar File String: The grammar file used to restrict text generation [Optional] (Default is to not use any grammar file. Examples: `json`, `list`)"
        },
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "text_completion": {
          "input_tokens": 1.8,
          "output_tokens": 1.4,
          "compute_cost": 1.0,
          "memory_cost": 0.9
        },
        "embedding": {
          "input_tokens": 0.7,
          "compute_cost": 0.8,
          "memory_cost": 0.5
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.2,
          "total_sentences": 0.1,
          "query_string_included": 0.4,
          "compute_cost": 1.2,
          "memory_cost": 0.8
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.5,
          "query_string_included": 1.5,
          "compute_cost": 1.2,
          "memory_cost": 1.0
        }
      }
    },
    {
      "model_name": "swiss_army_llama-Meta-Llama-3.1-8B-Instruct-Q5_K_M",
      "model_url": "https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
      "description": "The Meta-Llama-3.1-8B-Instruct model is designed for instruction-following tasks with optimized quantization for efficient computation.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Grammar File String: The grammar file used to restrict text generation [Optional] (Default is to not use any grammar file. Examples: `json`, `list`)"
        },
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "text_completion": {
          "input_tokens": 1.5,
          "output_tokens": 1.1,
          "compute_cost": 0.8,
          "memory_cost": 0.7
        },
        "embedding": {
          "input_tokens": 0.5,
          "compute_cost": 0.6,
          "memory_cost": 0.4
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.1,
          "total_sentences": 0.05,
          "query_string_included": 0.3,
          "compute_cost": 0.9,
          "memory_cost": 0.6
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.02,
          "query_string_included": 0.3,
          "compute_cost": 1.0,
          "memory_cost": 0.8
        }
      }
    },
    {
      "model_name": "swiss_army_llama-llava-llama-3-8b-v1_1-int4",
      "model_url": "https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf/resolve/main/llava-llama-3-8b-v1_1-int4.gguf",
      "description": "The LLaVA-LLaMA-3-8B-v1.1 model is a powerful language model developed by XTuner; it allows you to ask questions about images and get answers in a conversational manner.",
      "supported_inference_type_strings": ["ask_question_about_an_image"],
      "input_fields": [
        {
          "name": "image_data",
          "file_type": "base64",
          "optional": false
        },
        {
          "name": "question",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "ask_question_about_an_image": {
          "input_tokens": 1.5,
          "output_tokens": 1.1,
          "compute_cost": 1.2,
          "memory_cost": 1.0
        }
      }
    },
    {
      "model_name": "swiss_army_llama-bge-m3-q8_0",
      "model_url": "https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf",
      "description": "BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It performs dense retrieval, multi-vector retrieval, and sparse retrieval, providing a unified model foundation for real-world IR applications.",
      "supported_inference_type_strings": [
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "embeddings",
          "file_type": "json",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "embedding": {
          "input_tokens": 0.4,
          "compute_cost": 0.5,
          "memory_cost": 0.3
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.05,
          "total_sentences": 0.03,
          "query_string_included": 0.2,
          "compute_cost": 0.6,
          "memory_cost": 0.4
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.01,
          "query_string_included": 0.2,
          "compute_cost": 0.6,
          "memory_cost": 0.5
        }
      }
    },
    {
      "model_name": "swiss_army_llama-nomic-embed-text-v1.5.Q6_K",
      "model_url": "https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF/resolve/main/nomic-embed-text-v1.5.Q6_K.gguf",
      "description": "Nomic Embed Text v1.5 is a powerful language model developed by Nomic AI. It provides high-quality embeddings for semantic search and clustering tasks.",
      "supported_inference_type_strings": [
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "embeddings",
          "file_type": "json",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "embedding": {
          "input_tokens": 0.2,
          "compute_cost": 0.2,
          "memory_cost": 0.1
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.05,
          "total_sentences": 0.02,
          "query_string_included": 0.2,
          "compute_cost": 0.4,
          "memory_cost": 0.3
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.01,
          "query_string_included": 0.2,
          "compute_cost": 0.5,
          "memory_cost": 0.4
        }
      }
    },
    {
      "model_name": "openai-gpt-4o",
      "model_url": "",
      "description": "GPT-4o is OpenAI's most advanced multimodal model that is faster and cheaper than GPT-4 Turbo with stronger vision capabilities. The model has 128K context and an October 2023 knowledge cutoff.",
      "supported_inference_type_strings": ["text_completion"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": true
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openai-gpt-4o-vision",
      "model_url": "",
      "description": "GPT-4o is OpenAI's most advanced multimodal model that\u2019s faster and cheaper than GPT-4 Turbo with stronger vision capabilities. The model has 128K context and an October 2023 knowledge cutoff.",
      "supported_inference_type_strings": ["ask_question_about_an_image"],
      "input_fields": [
        {
          "name": "image_data",
          "file_type": "base64",
          "optional": false
        },
        {
          "name": "question",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openai-gpt-4-turbo",
      "model_url": "",
      "description": "GPT-4 Turbo is offered at 128K context with an April 2023 knowledge cutoff and basic support for vision.",
      "supported_inference_type_strings": ["text_completion"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": true
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openai-text-embedding-ada-002",
      "model_url": "",
      "description": "OpenAI's text embedding model. Provides high-quality embeddings for semantic search and clustering tasks.",
      "supported_inference_type_strings": ["embedding"],
      "input_fields": [
        {
          "name": "input_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "embedding",
          "file_type": "embedding",
          "optional": false
        }
      ],
      "model_parameters": [],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "claude3-haiku",
      "model_url": "",
      "description": "The Claude3 Haiku model is a powerful language model developed by Anthropic. It offers advanced natural language processing capabilities and can be accessed through the Claude API.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "inference_types_parameter_applies_to": "['text_completion']",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "claude3-opus",
      "model_url": "",
      "description": "The Claude3 Opus model is a powerful language model developed by Anthropic. It offers advanced natural language processing capabilities and can be accessed through the Claude API.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "claude3.5-sonnet",
      "model_url": "",
      "description": "The Claude3.5 Sonnet model is a powerful language model developed by Anthropic. It offers advanced natural language processing capabilities and can be accessed through the Claude API.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama-3.1-70b-versatile",
      "model_url": "",
      "description": "LLaMA3.1 70b versatile is a powerful language model developed by Meta with a context window of 131,072 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama-3.1-8b-instant",
      "model_url": "",
      "description": "LLaMA3.1 8b instant is a powerful language model developed by Meta with a context window of 131,072 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama3-70b-8192",
      "model_url": "",
      "description": "LLaMA3 70b is a powerful language model developed by Meta with a context window of 8,192 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama3-8b-8192",
      "model_url": "",
      "description": "LLaMA3 8b is a powerful but compact language model developed by Meta with a context window of 8,192 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-mixtral-8x7b-32768",
      "model_url": "",
      "description": "Mixtral 8x7b is a powerful language model developed by Mistral with a context window of 32,768 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-gemma2-9b-it",
      "model_url": "",
      "description": "Gemma 2 9b is an LLM developed by Google with a context window of 8,192 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-mistral-small-latest",
      "model_url": "",
      "description": "Mistral small is a powerful language model developed by Mistral AI.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-mistral-medium-latest",
      "model_url": "",
      "description": "Mistral medium is a powerful language model developed by Mistral AI.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-mistral-large-latest",
      "model_url": "",
      "description": "Mistral large is a powerful language model developed by Mistral AI.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-core",
      "model_url": "",
      "description": "Stable Image Core is the best quality achievable at high speed for text-to-image generation.",
      "supported_inference_type_strings": [
        "text_to_image"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "aspect_ratio",
          "type": "string",
          "default": "1:1",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The aspect ratio of the generated image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Random seed to use for generation [Optional]"
        },
        {
          "name": "style_preset",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Style Preset: Guides the image model towards a particular style (e.g., '3d-model', 'analog-film', 'anime', etc.) [Optional]"
        },
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The format of the generated image (e.g., 'png', 'jpeg', 'webp') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-sd3-large",
      "model_url": "",
      "description": "Stable Diffusion 3 Large model for high-quality image generation.",
      "supported_inference_type_strings": [
        "text_to_image"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "aspect_ratio",
          "type": "string",
          "default": "1:1",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The aspect ratio of the generated image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Random seed to use for generation [Optional]"
        },
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The format of the generated image (e.g., 'png', 'jpeg') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-conservative-upscale",
      "model_url": "",
      "description": "Conservative upscaling model that preserves image details.",
      "supported_inference_type_strings": [
        "conservative_upscale"
      ],
      "input_fields": [
        {
          "name": "image",
          "file_type": "image",
          "optional": false
        },
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "upscaled_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["conservative_upscale"],
          "description": "The format of the upscaled image (e.g., 'png', 'jpeg', 'webp') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["conservative_upscale"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["conservative_upscale"],
          "description": "Random seed to use for upscaling [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-creative-upscale",
      "model_url": "",
      "description": "Creative upscaling model that can enhance and add details to images.",
      "supported_inference_type_strings": [
        "creative_upscale"
      ],
      "input_fields": [
        {
          "name": "image",
          "file_type": "image",
          "optional": false
        },
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "upscaled_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["creative_upscale"],
          "description": "The format of the upscaled image (e.g., 'png', 'jpeg', 'webp') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["creative_upscale"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["creative_upscale"],
          "description": "Random seed to use for upscaling [Optional]"
        },
        {
          "name": "creativity",
          "type": "float",
          "default": 0.3,
          "min": 0,
          "max": 0.35,
          "inference_types_parameter_applies_to": ["creative_upscale"],
          "description": "Controls the creativity level of the upscaling process [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-sd3-medium",
      "model_url": "",
      "description": "Stable Diffusion 3 Medium model for balanced performance and quality in image generation.",
      "supported_inference_type_strings": [
        "text_to_image"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "aspect_ratio",
          "type": "string",
          "default": "1:1",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The aspect ratio of the generated image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Random seed to use for generation [Optional]"
        },
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The format of the generated image (e.g., 'png', 'jpeg') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-sd3-large-turbo",
      "model_url": "",
      "description": "Stable Diffusion 3 Large Turbo model for faster inference with high-quality results.",
      "supported_inference_type_strings": [
        "text_to_image"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "aspect_ratio",
          "type": "string",
          "default": "1:1",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The aspect ratio of the generated image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Random seed to use for generation [Optional]"
        },
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The format of the generated image (e.g., 'png', 'jpeg') [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/auto",
      "model_url": "",
      "description": "OpenRouter's auto-routing model that automatically selects the best available model for the given task.",
      "supported_inference_type_strings": ["text_completion"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    }
  ]
}


---
./pyproject.toml
---
[tool.ruff]
ignore = ["E501"]

---
./inference_server_code_as_single_file.txt
---
./setup_swiss_army_llama.py
---
import os
import subprocess
import httpx
import re
from logger_config import logger

def get_external_ip_func():
    providers = [
        "https://ipinfo.io/ip",
        "https://api.ipify.org",
        "https://checkip.amazonaws.com",
        "https://icanhazip.com"
    ]
    for provider in providers:
        try:
            response = httpx.get(provider)
            response.raise_for_status()
            return response.text.strip()
        except httpx.RequestError as e:
            logger.warning("Failed to retrieve external IP address from {}: {}".format(provider, e))
    logger.warning("Failed to retrieve external IP address from all providers.")
    return "Unknown"

def run_command(command, env=None, capture_output=False, check=False, timeout=None):
    shell = '/bin/zsh' if os.path.exists('/bin/zsh') else '/bin/bash'
    full_env = {**os.environ, **env} if env else os.environ.copy()
    command = ' '.join(command) if isinstance(command, list) else command
    try:
        result = subprocess.run(command, shell=True, env=full_env, capture_output=capture_output, text=True, executable=shell, check=check, timeout=timeout)
        if capture_output:
            if result.stdout:
                logger.info(result.stdout)
            if result.stderr:
                logger.error(result.stderr)
        return result
    except subprocess.TimeoutExpired:
        logger.warning("Command '{}' timed out after {} seconds".format(command, timeout))
    except subprocess.CalledProcessError as e:
        logger.error("Command '{}' failed with exit code {}".format(command, e.returncode))
        if capture_output:
            logger.error(e.output)
        raise

def is_port_available(port):
    result = run_command(["lsof", "-i", ":{}".format(port)], capture_output=True)
    return result.returncode != 0

def is_swiss_army_llama_responding(external_ip, port, security_token):
    try:
        url = "http://{}:{}/get_list_of_available_model_names/".format(external_ip, port)
        params = {'token': security_token}
        response = httpx.get(url, params=params)
        return response.status_code == 200
    except Exception as e:
        print("Error: {}".format(e))
        return False

def update_security_token(file_path, token):
    with open(file_path, 'r+') as file:
        content = file.read()
        file.seek(0)
        file.truncate()
        content = re.sub(r'SECURITY_TOKEN\s*=\s*"[^"]*"', 'SECURITY_TOKEN = "{}"'.format(token), content)
        file.write(content)

def is_pyenv_installed():
    result = run_command(["pyenv --version"], capture_output=True)
    return result.returncode == 0

def is_python_3_12_installed():
    result = run_command(["pyenv versions"], capture_output=True)
    return "3.12" in result.stdout

def is_rust_installed():
    try:
        result = run_command(["rustc", "--version"], capture_output=True)
        return result.returncode == 0
    except FileNotFoundError:
        return False

def setup_virtual_environment(swiss_army_llama_path):
    venv_path = os.path.join(swiss_army_llama_path, 'venv')
    if not os.path.exists(venv_path):
        os.makedirs(venv_path)
        run_command(['python3 -m venv {}'.format(venv_path)], check=True)
    pip_executable = os.path.join(venv_path, 'bin', 'pip')
    run_command(['{} install --upgrade pip'.format(pip_executable)], check=True)
    run_command(['{} install wheel'.format(pip_executable)], check=True)
    run_command(['{} install -r {}/requirements.txt'.format(pip_executable, swiss_army_llama_path)], check=True)
    return os.path.join(venv_path, 'bin', 'python')

def set_timezone_utc():
    os.environ['TZ'] = 'UTC'
    shell_profile_path = os.path.expanduser('~/.zshrc') if os.path.exists(os.path.expanduser('~/.zshrc')) else os.path.expanduser('~/.bashrc')
    if 'export TZ=UTC' not in open(shell_profile_path, 'r').read():
        run_command(['echo "export TZ=UTC" >> {}'.format(shell_profile_path)])

def check_systemd_service_exists(service_name):
    result = run_command("systemctl is-enabled {}".format(service_name), capture_output=True)
    return result.returncode == 0 and 'enabled' in result.stdout

def create_systemd_service(service_name, user, working_directory, exec_start):
    service_content = """[Unit]
Description=Swiss Army Llama service
After=network.target

[Service]
Type=simple
User={}
WorkingDirectory={}
ExecStart={}
Restart=always

[Install]
WantedBy=multi-user.target
""".format(user, working_directory, exec_start)
    service_path = "/etc/systemd/system/{}.service".format(service_name)
    temp_service_path = "/tmp/{}.service".format(service_name)
    with open(temp_service_path, 'w') as file:
        file.write(service_content)
    run_command("sudo mv {} {}".format(temp_service_path, service_path), check=True)
    logger.info("Systemd service file created at {}".format(service_path))
    run_command("sudo systemctl daemon-reload", check=True)
    run_command("sudo systemctl enable {}".format(service_name), check=True)
    run_command("sudo systemctl start {}".format(service_name), check=True)
    status_output = run_command("sudo systemctl status {}".format(service_name), capture_output=True, timeout=5)
    logger.info("Status of {} service:\n{}".format(service_name, status_output.stdout))

def ensure_pyenv_setup():
    if not is_pyenv_installed():
        logger.info("Installing pyenv...")
        run_command(["sudo apt-get update && sudo apt-get install -y build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python3-openssl git redis redis-server redis-tools"])
        run_command(["curl https://pyenv.run | bash"])
    if not is_python_3_12_installed():
        logger.info("Installing Python 3.12 using pyenv...")
        run_command(["pyenv install 3.12"])
        run_command(["pyenv global 3.12"])

def configure_shell_for_pyenv():
    shell_rc_path = os.path.expanduser("~/.zshrc") if os.path.exists(os.path.expanduser("~/.zshrc")) else os.path.expanduser("~/.bashrc")
    pyenv_init_str = 'export PYENV_ROOT="$HOME/.pyenv"\nexport PATH="$PYENV_ROOT/bin:$PATH"\neval "$(pyenv init --path)"\n'
    if pyenv_init_str not in open(shell_rc_path).read():
        with open(shell_rc_path, "a") as shell_rc:
            shell_rc.write(pyenv_init_str)
    os.environ["PYENV_ROOT"] = os.path.expanduser("~/.pyenv")
    os.environ["PATH"] = "{}:{}/bin".format(os.environ['PYENV_ROOT'], os.environ.get('PATH', ''))

def has_repo_been_updated(repo_path):
    try:
        run_command("git -C {} fetch".format(repo_path), check=True)
        local_commit = run_command("git -C {} rev-parse HEAD".format(repo_path), capture_output=True).stdout.strip()
        remote_commit = run_command("git -C {} rev-parse origin/main".format(repo_path), capture_output=True).stdout.strip()
        return local_commit != remote_commit
    except Exception as e:
        logger.error("Error checking repository updates: {}".format(e))
        return False

def setup_swiss_army_llama(security_token):
    set_timezone_utc()
    swiss_army_llama_path = os.path.expanduser("~/swiss_army_llama")
    swiss_army_llama_script = os.path.join(swiss_army_llama_path, "swiss_army_llama.py")
    if not os.path.exists(swiss_army_llama_path):
        logger.info("Cloning the Swiss Army Llama repository.")
        run_command("git clone https://github.com/Dicklesworthstone/swiss_army_llama {}".format(swiss_army_llama_path), check=True)
    else:
        logger.info("Swiss Army Llama repository already exists.")
    logger.info("Checking for updates to the Swiss Army Llama repository.")
    run_command("git -C {} fetch".format(swiss_army_llama_path), check=True)
    local_commit = run_command("git -C {} rev-parse HEAD".format(swiss_army_llama_path), capture_output=True).stdout.strip()
    remote_commit = run_command("git -C {} rev-parse origin/main".format(swiss_army_llama_path), capture_output=True).stdout.strip()
    if local_commit != remote_commit:
        logger.info("Updates found. Stashing local changes and pulling new changes from the repository.")
        run_command("git -C {} stash".format(swiss_army_llama_path), check=True)
        run_command("git -C {} pull".format(swiss_army_llama_path), check=True)
        update_security_token(swiss_army_llama_script, security_token)
    configure_shell_for_pyenv()
    if not is_pyenv_installed():
        ensure_pyenv_setup()
    if not is_python_3_12_installed():
        logger.info("Python 3.12 is not installed. Installing Python 3.12 using pyenv.")
        run_command("pyenv install 3.12", check=True)
        run_command("pyenv global 3.12", check=True)
    venv_path = os.path.join(swiss_army_llama_path, 'venv')
    if not os.path.exists(venv_path):
        python_executable = setup_virtual_environment(swiss_army_llama_path)
    else:
        python_executable = os.path.join(venv_path, 'bin', 'python')
        run_command(['pip install --upgrade -r {}/requirements.txt'.format(swiss_army_llama_path)], check=True) 
    if not is_rust_installed():
        logger.info("Rust is not installed. Installing Rust.")
        run_command("curl https://sh.rustup.rs -sSf | sh -s -- -y", check=True)
        os.environ.update({
            'PATH': "{}/.cargo/bin:{}".format(os.environ.get('HOME'), os.environ.get('PATH'))
        })
        run_command("rustup default nightly && rustup update nightly", check=True)
    if not check_systemd_service_exists("swiss_army_llama"):
        create_systemd_service("swiss_army_llama", os.getlogin(), swiss_army_llama_path, "{} {}".format(python_executable, swiss_army_llama_script))
    else:
        logger.info("Swiss Army Llama systemd service already exists; skipping installation, reloading systemd, and starting/enabling the service.")
        run_command("sudo systemctl daemon-reload", check=True)
        run_command("sudo systemctl enable swiss_army_llama", check=True)
        run_command("sudo systemctl start swiss_army_llama", check=True)
        status_output = run_command("sudo systemctl status swiss_army_llama", capture_output=True, timeout=5)
        logger.info("Status of swiss_army_llama service:\n{}".format(status_output.stdout))

def kill_running_instances_of_swiss_army_llama():
    logger.info("Stopping Swiss Army Llama service...")
    run_command("sudo systemctl stop swiss_army_llama", check=False)
    logger.info("Killing any remaining Swiss Army Llama processes...")
    run_command("ps -ef | grep 'swiss_army' | grep -v grep | awk '{print $2}' | xargs -r kill -9", check=False)

def check_and_setup_swiss_army_llama(security_token):
    swiss_army_llama_port = 8089
    remote_swiss_army_llama_mapped_port = 8087  # Local port for remote SSH tunnel (not used here)   # noqa: F841
    external_ip = get_external_ip_func()
    if external_ip == "Unknown":
        logger.error("Unable to reach external network providers. Network may be unreachable.")
        return
    repo_path = os.path.expanduser("~/swiss_army_llama")
    repo_updated = has_repo_been_updated(repo_path)
    service_responding = is_swiss_army_llama_responding("localhost", swiss_army_llama_port, security_token)
    swiss_army_llama_port_available = is_port_available(swiss_army_llama_port)
    if service_responding:
        logger.info("Swiss Army Llama service is responding on port {}.".format(swiss_army_llama_port))
        if not repo_updated:
            logger.info("Swiss Army Llama is already set up and running, and the repository has not been updated.")
            return
    elif not swiss_army_llama_port_available:
        logger.error("Swiss Army Llama service is NOT responding on port {}.".format(swiss_army_llama_port))
        logger.error("Port {} is not available. It may be in use by another process.".format(swiss_army_llama_port))
    if repo_updated or not service_responding:
        if repo_updated:
            logger.info("Repository has updates. Running setup.")
        else:
            logger.info("Repository has not been updated, but the service is not responding. Running setup.")
        kill_running_instances_of_swiss_army_llama()  # Ensure service is stopped before setup
        setup_swiss_army_llama(security_token)


---
./README.md
---
# Introduction and Overview of Pastel Inference Layer

![Illustration](https://raw.githubusercontent.com/pastelnetwork/inference_layer_documentation/master/inference_illustration.webp)


The new Pastel Inference Layer is [implemented](https://github.com/pastelnetwork/python_inference_layer_server) in Python on the server side. It works entirely with regular pasteld (`pasteld` is the Pastel Daemon binary, which is a compiled C++ binary that is the heart of the Pastel system) features accessed via the RPC (though some of this functionality has since been directly integrated as new or expanded pasteld RPC methods instead of being done externally all in Python).

There are 2 main components of the Inference Layer:


- **Credit Pack Tickets:** these are new Pastel blockchain tickets that can be created by users with the cooperation of a single responding Supernode (with verification/validation from other Supernodes before the ticket is finalized).
    - The purpose of the credit pack tickets is for the user to pre-pay for a certain amount of future inference requests ahead of time by paying a certain amount of PSL (which is actually burned) to create a credit pack ticket with a certain number of credits, where this number is determined by market prices and must be agreed upon by the majority of Supernodes. 
    - Once created, the credit pack ticket makes it simple to account for inference requests, the cost of which are denominated in terms of credits as opposed to in PSL.
        - They do this by specifying a particular PSL address as the “tracking address” for the credit pack, as well as a list of one or more PastelIDs that are authorized to spend credits from the credit pack.
        - Essentially, to pay for the cost of the credit pack, the end user has to send the full PSL cost of the credit pack to the central Pastel burn address, and this burn transaction must originate from the tracking address.
        - Once the credit pack has been created in the blockchain specifying an initial credit pack balance, credits can be consumed from the credit pack by the end user by creating inference requests. Each particular inference request has a certain cost measured in credits (e.g., generating a ChatGPT-type “text completion” of max length of 1,000 tokens might cost 41.2 inference credits), and this specific cost is told to the end user by the Responding Supernode before the request is actually done. 
        - If the user wants to move forward with the inference request at the quoted price in credits, the user does this by sending a particular small amount of PSL to the central burn address from the designated tracking address that is specified in the credit pack ticket they are using. The amount sent is a tiny bit of PSL that corresponds exactly to the number of credits that the user is authorizing to be spent from their credit pack ticket. For example, to authorize their text completion request for a cost of 41.2 credits, the end user would burn exactly 0.00412 PSL (or 412 “Patoshis”, the smallest individual unit of PSL) worth a small fraction of a penny.
        - The beauty of this system is that, beyond the initial credit pack ticket (the data for which is immutably stored forever in the Pastel blockchain as coin transactions), we don’t need to spend a lot of space in the blockchain and involve addition ticket overhead and complexity for tracking the current remaining balance of credits in a particular credit pack ticket; we can instead “encode” this data as tiny coin transactions which take up practically no space and which we already have various RPC methods in pasteld to keep track of (and we added additional methods for this purpose, such as the new `scanburntransactions` RPC method).
    - Unlike previous blockchain tickets in Pastel for Sense, Cascade, and NFTs, these new credit pack tickets work in a fundamentally different way: 
        - The cost of the credit pack ticket is paid by the end user, but this payment is burned rather than going to one or more specific Supernodes. This simplifies things a lot because we don’t need to have separate registration and activation stages and wait for block confirmations. 
        - The creation of a credit pack creates an obligation of the entire Pastel Network to honor future inference requests that are paid for using the credits in the credit pack; this is different from Cascade for example, where the registering Supernode (which receives most of the storage fee from the user rather than it being burned) is responsible for generating the storage symbols and spreading them through the network.
        - The new credit pack tickets are implemented in pasteld in a new “generic” way using a new ticket type called `contract`; the actual contents of these new `contract` tickets can be changed at will without requiring any changes in pasteld (or Gonode for that matter); pasteld is simply indexing them and making it quick and easy to retrieve them. This also means that we can use the same generic `contract` ticket for all sorts of new applications besides credit pack tickets without having to change anything in pasteld going forward.


- **Inference Requests:** these are simply REST requests made by the inference client (there is a reference client in Python, but the [primary client](https://github.com/pastelnetwork/pastel_inference_js_client) is written in JS and has a visual UI) to the Inference Layer server, which is implemented as a FastAPI application. 
    - Inference Requests can take many different forms, including:
        - Text completion (aka, LLMs, similar to ChatGPT) from a large variety of models/services, such as:
            - Various API-based services, including OpenAI, Claude3, Groq, Mistral, OpenRouter, etc. These services are *optional* according to the wishes of the Supernode operator. If the operator wants to support those models, they can include encrypted API keys for them in the [.env file](https://github.com/pastelnetwork/python_inference_layer_server/blob/master/.env) for the inference server, and they will automatically be used (and these API keys are automatically tested periodically to determine if they still work; if they don’t work, then that Supernode stops advertising support for that service). 
                - Note that there is a master list of supported models/services stored in github as the [model_menu.json](https://github.com/pastelnetwork/python_inference_layer_server/blob/master/model_menu.json) file; the local Supernode than filters this master list down to the list of models that it specifically supports, and these are advertised from a particular endpoint in the inference server code so that end users and other Supernodes can determine which models/services are supported by which specific Supernodes. In addition to specifying which models/services a Supernode supports, the model_menu.json has much more: it completely specifies in a uniform way which model parameters are available and which inference types they apply to, and the form of input and output that a model or service expects. The model_menu.json is also used in the JS Inference Client to automatically populate the UI elements instead of specifying these in a fixed way, so that updating the model_menu.json file means that everything updates automatically across the entire inference layer. It also makes adding support for new models and services easy, since you can just add new Swiss Army Llama models to the file and they will automatically work. For adding new services and model types, you also need to implement a function to submit the request to the particular service/model, but this is easy to do now with over 6 fully worked examples that all work in a similar way.
            - “Locally Hosted” LLMs using [Swiss Army Llama](https://github.com/Dicklesworthstone/swiss_army_llama); these do not require any API key and are completely decentralized/distributed, since they can be run on the Supernode machine itself doing inference on the CPU (which is slow, but still works). Such models allow for absolutely no censorship on the input prompts you can supply, and can also support models for which all the “safety” features have been removed so that they will answer questions which would be rejected by services such as OpenAI or Claude3 (e.g., “How do you hotwire a car?”). These models also support additional useful options that give you more control over the inference, such as being able to specify a grammar file to constrain the nature of the output of the model.
                - The reason why “Locally Hosted” is in quotes is because we also support running Swiss Army Llama on a remote machine. This is very useful because GPU enabled cloud instances are extremely expensive, often costing nearly $2/hour (over $1,000/month), which is much much more than the cost of running a regular CPU-only cloud instance for a Supernode, which can be as low as $30/month. But of course inference goes many times faster on a GPU enabled machine. Thus, Supernode operators can instead optionally use a service such as [vast.ai](https://vast.ai/) to rent a cheap GPU enabled instance with an Nvidia 4090 GPU for as little as $0.34/hour (around $250/month), which can be spun up and set up in just a couple minutes using a pre-made [Docker image template](https://cloud.vast.ai/?ref_id=78066&template_id=3c711e5ddd050882f0eb6f4ce1b8cc28) (which itself is mostly just using a regular [Docker image](https://hub.docker.com/repository/docker/jemanuel82/vastai_swiss_army_llama_template/general)) of Swiss Army Llama with full GPU mode enabled for everything. Then the details of this machine can be specified easily in the .env file for the inference server, and requests for Swiss Army Llama models will be seamlessly forwarded to the remote machine for execution (with a fallback to the local machine in case of an error). This means that a single GPU enabled instance can be shared between multiple Supernodes. This gives the best of both worlds in terms of decentralization/censorship resistance, inference performance, and economical low-cost operation.
        - Ask a question about an image that supports both Swiss Army Llama (via the Llava-Llama3 multimodal model) and via the GPT4o vision model.
        - Generate an image using Stable diffusion (with support easily added for others, such as Dall-e)
        - Using Swiss Army Llama to get embedding vectors for a whole document (e.g., text, html, pdf, doc, etc.) and do a semantic search against it.
        - Same for an audio file with speech, which will be automatically transcribed using Whisper and embeddings calculated optionally (along with an optional semantic search across those embeddings).


We explain above the main parts of the system, but not much about how they are actually implemented, which is quite involved and complex. The reason for the complexity is that we want the entire process to be very robust and secure. In particular, we want to ensure that:


- Users never spend money (in the form of either PSL or in Inference Credits) irreversibly but then don’t properly get the thing they paid for— either a new credit pack ticket that works, or the results of their inference request.

- Because obligations arising from the Inference Layer are “network wide” and thus shared by all Supernodes, this makes it a lot easier in theory to deal with problems. Even if a particular Supernode doesn’t follow through with what it is supposed to do for any reason (e.g., it’s being malicious, or it experienced technical issues like running out of disk space or poor network connectivity), then we want the other Supernodes to automatically realize that and have a robust process for automatically having the next Supernode step up to complete what the user has already paid for.

- We want to have confidence that each part of the complex processes involved in creating a new credit pack ticket or in creating and fulfilling a new inference request is 100% valid and correct and hasn’t been changed by any malicious parties, including the end user, the responding Supernode, or another Supernode, or even a “man in the middle”, like one of the ISPs involved in the process for the end user or the Supernodes. 
    - This means that each step in these processes is mediated by messages, and the messages are verified and validated in very particular ways. For example, the relevant fields of each message are combined in a repeatable way and hashed together, and this hash included in the message “envelope”; these message hashes are then signed by all parties involved using their PastelID private keys. This allows all parties to know for sure that the contents of the message hasn’t been altered in any way, because otherwise the hash wouldn’t match the hash that each party independently computes based on all the relevant message fields, and thus the signatures wouldn’t validate either. 
    - In addition to this, each message has the current UTC timestamp and current Pastel block height included. If a message arrives more than a minute after this timestamp or more than 2 blocks away from the specified block height, then the message is deemed invalid and is ignored.
    - For example, this means that if a user makes a request for a credit pack contain 1,000 credits at a cost of 30,000 PSL, say, then there is no way for a malicious responding Supernode to alter that request so that the end user pays the full 30,000 PSL but only gets a credit pack containing 100 credits. And the same goes for inference requests— the user is guaranteed to get what they requested and nothing more.
    
- We also want to avoid any pricing surprises for users. Before a user spends any PSL or Inference Credits, they need to fully understand what they are getting for their money. 
    - This means that requests for new credit packs are first given a price quote from the responding Supernode, and the end user has to agree to this price before moving forward, and the end user can specify a maximum PSL cost to the credit pack. Also, since end users might not know what a “reasonable” price level is for a credit pack in PSL terms, we automatically validate the pricing quote on the client side by checking the price quote using the current market price of PSL and the same rules that the Supernodes are supposed to use in arriving at the pricing of credits; if the offered price quote is too far from what it’s supposed to be, the client will automatically refuse to move forward even if the total credit pack price is below the maximum price specified by the end user.  
    - In terms of inference requests, this means that the user first fully specifies what they want to do in their inference request, and the responding Supernode does some calculations to determine how many credits the inference request will cost and tells the user; the user thus has a chance to see how much it will cost them before moving forward and spending any of their inference credits from their credit pack ticket until they are sure they want to do it at that price. Once they are ready, they indicate this by sending a confirmation transaction (we will describe this fully below), and only then are the credits permanently deducted from their credit pack.
    
- To make the system simpler and more reliable, the fundamental interaction is one between the end user and a single Supernode, called the “Responding Supernode”; in the case where we are creating a new credit pack, this is simply the Supernode that is “closest” to the end user’s PastelID using XOR distance. In the case of a new inference request, this would be the “closest” Supernode using XOR distance to the end user that also supports the exact model/service specified in the user’s inference request. But just because the end user primarily communicates with this single Responding Supernode, it doesn’t mean that the other Supernodes are kept in the dark, or that the single Responding Supernode is able to do whatever it wants without consulting the other Supernodes.
    - In the case of creating a new credit pack ticket, the Responding Supernode can propose a price quote to the end user, but then when the user confirms the price quote, that doesn’t mean that the process is over; the problem of course is that the creation of a new credit pack creates an obligation on the part of the entire network of Supernodes to honor those inference credits to complete future inference requests. 
    - So if the Responding Supernode decided to price the new credits at way below cost or the normal market price, while that might be great for that specific end user that got a special deal, it would be bad for the broader network because all the other Supernodes would then be stuck fulfilling inference requests that are unprofitable on a network level; i.e., where the users pro-rata amount of PSL burned in USD terms is much less than the USD cost of providing a particular inference request for those credits. This is an important concept here, since both the entire credit pack cost is burned, so nothing is *directly* profitable to the Supernode, but the Supernode benefits *indirectly* to the extent that PSL is burned and the supply reduced. 
    - In order to deal with this issue, there is a whole procedure by which the Responding Supernode, after giving the intial credit pack price quote in PSL to the end user and getting the end user’s agreement to the initial quote, must then communicate all the details of the new credit pack request to all the other Supernodes. Then a certain portion of all the Supernodes need to respond (the quorum percentage), and of the Supernodes that respond, a majority of those (the agreement percentage) must agree to the proposed pricing terms (i.e., agree that it’s OK to sell X inference credits for Y PSL). They indicate this agreement by signing their pastelid to the details of the entire credit pack proposal, and these signatures are included in the final credit pack data that is stored in the blockchain. 
    - Later, when the end user attempts to actually use the credit pack for an inference request, all of this information is re-validated to determine that it’s all correct and proper— that is, that the original Responding Supernode didn’t “go rogue” and offer terms that the other Supernodes wouldn’t agree to.

With those preliminaries out of the way, we can now get into the specifics of the implementations of both parts. We begin with the creation of credit pack tickets.

----------

## Credit Pack Ticket Implementation Details

Note: The following python message models are written using the [SQLModel library](https://sqlmodel.tiangolo.com/). This allows us to create a single class that can be used both as a database ORM (using sqlite via [SQLAlchemy](https://www.sqlalchemy.org/)) AND simultaneously as a response/validation model (using [Pydantic](https://docs.pydantic.dev/latest/)). Normally, we would need to specify both an ORM data model and a separate Pydantic model, since they are used for different things (ORM models are for adding, selecting, and deleting data from a database without having to use raw SQL queries, and Pydantic models are for automatically validating the form of inputs and responses to and from FastAPI endpoint functions and handling all the serialization to/from JSON).

As mentioned above, the entire flow can be viewed as an intricate exchange of messages between the end user and the Responding Supernode, and between the Responding Supernode and the rest of the Supernodes. These message exchanges are facilitated using standard REST API endpoints on the Supernodes.

For security purposes, all these endpoints require a challenge/response system to access them at all; in this process, before an end user or other Supernode (or anyone else— just call them the “client”) can access any of these endpoints, the client must first request a challenge string from the specific Supernode they want to connect to, and in this challenge request they must specify their PastelID. The Supernode then responds with a challenge, which has a random challenge ID and challenge string; the client then has to quickly sign this challenge string using their PastelID (which must match what they said their PastelID was in the original challenge request) and return both the challenge ID, challenge string, and signature, all before the challenge expires automatically. The Supernode then verifies that the signature is valid and matches the right PastelID, and only THEN will it allow the client to access the endpoint. 

Beyond that, there is security at the individual message level; you will see in the classes below that they all contain fields with names like `sha3_256_hash_of_credit_pack_purchase_request_fields` and `requesting_end_user_pastelid_signature_on_request_hash`. These are used to verify and validate ALL details of ALL messages at EVERY point in the whole process. 

Now, let’s introduce the actual flow for a user to request the creation of a new credit pack ticket:

1) First the end user picks a certain Supernode from the list ofSupernodes (the one whose pastelid has XOR distance closest to the best block hash on pastel now; call this Supernode the `responding_supernode`) and requests to purchase an credit pack ticket for a certain price using this message that is sent via a POST to the `/credit_purchase_initial_request` endpoint:

```python
    class CreditPackPurchaseRequest(SQLModel, table=True):
        id: uuid.UUID = Field(default_factory=uuid.uuid4, index=True, nullable=True)
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
        requesting_end_user_pastelid: str = Field(index=True)
        requested_initial_credits_in_credit_pack: int
        list_of_authorized_pastelids_allowed_to_use_credit_pack: str = Field(sa_column=Column(JSON))
        credit_usage_tracking_psl_address: str = Field(index=True)
        request_timestamp_utc_iso_string: str
        request_pastel_block_height: int
        credit_purchase_request_message_version_string: str
        requesting_end_user_pastelid_signature_on_request_hash: str
```

2) The `responding_supernode` evaluates the message from the end user to check that all fields are valid. If anything in the request is invalid, then it responds to the POST request to the `/credit_purchase_initial_request` endpoint with a rejection message:

```python
    class CreditPackPurchaseRequestRejection(SQLModel, table=True):
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
        credit_pack_purchase_request_fields_json_b64: str
        rejection_reason_string: str
        rejection_timestamp_utc_iso_string: str
        rejection_pastel_block_height: int
        credit_purchase_request_rejection_message_version_string: str
        responding_supernode_pastelid: str = Field(index=True)
        sha3_256_hash_of_credit_pack_purchase_request_rejection_fields: str = Field(unique=True, index=True)
        responding_supernode_signature_on_credit_pack_purchase_request_rejection_hash: str
```

3) If the purchase request message fields are all valid, then the `responding_supernode` first determines the best price (in PSL per credit) that they would be willing to accept for the credit pack. To avoid wasting time and communication overhead, this preliminary price quote (preliminary because it is not considered valid by the network until it has been agreed to by enough other Supernodes) is first sent to the end user to determine if the end user is willing to agree to the quoted price (since if the end user doesn't like the price or doesn't have enough PSL to buy the credit pack for the total price, then there is no point in continuing any further). So in that case the POST request from the end user is responded to with this message:

```python
    class CreditPackPurchaseRequestPreliminaryPriceQuote(SQLModel, table=True):
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
        credit_usage_tracking_psl_address: str = Field(index=True)
        credit_pack_purchase_request_fields_json_b64: str
        preliminary_quoted_price_per_credit_in_psl: float
        preliminary_total_cost_of_credit_pack_in_psl: float
        preliminary_price_quote_timestamp_utc_iso_string: str
        preliminary_price_quote_pastel_block_height: int
        preliminary_price_quote_message_version_string: str
        responding_supernode_pastelid: str = Field(index=True)
        sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields: str = Field(unique=True, index=True)
        responding_supernode_signature_on_credit_pack_purchase_request_preliminary_price_quote_hash: str
```

4) The user then calls another endpoint on the`responding_supernode` (the `/credit_purchase_preliminary_price_quote_response` endpoint) to POST their response to the preliminary price quote with this message:

```python
    class CreditPackPurchaseRequestPreliminaryPriceQuoteResponse(SQLModel, table=True):
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
        sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields: str = Field(index=True)
        credit_pack_purchase_request_fields_json_b64: str
        agree_with_preliminary_price_quote: bool
        credit_usage_tracking_psl_address: str = Field(index=True)
        preliminary_quoted_price_per_credit_in_psl: float
        preliminary_price_quote_response_timestamp_utc_iso_string: str
        preliminary_price_quote_response_pastel_block_height: int
        preliminary_price_quote_response_message_version_string: str
        requesting_end_user_pastelid: str = Field(index=True)
        sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_response_fields: str = Field(unique=True, index=True)
        requesting_end_user_pastelid_signature_on_preliminary_price_quote_response_hash: str
```

5) If the end user rejects the price quote (i.e., `agree_with_preliminary_price_quote` is false), then the process terminates now. The end user can repeat the process on the next block to try with a new `responding_supernode` that might be willing to accept a lower price. If the end user accepts the preliminary price quote (i.e., `agree_with_preliminary_price_quote` is true), then the `/credit_purchase_preliminary_price_quote_response` endpoint on the `responding_supernode` must now determine if enough other supernodes in the pastel network agree with the pricing it proposed to the end user. The `responding_supernode` does this by selecting the 12 supernodes whose hash(pastelid) has XOR distance closest to the best pastel block's merkle root; collectively these are known as the `potentially_agreeing_supernodes`. The `responding_supernode` calls a REST endpoint on each of the potentially_agreeing_supernodes called the `/credit_pack_price_agreement_request` endpoint and POSTs a message of the form:

```python
    class CreditPackPurchasePriceAgreementRequest(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        sha3_256_hash_of_credit_pack_purchase_request_response_fields: str = Field(index=True)
        supernode_requesting_price_agreement_pastelid: str = Field(index=True)
        credit_pack_purchase_request_fields_json_b64: str
        credit_usage_tracking_psl_address: str = Field(index=True)
        proposed_psl_price_per_credit: float
        price_agreement_request_timestamp_utc_iso_string: str
        price_agreement_request_pastel_block_height: int
        price_agreement_request_message_version_string: str
        sha3_256_hash_of_price_agreement_request_fields: str = Field(index=True)
        supernode_requesting_price_agreement_pastelid_signature_on_request_hash: str
```

6) Each of the `potentially_agreeing_supernodes` checks all the fields of the message, including the fields of the related messages (i.e., `credit_pack_purchase_request_response_fields_json`) to determine if all fields and signatures are valid so far. Then it determines if it is willing to go along with the quoted credit pricing proposed by the `responding_supernode`. It does this by responding to the POST request to `/credit_purchase_preliminary_price_quote_response` endpoint with the following message structure:

```python
    class CreditPackPurchasePriceAgreementRequestResponse(SQLModel, table=True):
        sha3_256_hash_of_price_agreement_request_fields: str = Field(primary_key=True, index=True)
        credit_pack_purchase_request_fields_json_b64: str
        agree_with_proposed_price: bool
        credit_usage_tracking_psl_address: str = Field(unique=True,index=True)
        proposed_psl_price_per_credit: float
        proposed_price_agreement_response_timestamp_utc_iso_string: str
        proposed_price_agreement_response_pastel_block_height: int
        proposed_price_agreement_response_message_version_string: str
        responding_supernode_signature_on_credit_pack_purchase_request_fields_json_b64: str
        responding_supernode_pastelid: str = Field(index=True)
        sha3_256_hash_of_price_agreement_request_response_fields: str = Field(unique=True, index=True)
        responding_supernode_signature_on_price_agreement_request_response_hash: str
```

7) The `responding_supernode` waits for as many of the `potentially_agreeing_supernodes` to respond as possible in a set period of time (say, 30 seconds) and then aggregates all of their responses for processing. It counts up the number of valid responses received (the `valid_price_agreement_request_responses` from all of the `potentially_agreeing_supernodes` and computes
`(len(valid_price_agreement_request_responses)/len(potentially_agreeing_supernodes))` and checks that this result exceeds `0.51`. If it does not, then the entire process terminates now and the `responding_supernode` prepares a message for the end user for the next time the end user calls the REST endpoint `/check_status_of_credit_purchase_request` on the `responding_supernode`; first the end user POSTs the message:

```python
    class CreditPackRequestStatusCheck(SQLModel, table=True):
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
        requesting_end_user_pastelid: str = Field(index=True)
        requesting_end_user_pastelid_signature_on_sha3_256_hash_of_credit_pack_purchase_request_fields: str
```

and in the case of a termination, the `responding_supernode` responds with this message detailing why the request failed (in this case because not enough of the `potentially_agreeing_supernodes` responded in time with a valid response to the `responding_supernode`):

```python
    class CreditPackPurchaseRequestResponseTermination(SQLModel, table=True):
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
        credit_pack_purchase_request_fields_json_b64: str
        termination_reason_string: str
        termination_timestamp_utc_iso_string: str
        termination_pastel_block_height: int
        credit_purchase_request_termination_message_version_string: str
        responding_supernode_pastelid: str = Field(index=True)
        sha3_256_hash_of_credit_pack_purchase_request_termination_fields: str = Field(unique=True, index=True)
        responding_supernode_signature_on_credit_pack_purchase_request_termination_hash: str
```

8) If enough of the `potentially_agreeing_supernodes` respond in time with valid responses, the next step is for the `responding_supernode` to tally up the responses to determine the number of `potentially_agreeing_supernodes agree` to the quoted credit price; call them the `agreeing_supernodes`. If `len(agreeing_supernodes)/len(valid_price_agreement_request_responses)` exceeds `0.85` then the price quote is deemed to be valid by the network as a whole. If `len(agreeing_supernodes)/len(valid_price_agreement_request_responses)` is LESS than `0.85`, then the entire process terminates, and when the end user next calls the `responding_supernode`'s `/check_status_of_credit_purchase_request` endpoint, the `responding_supernode` will again respond with a `CreditPackPurchaseRequestResponseTermination` (but now the `termination_reason_string` field will explain that the request terminated for a different reason— because not enough of the other Supernodes agreed to the proposed pricing). But if enough of them agreed to the pricing so that the propose credit pack is deemed to be valid by the network, then the `responding_supernode responds` with this message the next time the end user calls its `/check_status_of_credit_purchase_request` endpoint:

```python
    class CreditPackPurchaseRequestResponse(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(foreign_key="creditpackpurchaserequest.sha3_256_hash_of_credit_pack_purchase_request_fields", index=True)
        credit_pack_purchase_request_fields_json_b64: str
        psl_cost_per_credit: float
        proposed_total_cost_of_credit_pack_in_psl: float
        credit_usage_tracking_psl_address: str = Field(index=True)
        request_response_timestamp_utc_iso_string: str
        request_response_pastel_block_height: int
        best_block_merkle_root: str
        best_block_height: int
        credit_purchase_request_response_message_version_string: str
        responding_supernode_pastelid: str = Field(index=True)
        list_of_blacklisted_supernode_pastelids: str = Field(sa_column=Column(JSON))
        list_of_potentially_agreeing_supernodes: str = Field(sa_column=Column(JSON))
        list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms: str = Field(sa_column=Column(JSON))
        list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion: str = Field(sa_column=Column(JSON))
        selected_agreeing_supernodes_signatures_dict: str = Field(sa_column=Column(JSON))
        sha3_256_hash_of_credit_pack_purchase_request_response_fields: str = Field(unique=True, index=True)
        responding_supernode_signature_on_credit_pack_purchase_request_response_hash: str
```
, and the `responding_supernode` will also call the `/credit_pack_purchase_request_final_response_announcement` endpoint on each of the `agreeing_supernodes` and will also POST the same `CreditPackPurchaseRequestResponse` message so that all of the `agreeing_supernodes` know all the details of the ticket.

9) At this point, the deal is agreed to in all particulars, and all that is left is for the end user to actually burn `proposed_total_cost_of_credit_pack_in_psl` PSL coins by sending exactly this many coins to the burn address from the end user's `credit_usage_tracking_psl_address` within 50 blocks of `request_response_pastel_block_height`. Once the end user does this, the transaction's UTXO is then communicated to the responding_supernode by the end user calling the responding_supernode's `/confirm_credit_purchase_request` endpoint by POSTing a message of the form:

```python
    class CreditPackPurchaseRequestConfirmation(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(foreign_key="creditpackpurchaserequest.sha3_256_hash_of_credit_pack_purchase_request_fields", index=True)
        sha3_256_hash_of_credit_pack_purchase_request_response_fields: str = Field(foreign_key="creditpackpurchaserequestresponse.sha3_256_hash_of_credit_pack_purchase_request_response_fields", index=True)
        credit_pack_purchase_request_fields_json_b64: str
        requesting_end_user_pastelid: str = Field(index=True)
        txid_of_credit_purchase_burn_transaction: str = Field(index=True)
        credit_purchase_request_confirmation_utc_iso_string: str
        credit_purchase_request_confirmation_pastel_block_height: int
        credit_purchase_request_confirmation_message_version_string: str
        sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str = Field(unique=True, index=True)
        requesting_end_user_pastelid_signature_on_sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str
```

The end user also calls the `/credit_pack_purchase_completion_announcement` endpoint on each of the `agreeing_supernodes` and POSTs the same `CreditPackPurchaseRequestConfirmation` message to them to let them know that the payment was sent.

10) The `responding_supernode` then checks the details of this message and also checks the pastel blockchain directly to confirm that the transaction with txid `txid_of_credit_purchase_burn_transaction` really exists in the blockchain, was mined and confirmed by at least 3 new blocks (this check can be disabled to speed things up during debugging), and matches the expected amount exactly, and that it was indeed sent by the `credit_usage_tracking_psl_address` within 50 blocks of `request_response_pastel_block_height`. If all those things are true, then the `responding_supernode` writes the “combined ticket data” to the Pastel blockchain.

Essentially, this combined data is a nested JSON string that contains within it the JSON serialized messages for `CreditPackPurchaseRequest`, `CreditPackPurchaseRequestResponse`, and `CreditPackPurchaseRequestConfirmation`, which between them contain all the many salient pieces of information that are required to fully validate the legitimacy of the credit pack ticket— even by a newly joined Supernode that wasn’t around when the original credit pack was purchase and so never directly saw any of the many announcement messages that would have been sent to it if it were part of the network when the credit pack ticket was originally created. The process of writing this data to the blockchain, which is done by storing the Z-standard compressed JSON data in the form of Pay2FakeMultiSig coin transactions, is now completely handled within pasteld itself using a new RPC method, which is access in the Python Inference server code like this:

```python
    ticket_register_command_response = await rpc_connection.tickets('register', 'contract', ticket_json_b64, ticket_type_identifier, ticket_input_data_fully_parsed_sha3_256_hash)
```

This command returns a single Pastel TXID which uniquely picks out the credit pack ticket and is the primary way we refer to the completed credit pack ticket for all future operations (note that, of course, this TXID is itself not in the credit pack data, since that would create a “chicken and egg” problem.) For example, if we want to retrieve the credit pack from the blockchain we can use the RPC method in pasteld from Python like this and quickly parse out all the relevant data to get back a Python dict variable that has everything we’d need to fully validate the ticket contents:

```python
    ticket_get_command_response = await rpc_connection.tickets('get', ticket_txid , 1)
```

Because it would be wasteful to constantly get data from the blockchain and parse it, we only need to do this the first time (or if there is a chain re-org or rollback, which is detected and handled automatically in the Python Inference Server code) and the parsed data is serialized and turned into records in the Supernode’s local SQLite database. This is also true for all messages that the Supernode receives from end users or other Supernodes, as well as announcement messages that are sent using the built-in Pastel Masternode messaging system; all of these are automatically added to the database for ease of querying and efficiency (this is the beauty of using SQLmodel— all messages that are sent/received are automatically validated against the model and can be losslessly turned into the underlying ORM data models from the raw JSON, and can then be inserted easily into the database without a lot of annoying and verbose conversion/ingestion code).

After the ticket has been written to the blockchain successfully by the `responding_supernode`, it finally responds to the end user with this message:

```python
    class CreditPackPurchaseRequestConfirmationResponse(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(foreign_key="creditpackpurchaserequest.sha3_256_hash_of_credit_pack_purchase_request_fields", index=True)
        sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str = Field(foreign_key="creditpackpurchaserequestconfirmation.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields", index=True)
        credit_pack_confirmation_outcome_string: str
        pastel_api_credit_pack_ticket_registration_txid: str = Field(index=True)
        credit_pack_confirmation_failure_reason_if_applicable: str
        credit_purchase_request_confirmation_response_utc_iso_string: str
        credit_purchase_request_confirmation_response_pastel_block_height: int
        credit_purchase_request_confirmation_response_message_version_string: str
        responding_supernode_pastelid: str = Field(index=True)
        sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields: str = Field(unique=True, index=True)
        responding_supernode_signature_on_credit_pack_purchase_request_confirmation_response_hash: str
```

In addition, the `responding_supernode` calls the `/credit_pack_storage_completion_announcement` endpoint on all of the `agreeing_supernodes` and POSTs the same `CreditPackPurchaseRequestConfirmationResponse` message to them to let them know that the process has been successfully completed and everything is done and the credit pack ticket has been written to the blockchain correctly and is now valid and ready to be used, as well as the final TXID for the credit pack ticket.

11) If, for whatever reason, the `responding_supernode` is unable or unwilling to actually store the ticket data in the blockchain, but the end user has already burned the required PSL as needed, then we still need to deal with this situation, because otherwise it's very unfair to the end user. Luckily, any of the `agreeing_supernodes` can be called upon if needed by the end user to do this. But the end user is only permitted to ask one of the `agreeing_supernodes` to do this for them if more than 10 Pastel blocks have elapsed since the `CreditPackPurchaseRequestConfirmation` was sent by the end user without the original `responding_supernode` sending the `CreditPackPurchaseRequestConfirmationResponse` to the `agreeing_supernodes`. In that case, the end user chooses the "closest" of the `agreeing_supernodes` to the end user's pastelid (i.e., the XOR distance of `hash(agreeing_supernode_pastelid)` to `hash(end_user_pastelid)` is smallest of all the `agreeing_supernodes`; call this the `closest_agreeing_supernode`) and then the end user calls the `/credit_pack_storage_retry_request` endpoint on the `closest_agreeing_supernode` and POSTs the message:

```python
    class CreditPackStorageRetryRequest(SQLModel, table=True):
        sha3_256_hash_of_credit_pack_purchase_request_response_fields: str = Field(primary_key=True, index=True)
        credit_pack_purchase_request_fields_json_b64: str
        requesting_end_user_pastelid: str = Field(index=True)
        closest_agreeing_supernode_to_retry_storage_pastelid: str = Field(index=True)
        credit_pack_storage_retry_request_timestamp_utc_iso_string: str
        credit_pack_storage_retry_request_pastel_block_height: int
        credit_pack_storage_retry_request_message_version_string: str
        sha3_256_hash_of_credit_pack_storage_retry_request_fields: str
        requesting_end_user_pastelid_signature_on_credit_pack_storage_retry_request_hash: str
```

12) At this point, the `closest_agreeing_supernode` will check all the details to ensure that the retry request is valid (particularly, that the original `responding_supernode` has still not confirmed that the ticket storage took place successfully) in all respects, and if so, it will store the ticket in the Pastel blockchain itself. It can do this, including having the original `responding_supernode`'s pastelid signature on the ticket, because earlier in the process, the `responding_supernode` called the `closest_agreeing_supernode`'s `credit_pack_purchase_completion_announcement` endpoint, thus supplying all these details automatically. So now, the `closest_agreeing_supernode` simply stores that exact ticket data itself in the blockchain. When done, it responds to the end user's request to its `/credit_pack_storage_retry_request` endpoint with the following message:

```python
    class CreditPackStorageRetryRequestResponse(SQLModel, table=True):
        sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
        sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str
        credit_pack_storage_retry_confirmation_outcome_string: str
        pastel_api_credit_pack_ticket_registration_txid: str
        credit_pack_storage_retry_confirmation_failure_reason_if_applicable: str
        credit_pack_storage_retry_confirmation_response_utc_iso_string: str
        credit_pack_storage_retry_confirmation_response_pastel_block_height: int
        credit_pack_storage_retry_confirmation_response_message_version_string: str
        closest_agreeing_supernode_to_retry_storage_pastelid: str = Field(index=True)
        sha3_256_hash_of_credit_pack_storage_retry_confirmation_response_fields: str
        closest_agreeing_supernode_to_retry_storage_pastelid_signature_on_credit_pack_storage_retry_confirmation_response_hash: str    
```

That completes the entire flow from start to finish of creating a new credit pack ticket. 


----------

In this next section, we will go into more detail about how the various service functions that enable this functionality are implemented, with a particular focus on the provisioning and validation of new and existing credit pack tickets. 

First off, we should discuss how credit pack tickets are priced in PSL terms in the first place; that is, “for X number of inference credits, the total cost should be Y PSL.”  The basic idea here is to keep the price of inference credits relatively stable in USD terms even if the market price of PSL (i.e,. how much USD 1.0 PSL is worth) is moving around quite a bit. Another goal in the pricing is to keep the cost of inference credits in line with the underlying cost to serve requests, plus a 10% "theoretical profit margin" for the Supernodes that are serving these inference requests. Note that this isn't a *real* profit margin, since the Supernodes don't actually receive *any* of the cost of the credit pack tickets— this is all burned by the end user. However, this coin burning does reduce the total supply of PSL outstanding which benefits all holders, including and especially Supernode operators. The profit margin concept is more to ensure that the intrinsic economics of the broader pastel inference system are economically viable.

Various service functions are used to do this. Let's go through each function and explain their purpose and how they contribute to the estimation process:


- **`fetch_current_psl_market_price`**:
    - This function retrieves the current market price of PSL (Pastel) from two sources: CoinMarketCap and CoinGecko.
    - It sends HTTP requests to the respective APIs and extracts the PSL price in USD from the responses.
    - If the price cannot be retrieved from either source, it retries after a short delay.
    - It calculates the average price based on the available prices from the sources.
    - The function validates the average price to ensure it falls within a reasonable range.
    - It returns the average PSL price in USD.
    
- **`estimated_market_price_of_inference_credits_in_psl_terms`**:
    - This function estimates the market price of inference credits in PSL terms.
    - It first retrieves the current PSL market price in USD using the `fetch_current_psl_market_price` function.
    - It then calculates the cost per credit in USD, considering a target value per credit and a target profit margin.
    - The target value per credit represents the underlying cost to serve inference requests, while the profit margin ensures the economic viability of the Pastel inference system.
    - The cost per credit in USD is converted to PSL terms by dividing it by the current PSL market price.
    - The function returns the estimated market price of 1.0 inference credit in PSL.
    
- **`calculate_price_difference_percentage`** (in the inference client):
    - This function calculates the percentage difference between a quoted price and an estimated price.
    - It takes the quoted price and estimated price as input and computes the absolute difference between them.
    - The difference is then divided by the estimated price to obtain the percentage difference.
    - It raises an error if the estimated price is zero to avoid division by zero.
    - The function returns the price difference percentage.
    
- **`confirm_preliminary_price_quote`** (in the inference client):
    - This function confirms the preliminary price quote for a credit pack purchase.
    - It takes the preliminary price quote, maximum total credit pack price, and maximum per credit price as input.
    - If the maximum prices are not provided, it uses default values.
    - It extracts the quoted price per credit, quoted total price, and requested credits from the preliminary price quote.
    - It estimates the fair market price for the credits using the `estimated_market_price_of_inference_credits_in_psl_terms` function.
    - It calculates the price difference percentage between the quoted price and the estimated fair price using the `calculate_price_difference_percentage` function.
    - The function compares the quoted prices with the maximum prices and the estimated fair price.
    - If the quoted prices are within the acceptable range and the price difference percentage is below a certain threshold, it confirms the preliminary price quote.
    - Otherwise, it logs a warning message and rejects the price quote.
    internal_estimate_of_credit_pack_ticket_cost_in_psl (in the inference client):
    - This function provides an internal estimate of the cost of a credit pack ticket in PSL terms.
    - It takes the desired number of credits and a price cushion percentage as input.
    - It retrieves the estimated market price per credit in PSL using the `estimated_market_price_of_inference_credits_in_psl_terms` function.
    - It calculates the estimated total cost of the ticket by multiplying the desired number of credits, estimated price per credit, and the price cushion percentage.
    - The price cushion percentage allows for some flexibility in the pricing to account for market fluctuations or other factors.
    - The function returns the estimated total cost of the credit pack ticket in PSL.

These functions work together to estimate the cost of a credit pack ticket in PSL terms, considering the desired number of credits and the underlying cost to serve inference requests. The `fetch_current_psl_market_price` function retrieves the current PSL market price, which is used by the `estimated_market_price_of_inference_credits_in_psl_terms` function to estimate the fair market price of inference credits in PSL.

The `confirm_preliminary_price_quote` function in the inference client uses these estimates to validate the quoted prices from the responding supernode. It ensures that the quoted prices are within acceptable ranges and not significantly different from the estimated fair market price. This helps protect the user from potential price manipulation or uninformed pricing.

The `internal_estimate_of_credit_pack_ticket_cost_in_psl` function provides an internal estimate of the total cost of a credit pack ticket based on the desired number of credits and a price cushion percentage. This estimate can be used by the client to set reasonable maximum prices when requesting a credit pack. Overall, these functions contribute to the estimation and validation of credit pack ticket costs in PSL terms, ensuring that the prices are economically viable for the Pastel inference system and fair for the users.


----------

Next, we go into the various service functions for validating existing credit pack tickets. Before any credit pack ticket can be used, it’s not enough for the end user to have a TXID that points to a credit pack ticket in the Pastel Blockchain; theoretically, something that looks like a credit pack ticket could be created by ANY user without it being valid. So the mere fact that a ticket exists in the blockchain doesn’t mean that it’s valid. We also can’t rely on pasteld to automatically validate the ticket in the same way that it does for Sense, Cascade, and NFT tickets, because in this new `contract` ticket type, pasteld just stores the raw ticket data without ever having to decode and parse it, let alone validate it.

This is what gives us the flexibility and power to dynamically revise the credit pack ticket fields/structure (which is also facilitated by the inclusion of a “message_version_string” in nearly every message, so we can deal with changes in a robust way to the inference protocol) and also to create completely new kinds of `contract` tickets for new applications (e.g., some kind of generic “smart contract” application that can run little WASM containers, similar to AWS Lambda functions). But it also means that all of the detailed level validation of the credit pack ticket data itself needs to be done by us in the Inference Layer server code in Python.

That’s no problem though— it’s easy enough to validate a credit pack ticket in Python. We just need to get the ticket data from the blockchain, parse out the component messages, and run them through the same validation process that we apply when the Supernode receives a message via a REST endpoint. In fact, we should review that function, because it’s quite long and intricate, because we use the same validation function to check ALL the various kinds of messages that are sent around during the process of creating a new credit pack (in fact, we also reuse this same function for inference request message validation). You can read the entire code for the function [here](https://github.com/pastelnetwork/python_inference_layer_server/blob/4176bb6e3e52cdeb6e3c61f291b2944e800b764f/service_functions.py#L5885).




Below is a detailed explanation in words of how the function works:

The `validate_credit_pack_ticket_message_data_func` is a comprehensive validation function that performs several checks on a received SQLModel instance to ensure the integrity and validity of the data. Here's a detailed explanation of how the function verifies the correctness of the message and ensures it hasn't been tampered with:


1. Timestamp Validation:
    - The function iterates through all the fields in the model instance and checks for fields ending with "_timestamp_utc_iso_string".
    - For each timestamp field, it attempts to convert the field value to a datetime object using `pd.to_datetime()`. If the conversion fails, it indicates an invalid timestamp format, and an error is appended to the `validation_errors` list.
    - Additionally, the function compares the timestamp in the field with the current timestamp using the `compare_datetimes()` function. If the difference between the timestamps exceeds a certain threshold (`MAXIMUM_LOCAL_PASTEL_BLOCK_HEIGHT_DIFFERENCE_IN_BLOCKS`), it suggests that the timestamp is too far from the current time, and an error is appended to the `validation_errors` list.
    
2. Pastel Block Height Validation:
    - The function retrieves the best block hash, Merkle root, and block height using the `get_best_block_hash_and_merkle_root_func()` function.
    - It then iterates through the fields in the model instance and checks for fields ending with "_pastel_block_height".
    - For each block height field, it compares the field value with the current block height obtained from the Pastel network. If the absolute difference between the field value and the current block height exceeds `MAXIMUM_LOCAL_PASTEL_BLOCK_HEIGHT_DIFFERENCE_IN_BLOCKS`, it indicates a discrepancy, and an error is appended to the `validation_errors` list.
    
3. Hash Validation:
    - The function computes the expected SHA3-256 hash of the response fields using the `compute_sha3_256_hash_of_sqlmodel_response_fields()` function.
    - It then searches for a field name starting with "sha3_256_hash_of" and ending with "_fields" to locate the hash field in the model instance.
    - If a hash field is found, the function compares the actual hash value stored in the field with the computed expected hash. If the hashes don't match, it indicates that the response fields have been modified, and an error is appended to the `validation_errors` list.
    
4. Pastel ID Signature Validation:
    - The function handles signature validation differently depending on the type of the model instance.
    - For `CreditPackPurchasePriceAgreementRequestResponse` instances:
        - It identifies the first Pastel ID field and signature fields based on naming conventions.
        - If the corresponding Pastel ID field is found, it verifies each signature field using the `verify_message_with_pastelid_func()` function, passing the Pastel ID, the message to verify (either the hash or the JSON-encoded request fields), and the signature.
        - If any signature verification fails, an error is appended to the `validation_errors` list.
    - For other model instances:
        - It identifies the last signature field and the last hash field based on naming conventions.
        - If both fields are found, it checks for the presence of the corresponding Pastel ID field.
        - If the Pastel ID field is found or not applicable (in case of combined Pastel ID and signature fields), it extracts the Pastel ID and signature from the fields.
        - It then verifies the signature using the `verify_message_with_pastelid_func()` function, passing the Pastel ID, the message to verify (the hash field value), and the signature.
        - If the signature verification fails, an error is appended to the `validation_errors` list.
        

Finally, the function returns the list of validation errors (`validation_errors`). If the list is empty, it indicates that the message passed all the validation checks successfully. If there are any errors, they are included in the returned list for further handling or reporting. By performing these comprehensive validations, the function ensures that the received message adheres to the expected format, timestamps are within acceptable ranges, block heights match the current network state, hashes are consistent, and signatures are valid. This helps in detecting any modifications or tampering of the message during transmission or by malicious actors.

In addition to the main validation function, several helper functions support the detailed validation process:


1. `compare_datetimes(datetime_input1, datetime_input2)`:
    - This function ensures that the datetime inputs are converted to timezone-aware datetime objects. It calculates the difference in seconds between the two datetime inputs and checks if they are within an acceptable range (`MAXIMUM_LOCAL_UTC_TIMESTAMP_DIFFERENCE_IN_SECONDS`). If the difference is too large, a warning is logged.
    
2. `get_sha256_hash_of_input_data_func(input_data_or_string)`:
    - This function computes the SHA3-256 hash of the input data. If the input is a string, it is encoded to bytes before hashing. The function returns the hexadecimal representation of the hash.
    
3. `sort_dict_by_keys(input_dict)`:
    - This function sorts a dictionary and its nested dictionaries by keys. It converts the sorted dictionary into a JSON string, which can be useful for consistent and reproducible hash calculations.
    
4. `extract_response_fields_from_credit_pack_ticket_message_data_as_json_func(model_instance: SQLModel)`:
    - This asynchronous function extracts the response fields from a SQLModel instance and converts them into a JSON string. It handles various data types (e.g., datetime, list, dict, decimal) and ensures the fields are sorted for consistent hash computation.
    
5. `async def compute_sha3_256_hash_of_sqlmodel_response_fields(model_instance: SQLModel)`:
    - This asynchronous function uses `extract_response_fields_from_credit_pack_ticket_message_data_as_json_func` to get the JSON representation of the response fields and then computes the SHA3-256 hash of this JSON string.
    
6. `validate_credit_pack_blockchain_ticket_data_field_hashes(model_instance: SQLModel)`:
    - This asynchronous function validates the hashes of the response fields in a SQLModel instance. It compares the expected hash (computed from the response fields) with the actual hash stored in the model instance. If the hashes don't match, it appends an error to the `validation_errors` list.
    

Now we can also look at the full function for validating an existing credit pack ticket (`validate_existing_credit_pack_ticket`); the code for this function can be found [here](https://github.com/pastelnetwork/python_inference_layer_server/blob/4176bb6e3e52cdeb6e3c61f291b2944e800b764f/service_functions.py#L2969).


And a detailed explanation of how it works in words is below:

The `validate_existing_credit_pack_ticket` function is an asynchronous function that takes a `credit_pack_ticket_txid` as input and performs a comprehensive validation of an existing Pastel credit pack ticket. The purpose of this function is to ensure that the credit pack ticket is valid and has not been tampered with, even if the Supernode performing the validation was not present when the ticket was initially created. Here's a detailed breakdown of how the function works:


1. The function starts by retrieving the credit pack ticket data from the blockchain using the `retrieve_credit_pack_ticket_from_blockchain_using_txid` function. This function returns three objects: `credit_pack_purchase_request`, `credit_pack_purchase_request_response`, and `credit_pack_purchase_request_confirmation`.
2. It initializes a `validation_results` dictionary to store the validation results, including the overall validity of the ticket, individual validation checks, and any validation failure reasons.
3. Payment Validation:
    - The function calls the `check_burn_transaction` function to validate the payment associated with the credit pack ticket.
    - It checks if a matching or exceeding burn transaction exists for the specified `txid_of_credit_purchase_burn_transaction`, `credit_usage_tracking_psl_address`, `proposed_total_cost_of_credit_pack_in_psl`, and `request_response_pastel_block_height`.
    - If a matching or exceeding transaction is found, the payment is considered valid. Otherwise, the ticket is marked as invalid, and the reason is added to the `validation_failure_reasons_list`.
4. Supernode Validation:
    - The function retrieves the count and details of active supernodes at the time of the credit pack purchase using the `fetch_active_supernodes_count_and_details` function, based on the `request_response_pastel_block_height`.
    - It checks if all the potentially agreeing supernodes and agreeing supernodes listed in the ticket were valid supernodes at the time of the purchase by comparing their pastelids with the list of active supernodes.
    - If any supernode is not found in the list of active supernodes, the ticket is marked as invalid, and the reason is added to the `validation_failure_reasons_list`.
5. Ticket Response Hash Validation:
    - The function validates the hashes of the credit pack purchase request response and confirmation objects using the `validate_credit_pack_blockchain_ticket_data_field_hashes` function.
    - It checks if the computed hashes match the hashes included in the ticket objects.
    - If any hash mismatch is detected, the ticket is marked as invalid, and the reason is added to the `validation_failure_reasons_list`.
6. Signature Validation:
    - The function verifies the signatures of the agreeing supernodes using the `verify_message_with_pastelid_func` function.
    - It iterates over each agreeing supernode pastelid and verifies their signature on the `credit_pack_purchase_request_fields_json_b64` field.
    - If any signature fails validation, the ticket is marked as invalid, and the reason is added to the `validation_failure_reasons_list`.
7. Agreeing Supernodes Validation:
    - The function calculates the quorum percentage and agreeing percentage based on the number of potentially agreeing supernodes, agreeing supernodes, and total active supernodes at the time of the purchase.
    - It checks if the quorum percentage and agreeing percentage meet the required thresholds (`SUPERNODE_CREDIT_PRICE_AGREEMENT_QUORUM_PERCENTAGE` and `SUPERNODE_CREDIT_PRICE_AGREEMENT_MAJORITY_PERCENTAGE`).
    - If either the quorum or agreeing percentage is not met, the ticket is marked as invalid, and the reason is added to the `validation_failure_reasons_list`.
8. Handling Validation Failures:
    - If any validation failures are detected, the function logs the failure reasons and adds the invalid credit pack ticket TXID to a "known bad" table in the database using the `insert_credit_pack_ticket_txid_into_known_bad_table_in_db` function.
9. Finally, the function returns the `validation_results` dictionary, which includes the overall validity of the ticket, individual validation checks, and any validation failure reasons.

This function ensures the integrity and validity of an existing Pastel credit pack ticket by performing comprehensive validations, including payment validation, supernode validation, hash validation, signature validation, and agreeing supernodes validation. By checking against historical data and verifying the consistency of the ticket data, the function can determine the validity of a credit pack ticket even if the validating supernode was not present during the ticket's creation.

We also offer an convenience endpoint for use by an end user where the user can specify their PastelID and get back a list of all their valid credit pack tickets (this function can also determine the current credit balance of those tickets; how this works will be explained in more detail below when we explain the flow for Inference Requests). The code for that function can be found [here](https://github.com/pastelnetwork/python_inference_layer_server/blob/4176bb6e3e52cdeb6e3c61f291b2944e800b764f/service_functions.py#L3091).

And here is the breakdown of how that function works:

The `get_valid_credit_pack_tickets_for_pastelid` function is an asynchronous function that retrieves a list of valid credit pack tickets for a given PastelID. It provides a convenient way for an end user to retrieve all their credit pack tickets and their current credit balances. Here's a detailed breakdown of how the function works:


1. The function starts by querying the database using SQLAlchemy's `select` statement to retrieve all the `CreditPackPurchaseRequestConfirmation` records associated with the given `pastelid`. This is done within an asynchronous database session using `db_code.Session()`.
2. It initializes an empty list called `complete_tickets` to store the complete credit pack tickets.
3. For each `request_confirmation` retrieved from the database, the function performs the following steps:
    - Extracts the `sha3_256_hash_of_credit_pack_purchase_request_fields` from the `request_confirmation`.
    - Queries the database to find the corresponding `CreditPackPurchaseRequestResponseTxidMapping` record using the extracted hash. This mapping contains the `pastel_api_credit_pack_ticket_registration_txid` (TXID) associated with the credit pack ticket.
    - If a `txid_mapping` is found, the function proceeds to retrieve the complete credit pack ticket data.
4. Retrieving Complete Credit Pack Ticket Data:
    - If a `txid_mapping` is found, the function queries the database to check if there is an existing `CreditPackCompleteTicketWithBalance` record associated with the TXID.
    - If an existing record is found (`existing_data`), it means the complete credit pack ticket data is already stored in the database. In this case:
        - The function loads the `complete_credit_pack_data_json` from the `existing_data` record and parses it into a `complete_ticket` dictionary.
        - It calls the `determine_current_credit_pack_balance_based_on_tracking_transactions` function to determine the current credit balance and the number of confirmation transactions for the credit pack ticket based on the TXID.
        - The `credit_pack_current_credit_balance` and `balance_as_of_datetime` fields are added to the `complete_ticket` dictionary.
        - The updated `complete_ticket` is then converted back to JSON format and stored in the `existing_data` record, along with the updated `datetime_last_updated`.
        - The changes are committed to the database within an asynchronous database session.
    - If an existing record is not found, it means the complete credit pack ticket data needs to be retrieved and stored in the database. In this case:
        - The function calls the `determine_current_credit_pack_balance_based_on_tracking_transactions` function to determine the current credit balance and the number of confirmation transactions for the credit pack ticket based on the TXID.
        - It retrieves the `credit_pack_purchase_request_response` and `credit_pack_purchase_request_confirmation` using the `retrieve_credit_pack_ticket_using_txid` function.
        - If both the response and confirmation are successfully retrieved, the function proceeds to create a `complete_ticket` dictionary containing the `credit_pack_purchase_request`, `credit_pack_purchase_request_response`, `credit_pack_purchase_request_confirmation`, `credit_pack_registration_txid`, `credit_pack_current_credit_balance`, and `balance_as_of_datetime`.
        - The `complete_ticket` dictionary is then processed by converting UUIDs to strings and normalizing the data using the `convert_uuids_to_strings` and `normalize_data` functions.
        - The processed `complete_ticket` is converted to JSON format.
        - If an `existing_data` record exists, the `complete_credit_pack_data_json` and `datetime_last_updated` fields are updated with the new data.
        - If an `existing_data` record doesn't exist, a new `CreditPackCompleteTicketWithBalance` record is created with the TXID, `complete_credit_pack_data_json`, and `datetime_last_updated`.
        - The changes are committed to the database within an asynchronous database session.
5. Finally, the `complete_ticket` is appended to the `complete_tickets` list.
6. After processing all the `request_confirmations`, the function returns the `complete_tickets` list containing all the valid credit pack tickets for the given `pastelid`.

This function efficiently retrieves and processes credit pack tickets for a specific PastelID. It leverages the database to store and retrieve complete ticket data, including the current credit balance. By caching the ticket data in the database, subsequent requests for the same ticket can be served faster without the need to retrieve the data from the blockchain every time. The function also ensures that the ticket data is updated with the latest credit balance and timestamp whenever it is retrieved.

The process of retrieving valid credit pack tickets for a specific PastelID using the `get_valid_credit_pack_tickets_for_pastelid` function relies on the local Supernode's SQLite database. However, to ensure that the local database contains up-to-date information about all the credit pack tickets in the blockchain, the Supernode periodically runs background tasks to gather ticket data from the blockchain, parse it, and ingest it into the local database, including the following:


1. `retrieve_generic_ticket_data_from_blockchain`:
    - This function retrieves the ticket data for a specific ticket TXID from the blockchain using the `rpc_connection.tickets('get', ...)` command.
    - It retrieves the ticket's input data, including the fully parsed SHA3-256 hash and the ticket input data dictionary.
    - It computes the SHA3-256 hash of the retrieved ticket input data and compares it with the retrieved fully parsed hash to ensure data integrity.
    - If the hashes match, it returns the credit pack combined blockchain ticket data as a JSON string.
2. `get_list_of_credit_pack_ticket_txids_already_in_db`:
    - This function retrieves the list of credit pack ticket TXIDs that are already stored in the local SQLite database.
    - It queries the `CreditPackPurchaseRequestResponseTxidMapping` table to get the list of TXIDs.
    - It returns a list of unique TXIDs that are already stored in the database.
3. `list_generic_tickets_in_blockchain_and_parse_and_validate_and_store_them`:
    - This function retrieves all the blockchain tickets of a specific type (e.g., "INFERENCE_API_CREDIT_PACK_TICKET") starting from a given block height.
    - It uses the `rpc_connection.tickets('list', ...)` command to retrieve the ticket data from the blockchain.
    - It checks the internal consistency of the retrieved tickets by comparing the computed SHA3-256 hashes of the ticket input data with the retrieved fully parsed hashes.
    - It retrieves the list of already stored credit pack TXIDs and the list of known bad credit pack TXIDs from the local database.
    - If `force_revalidate_all_tickets` is set to `True`, it attempts to validate all retrieved tickets, even those already stored in the database or in the known bad list.
    - If `force_revalidate_all_tickets` is set to `False`, it only attempts to validate tickets that are not already stored in the database or in the known bad list.
    - For each ticket TXID that needs validation, it calls the `validate_existing_credit_pack_ticket` function to perform in-depth validation of all aspects of the ticket.
    - If a ticket passes validation, it is saved to the local database using the `retrieve_credit_pack_ticket_using_txid` function.
    - It returns the list of retrieved ticket input data as JSON strings and the list of fully validated ticket TXIDs.
4. `periodic_ticket_listing_and_validation`:
    - This function is an infinite loop that periodically calls the `list_generic_tickets_in_blockchain_and_parse_and_validate_and_store_them` function.
    - It runs every 30 minutes (adjustable) to continuously update the local database with the latest credit pack ticket data from the blockchain.
    - If an error occurs during the periodic execution, it logs the error and continues the loop.

The `startup` function is called when the Supernode starts up. It initializes the database, generates or loads the encryption key, decrypts sensitive fields, and creates background tasks for various operations, including the `periodic_ticket_listing_and_validation` task.
By periodically running the `periodic_ticket_listing_and_validation` task, the Supernode ensures that its local SQLite database is continuously updated with the latest credit pack ticket data from the blockchain. This allows the `get_valid_credit_pack_tickets_for_pastelid` function to retrieve valid credit pack tickets for a specific PastelID efficiently from the local database, without having to query the blockchain every time.

This approach optimizes performance and reduces the load on the blockchain by caching the ticket data in the local database and periodically synchronizing it with the blockchain data. It enables fast retrieval of valid credit pack tickets for a given PastelID while ensuring data consistency and integrity through periodic updates and validations.

One obvious issue with this approach is that we need it to be robust to possible Pastel blockchain level re-orgs and rollbacks; essentially, the local database can go “out of sync” with the underlying blockchain if the blockchain itself changes (this obviously doesn’t happen in normal operation). We do this by using various additional functions and background services which are periodically running:

The code provided includes several functions that work together to handle chain reorgs and rollbacks in the process of tracking and storing burn transactions and block hashes. Let's break down how these functions contribute to handling chain reorgs and rollbacks:


1. `detect_chain_reorg_and_rescan`:
    - This function runs continuously in the background, periodically checking for chain reorgs.
    - It retrieves the latest stored block hash from the database and compares it with the block hash at the same height in the blockchain.
    - If the stored block hash doesn't match the block hash in the blockchain, it indicates that a chain reorg has occurred.
    - When a chain reorg is detected, the function triggers a full rescan of burn transactions by deleting all existing records in the `BurnAddressTransaction` and `BlockHash` tables and calling the `full_rescan_burn_transactions` function.
    - The function sleeps for a specified interval (e.g., 6000 seconds) before checking for chain reorgs again.
2. `full_rescan_burn_transactions`:
    - This function is called when a chain reorg is detected or when no burn transaction records are found in the database.
    - It performs a full rescan of burn transactions starting from the genesis block.
    - It uses the `rpc_connection.scanburntransactions("*")` method to retrieve all burn transactions from any address.
    - The retrieved burn transactions are then processed in chunks using the `process_transactions_in_chunks` function, which inserts the transactions into the database.
    - After processing the burn transactions, the function checks if block hash records exist in the database. If not, it calls the `fetch_and_insert_block_hashes` function to retrieve and store block hashes for the entire blockchain.
3. `fetch_and_insert_block_hashes`:
    - This function fetches and inserts block hashes into the database for a specified range of block heights.
    - It uses the `rpc_connection.getblockhash` method to retrieve block hashes in batches.
    - The retrieved block hashes are processed to construct potential block insert tuples, including the previous and next block hashes if available.
    - The function filters out duplicate heights within the batch and checks the database for existing block heights to avoid inserting duplicates.
    - The final block hashes to be inserted are passed to the `bulk_insert_block_hashes` function for bulk insertion into the database.
    - The function logs progress after inserting a certain number of block hashes (e.g., every 500 inserts) and continues processing the next range of block heights.
4. `bulk_insert_block_hashes`:
    - This function performs the bulk insertion of block hashes into the database.
    - It rechecks the existing block heights just before inserting to handle potential race conditions.
    - The function creates new `BlockHash` instances for the block hashes that don't already exist in the database.
    - It adds the new block hashes to the database session and attempts to commit the changes.
    - If an exception occurs during the bulk insert, the changes are rolled back, and an error is logged.
    

By using these functions together, the code can handle chain reorgs and rollbacks effectively:


- The `detect_chain_reorg_and_rescan` function continuously monitors for chain reorgs by comparing the stored block hashes with the actual block hashes in the blockchain.
- When a chain reorg is detected, the function triggers a full rescan of burn transactions and block hashes by calling the `full_rescan_burn_transactions` function.
- The `full_rescan_burn_transactions` function retrieves all burn transactions from the genesis block and processes them in chunks, inserting them into the database.
- If block hash records are missing, the `fetch_and_insert_block_hashes` function is called to retrieve and store block hashes for the entire blockchain.
- The `fetch_and_insert_block_hashes` function fetches block hashes in batches, filters out duplicates, and passes them to the `bulk_insert_block_hashes` function for bulk insertion into the database.

By performing a full rescan of burn transactions and block hashes when a chain reorg is detected, the code ensures that the stored data is consistent with the current state of the blockchain, effectively handling chain reorgs and rollbacks. Also, as we will see in the next section, these background tasks cache certain information about burn transactions in the local database that are critical for efficiently computing the remaining available balance on a credit pack ticket, so that Supernodes don’t accept inference requests from credit pack tickets that don’t have enough credit value left on them to cover the full cost of the inference request. 

OK, we are now ready to discuss how Inference Requests work and how they interact with credit pack tickets in Pastel.


----------
## Inference Requests

We already gave a pretty detailed background on how inference requests work in the abstract, and in particular, how the system tracks the usage of credits from credit packs in order to determine the remaining available balance of each credit pack (again, only the credit pack’s initial balance amount is stored in the original blockchain ticket— all the information for tracking the current remaining available credit balance must be dynamically computed by the Supernodes by keeping track of all the various burn transactions sent from the designated PSL tracking address that is specified in the original credit pack ticket to determine how many credits have already been used from that specific credit pack ticket). 

We now introduce the detailed flow involved in an end user creating a new inference request and how that request is fulfilled by the responding_supernode:

**Inference Request Flow:**

1. **Initiating the Inference Request:**
    - The user creates a new inference request by sending a POST request to the `/make_inference_api_usage_request` endpoint with an `InferenceAPIUsageRequest` message, which looks like this:

```python
    class InferenceAPIUsageRequest(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        inference_request_id: str = Field(unique=True, index=True)
        requesting_pastelid: str = Field(index=True)
        credit_pack_ticket_pastel_txid: str = Field(index=True)
        requested_model_canonical_string: str
        model_inference_type_string: str
        model_parameters_json_b64: str
        model_input_data_json_b64: str
        inference_request_utc_iso_string: str
        inference_request_pastel_block_height: int
        status: str = Field(index=True)
        inference_request_message_version_string: str
        sha3_256_hash_of_inference_request_fields: str
        requesting_pastelid_signature_on_request_hash: str
```

    - This message contains essential information such as the user's PastelID, the credit pack ticket TXID, the requested model, inference type, model parameters, input data, and various other details.
    - To avoid any issues with quoted JSON or escaping text, all model inputs and all model parameters are supplied as base64 encoded JSON. This also provides flexibility to work with binary inputs, such as input images for the “Ask a Question About an Image” inference requests.
    - The user signs the hash of the inference request fields using their PastelID to ensure the integrity and authenticity of the request.
    - The inclusion of the credit pack ticket TXID allows the responding supernode to verify that the user has sufficient credits to cover the cost of the inference request.
    
2. **Processing the Inference Request:**
    - The responding supernode, determined by the XOR distance between the user's PastelID and the supernode's PastelID, receives the inference request.
    - The supernode first validates the request by checking the signature, ensuring that the request comes from the claimed PastelID and that the data hasn't been tampered with.
    - Next, the supernode checks if it supports the requested model and inference type. This is crucial because different supernodes may have different capabilities and support different models/services.
    - If the supernode supports the requested model, it then ensures that the provided input data matches the expected format for that specific model/service. For example, if the inference request is for text completion, the supernode verifies that the input is indeed a text prompt and not an image file.
    - The supernode then calculates the cost of the inference request based on the specific details provided. This cost calculation varies depending on the type of inference:
        - For text completion requests, the supernode uses the relevant tokenizer for the requested model/service to count the number of tokens in the input prompt. The cost is determined based on the token count and the specific pricing implemented by the API provider in the case of service based offerings; a target “profit margin” is added on top of this to determine the cost of the inference request in credit pack credits. 
        - In the case of models that are hosted locally or remotely using Swiss Army Llama, where there is no API based pricing, we estimate the cost using parameters stored in the model_menu.json file particular to each model that takes into account the memory and processing load of running the model. Eventually, when we have gathered more operating data, we will convert these into estimates of total time used on a GPU enabled instance, and use the average live pricing level for a service like vast.ai to come up with more accurate pricing that covers the true underlying cost of using each model locally or remotely.
        - For audio transcription and embedding requests, the cost is calculated based on the duration of the audio file supplied by the user, measured in seconds.
        - For image generation requests, the relevant API based pricing is again used to arrive at an estimated cost in credits that should cover the Supernode owner’s operating expenses from using the relevant API for that request.
        - For “Ask a Question About an Image” requests, the relevant pricing for the API is again used, taking into account the actual resolution of the input image. 
    - Once the cost is determined, the supernode generates an `InferenceAPIUsageResponse` message, which includes the proposed cost in inference credits, the remaining credits in the user's credit pack after processing the request, and other relevant details, which looks like this:

```python
    class InferenceAPIUsageResponse(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        inference_response_id: str = Field(unique=True, index=True)
        inference_request_id: str = Field(foreign_key="inferenceapiusagerequest.inference_request_id", index=True)
        proposed_cost_of_request_in_inference_credits: float
        remaining_credits_in_pack_after_request_processed: float
        credit_usage_tracking_psl_address: str = Field(index=True)
        request_confirmation_message_amount_in_patoshis: int
        max_block_height_to_include_confirmation_transaction: int
        inference_request_response_utc_iso_string: str
        inference_request_response_pastel_block_height: int
        inference_request_response_message_version_string: str    
        sha3_256_hash_of_inference_request_response_fields: str
        supernode_pastelid_and_signature_on_inference_request_response_hash: str
    ```

    - The supernode sends this response back to the user, providing them with the cost estimate for their inference request.
    
2. **Broadcasting the Inference Request and Response:**
    - After sending the `InferenceAPIUsageResponse` back to the user, the responding supernode broadcasts a combined message containing both the `InferenceAPIUsageRequest` and `InferenceAPIUsageResponse` to the nearest supernodes based on the XOR distance to the user's PastelID.
    - This broadcast ensures that multiple supernodes are aware of the inference request and can assist in processing it if needed, providing redundancy and fault tolerance.
    - The broadcast message includes the original inference request details, the proposed cost, and the remaining credits in the user's credit pack.
    
3. **Confirming the Inference Request:**
    - Upon receiving the `InferenceAPIUsageResponse` from the responding supernode, the user reviews the proposed cost and decides whether to proceed with the inference request.
    - If the user agrees to the cost, they confirm the inference request by first sending the corresponding number Patoshis from the designated PSL tracking address to the Pastel burn address; once they do that and get a TXID for that burn transactions, they send a POST request to the `/confirm_inference_request` endpoint with an `InferenceConfirmation` message:

```python
    class InferenceConfirmation(SQLModel):
        inference_request_id: str
        requesting_pastelid: str
        confirmation_transaction: dict
```

    - The `InferenceConfirmation` message includes the inference request ID, the user's PastelID, and a confirmation transaction.
    - The confirmation transaction serves as proof that the user has agreed to the proposed cost and has authorized the deduction of the required inference credits from their credit pack. Since only the authorized creator of the credit pack would have control over the tracking address, everyone knows for sure that the request originated with the user for all intents and purposes (ignoring cases where the user got hacked, for example). 
    - Note that multiple different PastelIDs (the ones specifically listed in the original credit pack ticket data) can create and authorize inference requests from the credit pack ticket; this means that multiple users (each with their own PastelID) could in theory share a single credit pack and easily keep track of who has used what in terms of credits from the pack, but each of these would need to have the private key of the tracking address imported into their local wallet. 
    - However, since only a very tiny amount of PSL is required for the tracking transactions, this address could contain, say, 10 PSL or less, so there would be very little risk in sharing the private key, since there isn’t much PSL value to steal. Critically, even if a third party were able to get the private key to the tracking address, they still wouldn’t be able to “steal” inference requests using it unless they had ALSO stolen the PastelID private keys for one of the PastelIDs included in the list of authorized PastelIDs for that specific credit pack.
    
4. **Executing the Inference Request:**
    - Once the `responding_supernode` receives the `InferenceConfirmation` from the user, it proceeds to process and execute the inference request.
    - The Supernode verifies the confirmation transaction to ensure that the user has authorized the deduction of the inference credits.
    - If the confirmation is valid, the Supernode begins executing the inference request using the specified model and parameters.
    - The execution process varies depending on the type of inference; for example:
        - For text completion requests, the Supernode feeds the input prompt to the specified language model or service using the (optional) user-supplied parameters (e.g., number of completions; number of tokens in each completion; sampling temperature) and generates the completion text based on the model's output.
        - For image generation requests, the Supernode feeds the image prompt and specific user parameters to the image generation model (e.g., Stable Diffusion) and generates an output image to be sent back to the user.
        - For “Ask a Question About an Image” requests, the user supplies an input image and question string, and the Supernode sends that to the multi-modal model (e.g., GPT4o, Llava, etc.) and returns the output response text to the user.
        - For document embedding requests, the supernode uses the specified embedding model to generate vector representations of the extracted sentences in the input document, which can be extracted from various common document formats such as PDF, DOC, HTML, TXT, and even scanned images and PDF using OCR. The user can also optionally supply a semantic query string which will be used to search across the embeddings, returning the most relevant parts of the document text.
        - For audio transcription requests, the Supernode uses the specified audio transcription model to convert the provided audio file into written text and optionally computes embedding vectors for the transcribed text using the specified LLM for calculating the embeddings. The user can also optionally supply a semantic query string which will be used to search across the embeddings, returning the most relevant parts of the transcribed text.
    - During the execution process, the Supernode may need to communicate with external APIs or services, depending on the requested model/service.
    - The Supernode monitors the progress of the inference execution and handles any errors or exceptions that may occur.
    
5. **Generating the Inference Output Result:**
    - Once the inference execution is completed, the `responding_supernode` generates an `InferenceAPIOutputResult` message, which looks like this:

```python
    class InferenceAPIOutputResult(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        inference_result_id: str = Field(unique=True, index=True)
        inference_request_id: str = Field(foreign_key="inferenceapiusagerequest.inference_request_id", index=True)
        inference_response_id: str = Field(foreign_key="inferenceapiusageresponse.inference_response_id", index=True)
        responding_supernode_pastelid: str = Field(index=True)
        inference_result_json_base64: str
        inference_result_file_type_strings: str
        inference_result_utc_iso_string: str
        inference_result_pastel_block_height: int
        inference_result_message_version_string: str    
        sha3_256_hash_of_inference_result_fields: str    
        responding_supernode_signature_on_inference_result_id: str
```

    - This message contains the inference result ID, the original inference request ID, the inference response ID, the `responding_supernode`'s PastelID, and the actual inference output data.
    - The inference output data is serialized and stored in the `inference_result_json_base64` field as a base64-encoded JSON string.
    - The Supernode also includes the file type of the inference output (e.g., JSON, text, image) in the `inference_result_file_type_strings` field.
    - The Supernode signs the hash of the inference result fields using its PastelID to ensure the integrity and authenticity of the output.
    
6. **Checking the Status of Inference Request Results:**
    - The user can check the status of their inference request by sending a GET request to the `/check_status_of_inference_request_results/{inference_response_id}` endpoint.
    - The endpoint expects the `inference_response_id` as a path parameter, which uniquely identifies the inference response associated with the user's request.
    - The `responding_supernode` receives the status check request and verifies that the `inference_response_id` exists and is associated with the requesting user.
    - If the inference request is still being processed, the Supernode responds with a status indicating that the results are not yet available.
    - If the inference request has been completed, the Supernode responds with a status indicating that the results are ready for retrieval.
    
7. **Retrieving the Inference Output Results:**
    - When the inference results are available, the user can retrieve them by sending a POST request to the `/retrieve_inference_output_results` endpoint.
    - The user provides the `inference_response_id`, their PastelID, and a challenge-response signature for authentication purposes.
    - The `responding_supernode` verifies the user's authentication by checking the provided PastelID and challenge-response signature.
    - If the authentication is successful, the Supernode retrieves the `InferenceAPIOutputResult` associated with the provided `inference_response_id`.
    - The Supernode sends the `InferenceAPIOutputResult` back to the user, allowing them to access the inference output data.
    - Additionally, the Supernode broadcasts the `InferenceAPIOutputResult` to the nearest Supernodes based on the XOR distance to the user's PastelID.
    - This broadcast ensures that multiple Supernodes have a record of the inference output and can provide the results to the user if the original `responding_supernode` becomes unavailable.
    
8. **Auditing the Inference Request Response:**
    - The user has the option to audit the inference request response by sending a POST request to the `/audit_inference_request_response` endpoint.
    - The user provides the `inference_response_id`, their PastelID, and a signature to prove their identity and authorization.
    - The `responding_supernode` verifies the user's signature and checks if the provided PastelID matches the one associated with the inference request.
    - If the authentication and authorization are successful, the Supernode retrieves the `InferenceAPIUsageResponse` associated with the provided `inference_response_id`.
    - The Supernode returns the `InferenceAPIUsageResponse` to the user, allowing them to review the details of the inference request response, such as the proposed cost and remaining credits.
    - This audit process provides transparency and allows the user to verify that the inference request was processed correctly and that the appropriate amount of credits were deducted from their credit pack.
    
9. **Auditing the Inference Request Result:**
    - Similar to auditing the inference request response, the user can also audit the inference request result by sending a POST request to the `/audit_inference_request_result` endpoint.
    - The user provides the `inference_response_id`, their PastelID, and a signature for authentication and authorization.
    - The `responding_supernode` verifies the user's signature and checks if the provided PastelID matches the one associated with the inference request.
    - If the authentication and authorization are successful, the supernode retrieves the `InferenceAPIOutputResult` associated with the provided `inference_response_id`.
    - The Supernode returns the `InferenceAPIOutputResult` to the user, allowing them to review the details of the inference output, such as the actual inference result data and file type.
    - This audit process ensures that the user can verify the correctness and integrity of the inference output received from the Supernode.
    

**Additional Endpoints:**

- `/get_inference_model_menu`:
    - This endpoint allows users to retrieve information about the available inference models and their parameters.
    - The `responding_supernode` maintains a menu of supported models and their corresponding parameters, such as input formats, output formats, and any specific options or configurations.
    - When a user sends a GET request to this endpoint, the Supernode responds with the inference model menu, providing the user with the necessary information to construct a valid inference request.
    - The model menu helps users understand the capabilities and requirements of each supported model, enabling them to select the appropriate model for their specific inference needs.
    
- `/download/{file_name}`:
    - This endpoint facilitates the download of inference result files, particularly when the inference output is not easily representable as a JSON string (e.g., generated images or audio files).
    - When an inference request results in a file output, the `responding_supernode` stores the file temporarily and provides a unique file name to the user as part of the `InferenceAPIOutputResult`.
    - The user can then send a GET request to the `/download/{file_name}` endpoint, specifying the file name they received.
    - The Supernode verifies that the requested file exists and is associated with the user's inference request.
    - If the file is found and the user is authorized to access it, the Supernode initiates a file download response, allowing the user to retrieve the inference output file.
    - The Supernode may implement additional security measures, such as file expiration or authentication, to ensure that only authorized users can download the inference output files.


----------

Now let’s get into more details about how this functionality is actually implemented in the Inference Layer server code.  We will introduce a category of inference related service functions, and then give detailed breakdowns of the names of these functions and how they work (inputs, outputs, and purpose/rationale). The code for all these functions can be found [here](https://github.com/pastelnetwork/python_inference_layer_server/blob/master/service_functions.py).


**Messaging Related Service Functions:**

These functions use the built-in Masternode messaging system that is already part of pasteld, and build additional abstractions on top of these so we can use PastelIDs as the basis of identity and security/signing. These messages are used internally by the Supernodes for helping with the flows for both credit pack creation and inference request processing, but we also expose “user oriented” messaging functionality that permits any Pastel user (i.e., not just a Supernode) to use their PastelID as a sort of “email address” for sending and receiving validated and signed messages (that is, users can know for sure that a given message really came from the specified PastelID sender and that the message wasn’t modified in transmit by one or more Supernodes; it does so by checking the PastelID signature on the signed message hash). In this scenario, the Supernodes (which are the only nodes that are able to actually send and receive the lower level messages using the built-in masternode messaging system) act as a kind of “[SMTP](https://en.wikipedia.org/wiki/Simple_Mail_Transfer_Protocol)” mail server on behalf of regular Pastel users.

Note that messages are not currently encrypted, so any Supernode can see the contents of a message sent between two ordinary users. However, it should be relatively simple to add some form of encryption using the pastelIDs of the users to automatically create shared secrets between pairs of users (this is on the list of future features). 

There are several functions crucial for handling inference requests and processing broadcast messages in the Inference Layer server. Let's go through each function and explain how they tie into the various steps of the inference request flow:


- **`broadcast_message_to_n_closest_supernodes_to_given_pastelid`**
    - This function is responsible for broadcasting a message to the closest supernodes based on a given PastelID. It takes the input PastelID, message body, and message type as parameters. First, it retrieves the list of supernodes and filters them based on their support for the desired model using the `is_model_supported` function. It excludes the local supernode from the list of supported supernodes. If no other supported supernodes are found, it falls back to using all supernodes except the local one. The function then selects the N closest supernodes to the input PastelID from the supported supernodes list. It signs the message using the local supernode's PastelID and broadcasts it to the selected supernodes. This function is used in the "Broadcasting the Inference Request and Response" step to inform nearby supernodes about the inference request and response.
    
- **`process_broadcast_messages`**
    This function processes broadcast messages received by the supernode. It takes the message and a database session as input. The function first checks the message type to determine how to handle it.
    For `inference_request_response_announcement_message` messages:
    - It checks if the corresponding `InferenceAPIUsageRequest` and `InferenceAPIUsageResponse` records already exist in the database.
    - If neither record exists, it creates new entries for both the request and response in the database.
    - If either record already exists, it skips the insertion to avoid duplicates.
    For `inference_request_result_announcement_message` messages:
    - It checks if the corresponding `InferenceAPIOutputResult` record already exists in the database.
    - If the record doesn't exist, it creates a new entry for the output result in the database.
    - If the record already exists, it skips the insertion to avoid duplicates.
    This function is used to process and store the information received from broadcast messages related to inference requests and results.
    
- **`monitor_new_messages`**
    This function continuously monitors for new messages received by the Supernode. It runs in an infinite loop, periodically checking for new messages. It retrieves the last processed timestamp from the database to determine the starting point for processing new messages. The function fetches new messages from the masternode messaging system using the `list_sn_messages_func`. For each new message, it checks if the message already exists in the database to avoid duplicates. If the message is new, it updates various metadata tables in the database, including:
    - `MessageSenderMetadata`: Tracks the total messages sent and data sent by each sending Supernode.
    - `MessageReceiverMetadata`: Tracks the total messages received and data received by each receiving Supernode.
    - `MessageSenderReceiverMetadata`: Tracks the total messages and data exchanged between each pair of sending and receiving Supernodes.
    - `MessageMetadata`: Tracks the overall total messages, senders, and receivers in the system.
    After updating the metadata, the function processes the new messages concurrently using the `process_broadcast_messages` function. This function ensures that the Supernode stays up to date with new messages and processes them accordingly, tying into the overall flow of handling inference requests and responses.

**Corresponding SQLModel Data Models**
The data models used to support these functions are defined using SQLModel, which combines SQLAlchemy ORM models with Pydantic response models. Some of the key models include:


- `Message` - Represents a message exchanged between Supernodes:

```python
    class Message(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        sending_sn_pastelid: str = Field(index=True)
        receiving_sn_pastelid: str = Field(index=True)
        sending_sn_txid_vout: str = Field(index=True)
        receiving_sn_txid_vout: str = Field(index=True)
        message_type: str = Field(index=True)
        message_body: str = Field(sa_column=Column(JSON))
        signature: str
        timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
```

- `MessageMetadata` - Tracks the overall total messages, senders, and receivers in the system:

```python
    class MessageMetadata(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        total_messages: int
        total_senders: int
        total_receivers: int
        timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
```

- `MessageSenderMetadata` - Tracks the total messages sent and data sent by each sending supernode:

```python
    class MessageSenderMetadata(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        sending_sn_pastelid: str = Field(index=True)
        sending_sn_txid_vout: str = Field(index=True)
        sending_sn_pubkey: str = Field(index=True)
        total_messages_sent: int
        total_data_sent_bytes: float
        timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
```

- `MessageReceiverMetadata` - Tracks the total messages received and data received by each receiving supernode:

```python
    class MessageReceiverMetadata(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        receiving_sn_pastelid: str = Field(index=True)
        receiving_sn_txid_vout: str = Field(index=True)
        total_messages_received: int
        total_data_received_bytes: float
        timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
```

- `MessageSenderReceiverMetadata` - Tracks the total messages and data exchanged between each pair of sending and receiving Supernodes:

```python
    class MessageSenderReceiverMetadata(SQLModel, table=True):
        id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
        sending_sn_pastelid: str = Field(index=True)
        receiving_sn_pastelid: str = Field(index=True)
        total_messages: int
        total_data_bytes: float
        timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
```

These models work together with the functions to facilitate the processing of inference requests, broadcasting of messages, and handling of received broadcast messages in the Inference Layer server. They enable the Supernodes to communicate and coordinate effectively, ensuring that inference requests are properly processed and the necessary information is stored and tracked in the database.


----------

**Model Menu and API Based Service Related Functions:** 

The next group of service functions we will review are related to how Supernodes determine which models they themselves support, and which ones are supported by other Supernodes. It also includes functions which are used by the Supernodes to automatically test and verify API keys for use with API based services such as OpenAI, Stability, Groq, Mistral, etc. These functions include the following:

- **`is_model_supported`**:
    - This function checks if a desired model is supported by a Supernode. It takes the model menu, desired model canonical string, desired model inference type string, and desired model parameters JSON as input. 
    - The function compares the desired model information with the models available in the model menu, ensuring that the desired model canonical string matches any of the models in the menu with a similarity threshold of 95% using fuzzy matching (via the [FuzzyWuzzy](https://pypi.org/project/fuzzywuzzy/) library). 
    - If a match is found, it further checks if the desired inference type string is supported by the matched model and verifies that the desired model parameters are valid according to the parameter specifications in the model menu. 
    - This function is used in the "Processing the Inference Request" step to ensure that the `responding_supernode` supports the requested model and inference type.


- **`get_inference_model_menu`**:
    - This function is responsible for retrieving the inference model menu, which contains information about the available models and their supported parameters.
    - It first loads the API key test results from a file using the `load_api_key_tests` function.
    - It then fetches the latest model menu from a GitHub URL using an asynchronous HTTP client.
    - The function filters the model menu based on the availability of valid API keys for each model provider (e.g., Stability, OpenAI, Mistral, Groq, Anthropic, OpenRouter).
    - It checks the validity of each API key using the `is_api_key_valid` function, which either retrieves the test result from the loaded API key tests or runs a new test using the `run_api_key_test` function.
    - Models that don't require API keys are automatically included in the filtered model menu.
    - The filtered model menu is saved locally as a JSON file.
    - Finally, the function returns the filtered model menu.
    - This function is used in the "Processing the Inference Request" step to provide the available models and their parameters to the `is_model_supported` function.
    
- **`load_api_key_tests`**:
    - This function loads the API key test results from a JSON file.
    - It reads the contents of the file and returns the loaded JSON data.
    - If the file is not found, it returns an empty dictionary.
    - This function is used by `get_inference_model_menu` to load the existing API key test results.
    
- **`save_api_key_tests`**:
    - This function saves the API key test results to a JSON file.
    - It takes the `api_key_tests` dictionary as input and writes it to the specified file path.
    - This function is used by `get_inference_model_menu` to save the updated API key test results after running new tests.


- **`is_api_key_valid`**:
    - This function checks the validity of an API key for a specific API provider.
    - It takes the `api_name` and `api_key_tests` dictionary as input.
    - If the API name is not found in the `api_key_tests` dictionary or the existing test result is outdated (based on the `is_test_result_valid` function), it runs a new API key test using the `run_api_key_test` function.
    - If the test passes, it updates the `api_key_tests` dictionary with the new test result and timestamp.
    - It returns the validity status of the API key (True or False).
    - This function is used by `get_inference_model_menu` to determine which models should be included in the filtered model menu based on the availability of valid API keys.
    
- **`is_test_result_valid`**:
    - This function checks if an API key test result is still valid based on a specified validity duration.
    - It takes the `test_timestamp` as input and compares it with the current timestamp.
    - If the difference between the current timestamp and the test timestamp is less than the specified validity duration (e.g., 24 hours), it considers the test result as valid.
    - This function is used by `is_api_key_valid` to determine if a new API key test needs to be run.
    
- **`run_api_key_test`**:
    - This function runs an API key test for a specific API provider.
    - It takes the `api_name` as input and calls the corresponding test function based on the API name.
    - It supports testing API keys for Stability, OpenAI, Mistral, Groq, Anthropic (Claude), and OpenRouter.
    - Each test function sends a test request to the respective API endpoint using the provided API key and checks the response status or content to determine if the API key is valid.
    - If the test passes, it returns True; otherwise, it returns False.
    - This function is used by `is_api_key_valid` to run a new API key test when needed.
    

These functions work together to manage the inference model menu and ensure that only models with valid API keys are included. The `get_inference_model_menu` function is a key part of the "Processing the Inference Request" step, where it provides the available models and their parameters to the `is_model_supported` function. By regularly testing the API keys and filtering the model menu accordingly, the system ensures that inference requests are only processed for models with valid API keys, preventing potential errors or failures during the execution of the inference request.

It also allows Supernodes to accurately advertise the specific models and services they support. This allows the Inference Layer to provide a lot more options to users without introducing necessary centralization, since Supernode Operators are free to decide which API based services, if any, they want to support with their own Supernodes; if they do want to support a particular API service, such as *OpenAI* or *Groq*, then it’s totally up to the Supernode operators themselves to procure a valid API key and include it in their .env file in encrypted form. The result of all this are exposed by each Supernode at the following endpoint:

```python
    @router.get("/get_inference_model_menu")
    async def get_inference_model_menu_endpoint(
        rpc_connection=Depends(get_rpc_connection),
    ):
        model_menu = await service_functions.get_inference_model_menu()
        return model_menu 
```

which sends back the filtered model menu after removing any entries in the master model_menu.json that the particular Supernode doesn’t support. For instance, if that particular Supernode is controlled by a Supernode operator who never bothered to get a Stability API key, then the returned model menu from that Supernode will not include the “text_to_image” inference type and associated Stability models; if an end user wants to use that inference type, they will need to select a different Supernode as their closest model-supporting `responding_supernode`.

To really understand how the `model_menu.json`  file works and its importance in the wider system, we need to go into a lot more detail. For starters, here is what the beginning of this file looks like:

```JSON
    {
      "models": [
        {
          "model_name": "swiss_army_llama-Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M",
          "model_url": "https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf",
          "description": "Hermes 2 Pro is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house. This version excels at Function Calling, JSON Structured Outputs, and has improved on several other metrics, scoring 90% on function calling evaluation and 84% on structured JSON Output evaluation.",
          "supported_inference_type_strings": [
            "text_completion",
            "embedding",
            "embedding_document",
            "embedding_audio"
          ],
          "input_fields": [
            {
              "name": "input_prompt",
              "file_type": "text",
              "optional": false
            }
          ],
          "output_fields": [
            {
              "name": "generated_text",
              "file_type": "text",
              "optional": false
            }
          ],
          "model_parameters": [
            {
              "name": "number_of_tokens_to_generate",
              "type": "int",
              "default": 1000,
              "inference_types_parameter_applies_to": "['text_completion']",
              "description": "The maximum number of tokens to generate [Optional]"
            },
            {
              "name": "temperature",
              "type": "float",
              "default": 0.7,
              "inference_types_parameter_applies_to": "['text_completion']",
              "description": "The temperature for sampling [Optional]"
            },
            {
              "name": "number_of_completions_to_generate [Optional]",
              "type": "int",
              "default": 1,
              "inference_types_parameter_applies_to": "['text_completion']",
              "description": "The number of completions to generate"
            },
            {
              "name": "grammar_file_string",
              "type": "string",
              "default": "",
              "inference_types_parameter_applies_to": "['text_completion']",
              "description": "Grammar File String: The grammar file used to restrict text generation [Optional] (Default is to not use any grammar file. Examples: `json`, `list`)"
            },
            {
              "name": "corpus_identifier_string",
              "type": "string",
              "default": "",
              "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
              "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
            }
          ],
          "credit_costs": {
            "text_completion": {
              "input_tokens": 1.8,
              "output_tokens": 1.4,
              "compute_cost": 1.0,
              "memory_cost": 0.9
            },
            "embedding": {
              "input_tokens": 0.7,
              "compute_cost": 0.8,
              "memory_cost": 0.5
            },
            "embedding_document": {
              "average_tokens_per_sentence": 0.2,
              "total_sentences": 0.1,
              "query_string_included": 0.4,
              "compute_cost": 1.2,
              "memory_cost": 0.8
            },
            "embedding_audio": {
              "audio_file_length_in_seconds": 0.5,
              "query_string_included": 1.5,
              "compute_cost": 1.2,
              "memory_cost": 1.0
            }
          }
        },
```
Compare the above entry for a “locally hosted” Swiss Army Llama based LLM to the entry below for an API service based model, which has fewer options to specify and which uses a very different approach to calculating the cost of a specific inference request (the API service based models try to estimate the actual cost to the Supernode operator of making that particular inference request, whereas the Swiss Army Llama based models instead try to quantify the compute/memory usage of a particular inference request):

```JSON
        {
          "model_name": "openai-gpt-4o",
          "model_url": "",
          "description": "GPT-4o is OpenAI's most advanced multimodal model that\u2019s faster and cheaper than GPT-4 Turbo with stronger vision capabilities. The model has 128K context and an October 2023 knowledge cutoff.",
          "supported_inference_type_strings": [
            "text_completion"
          ],
          "input_fields": [
            {
              "name": "input_prompt",
              "file_type": "text",
              "optional": true
            }
          ],
          "output_fields": [
            {
              "name": "generated_text",
              "file_type": "text",
              "optional": false
            }
          ],
          "model_parameters": [
            {
              "name": "number_of_tokens_to_generate",
              "type": "int",
              "default": 1000,
              "inference_types_parameter_applies_to": "['text_completion']",
              "description": "The maximum number of tokens to generate [Optional]"
            },
            {
              "name": "temperature",
              "type": "float",
              "default": 0.7,
              "inference_types_parameter_applies_to": "['text_completion']",
              "description": "The temperature for sampling [Optional]"
            },
            {
              "name": "number_of_completions_to_generate [Optional]",
              "type": "int",
              "default": 1,
              "inference_types_parameter_applies_to": "['text_completion']",
              "description": "The number of completions to generate"
            }
          ],
          "credit_costs": {
            "api_based_pricing": 1
          }
        }, 
```

Or for an image generation (“text_to_image” inference type) model’s entry, which specifies wildly different input parameters and output type (a binary image encoded as base64 text rather than a plain text output like you would get from an LLM model doing a text_completion inference request):

```JSON
     {
          "model_name": "stability-core",
          "model_url": "",
          "description": "Stable Image Core is the best quality achievable at high speed for text-to-image generation.",
          "supported_inference_type_strings": [
            "text_to_image",
            "creative_upscale"
          ],
          "input_fields": [
            {
              "name": "prompt",
              "file_type": "text",
              "optional": false
            }
          ],
          "output_fields": [
            {
              "name": "generated_image",
              "file_type": "base64_image",
              "optional": false
            }
          ],
          "model_parameters": [
            {
              "name": "aspect_ratio",
              "type": "string",
              "default": "1:1",
              "inference_types_parameter_applies_to": "['text_to_image']",
              "description": "The aspect ratio of the generated image [Optional]"
            },
            {
              "name": "seed",
              "type": "int",
              "default": 0,
              "inference_types_parameter_applies_to": "['text_to_image']",
              "description": "Random seed to use for generation [Optional]"
            },
            {
              "name": "style_preset",
              "type": "string",
              "default": null,
              "inference_types_parameter_applies_to": "['text_to_image']",
              "description": "Style Preset: Guides the image model towards a particular style (e.g., '3d-model', 'analog-film', 'anime', etc.) [Optional]"
            },
            {
              "name": "output_format",
              "type": "string",
              "default": "jpeg",
              "inference_types_parameter_applies_to": "['text_to_image']",
              "description": "The format of the generated image (e.g., 'png', 'jpeg', 'webp', etc.) [Optional]"
            },
            {
              "name": "negative_prompt",
              "type": "string",
              "default": null,
              "inference_types_parameter_applies_to": "['text_to_image']",
              "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
            }
          ],
          "credit_costs": {
            "api_based_pricing": 1
          }
        },
```

As you can probably see from the examples, the structure is quite generic and covers all the functionality we might need from any kind of model, with clear methods indicated for how to expand the coverage to new and different inference types, services, and models. The `model_menu.json` file plays a crucial role in the Inference Layer server code. It serves as a central configuration file that defines the available models, their capabilities, and the parameters they support for different inference types. The file is designed to be flexible and extensible, allowing easy addition of new models and services without requiring significant changes to the codebase.

Let's take a detailed look at the structure of the `model_menu.json` file and explain the rationale behind its design:


1. The root of the JSON file contains a single key, "models", which is an array of model objects.


2. Each model object represents a specific model or service that the Inference Layer supports. It contains various properties that describe the model's characteristics and capabilities. The key properties of a model object are:
    - `model_name`: A unique identifier for the model, used throughout the system to refer to the specific model.
    - `model_url`: The URL or path to the model file, if applicable. This is used for locally hosted models using Swiss Army Llama.
    - `description`: A brief description of the model, providing information about its capabilities, training data, or any other relevant details.
    - `supported_inference_type_strings`: An array of strings indicating the inference types supported by the model. This allows the system to determine which models can be used for a particular inference request.
    - `input_fields`: An array of objects specifying the input fields required by the model for each inference type. Each input field object contains the following properties:
        - `name`: The name of the input field.
        - `file_type`: The type of data expected for the input field (e.g., text, base64, etc.).
        - `optional`: A boolean indicating whether the input field is optional or required.
    - `output_fields`: An array of objects specifying the output fields returned by the model for each inference type. Each output field object contains the following properties:
        - `name`: The name of the output field.
        - `file_type`: The type of data returned in the output field (e.g., text, base64_image, etc.).
        - `optional`: A boolean indicating whether the output field is optional or guaranteed to be present.
    - `model_parameters`: An array of objects defining the parameters supported by the model for each inference type. Each parameter object contains the following properties:
        - `name`: The name of the parameter.
        - `type`: The data type of the parameter (e.g., int, float, string, etc.).
        - `default`: The default value for the parameter, if applicable.
        - `inference_types_parameter_applies_to`: An array of inference types to which the parameter applies. This allows for parameters to be specific to certain inference types.
        - `description`: A description of the parameter, providing information about its purpose and usage.
    - `credit_costs`: An object that defines the credit costs associated with using the model for different inference types. It can contain either specific cost breakdowns for each inference type or a flag indicating that the model uses API-based pricing.
    

The rationale behind this design is to provide a declarative and data-driven approach to configuring the available models and their capabilities. By storing all the necessary information about models in the `model_menu.json` file, the Inference Layer server code can dynamically adapt to changes in the model lineup without requiring code modifications.

One of the key benefits of this design is that it enables the automatic generation of UI elements in the visual frontend for each model and inference type. The frontend code can iterate over the models defined in the `model_menu.json` file and dynamically create the appropriate input fields, parameter controls, and output displays based on the specifications provided in the JSON file.
For example, when a user selects a specific model and inference type in the frontend, the UI can be dynamically populated with the relevant input fields and parameter controls as defined in the `model_menu.json` file. This eliminates the need for hard-coded UI elements and allows for a highly flexible and adaptable user interface.

Furthermore, the `model_menu.json` file serves as a single source of truth for model configuration. It provides a clear and centralized location for defining the available models, their capabilities, and the parameters they support. This makes it easier to manage and update the model lineup, as well as maintain consistency across different parts of the system.



----------

**Inference Request Cost Estimation Service Functions**

The next batch of functions pertain to estimating the cost of performing specific inference requests depending on the nature of the request and specified models and model parameters. Let's go through each function and explain their purpose and how they contribute to the cost estimation process.


1. **`get_tokenizer`** **and** **`count_tokens`**:
- The `get_tokenizer` function maps model names to their corresponding tokenizer names using a predefined mapping. It uses fuzzy string matching to find the best match for the given model name and returns the appropriate tokenizer name.
- The `count_tokens` function takes a model name and input data as parameters and counts the number of tokens in the input data using the appropriate tokenizer for the specified model.
- It handles different types of tokenizers, such as GPT-2, GPT-Neo, BERT, RoBERTa, and specific tokenizers for models like Claude, Whisper, and CLIP.
- The token count is important for estimating the cost of inference requests, as many pricing models are based on the number of input and output tokens.


2. **`calculate_api_cost`**:
- This function calculates the estimated cost of an inference request based on the pricing data for each API service and model.
- It takes the model name, input data, and model parameters as input and uses fuzzy string matching to find the best match for the model name in the predefined pricing data.
- For Stability models, the cost is calculated based on the credits per call and the number of completions to generate.
- For OpenAI's GPT-4o-vision model, the cost is calculated based on the input image resolution, number of tiles, and tokens, as well as the question tokens.
- For other models, the cost is calculated based on the number of input tokens, output tokens to generate, number of completions, and per-call cost.
- The function returns the estimated cost in dollars.


3. **`convert_document_to_sentences`**:
- This function converts a document file to sentences using the Swiss Army Llama service.
- It checks if either the local or remote Swiss Army Llama service is responding and selects the appropriate port.
- It uploads the document file and retrieves the file metadata (URL, hash, size) using the `upload_and_get_file_metadata` function.
- It sends a POST request to the Swiss Army Llama service with the file metadata and security token to convert the document to sentences.
- If the remote Swiss Army Llama service fails, it falls back to the local service if available.
- The function returns a dictionary containing the individual sentences and the total number of sentences in the document.


4. **`calculate_proposed_inference_cost_in_credits`**:
- This function calculates the proposed cost of an inference request in credits based on the requested model, model parameters, inference type, and input data.
- It distinguishes between API-based models and local LLM models (Swiss Army Llama).
- For API-based models, it calls the `calculate_api_cost` function to estimate the cost in dollars and then converts it to credits based on a target value per credit and profit margin.
- For local LLM models, it retrieves the credit costs from the model data and calculates the cost based on the inference type:
- For text completion and question-answering on images, it considers the input tokens, output tokens, number of completions, compute cost, and memory cost.
- For embedding documents, it converts the document to sentences using the `convert_document_to_sentences` function and calculates the cost based on the total tokens, total sentences, query string inclusion, compute cost, and memory cost.
- For embedding audio, it calculates the cost based on the audio length in seconds, query string inclusion, compute cost, and memory cost.
- The function applies a credit cost multiplier factor and ensures a minimum cost in credits.
- It returns the final proposed cost in credits for the inference request.

These functions work together to estimate the cost of performing specific inference requests based on various factors such as the model, input data, model parameters, and inference type. They take into account the pricing models of different API services and local LLM models, converting the estimated costs to credits for a consistent pricing scheme. The `calculate_proposed_inference_cost_in_credits` function serves as the main entry point for estimating the cost of an inference request, utilizing the other functions as needed.


----------

**Misc Utility Functions for Inference Requests:**

Next up are a couple utility functions that are used with Swiss Army Llama:


- **`is_swiss_army_llama_responding`**:
    - This function checks if the Swiss Army Llama service is responding, either locally or remotely.
    - It takes a boolean parameter `local` to determine whether to check the local or remote service.
    - Based on the `local` parameter, it sets the appropriate port number for the service.
    - It sends a GET request to the `/get_list_of_available_model_names/` endpoint of the Swiss Army Llama service, including a security token as a parameter.
    - If the response status code is 200 (OK), it means the service is responding, and the function returns `True`. Otherwise, it returns `False`.
    - Checking the responsiveness of the Swiss Army Llama service is important to ensure that it is available for performing inference requests and other tasks.
    
- **`check_if_input_text_would_get_rejected_from_api_services`**:
    - This is an asynchronous function that checks if the input text for an inference request is likely to be rejected or cause problems with API services like OpenAI.
    - It first checks if the Swiss Army Llama service is responding, either locally or remotely, based on the configuration settings.
    - If neither the local nor the remote Swiss Army Llama service is responding, it logs an error message and returns `None`, indicating that the check cannot be performed.
    - If the Swiss Army Llama service is responding, it constructs an input prompt that asks whether the given inference request text is problematic or offensive and likely to be rejected by OpenAI.
    - It sends a POST request to the `/get_text_completions_from_input_prompt/` endpoint of the Swiss Army Llama service, passing the input prompt, model name, and other parameters.
    - The function expects the Swiss Army Llama service to respond with either "ACCEPT" or "REJECT" based on its assessment of the input text (it constrains the results of the model by using a grammar file which only allows responses that conform to either “ACCEPT” or “REJECT”
    - If the response is "ACCEPT", it means the inference request is not problematic, and the function returns `True`, indicating that the request can be passed on to the API service.
    - If the response is "REJECT", it means the inference request is determined to be problematic and likely to result in a rejection or ban from the API service. In this case, the function logs an error message and returns `False`.
    - If the response is neither "ACCEPT" nor "REJECT", it logs a warning message and returns the current value of `inference_request_allowed`.
    - In case of any exceptions during the Swiss Army Llama request, it logs an error message and attempts to fall back to the local Swiss Army Llama service if the remote service was being used.
    

The `check_if_input_text_would_get_rejected_from_api_services` function plays a crucial role in the inference request flow. Before sending an inference request to an external API service, it is important to ensure that the input text is not problematic or offensive, as sending such requests could result in the API key being banned or the request being rejected.

By leveraging the Swiss Army Llama service, this function performs a pre-check on the input text to determine its suitability for the API service. It uses a specific model and parameters to assess the input text and expects a clear "ACCEPT" or "REJECT" response.

This pre-check step helps to mitigate the risk of sending inappropriate or offensive requests to the API service, protecting the Pastel inference system (more specifically, protecting individual Supernode operators themselves) from potential bans or rejections. It allows the system to proactively filter out problematic requests and ensure a smoother and more reliable inference request flow.

The function also handles scenarios where the Swiss Army Llama service may not be responding and provides fallback mechanisms to ensure the pre-check can still be performed using the local service if available.

----------

**Validating and Processing New Inference Requests:**

The next batch of service functions play crucial roles in the inference request flow, specifically in validating and processing incoming inference requests. They also help us to determine how much a given credit pack ticket has been used so we can determine if it contains enough remaining credits to cover the cost of the currently contemplated inference request, as well as detecting the corresponding confirmation tracking transactions that are used by end users to authorize a new inference request. Let's go through each function and explain their purpose and how they tie into the inference flow:

- **`validate_inference_api_usage_request`**:
    - This function takes an `InferenceAPIUsageRequest` object as input and performs a series of validations to ensure the integrity and validity of the inference request.
    - It first calls the `validate_inference_request_message_data_func` to validate the request message data against predefined validation rules. If any validation errors are found, an exception is raised.
    - It extracts relevant information from the request, such as the requesting PastelID, credit pack ticket TXID, requested model, inference type, model parameters, and input data.
    - The function validates the credit pack ticket TXID using the `validate_pastel_txid_string` function to ensure it follows the expected format.
    - It retrieves the credit pack purchase request response using the `retrieve_credit_pack_ticket_using_txid` function and checks if the requesting PastelID is authorized to use the credit pack.
    - The function then verifies if the requested model is available in the model menu obtained using the `get_inference_model_menu` function. If the model is not found or the requested inference type is not supported by the model, the function returns `False`.
    - If the requested model is a Swiss Army Llama model (not API-based), the function checks if the Swiss Army Llama service is available, either locally or remotely. If the service is not responding, the function returns `False`.
    - The function decodes the model parameters and input data from base64 format and performs additional checks based on the detected data type.
    - If the inference request is for an API-based model and local checking is enabled, the function calls the `check_if_input_text_would_get_rejected_from_api_services` function to determine if the input text is likely to be rejected by the API service. If the request is deemed risky, the function returns `False`.
    - The function calculates the proposed cost of the inference request in credits using the `calculate_proposed_inference_cost_in_credits` function based on the requested model, model parameters, inference type, and input data.
    - It validates the credit pack ticket using the `validate_existing_credit_pack_ticket` function to ensure its validity.
    - The function determines the current credit balance of the credit pack ticket using the `determine_current_credit_pack_balance_based_on_tracking_transactions` function and checks if there are sufficient credits available for the requested inference. If the balance is insufficient, the function returns `False`.
    - Finally, the function returns a tuple indicating the validity of the request, the proposed cost in credits, and the remaining credits after the request.

- **`check_burn_address_for_tracking_transaction`**:
    - This function checks the burn address for a specific tracking transaction related to an inference request confirmation.
    - It takes the tracking address, expected amount, transaction ID (TXID), maximum block height, and optional retry parameters as input.
    - The function uses the RPC connection to retrieve transactions from the blockchain and searches for a transaction that matches the specified criteria.
    - If a transaction with the exact expected amount is found and has sufficient confirmations (or if the confirmation check is skipped), the function returns `True` along with the transaction details.
    - If a transaction with an amount greater than or equal to the expected amount is found, the function returns `False` for an exact match but `True` for an exceeding transaction, along with the transaction details.
    - If no matching transaction is found after the specified number of retries, the function returns `False`.
    - This function is used to verify that the user has indeed sent the required tracking transaction to the burn address, confirming their agreement to pay for the inference request using their credits.

-  **`determine_current_credit_pack_balance_based_on_tracking_transactions`**:
    - This function is a crucial part of the inference flow, as it calculates the current balance of credits available in a specific credit pack ticket. This function is essential for ensuring that users have sufficient credits to cover the cost of their inference requests and for keeping track of the credits consumed over time.
    - The function takes the `credit_pack_ticket_txid` as input, which uniquely identifies the credit pack ticket.
    - It retrieves the credit pack ticket data using the `retrieve_credit_pack_ticket_using_txid` function, which fetches the ticket information from the blockchain using the provided TXID.
    - From the retrieved ticket data, the function extracts the initial credit balance (`requested_initial_credits_in_credit_pack`) and the credit usage tracking PSL address (`credit_usage_tracking_psl_address`). These pieces of information are necessary for calculating the current credit balance.
    - The function then queries the database to retrieve all the burn address transactions associated with the specific credit usage tracking address. These transactions represent the inference requests that have consumed credits from the credit pack.
    - It calculates the total credits consumed by summing up the amounts of all the retrieved burn address transactions. The transaction amounts are adjusted using the `CREDIT_USAGE_TO_TRACKING_AMOUNT_MULTIPLIER` to convert them from the tracking amount to the actual credit amount.
    - The function then checks if there are any new blocks added to the blockchain since the last update of the database. It does this by comparing the current block height (`current_block_height`) with the latest block height stored in the database (`latest_db_block_height`).
    - If there are new blocks, the function retrieves the new burn transactions from those blocks using the `listsinceblock` RPC method. It filters out the transactions that are not relevant to the specific credit pack ticket based on the burn address and tracking address.
    - The function decodes the new burn transactions using the `process_transactions_in_chunks` function, which retrieves the detailed transaction data in chunks to handle a large number of transactions efficiently.
    - It then queries the database again to retrieve any new tracking transactions associated with the credit pack ticket that occurred in the new blocks or are still pending.
    - The total credits consumed is updated by adding the amounts of the new tracking transactions to the previously calculated total.
    - Optionally, the function can update the block hashes in the database for the new blocks using the `fetch_and_insert_block_hashes` function. This step is useful for maintaining a local copy of the block hashes for faster lookups.
    - Finally, the function calculates the current credit balance by subtracting the total credits consumed from the initial credit balance. It also counts the total number of confirmation transactions from the tracking address to the burn address.
    - The function returns the current credit balance and the number of confirmation transactions.
    - The `determine_current_credit_pack_balance_based_on_tracking_transactions` function is closely tied to the `check_burn_address_for_tracking_transaction` function, which is used to verify the tracking transactions sent by users to confirm their inference requests.
        - When a user sends a confirmation transaction to the burn address, the `check_burn_address_for_tracking_transaction` function checks if the transaction meets the expected criteria, such as the correct amount and sufficient confirmations. If the transaction is valid, it indicates that the user has authorized the consumption of credits from their credit pack for the inference request.
        - The `determine_current_credit_pack_balance_based_on_tracking_transactions` function, in turn, relies on these tracking transactions to calculate the current balance of the credit pack. By querying the database and retrieving the relevant tracking transactions, the function can accurately determine how many credits have been consumed and update the credit pack balance accordingly.
    - This function is crucial for maintaining the integrity of the credit system and ensuring that users can only consume the credits they have available in their credit packs. It allows the inference flow to proceed smoothly, with the `responding_supernode` being able to verify that the user has sufficient credits before executing the inference request.
    - Moreover, the function's ability to handle new blocks and transactions ensures that the credit balance remains up to date, even as new inference requests are processed and new tracking transactions are added to the blockchain.

- **`process_inference_api_usage_request`**:
    - This function takes an `InferenceAPIUsageRequest` object as input and processes the inference request.
    - It first calls the `validate_inference_api_usage_request` function to validate the request. If the request is invalid, an exception is raised.
    - If the request is valid, the function saves the inference API usage request using the `save_inference_api_usage_request` function.
    - It retrieves the credit pack purchase request response using the `retrieve_credit_pack_ticket_using_txid` function to obtain the credit usage tracking PSL address.
    - The function then calls the `create_and_save_inference_api_usage_response` function to create and save an `InferenceAPIUsageResponse` object based on the saved request, proposed cost in credits, remaining credits after the request, and credit usage tracking PSL address.
    - Finally, the function returns the created `InferenceAPIUsageResponse` object.

- **`create_and_save_inference_api_usage_response`**:
    - This is a helper function used by `process_inference_api_usage_request` to create and save an `InferenceAPIUsageResponse` object.
    - It takes the saved inference API usage request, proposed cost in credits, remaining credits after the request, and credit usage tracking PSL address as input.
    - The function generates a unique identifier for the inference response and creates an `InferenceAPIUsageResponse` instance with the relevant information.
    - It computes the hash of the response fields using the `compute_sha3_256_hash_of_sqlmodel_response_fields` function and signs the hash with the local Supernode's PastelID using the `sign_message_with_pastelid_func` function.
    - The function then saves the `InferenceAPIUsageResponse` object to the database using an asynchronous database session.
    - Finally, it returns the saved `InferenceAPIUsageResponse` object.

These functions play a critical role in the inference request flow by validating incoming requests, checking the availability of requested models and services, calculating the cost of the inference in credits, and ensuring sufficient credits are available in the associated credit pack ticket. They also handle the creation and saving of the `InferenceAPIUsageResponse` object, which represents the Supernode's response to the inference request, including the proposed cost and remaining credits.

The `validate_inference_api_usage_request` function acts as a gatekeeper, performing various checks and validations to ensure the integrity and validity of the inference request before proceeding with the actual processing. It helps prevent invalid or unauthorized requests from being processed and ensures that the requested model and inference type are supported.

The `process_inference_api_usage_request` function orchestrates the overall processing of the inference request. It relies on the `validate_inference_api_usage_request` function to validate the request and then saves the request, retrieves necessary information, and creates and saves the corresponding `InferenceAPIUsageResponse` object.

----------

**Processing Inference Confirmations and Saving Output Results:**

The next batch of service functions pertain to several important functions related to processing inference confirmations, saving inference output results. Let's go through each function and explain their purpose and how they tie into the inference flow:

- **`process_inference_confirmation`**:
    - This function processes the confirmation of an inference request sent by the user.
    - It takes the inference request ID and an `InferenceConfirmation` object as input.
    - The function retrieves the corresponding `InferenceAPIUsageRequest` and `InferenceAPIUsageResponse` from the database using the provided inference request ID.
    - It ensures that the burn address is tracked by the local wallet by importing it if necessary.
    - The function then calls the `check_burn_address_for_tracking_transaction` function to check if the burn address has received the expected tracking transaction from the user.
    - If a matching transaction is found, the function computes the current credit pack balance based on the tracking transactions using the `determine_current_credit_pack_balance_based_on_tracking_transactions` function.
    - It updates the status of the inference request to "confirmed" in the database.
    - Finally, the function triggers the execution of the inference request by creating a new task using `asyncio.create_task(execute_inference_request(inference_request_id))`.
    - If any error occurs during the process, the function logs the error and re-raises the exception.
    
- **`save_inference_output_results`**:
    - This function saves the output results of an inference request to the database.
    - It takes the inference request ID, inference response ID, output results dictionary, and output results file type strings as input.
    - The function generates a unique identifier for the inference result and creates an `InferenceAPIOutputResult` record without the hash and signature fields.
    - It populates the record with the relevant information, including the`responding_supernode`'s PastelID, the base64-encoded output results JSON, file type strings, timestamp, block height, and message version.
    - The function computes the hash of the inference result fields using the `compute_sha3_256_hash_of_sqlmodel_response_fields` function and signs the hash with the `responding_supernode`'s PastelID using the `sign_message_with_pastelid_func` function.
    - Finally, it saves the `InferenceAPIOutputResult` record to the database using an asynchronous database session.
    - If any error occurs during the process, the function logs the error and re-raises the exception.
    
These functions play important roles in the inference flow, specifically in the confirmation and result saving stages. The `process_inference_confirmation` function is responsible for handling the user's confirmation of an inference request. When the user sends a tracking transaction to the burn address, indicating their agreement to pay for the inference using their credits, this function verifies the transaction and updates the status of the inference request to "confirmed". It also triggers the execution of the inference request by creating a new task.

The `save_inference_output_results` function is called after the inference request has been executed and the output results are available. It takes the output results, along with other relevant information, and creates an `InferenceAPIOutputResult` record in the database. This record contains the inference results, file type information, `responding_supernode`'s PastelID, and other metadata. The function also computes a hash of the result fields and signs it with the Supernode's PastelID to ensure the integrity and authenticity of the results.


----------


**Inference Request Execution Functions for API-Based Services and Models**

The next batch of service functions we will review are related to how inference requests are executed for API-based services and models, such as those from Stability, OpenAI, Anthropic, Mistral, Groq, and OpenRouter. These functions are responsible for submitting the inference requests to the respective APIs, handling the responses, and processing the output results. Let's go through each function and explain their purpose and how they contribute to the execution of inference requests:

1. **`get_claude3_model_name`**:
    - This is a helper function specific to Anthropic's Claude API. It maps the model names used in the Pastel Inference Layer to the corresponding model names recognized by the Claude API.
    - It takes the `model_name` as input and returns the corresponding Claude API model name using a predefined mapping.
    - If the provided `model_name` is not found in the mapping, it returns an empty string.
    - This function ensures that the correct model name is used when submitting inference requests to the Claude API.
    
2. **`submit_inference_request_to_stability_api`**:
    - This function is responsible for submitting inference requests to the Stability API for image generation tasks.
    - It supports two types of inference requests: "text_to_image" and "creative_upscale".
    - For "text_to_image" requests:
        - It extracts the model parameters and input prompt from the `inference_request` object.
        - It constructs the necessary data payload based on the model parameters, such as aspect ratio, output format, negative prompt, seed, and style preset.
        - It sends a POST request to the Stability API endpoint with the appropriate headers and data payload.
        - If the response is successful (status code 200), it processes the response JSON and extracts the generated image as base64-encoded data.
        - It returns the output results and file type strings.
    - For "creative_upscale" requests:
        - It extracts the model parameters and input image from the `inference_request` object.
        - It constructs the necessary data payload based on the model parameters, such as prompt, output format, creativity, seed, and negative prompt.
        - It sends a POST request to the Stability API endpoint with the appropriate headers, files, and data payload.
        - If the response is successful (status code 200), it retrieves the `generation_id` from the response.
        - It then enters a loop to poll for the upscaled image result using the `generation_id`.
        - Once the upscaled image is available, it encodes the image as base64 and returns the output results and file type strings.
    - If any errors occur during the process or if the inference type is not supported, it logs an error message and returns `None`.
    
3. **`submit_inference_request_to_openai_api`**:
    - This function is responsible for submitting inference requests to the OpenAI API for various tasks, including text completion, embedding, and question-answering on images.
    - For "text_completion" requests:
        - It extracts the model parameters and input prompt from the `inference_request` object.
        - It determines the number of completions to generate based on the model parameters.
        - It sends a POST request to the OpenAI API endpoint with the appropriate headers and JSON payload, including the model, input prompt, max tokens, temperature, and number of completions.
        - If the response is successful (status code 200), it processes the response JSON and extracts the generated text completions.
        - It keeps track of the total input and output tokens used for the request.
        - It returns the output results and file type strings.
    - For "embedding" requests:
        - It extracts the input text from the `inference_request` object.
        - It sends a POST request to the OpenAI API endpoint with the appropriate headers and JSON payload, including the model and input text.
        - If the response is successful (status code 200), it processes the response JSON and extracts the embedding vector.
        - It returns the output results and file type strings.
    - For "ask_question_about_an_image" requests:
        - It extracts the model parameters, number of completions, input image, and question from the `inference_request` object.
        - It encodes the input image as base64.
        - It sends a POST request to the OpenAI API endpoint with the appropriate headers and JSON payload, including the model, question, image URL, and max tokens.
        - If the response is successful (status code 200), it processes the response JSON and extracts the generated text responses.
        - It keeps track of the total input and output tokens used for the request.
        - It returns the output results and file type strings.
    - If any errors occur during the process or if the inference type is not supported, it logs an error message and returns `None`.
    
4. **`submit_inference_request_to_openrouter`**:
    - This function is responsible for submitting inference requests to the OpenRouter API for text completion tasks.
    - It extracts the model parameters and input prompt from the `inference_request` object.
    - It constructs the necessary JSON payload based on the model parameters, such as model, input prompt, max tokens, and temperature.
    - It sends a POST request to the OpenRouter API endpoint with the appropriate headers and JSON payload.
    - If the response is successful (status code 200), it processes the response JSON and extracts the generated text completion.
    - It returns the output results and file type strings.
    - If any errors occur during the process or if the inference type is not supported, it logs an error message and returns `None`.
    
5. **`submit_inference_request_to_mistral_api`**:
    - This function is responsible for submitting inference requests to the Mistral API for text completion and embedding tasks.
    - For "text_completion" requests:
        - It extracts the model parameters, input prompt, and number of completions from the `inference_request` object.
        - It uses the Mistral API client to send a streaming request for text completion, providing the model, input prompt, max tokens, and temperature.
        - It processes the streaming response chunks and accumulates the generated text completion.
        - It keeps track of the total input and output tokens used for the request.
        - It returns the output results and file type strings.
    - For "embedding" requests:
        - It extracts the input text from the `inference_request` object.
        - It uses the Mistral API client to send a request for embedding, providing the model and input text.
        - It processes the response and extracts the embedding vector.
        - It returns the output results and file type strings.
    - If any errors occur during the process or if the inference type is not supported, it logs an error message and returns `None`.
    
6. **`submit_inference_request_to_groq_api`**:
    - This function is responsible for submitting inference requests to the Groq API for text completion and embedding tasks.
    - For "text_completion" requests:
        - It extracts the model parameters, input prompt, and number of completions from the `inference_request` object.
        - It uses the Groq API client to send a request for text completion, providing the model, input prompt, max tokens, and temperature.
        - It processes the response and extracts the generated text completions.
        - It keeps track of the total input and output tokens used for the request.
        - It returns the output results and file type strings.
    - For "embedding" requests:
        - It extracts the input text from the `inference_request` object.
        - It uses the Groq API client to send a request for embedding, providing the model and input text.
        - It processes the response and extracts the embedding vector.
        - It returns the output results and file type strings.
    - If any errors occur during the process or if the inference type is not supported, it logs an error message and returns `None`.
    
7. **`submit_inference_request_to_claude_api`**:
    - This function is responsible for submitting inference requests to the Claude API (by Anthropic) for text completion and embedding tasks.
    - It uses the `get_claude3_model_name` helper function to map the model name from the `inference_request` to the corresponding Claude API model name.
    - For "text_completion" requests:
        - It extracts the model parameters, input prompt, and number of completions from the `inference_request` object.
        - It uses the Claude API client to send a streaming request for text completion, providing the model, input prompt, max tokens, and temperature.
        - It processes the streaming response and accumulates the generated text completions.
        - It keeps track of the total input and output tokens used for the request.
        - It returns the output results and file type strings.
    - For "embedding" requests:
        - It extracts the input text from the `inference_request` object.
        - It uses the Claude API client to send a request for embedding, providing the model and input text.
        - It processes the response and extracts the embedding vector.
        - It returns the output results and file type strings.
    - If any errors occur during the process or if the inference type is not supported, it logs an error message and returns `None`.
    
These functions play an important role in executing inference requests for API-based services and models. They handle the communication with the respective APIs, construct the necessary payloads based on the inference request details, and process the API responses to extract the relevant output results.

The rationale behind having separate functions for each API is to provide a modular and extensible approach to integrating different APIs into the Pastel Inference Layer. Each API has its own specific requirements, endpoints, and response formats, so having dedicated functions allows for customized handling of each API's peculiarities.

These functions also ensure that the output results are properly formatted and returned along with the appropriate file type strings. This is important for the Pastel Inference Layer to correctly interpret and handle the output results based on their data types. The functions enable the system to seamlessly integrate with various APIs, execute inference requests based on user specifications, and retrieve the generated output results. By providing a consistent interface for different APIs, these functions contribute to the flexibility and extensibility of the Pastel Inference Layer, allowing it to support a wide range of models and services.

----------

**Inference Request Execution Functions for Swiss Army Llama**

The next batch of service functions we will review are related to how inference requests are executed for models hosted using [Swiss Army Llama](https://github.com/Dicklesworthstone/swiss_army_llama), which is an open-source framework for running large language models (LLMs) and computing and storing embedding vectors of documents (and also of audio files, which are transcribed to text using [Whisper](https://github.com/SYSTRAN/faster-whisper)) in one of two ways:

- **Locally**— i.e., running on the CPU on the actual Supernode server itself;

- **On a remote server**— i.e., a GPU-enabled instance set up by the Supernode using a cost effective service such as [vast.ai](https://vast.ai/) and a [template image](https://cloud.vast.ai/?ref_id=78066&template_id=3c711e5ddd050882f0eb6f4ce1b8cc28) already set up for Swiss Army Llama; this remote GPU instance can then be shared across all of the Supernode operators various Supernodes; 

These functions are responsible for submitting the inference requests to the Swiss Army Llama service, handling the responses, and processing the output results. Let's go through each function and explain their purpose and how they contribute to the execution of inference requests:

1. **`determine_swiss_army_llama_port`**:
    - This function determines the appropriate port to use for communicating with the Swiss Army Llama service.
    - It checks if the local Swiss Army Llama service is responding using the `is_swiss_army_llama_responding` function with the `local` parameter set to `True`.
    - If the `USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE` configuration is set to `True`, it also checks if the remote Swiss Army Llama service is responding by calling `is_swiss_army_llama_responding` with `local` set to `False`.
    - If the remote Swiss Army Llama service is responding and the configuration allows its usage, the function returns the `REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT`.
    - If the local Swiss Army Llama service is responding, it returns the `SWISS_ARMY_LLAMA_PORT`.
    - If neither service is responding, it returns `None`.
    - This function helps in selecting the appropriate port based on the availability and configuration of the Swiss Army Llama services.
    
2. **`handle_swiss_army_llama_exception`**:
    - This is an exception handling function specific to Swiss Army Llama requests.
    - It takes the exception object `e`, the `client` instance, the `inference_request`, `model_parameters`, `port`, `is_fallback` flag, and the `handler_function` as parameters.
    - If the exception occurs while using the remote Swiss Army Llama service (indicated by the `port` being `REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT`) and it's not a fallback attempt, the function logs a message indicating a fallback to the local Swiss Army Llama service.
    - It then recursively calls the `handler_function` with the `port` set to `SWISS_ARMY_LLAMA_PORT` and `is_fallback` set to `True`.
    - If the exception occurs in any other scenario, it returns `None` for both the output results and file type strings.
    - This function provides a mechanism to handle exceptions during Swiss Army Llama requests and fallback to the local service if necessary.
    
3. **`handle_swiss_army_llama_text_completion`**:
    - This function handles text completion requests using the Swiss Army Llama service.
    - It takes the `client` instance, `inference_request`, `model_parameters`, `port`, and `is_fallback` flag as parameters.
    - It constructs the payload for the text completion request, including the input prompt, LLM model name, temperature, number of tokens to generate, number of completions to generate, and grammar file string.
    - It sends a POST request to the Swiss Army Llama service endpoint `/get_text_completions_from_input_prompt/` with the payload and security token.
    - If the request is successful (status code 200), it processes the response JSON and extracts the generated text completion.
    - It identifies the data type of the output text using the `magika` library and sets the appropriate output file type strings.
    - If an exception occurs during the request, it calls the `handle_swiss_army_llama_exception` function to handle the exception and potentially fallback to the local service.
    - It returns the output text and file type strings.
    
4. **`handle_swiss_army_llama_image_question`**:
    - This function handles image question requests using the Swiss Army Llama service.
    - It takes the `client` instance, `inference_request`, `model_parameters`, `port`, and `is_fallback` flag as parameters.
    - It extracts the image data and question from the input data, which is assumed to be base64-encoded JSON.
    - It constructs the payload for the image question request, including the question, LLM model name, temperature, number of tokens to generate, and number of completions to generate.
    - It sends a POST request to the Swiss Army Llama service endpoint `/ask_question_about_image/` with the payload, image file, and security token.
    - If the request is successful (status code 200), it processes the response JSON and extracts the generated text answer.
    - It identifies the data type of the output text using the `magika` library and sets the appropriate output file type strings.
    - If an exception occurs during the request, it calls the `handle_swiss_army_llama_exception` function to handle the exception and potentially fallback to the local service.
    - It returns the output text and file type strings.
    
5. **`handle_swiss_army_llama_embedding_document`**:
    - This function handles document embedding requests using the Swiss Army Llama service.
    - It takes the `client` instance, `inference_request`, `model_parameters`, `port`, and `is_fallback` flag as parameters.
    - It extracts the document file data from the input data, which is assumed to be base64-encoded JSON.
    - It uploads the document file using the `upload_and_get_file_metadata` function and retrieves the file metadata (URL, hash, size).
    - It constructs the parameters for the document embedding request, including the LLM model name, embedding pooling method, corpus identifier string, JSON format, and whether to send back a JSON or ZIP file.
    - It sends a POST request to the Swiss Army Llama service endpoint `/get_all_embedding_vectors_for_document/` with the parameters, file metadata, and security token.
    - If the request is successful (status code 200), it processes the response based on the specified format (JSON or ZIP).
    - For JSON format, it directly returns the response JSON as the output results.
    - For ZIP format, it encodes the ZIP file content as base64 and sets the appropriate output file type strings.
    - If an exception occurs during the request, it calls the `handle_swiss_army_llama_exception` function to handle the exception and potentially fallback to the local service.
    - It returns the output results and file type strings.
    
6. **`handle_swiss_army_llama_embedding_audio`**:
    - This function handles audio embedding requests using the Swiss Army Llama service.
    - It takes the `client` instance, `inference_request`, `model_parameters`, `port`, and `is_fallback` flag as parameters.
    - It extracts the audio file data from the input data, which is assumed to be base64-encoded JSON.
    - It uploads the audio file using the `upload_and_get_file_metadata` function and retrieves the file metadata (URL, hash, size).
    - It constructs the parameters for the audio embedding request, including the option to compute embeddings for the resulting transcript document, LLM model name, embedding pooling method, and corpus identifier string.
    - It sends a POST request to the Swiss Army Llama service endpoint `/compute_transcript_with_whisper_from_audio/` with the parameters, file metadata, and security token.
    - If the request is successful (status code 200), it processes the response JSON as the output results.
    - If a query text is provided in the model parameters, it sends an additional request to the `/search_stored_embeddings_with_query_string_for_semantic_similarity/` endpoint to perform a semantic search on the embeddings.
    - It appends the search results to the output results.
    - If an exception occurs during the request, it calls the `handle_swiss_army_llama_exception` function to handle the exception and potentially fallback to the local service.
    - It returns the output results and file type strings.
    
7. **`handle_swiss_army_llama_semantic_search`**:
    - This function handles semantic search requests using the Swiss Army Llama service.
    - It takes the `client` instance, `inference_request`, `model_parameters`, `port`, and `is_fallback` flag as parameters.
    - It constructs the payload for the semantic search request, including the query text, number of most similar strings to return, LLM model name, embedding pooling method, and corpus identifier string.
    - It sends a POST request to the Swiss Army Llama service endpoint `/search_stored_embeddings_with_query_string_for_semantic_similarity/` with the payload and security token.
    - If the request is successful (status code 200), it processes the response JSON as the output results.
    - If an exception occurs during the request, it calls the `handle_swiss_army_llama_exception` function to handle the exception and potentially fallback to the local service.
    - It returns the output results and file type strings.
    
8. **`handle_swiss_army_llama_advanced_semantic_search`**:
    - This function handles advanced semantic search requests using the Swiss Army Llama service.
    - It takes the `client` instance, `inference_request`, `model_parameters`, `port`, and `is_fallback` flag as parameters.
    - It constructs the payload for the advanced semantic search request, including the query text, LLM model name, embedding pooling method, corpus identifier string, similarity filter percentage, number of most similar strings to return, and result sorting metric.
    - It sends a POST request to the Swiss Army Llama service endpoint `/advanced_search_stored_embeddings_with_query_string_for_semantic_similarity/` with the payload and security token.
    - If the request is successful (status code 200), it processes the response JSON as the output results.
    - If an exception occurs during the request, it calls the `handle_swiss_army_llama_exception` function to handle the exception and potentially fallback to the local service.
    - It returns the output results and file type strings.
    
9. **`submit_inference_request_to_swiss_army_llama`**:
    - This is the main function for submitting inference requests to the Swiss Army Llama service.
    - It takes the `inference_request` and an optional `is_fallback` flag as parameters.
    - It determines the appropriate port for the Swiss Army Llama service using the `determine_swiss_army_llama_port` function.
    - If no valid port is found (indicating that neither the local nor remote service is responding), it logs an error message and returns `None` for both the output results and file type strings.
    - It decodes the model parameters from base64-encoded JSON.
    - Based on the `model_inference_type_string` of the inference request, it calls the corresponding handler function:
        - For "text_completion", it calls `handle_swiss_army_llama_text_completion`.
        - For "embedding_document", it calls `handle_swiss_army_llama_embedding_document`.
        - For "embedding_audio", it calls `handle_swiss_army_llama_embedding_audio`.
        - For "ask_question_about_an_image", it calls `handle_swiss_army_llama_image_question`.
        - For "semantic_search", it calls `handle_swiss_army_llama_semantic_search`.
        - For "advanced_semantic_search", it calls `handle_swiss_army_llama_advanced_semantic_search`.
    - If the inference type is not supported, it logs a warning message and returns `None` for both the output results and file type strings.
    - It returns the output results and file type strings obtained from the respective handler function.
    
These functions play a crucial role in executing inference requests using the Swiss Army Llama service: they handle the communication with the Swiss Army Llama service, construct the necessary payloads based on the inference request details, and process the service responses to extract the relevant output results.

The rationale behind having separate handler functions for different inference types is to provide a modular and extensible approach to supporting various capabilities of the Swiss Army Llama service. Each inference type has its own specific requirements, parameters, and response formats, so having dedicated handler functions allows for customized handling of each type.

The `submit_inference_request_to_swiss_army_llama` function serves as the main entry point for submitting inference requests to the Swiss Army Llama service. It determines the appropriate port to use based on the availability and configuration of the local and remote services. It then delegates the request to the corresponding handler function based on the inference type.

The exception handling mechanism implemented in the `handle_swiss_army_llama_exception` function provides a way to gracefully handle errors that may occur during the communication with the Swiss Army Llama service. It allows for fallback to the local service if the remote service encounters an exception, ensuring that the inference request can still be processed.

Overall, these functions enable the Pastel Inference Layer to seamlessly integrate with the Swiss Army Llama service, providing users with the ability to run large language models locally or on a remote server. They handle the execution of various inference types, such as text completion, document and audio embedding, image question answering, and semantic search. By offering flexibility in the deployment options and supporting different inference capabilities, these functions contribute to the versatility and robustness of the Pastel Inference Layer.

----------

**Inference Request Execution and Result Retrieval Functions**

The final batch of service functions we will review are related to the overall execution of inference requests, checking the status of inference results, and retrieving the output results while verifying authorization. These functions tie together the various components of the inference request flow and provide the necessary endpoints for users to interact with the system. Let's go through each function and explain their purpose and how they contribute to the inference request execution and result retrieval process:

1. **`execute_inference_request`**:
    - This is the main function responsible for executing an inference request.
    - It takes the `inference_request_id` as a parameter, which uniquely identifies the inference request.
    - It retrieves the corresponding `InferenceAPIUsageRequest` from the database using the provided `inference_request_id`.
    - If the `InferenceAPIUsageRequest` is not found, it logs a warning message and returns, indicating an invalid inference request ID.
    - It also retrieves the associated `InferenceAPIUsageResponse` from the database using the same `inference_request_id`.
    - Based on the `requested_model_canonical_string` of the `InferenceAPIUsageRequest`, it determines the appropriate API or service to submit the inference request to:
        - If the model starts with "stability-", it calls the `submit_inference_request_to_stability_api` function.
        - If the model starts with "openai-", it calls the `submit_inference_request_to_openai_api` function.
        - If the model starts with "mistralapi-", it calls the `submit_inference_request_to_mistral_api` function.
        - If the model starts with "groq-", it calls the `submit_inference_request_to_groq_api` function.
        - If the model contains "claude" (case-insensitive), it calls the `submit_inference_request_to_claude_api` function.
        - If the model starts with "openrouter/", it calls the `submit_inference_request_to_openrouter` function.
        - If the model starts with "swiss_army_llama-", it calls the `submit_inference_request_to_swiss_army_llama` function.
        - If none of the above conditions match, it raises a `ValueError` indicating an unsupported provider or model.
    - The selected submission function is called with the `InferenceAPIUsageRequest` object, and it returns the `output_results` and `output_results_file_type_strings`.
    - If the `output_results` and `output_results_file_type_strings` are not `None`, it calls the `save_inference_output_results` function to save the inference output results to the database.
    - If any exception occurs during the execution process, it logs the error, prints the traceback, and re-raises the exception.
    
2. **`check_status_of_inference_request_results`**:
    - This function checks the status of an inference request's results.
    - It takes the `inference_response_id` as a parameter, which uniquely identifies the inference response.
    - It retrieves the corresponding `InferenceAPIOutputResult` from the database using the provided `inference_response_id`.
    - If the `InferenceAPIOutputResult` is found, it means the inference request has been processed and the results are available, so the function returns `True`.
    - If the `InferenceAPIOutputResult` is not found, it means the inference request is still being processed or has not yet started, so the function returns `False`.
    - If any exception occurs during the status check, it logs the error and re-raises the exception.
    
3. **`get_inference_output_results_and_verify_authorization`**:
    - This function retrieves the inference output results and verifies the authorization of the requesting user.
    - It takes the `inference_response_id` and `requesting_pastelid` as parameters.
    - It retrieves the corresponding `InferenceAPIOutputResult` from the database using the provided `inference_response_id`.
    - If the `InferenceAPIOutputResult` is not found, it raises a `ValueError` indicating that the inference output results are not found.
    - It then retrieves the associated `InferenceAPIUsageRequest` from the database using the `inference_request_id` from the `InferenceAPIOutputResult`.
    - If the `InferenceAPIUsageRequest` is not found or the `requesting_pastelid` does not match the `requesting_pastelid` of the `InferenceAPIUsageRequest`, it raises a `ValueError` indicating unauthorized access to the inference output results.
    - If the authorization is successful, it returns the `InferenceAPIOutputResult` object.
    
These functions play a critical role in the overall execution and management of inference requests in the Pastel Inference Layer. The `execute_inference_request` function serves as the central point of control for executing an inference request. It determines the appropriate API or service to submit the request to based on the model specified in the `InferenceAPIUsageRequest`. By delegating the actual submission to the respective functions for each API or service, it maintains a modular and extensible architecture.

The `check_status_of_inference_request_results` function provides a way for users to check the status of their inference requests. It allows them to determine whether the results are available or if the request is still being processed. This function is typically called by the user periodically to poll for the availability of the results.

The `get_inference_output_results_and_verify_authorization` function is responsible for retrieving the inference output results and verifying the authorization of the requesting user. It ensures that only the user who initiated the inference request can access the corresponding output results. This function is called when the user wants to retrieve the results after confirming their availability through the status check.

The use of database queries and transactions in these functions ensures data consistency and integrity. The functions retrieve the necessary objects from the database, such as `InferenceAPIUsageRequest`, `InferenceAPIUsageResponse`, and `InferenceAPIOutputResult`, to perform the required operations. The database transactions are managed using the `db_code.Session` context manager, which handles the session lifecycle and ensures proper resource management.

Error handling is also incorporated into these functions. If any exceptions occur during the execution or retrieval process, the functions log the errors, print the tracebacks, and re-raise the exceptions. This allows for proper error propagation and handling at higher levels of the system.

Overall, these functions provide the necessary functionality for executing inference requests, checking the status of results, and retrieving the output results while verifying authorization. They encapsulate the complexity of interacting with different APIs and services, and provide a consistent interface for users to interact with the Pastel Inference Layer. By leveraging the modular design and database transactions, these functions ensure a reliable and secure execution and retrieval process for inference requests.

----------

## Misc Other Infrastructure Code

Here, we explain other parts of the Inference Server system code that we haven't touched on yet above. First of these is the code used by the Inference Server to initially install, set up, configure, and update Swiss Army Llama for local operation on the Supernode server itself (setting Swiss Army Llama up on a remote machine for use with the Inference Server is a separate process that we will detail in a later section). This is handled from the code file `setup_swiss_army_llama.py`, which is called from the `main.py` entrypoint code file in the Inference Server as a startup task of the FastAPI server. The complete code for `setup_swiss_army_llama.py` can be found [here](https://github.com/pastelnetwork/python_inference_layer_server/blob/master/setup_swiss_army_llama.py).

The `setup_swiss_army_llama.py` file contains various utility functions and a main function `check_and_setup_swiss_army_llama` that orchestrates the entire setup process. Let's go through each function and explain their purpose and how they contribute to setting up and managing the Swiss Army Llama service:

1. `get_external_ip_func`:
    - This function attempts to retrieve the external IP address of the server by querying several public IP address providers.
    - It iterates through a list of providers and sends HTTP GET requests to each provider until a successful response is received.
    - If a provider returns a valid IP address, the function returns it.
    - If all providers fail to provide an IP address, the function returns "Unknown".
    
2. `run_command`:
    - This function is a wrapper around the `subprocess.run` function to execute shell commands.
    - It takes the command as a string or a list of command arguments, along with optional parameters for environment variables, output capture, error checking, and timeout.
    - It uses the default shell (`/bin/zsh` if available, otherwise `/bin/bash`) to execute the command.
    - If `capture_output` is set to `True`, the function captures and logs the command's stdout and stderr.
    - If the command times out or fails, the function logs the appropriate error message.
    - It returns the `subprocess.CompletedProcess` object representing the executed command.
    
3. `is_port_available`:
    - This function checks if a specified port is available (not in use) on the server.
    - It uses the `lsof` command to list open files and sockets associated with the port.
    - If the command's return code is non-zero, it means the port is available.
    
4. `is_swiss_army_llama_responding`:
    - This function checks if the Swiss Army Llama service is responding on a specified IP address and port.
    - It constructs a URL using the provided IP address, port, and the `/get_list_of_available_model_names/` endpoint.
    - It sends an HTTP GET request to the URL with the security token as a parameter.
    - If the response status code is 200, it means the service is responding.
    
5. `update_security_token`:
    - This function updates the security token in the Swiss Army Llama configuration file.
    - It reads the content of the specified file and uses regular expressions to replace the existing security token with the provided new token.
    - It writes the updated content back to the file.
    
6. `is_pyenv_installed`:
    - This function checks if the `pyenv` tool is installed on the server.
    - It runs the `pyenv --version` command and checks the return code.
    - If the return code is 0, it means `pyenv` is installed.
    
7. `is_python_3_12_installed`:
    - This function checks if Python 3.12 is installed using `pyenv`.
    - It runs the `pyenv versions` command and checks if "3.12" is present in the output.
    
8. `is_rust_installed`:
    - This function checks if the Rust programming language is installed on the server.
    - It runs the `rustc --version` command and checks the return code.
    - If the return code is 0, it means Rust is installed.
    - If the command throws a `FileNotFoundError`, it means Rust is not installed.
    
9. `setup_virtual_environment`:
    - This function sets up a Python virtual environment for Swiss Army Llama.
    - It creates a virtual environment directory if it doesn't exist.
    - It upgrades `pip`, installs the `wheel` package, and installs the dependencies from the `requirements.txt` file.
    - It returns the path to the Python executable within the virtual environment.
    
10. `set_timezone_utc`:
    - This function sets the timezone to UTC.
    - It sets the `TZ` environment variable to 'UTC'.
    - It adds the `export TZ=UTC` line to the user's shell profile file (`~/.zshrc` or `~/.bashrc`).
    
11. `check_systemd_service_exists`:
    - This function checks if a systemd service with the specified name exists and is enabled.
    - It runs the `systemctl is-enabled <service_name>` command and checks the return code and output.
    - If the return code is 0 and the output contains 'enabled', it means the service exists.
    
12. `create_systemd_service`:
    - This function creates a systemd service file for Swiss Army Llama.
    - It constructs the service file content with the provided service name, user, working directory, and command to execute.
    - It writes the service file content to a temporary file and moves it to the systemd directory.
    - It reloads the systemd daemon, enables the service, and starts the service.
    - It logs the status of the service after starting it.
    
13. `ensure_pyenv_setup`:
    - This function ensures that `pyenv` is installed and Python 3.12 is set up.
    - If `pyenv` is not installed, it installs the necessary dependencies and installs `pyenv`.
    - If Python 3.12 is not installed, it installs Python 3.12 using `pyenv` and sets it as the global Python version.
    
14. `configure_shell_for_pyenv`:
    - This function configures the shell environment for `pyenv`.
    - It adds the necessary `pyenv` initialization commands to the user's shell profile file (`~/.zshrc` or `~/.bashrc`).
    - It updates the `PYENV_ROOT` and `PATH` environment variables to include `pyenv` paths.
    
15. `has_repo_been_updated`:
    - This function checks if the Swiss Army Llama repository has been updated.
    - It fetches the latest changes from the remote repository.
    - It compares the local commit hash with the remote commit hash of the `main` branch.
    - If the commit hashes are different, it means the repository has been updated.
    
16. `setup_swiss_army_llama`:
    - This function performs the actual setup of Swiss Army Llama.
    - It sets the timezone to UTC.
    - It clones the Swiss Army Llama repository if it doesn't exist.
    - It checks for updates in the repository and pulls the latest changes if updates are available.
    - It configures the shell environment for `pyenv` and ensures `pyenv` and Python 3.12 are installed.
    - It sets up a Python virtual environment for Swiss Army Llama and installs the dependencies.
    - If Rust is not installed, it installs Rust and sets up the necessary environment variables.
    - If the Swiss Army Llama systemd service doesn't exist, it creates the service file and starts the service.
    - If the service already exists, it reloads the systemd daemon, enables the service, and starts it.
    
17. `kill_running_instances_of_swiss_army_llama`:
    - This function kills any running instances of Swiss Army Llama.
    - It stops the Swiss Army Llama systemd service.
    - It finds and kills any remaining Swiss Army Llama processes using the `ps` and `kill` commands.
    
18. `check_and_setup_swiss_army_llama`:
    - This is the main function that orchestrates the setup process.
    - It retrieves the external IP address of the server.
    - It checks if the Swiss Army Llama repository has been updated.
    - It checks if the Swiss Army Llama service is responding on the specified port.
    - It checks if the Swiss Army Llama port is available.
    - If the service is responding and the repository hasn't been updated, it considers Swiss Army Llama to be already set up and skips the setup process.
    - If the service is not responding or the repository has been updated, it kills any running instances of Swiss Army Llama and runs the setup process.

These functions work together to automate the setup and management of the Swiss Army Llama service on the Supernode server. The `check_and_setup_swiss_army_llama` function serves as the entry point and is called from the `main.py` file as a startup task of the FastAPI server.

By checking for updates in the Swiss Army Llama repository, ensuring the necessary dependencies are installed, configuring the environment, and managing the systemd service, this code ensures that the Swiss Army Llama service is properly set up and running on the Supernode server. It also handles scenarios where the service may need to be restarted or updated based on changes in the repository or the service's responsiveness.

----------

**Environment Variable Setup and Processing, Including Decryption of API Keys and Passwords**

The Pastel Inference Layer server relies on environment variables to store and manage various configuration settings, including sensitive information such as API keys and passwords. These environment variables are typically stored in a `.env` file, which is loaded and processed during the server startup.

The `.env` file contains a wide range of configuration settings, covering aspects such as:

- Server port numbers
- Timeout values
- Transaction settings
- Credit pack settings
- API keys for external services (e.g., OpenAI, Anthropic, Stability)
- Passphrases for local and remote Pastel IDs

You can see the sample .env file below. Not that the very long values are actually encrypted secrets that rely upon the existence of a saved decryption key stored somewhere on the machine:

```Dockerfile
    UVICORN_PORT=7123
    TEMP_OVERRIDE_LOCALHOST_ONLY=1
    CHALLENGE_EXPIRATION_TIME_IN_SECONDS=300
    NUMBER_OF_DAYS_BEFORE_MESSAGES_ARE_CONSIDERED_OBSOLETE=3
    GITHUB_MODEL_MENU_URL=https://raw.githubusercontent.com/pastelnetwork/python_inference_layer_server/master/model_menu.json
    MESSAGING_TIMEOUT_IN_SECONDS=60
    MAXIMUM_NUMBER_OF_PASTEL_BLOCKS_FOR_USER_TO_SEND_BURN_AMOUNT_FOR_CREDIT_TICKET=50
    MAXIMUM_LOCAL_CREDIT_PRICE_DIFFERENCE_TO_ACCEPT_CREDIT_PRICING=0.25
    MAXIMUM_LOCAL_PASTEL_BLOCK_HEIGHT_DIFFERENCE_IN_BLOCKS=1
    MINIMUM_NUMBER_OF_PASTEL_BLOCKS_BEFORE_TICKET_STORAGE_RETRY_ALLOWED=10
    MINIMUM_CONFIRMATION_BLOCKS_FOR_CREDIT_PACK_BURN_TRANSACTION=1
    MAXIMUM_LOCAL_UTC_TIMESTAMP_DIFFERENCE_IN_SECONDS=55.0
    BURN_TRANSACTION_MAXIMUM_AGE_IN_DAYS=3
    SUPERNODE_CREDIT_PRICE_AGREEMENT_QUORUM_PERCENTAGE=0.51
    SUPERNODE_CREDIT_PRICE_AGREEMENT_MAJORITY_PERCENTAGE=0.85
    CREDIT_USAGE_TO_TRACKING_AMOUNT_MULTIPLIER=10
    CREDIT_COST_MULTIPLIER_FACTOR=0.02
    API_KEY_TEST_VALIDITY_HOURS=72
    TARGET_VALUE_PER_CREDIT_IN_USD=0.01
    TARGET_PROFIT_MARGIN=0.15
    MAXIMUM_PER_CREDIT_PRICE_IN_PSL_FOR_CLIENT=100.0
    MINIMUM_COST_IN_CREDITS=0.5
    SWISS_ARMY_LLAMA_PORT=8089
    USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE=1
    REMOTE_SWISS_ARMY_LLAMA_INSTANCE_SSH_KEY_PATH=/home/ubuntu/vastai_privkey
    REMOTE_SWISS_ARMY_LLAMA_INSTANCE_IP_ADDRESS=106.185.159.136
    REMOTE_SWISS_ARMY_LLAMA_INSTANCE_PORT=20765
    REMOTE_SWISS_ARMY_LLAMA_EXPOSED_PORT=8089
    REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT=8087
    SWISS_ARMY_LLAMA_SECURITY_TOKEN=gAAAAABmEFF7K1lIlVGm3P8HxZoRJMdr0EIioE3Ug1x9PKaqLi620LOvSALMEcG7drPr61deEkcP2SRplE9qv-sjgwjE964axw%3D%3D
    OPENAI_API_KEY=gAAAAABmEXkqBi83ORWgoTMTvvg0d4ehGjTuCVFguKPVETPAsg7VhRE_Ecs4tb0WFylElFkhQTtHthcDQtzHQ_Do2M0SO2hYqG53vTANcBhPUbSAxwM0sRXcmqeEX1OWhaHbRze67DzkvzqBS1KnQtaUzBtUuusQ6w%3D%3D
    CLAUDE3_API_KEY=gAAAAABmEFF73nD1mrch6DKztNtk9-Kv6PvPpnHSYknatGR4ejc2tDyYc7aqqmxPR03AO23eR_VXIChhNt9q0ZSHB4A2tDVtTCRpsB3xywCe91_TTViyg0VDXj1hlKM-urXGHoKLnyn_xWqsrhTj6WP-ebjqWhdyK0cM-izqWsHFuXWvac1RGwmgZpadweFWYJOAvviTLClcBJaHANviG2_HGkvxfWr8GQ%3D%3D
    GROQ_API_KEY=gAAAAABmEJe9xW7Fejwg2buHRymIH5v7TcJGk7P_lLPEG91RJ28iLwHIJ7PCt77drz_Hz0z-aOAp-GgC6h6_DhZ0yw9H5vFsCtMAMmEOKRcJOMnbrGs-hcJRm6r3Cg3_tqwcSUG1XjVk4MeEYim9ZkbTBcG_AeHi0g%3D%3D
    MISTRAL_API_KEY=gAAAAABmEJe98LKZmYTux4IMIiSLHV3jftgxCr7zR_RKZx3xuMwiw5xv2rCy4hOY9Inyg2kkoFke8_o55RBLxNUe8UHOZIQmoa38o6XhHCxpydLlJORytFkb7fI6pKWv0hPNgnFiMjw6
    STABILITY_API_KEY=gAAAAABmEM0J9Y4HtTRPcqbhy5R3UZpDQvHZrUY6UKbcMRh-RsV7JROD1nCxGcDElDkdt61qgVZWOUiCpNDdE5w_hxV7wmAuQosh-Vsfapp25cb2qMA3xJzK8Aoo1X1gdyakDq2xHJWhhtFWpCDTBcrt6p799tMfCQ%3D%3D
    OPENROUTER_API_KEY=gAAAAABmFzAGXWhWQZox78SWuj5SUvesTkCU-ry5uf2YZyqfUaF1vZjkR6zwbghs9vdbqLEv4rSqu0FSGewDiIG-jFw8JpY2p9AIt9h2M0Mhc8JkRp9fXGVnCR8YFqaw3aQZmmbpwSobshjqw9oWdgnl3rmpd1SSRXtX5RLpXX7vWPiFbrBZj_8%3D
    LOCAL_PASTEL_ID_PASSPHRASE=gAAAAABmEFF72YuPdwL_cNZSu54NLVxiT9s6QwGpUn0dnHoBbHRq7T-vK_vAuBV0HFrpnlquzxrmknsYKuaiWktTjOSH4knDnA%3D%3D
    MY_LOCAL_PASTELID=jXYdog1FfN1YBphHrrRuMVsXT76gdfMTvDBo2aJyjQnLdz2HWtHUdE376imdgeVjQNK93drAmwWoc7A3G4t2Pj
    MY_PASTELID_PASSPHRASE=gAAAAABmEFF7yMxMmyz4xaSa7ykie-487B42GOgzlR0HGa0K08M7Ow3DZHNt6W46M_SQQQaKtFMM-OTVJDIE51AQh4CBchmU1g%3D%3D
```

The use of environment variables allows for flexible configuration of the server without the need to modify the codebase directly. It also enables easy management of different configurations for various environments (e.g., development, staging, production).

To process the environment variables, the Pastel Inference Layer server utilizes the `python-decouple` library. This library provides a convenient way to load environment variables from a file and access them throughout the application. The `Config` class from `python-decouple` is used to create a `config` object, which acts as a centralized point of access for the environment variables.
One critical aspect of the environment variable setup is the handling of sensitive information, such as API keys and passphrases. To ensure the security of these sensitive values, the Pastel Inference Layer server employs encryption techniques. The sensitive fields in the `.env` file are encrypted using the `Fernet` symmetric encryption algorithm from the `cryptography` library.

The encryption and decryption process is handled by a set of dedicated functions in the `service_functions.py` module. These functions include:


- `generate_or_load_encryption_key_sync`: Generates or loads the encryption key synchronously.
- `encrypt_sensitive_fields`: Encrypts the sensitive fields in the `.env` file.
- `decrypt_sensitive_data`: Decrypts the encrypted data using the provided encryption key.
- `encrypt_sensitive_data`: Encrypts the sensitive data using the provided encryption key.
- `decrypt_sensitive_fields`: Decrypts the sensitive fields and assigns them to global variables.

The encryption key is generated or loaded during the server startup process. If an encryption key file doesn't exist, a new key is generated using the `Fernet.generate_key()` method and saved to a file for future use. The encryption key is then used to encrypt the sensitive fields in the `.env` file, replacing the plain-text values with encrypted strings.

During runtime, the `decrypt_sensitive_fields` function is called to decrypt the sensitive fields using the encryption key. The decrypted values are assigned to global variables, making them accessible throughout the application. This approach ensures that the sensitive information remains protected at rest (in the `.env` file) and is only decrypted when needed during execution.
The encryption and decryption process strikes a balance between security and usability. By encrypting the sensitive fields, the Pastel Inference Layer server minimizes the risk of unauthorized access to critical information. At the same time, the decryption of these fields during runtime allows the server to utilize the necessary API keys and passphrases for its operations.

In summary, the environment variable setup and processing in the Pastel Inference Layer server play a vital role in the configuration and security of the application. The use of a `.env` file and the `python-decouple` library enables flexible and centralized management of configuration settings. The encryption and decryption of sensitive fields using the `Fernet` algorithm ensures the protection of critical information while still allowing seamless integration with the server's functionality. This setup demonstrates a thoughtful approach to balancing configuration flexibility, security, and ease of use in the Pastel Inference Layer server.

----------

**Ansible Playbook for Deploying Inference Server**

The Inference Server repo includes an ansible playbook called [pastel_inference_layer_deployment_playbook.yml](https://github.com/pastelnetwork/python_inference_layer_server/blob/master/setup_scripts_and_playbooks/pastel_inference_layer_deployment_playbook.yml) that largely automates the process of deploying the system on a fresh machine, or updating a configured inference server Supernode when the inference code is updated. 

The complete playbook is shown below, with explanation to follow:

```YAML
---
- name: Pastel Inference Layer Deployment
  hosts: all
  become: yes
  vars:
    ubuntu_user: ubuntu
    oh_my_zsh_install_script: "https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh"
    rust_install_script: "https://sh.rustup.rs"
    atuin_install_script: "https://setup.atuin.sh"
    zshrc_path: "/home/{{ ubuntu_user }}/.zshrc"
    bashrc_path: "/home/{{ ubuntu_user }}/.bashrc"
    oh_my_zsh_install_flag: "/home/{{ ubuntu_user }}/.oh-my-zsh"
    home_dir: "/home/{{ ubuntu_user }}"

  tasks:
    - name: Update and upgrade apt packages
      ansible.builtin.apt:
        update_cache: yes
        upgrade: dist
        autoremove: yes

    - name: Check if zsh is installed
      command: which zsh
      register: zsh_installed
      ignore_errors: yes

    - name: Ensure zsh_installed is defined
      set_fact:
        zsh_installed: { "rc": 1 }
      when: zsh_installed is not defined

    - name: Check if Oh My Zsh is installed
      stat:
        path: "{{ oh_my_zsh_install_flag }}"
      register: oh_my_zsh_installed

    - name: Install Oh My Zsh
      become_user: "{{ ubuntu_user }}"
      shell: >
        sh -c "$(curl -fsSL {{ oh_my_zsh_install_script }})" && touch {{ oh_my_zsh_install_flag }}
      when: not oh_my_zsh_installed.stat.exists

    - name: Install Rust
      become_user: "{{ ubuntu_user }}"
      shell: >
        curl -fsSL {{ rust_install_script }} | sh -s -- -y

    - name: Ensure Rust environment is loaded
      lineinfile:
        path: "{{ zshrc_path }}"
        regexp: 'source $HOME/.cargo/env'
        line: 'source $HOME/.cargo/env'
        state: present
      become_user: "{{ ubuntu_user }}"

    - name: Install Atuin
      become_user: "{{ ubuntu_user }}"
      shell: >
        /bin/bash -c "$(curl --proto '=https' --tlsv1.2 -sSf {{ atuin_install_script }})"

    - name: Ensure Atuin environment is loaded
      lineinfile:
        path: "{{ zshrc_path }}"
        regexp: 'eval "$(atuin init zsh)"'
        line: 'eval "$(atuin init zsh)"'
        state: present
      become_user: "{{ ubuntu_user }}"

    - name: Install dependencies for pyenv
      apt:
        name:
          - build-essential
          - libssl-dev
          - zlib1g-dev
          - libbz2-dev
          - libreadline-dev
          - libsqlite3-dev
          - wget
          - curl
          - llvm
          - libncurses5-dev
          - libncursesw5-dev
          - xz-utils
          - tk-dev
          - libffi-dev
          - liblzma-dev
          - python3-openssl
          - git
        state: present
        update_cache: yes

    - name: Clone pyenv repository
      git:
        repo: 'https://github.com/pyenv/pyenv.git'
        dest: '{{ home_dir }}/.pyenv'
        update: yes
        force: yes

    - name: Set pyenv environment variables in .zshrc if zsh is found
      blockinfile:
        path: '{{ zshrc_path }}'
        block: |
          export PYENV_ROOT="$HOME/.pyenv"
          export PATH="$PYENV_ROOT/bin:$PATH"
          eval "$(pyenv init --path)"
      when: zsh_installed.rc == 0

    - name: Set pyenv environment variables in .bashrc if zsh is not found
      blockinfile:
        path: '{{ bashrc_path }}'
        block: |
          export PYENV_ROOT="$HOME/.pyenv"
          export PATH="$PYENV_ROOT/bin:$PATH"
          eval "$(pyenv init --path)"
      when: zsh_installed.rc != 0

    - name: Ensure pyenv directories have correct permissions
      file:
        path: /home/ubuntu/.pyenv
        state: directory
        owner: ubuntu
        group: ubuntu
        mode: '0755'
        recurse: yes

    - name: Ensure pyenv is available and install Python 3.12
      become: no
      shell: |
        export PATH="/home/ubuntu/.pyenv/bin:$PATH"
        eval "$(pyenv init --path)"
        eval "$(pyenv init -)"
        pyenv install 3.12
      args:
        executable: /bin/bash
      environment:
        PYENV_ROOT: "/home/ubuntu/.pyenv"
      become_user: ubuntu

    - name: Check again if zsh is installed
      command: which zsh
      register: zsh_installed
      ignore_errors: yes
      failed_when: false

    - name: Replace /root/ with /home/ubuntu/ in .zshrc
      replace:
        path: "/home/{{ ubuntu_user }}/.zshrc"
        regexp: "/root/"
        replace: "/home/{{ ubuntu_user }}/"

    - name: Determine the user shell
      shell: echo $SHELL
      register: user_shell

    - name: Set shell path and profile file
      set_fact:
        shell_path: "{{ user_shell.stdout }}"
        profile_file: "{{ 'zshrc' if '/zsh' in user_shell.stdout else 'bashrc' }}"

    - name: Check if the application directory exists
      stat:
        path: /home/{{ ubuntu_user }}/python_inference_layer_server
      register: app_dir

    - name: Clone the repository if the directory doesn't exist
      git:
        repo: https://github.com/pastelnetwork/python_inference_layer_server
        dest: /home/{{ ubuntu_user }}/python_inference_layer_server
      when: not app_dir.stat.exists

    - name: Run initial setup script if the directory was just created
      shell: |
        chmod +x setup_scripts_and_playbooks/initial_inference_server_setup_script.sh
        ./setup_scripts_and_playbooks/initial_inference_server_setup_script.sh
      args:
        chdir: /home/{{ ubuntu_user }}/python_inference_layer_server
      when: not app_dir.stat.exists

    - name: Update code
      shell: |
        source /home/{{ ubuntu_user }}/.{{ profile_file }}
        git stash
        git pull
      args:
        chdir: /home/{{ ubuntu_user }}/python_inference_layer_server
        executable: "{{ shell_path }}"

    - name: Get the name of the existing tmux session
      shell: tmux list-sessions -F '#{session_name}' | head -1
      register: tmux_session_name
      ignore_errors: true

    - name: Create tmux session if it doesn't exist
      shell: tmux new-session -d -s default_session
      when: tmux_session_name.stdout == ""
      args:
        executable: "{{ shell_path }}"

    - name: Set the tmux session name
      set_fact:
        session_name: "{{ tmux_session_name.stdout if tmux_session_name.stdout else 'default_session' }}"

    - name: Check if supernode_script window exists
      shell: tmux list-windows -t {{ session_name }} -F '#{window_name}' | grep -q '^supernode_script$'
      register: window_exists
      ignore_errors: true

    - name: Kill supernode_script window if it exists
      shell: tmux kill-window -t {{ session_name }}:supernode_script
      when: window_exists.rc == 0

    - name: Create temporary script
      copy:
        content: |
          #!/bin/{{ 'zsh' if '/zsh' in shell_path else 'bash' }}
          source /home/{{ ubuntu_user }}/.{{ profile_file }}
          cd /home/{{ ubuntu_user }}/python_inference_layer_server
          pyenv local 3.12
          source venv/bin/activate
          python -m pip install --upgrade pip
          python -m pip install --upgrade setuptools
          python -m pip install wheel
          pip install -r requirements.txt
          python main.py
        dest: /home/{{ ubuntu_user }}/run_script.sh
        mode: '0755'

    - name: Launch script in new tmux window
      shell: |
        tmux new-window -t {{ session_name }}: -n supernode_script -d "{{ shell_path }} -c '/home/{{ ubuntu_user }}/run_script.sh'"
      args:
        executable: "{{ shell_path }}"

    - name: Remove temporary script
      file:
        path: /home/{{ ubuntu_user }}/run_script.sh
        state: absent

```

**Detailed Breakdown of the Ansible Playbook for the Pastel Inference Layer Deployment:**

The Ansible playbook above is designed to automate the deployment of the Pastel Inference Layer on a server. It performs various tasks to prepare the environment, install necessary software, and set up the application for running within a `tmux` session. Below is a comprehensive breakdown of each step:

#### Initial Setup and Package Management
The playbook begins by updating the package lists and upgrading the installed packages. This ensures that the system is up to date with the latest security patches and software versions. The `autoremove` option cleans up unnecessary packages, keeping the system lean.

#### Checking and Installing Oh My Zsh
The playbook checks if Oh My Zsh is already installed by looking for a specific file. If it is not installed, it uses a shell script to install Oh My Zsh, which provides an enhanced shell experience.

#### Installing Rust
Rust is installed using a provided script. Rust is essential for compiling certain dependencies and tools that may be required by the Pastel Inference Layer.

#### Configuring Shell Environments
The playbook ensures that the Rust environment variables are loaded by adding them to the `.zshrc` file. This allows the user to use Rust tools seamlessly.

#### Installing Atuin
Atuin, a tool for enhancing shell history, is installed. The playbook adds the necessary initialization commands to `.zshrc` to ensure it is available in the shell.

#### Setting Up pyenv and Python
The playbook installs dependencies required for building Python from source and then clones the `pyenv` repository. `pyenv` allows for easy management of multiple Python versions.

Depending on whether `zsh` or `bash` is the default shell, it adds `pyenv` initialization commands to either `.zshrc` or `.bashrc`. This ensures `pyenv` is set up correctly regardless of the shell in use.

#### Ensuring pyenv Initialization
The playbook runs commands to initialize `pyenv` in the respective shell and checks if `pyenv` is correctly installed by verifying its version. This step ensures that the environment is correctly set up for Python version management.

#### Installing Python 3.12 and Setting Up Virtual Environment
The playbook installs Python 3.12 using `pyenv`. It then creates and activates a virtual environment within the project directory. This isolated environment ensures that dependencies for the Pastel Inference Layer do not interfere with other Python projects on the system.

#### Cloning the Application Repository
The playbook checks if the application directory exists. If it does not, it clones the repository from GitHub and runs an initial setup script. This script likely sets up necessary configurations and dependencies specific to the Pastel Inference Layer.

#### Updating the Application Code
If the repository already exists, the playbook stashes any local changes and pulls the latest code from the repository. This ensures the application is up to date with the latest changes.

#### Setting Up and Managing tmux Session
The playbook uses `tmux` to create a new session or attach to an existing one. This allows the application to run in the background, even if the SSH session is disconnected.

It checks if a specific `tmux` window for running the application exists. If it does, it kills the window to prevent multiple instances from running. It then creates a new window within the `tmux` session to run the application.

A temporary script is created and executed within the `tmux` window to start the application. This script sets up the environment, activates the virtual environment, installs any missing dependencies, and starts the application.

#### Cleaning Up
After launching the application, the playbook removes the temporary script to keep the system clean.

----------

**Using a Remote GPU-Enabled Instance of Swiss Army Llama on Vast.ai to Speed Up Inference**
 
The Pastel Inference Layer server supports the use of a remote GPU-enabled instance of Swiss Army Llama hosted on Vast.ai to significantly speed up inference tasks. This approach leverages the power of GPU acceleration while maintaining a cost-effective and decentralized infrastructure.
The process involves using a custom Docker image that encapsulates the necessary dependencies and configurations for running Swiss Army Llama on a GPU-enabled instance. The Docker image is based on the PyTorch image with CUDA and cuDNN pre-installed, ensuring compatibility with GPU acceleration.

The Dockerfile performs the following steps:

1. Sets environment variables for FAISS GPU build and CUDA support.
2. Installs necessary packages and dependencies, including build tools, libraries, and utilities.
3. Installs Rust and Atuin for additional functionality.
4. Clones the Swiss Army Llama repository and sets the working directory.
5. Installs Python dependencies using conda and pip, including the GPU-enabled version of FAISS.
6. Exposes the necessary ports for the Swiss Army Llama server and Redis.
7. Creates a benchmark script to assess the performance of the host machine.
8. Defines the command to start the benchmark, Redis server, and Swiss Army Llama server when the container starts.
9. 

The resulting Docker image is published on Docker Hub [here](https://hub.docker.com/repository/docker/jemanuel82/vastai_swiss_army_llama_template/general), making it easily accessible for deployment. The exact Dockerfile contents is shown below:

```bash
    # Use the PyTorch image with CUDA and cuDNN pre-installed
    FROM pytorch/pytorch:2.2.0-cuda12.1-cudnn8-devel
    
    # Set environment variables for FAISS GPU build and CUDA support
    ENV DEBIAN_FRONTEND=noninteractive
    ENV PYTHONUNBUFFERED=1
    ENV DATA_DIRECTORY=/swiss_army_llama
    ENV FAISS_ENABLE_GPU=ON
    ENV LLAMA_CUDA=on
    ENV PATH="/root/.cargo/bin:${PATH}"
    
    # Pre-accept the Microsoft EULA for ttf-mscorefonts-installer and update/install packages
    RUN echo ttf-mscorefonts-installer msttcorefonts/accepted-mscorefonts-eula select true | debconf-set-selections && \
        apt-get update && \
        apt-get install -y \
        build-essential curl pkg-config ffmpeg gcc g++ gdb gdisk git cmake htop psmisc sysbench \
        libboost-all-dev libssl-dev make nano nginx nodejs npm openssh-client iperf3 \
        openssh-server openssl rsync libpulse-dev software-properties-common inetutils-ping \
        ubuntu-release-upgrader-core ubuntu-restricted-extras ufw unzip vcsh vim vpnc \
        zip zlib1g-dev zstd libpq-dev libmagic1 libxml2-dev libxslt1-dev antiword \
        unrtf poppler-utils tesseract-ocr flac lame libmad0 libsox-fmt-mp3 sox \
        libjpeg-dev swig redis-server wget libffi-dev libbz2-dev libreadline-dev \
        libsqlite3-dev llvm libncurses5-dev xz-utils tk-dev libxmlsec1-dev liblzma-dev \
        libncursesw5-dev python3-openssl libpoppler-cpp-dev pstotext net-tools iproute2 multitail && \
        apt-get upgrade -y && \
        apt-get autoremove -y && \
        rm -rf /var/lib/apt/lists/*
    
    # Install Rust and Atuin
    RUN curl https://sh.rustup.rs -sSf | sh -s -- -y && \
        /bin/bash -c "$(curl --proto '=https' --tlsv1.2 -sSf https://setup.atuin.sh)"
    
    # Clone the repository
    RUN git clone https://github.com/Dicklesworthstone/swiss_army_llama.git /swiss_army_llama
    WORKDIR /swiss_army_llama
    
    # Install Python dependencies using conda and pip
    RUN conda update -n base -c defaults conda && \
        conda install -c conda-forge python=3.12 pip setuptools wheel && \
        conda install -c conda-forge faiss-gpu && \
        CMAKE_ARGS="-DLLAMA_CUDA=on" /opt/conda/bin/pip install --no-cache-dir -r requirements.txt
    
    # Expose the port the app runs on and Redis default port
    EXPOSE 8089 6379
    
    # Create benchmark script
    RUN echo '#!/bin/bash' > /swiss_army_llama/benchmark.sh && \
        echo 'echo "CPU Benchmark:" > /swiss_army_llama/host_machine_benchmark_results.txt' >> /swiss_army_llama/benchmark.sh && \
        echo 'sysbench --test=cpu --cpu-max-prime=5000 run >> /swiss_army_llama/host_machine_benchmark_results.txt' >> /swiss_army_llama/benchmark.sh && \
        echo 'echo "\nMemory Benchmark:" >> /swiss_army_llama/host_machine_benchmark_results.txt' >> /swiss_army_llama/benchmark.sh && \
        echo 'sysbench --test=memory --memory-block-size=1M --memory-total-size=2G run >> /swiss_army_llama/host_machine_benchmark_results.txt' >> /swiss_army_llama/benchmark.sh && \
        echo 'echo "\nDisk Benchmark:" >> /swiss_army_llama/host_machine_benchmark_results.txt' >> /swiss_army_llama/benchmark.sh && \
        echo 'dd if=/dev/zero of=/tmp/test1.img bs=1G count=1 oflag=dsync 2>> /swiss_army_llama/host_machine_benchmark_results.txt' >> /swiss_army_llama/benchmark.sh && \
        echo 'rm -f /tmp/test1.img' >> /swiss_army_llama/benchmark.sh && \
        echo 'echo "\nNetwork Benchmark (localhost):" >> /swiss_army_llama/host_machine_benchmark_results.txt' >> /swiss_army_llama/benchmark.sh && \
        echo 'iperf3 -s & sleep 1' >> /swiss_army_llama/benchmark.sh && \
        echo 'iperf3 -c localhost -t 10 >> /swiss_army_llama/host_machine_benchmark_results.txt' >> /swiss_army_llama/benchmark.sh && \
        echo 'pkill iperf3' >> /swiss_army_llama/benchmark.sh && \
        chmod +x /swiss_army_llama/benchmark.sh
    
    # Start benchmark, Redis, and swiss_army_llama server when the container starts
    CMD /swiss_army_llama/benchmark.sh && service redis-server start && python /swiss_army_llama/swiss_army_llama.py
```


To utilize this GPU-enabled Swiss Army Llama instance, users can leverage the Vast.ai platform. Vast.ai is a decentralized marketplace that allows individuals to rent out their spare GPU capacity at affordable rates. Users can spin up a powerful machine with a 4090 GPU and ample RAM for as low as 34 cents per hour, significantly lower than traditional cloud providers like AWS, Azure, or Lambda. It’s sort of like the “UberX” of GPU cloud instance providers, with a disruptive and compelling business model. 

To get started with Vast.ai, users need to follow these steps:


1. Create an account on Vast.ai and purchase credits using a credit card (via Stripe) or by paying with cryptocurrency.
2. Generate an SSH key and add it to their Vast.ai account for secure access to provisioned instances.
3. Select the [Swiss Army Llama template](https://cloud.vast.ai/?ref_id=78066&template_id=3c711e5ddd050882f0eb6f4ce1b8cc28](https://cloud.vast.ai/?ref_id=78066&template_id=3c711e5ddd050882f0eb6f4ce1b8cc28) to speed up the deployment process. This template specifies the custom Docker image and the required storage of 60GB.
4. Provision the instance on Vast.ai, which will provide the IP address and port for SSH access.
5. Access the provisioned machine using SSH with the command provided by Vast.ai, similar to:

```bash
    ssh -i ~/vastai_privkey -p 20765 root@106.185.159.136
```

6. Once connected to the instance, navigate to the Swiss Army Llama directory and start the server:

```bash
    cd /swiss_army_llama/ 
    git update python swiss_army_llama.py
    cd /swiss_army_llama/ git update python swiss_army_llama.py
```

7. This will initiate the process of downloading the default Swiss Army Llama model files from Hugging Face.

To integrate the remote Swiss Army Llama instance with the local Supernode inference servers, users need to add the following optional fields to their `.env` file:

```ini
    USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE=1
    REMOTE_SWISS_ARMY_LLAMA_INSTANCE_SSH_KEY_PATH=/home/ubuntu/vastai_privkey
    REMOTE_SWISS_ARMY_LLAMA_INSTANCE_IP_ADDRESS=106.185.159.136
    REMOTE_SWISS_ARMY_LLAMA_INSTANCE_PORT=20765
    REMOTE_SWISS_ARMY_LLAMA_EXPOSED_PORT=8089
    REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT=8087
```

With these configurations in place, the Pastel Inference Layer server will automatically detect the availability of the remote Swiss Army Llama instance and establish an SSH tunnel to the Vast.ai machine. If the connection is successful, all Swiss Army Llama requests from the Supernode inference servers will be routed to the associated remote GPU-enabled instance.
This setup offers several benefits:

1. Improved Inference Performance: By utilizing a GPU-enabled instance, inference tasks can be accelerated significantly, reducing latency and improving overall performance.
2. Cost-Effectiveness: Vast.ai's decentralized marketplace allows users to rent GPU resources at affordable rates, making it more cost-effective compared to traditional cloud providers.
3. Decentralization and Censorship Resistance: By leveraging a decentralized platform like Vast.ai, the Pastel Inference Layer server maintains its decentralized nature and resists censorship attempts.
4. Resource Sharing: Multiple Supernode inference servers can share a single remote GPU instance, optimizing resource utilization and reducing costs.

By integrating a remote GPU-enabled instance of Swiss Army Llama hosted on Vast.ai, the Pastel Inference Layer server achieves a balance between inference performance, cost-effectiveness, and decentralization. This approach empowers users to harness the power of GPU acceleration while maintaining the core principles of the Pastel network.




---
./main.py
---
import warnings
from cryptography.utils import CryptographyDeprecationWarning
from logger_config import setup_logger
from endpoint_functions import router
import asyncio
import os
import random
import traceback
import fastapi
import threading
from fastapi import Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.exceptions import RequestValidationError
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.middleware.base import RequestResponseEndpoint
from starlette.responses import Response
import uvloop
from uvicorn import Config, Server
from decouple import Config as DecoupleConfig, RepositoryEnv
from database_code import initialize_db
from setup_swiss_army_llama import check_and_setup_swiss_army_llama
from service_functions import (monitor_new_messages, generate_or_load_encryption_key_sync, decrypt_sensitive_data, get_env_value, fetch_all_mnid_tickets_details, establish_ssh_tunnel, schedule_micro_benchmark_periodically,
                                list_generic_tickets_in_blockchain_and_parse_and_validate_and_store_them, generate_supernode_inference_ip_blacklist)
warnings.filterwarnings("ignore", category=CryptographyDeprecationWarning)
config = DecoupleConfig(RepositoryEnv('.env'))
UVICORN_PORT = config.get("UVICORN_PORT", cast=int)
USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE = config.get("USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE", default=0, cast=int)
SWISS_ARMY_LLAMA_SECURITY_TOKEN = config.get("SWISS_ARMY_LLAMA_SECURITY_TOKEN", cast=str)
os.environ['TZ'] = 'UTC' # Set timezone to UTC for the current session
asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
logger = setup_logger()

app = fastapi.FastAPI(
    title="Pastel-Supernode-Inference-Layer",
    description="Pastel Supernode Inference Layer API",
    docs_url="/",
    redoc_url="/redoc"
)

class LimitRequestSizeMiddleware(BaseHTTPMiddleware):
    def __init__(self, app, max_request_size: int):
        super().__init__(app)
        self.max_request_size = max_request_size

    async def dispatch(self, request: Request, call_next: RequestResponseEndpoint) -> Response:
        request_size = int(request.headers.get('content-length', 0))
        if request_size > self.max_request_size:
            return Response("Request size exceeds the limit", status_code=413)
        return await call_next(request)
    
app.add_middleware(LimitRequestSizeMiddleware, max_request_size=50 * 1024 * 1024)
app.include_router(router, prefix='', tags=['main'])

# Custom Exception Handling Middleware
@app.middleware("http")
async def custom_exception_handling(request: Request, call_next):
    try:
        return await call_next(request)
    except RequestValidationError as ve:
        logger.error(f"Validation error: {ve}")
        return JSONResponse(status_code=ve.status_code, content={"detail": ve.error_msg})
    except Exception as e:
        tb = traceback.format_exc()  # Get the full traceback
        logger.error(f"Unhandled exception: {e}\n{tb}")  # Log the exception with traceback
        return JSONResponse(status_code=500, content={"detail": str(e)})
    
# CORS Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=['*'],
    allow_credentials=True,
    allow_methods=['*'],
    allow_headers=['*'],
    expose_headers=["Authorization"]
)

def decrypt_sensitive_fields():
    global LOCAL_PASTEL_ID_PASSPHRASE, SWISS_ARMY_LLAMA_SECURITY_TOKEN, OPENAI_API_KEY, CLAUDE3_API_KEY, GROQ_API_KEY, MISTRAL_API_KEY, STABILITY_API_KEY, OPENROUTER_API_KEY, encryption_key
    LOCAL_PASTEL_ID_PASSPHRASE = decrypt_sensitive_data(get_env_value("LOCAL_PASTEL_ID_PASSPHRASE"), encryption_key)
    SWISS_ARMY_LLAMA_SECURITY_TOKEN = decrypt_sensitive_data(get_env_value("SWISS_ARMY_LLAMA_SECURITY_TOKEN"), encryption_key)
    OPENAI_API_KEY = decrypt_sensitive_data(get_env_value("OPENAI_API_KEY"), encryption_key)
    CLAUDE3_API_KEY = decrypt_sensitive_data(get_env_value("CLAUDE3_API_KEY"), encryption_key)
    GROQ_API_KEY = decrypt_sensitive_data(get_env_value("GROQ_API_KEY"), encryption_key)
    MISTRAL_API_KEY = decrypt_sensitive_data(get_env_value("MISTRAL_API_KEY"), encryption_key)
    STABILITY_API_KEY = decrypt_sensitive_data(get_env_value("STABILITY_API_KEY"), encryption_key)
    OPENROUTER_API_KEY = decrypt_sensitive_data(get_env_value("OPENROUTER_API_KEY"), encryption_key)
    
async def startup():
    global encryption_key  # Declare encryption_key as global
    try:
        db_init_complete = await initialize_db()
        logger.info(f"Database initialization complete: {db_init_complete}")
        encryption_key = generate_or_load_encryption_key_sync()  # Generate or load the encryption key synchronously    
        decrypt_sensitive_fields() # Now decrypt sensitive fields        
        asyncio.create_task(monitor_new_messages())  # Create a background task
        asyncio.create_task(fetch_all_mnid_tickets_details())
        asyncio.create_task(list_generic_tickets_in_blockchain_and_parse_and_validate_and_store_them())
        # asyncio.create_task(periodic_ticket_listing_and_validation())
        asyncio.create_task(asyncio.to_thread(check_and_setup_swiss_army_llama, SWISS_ARMY_LLAMA_SECURITY_TOKEN)) # Check and setup Swiss Army Llama asynchronously
        await generate_supernode_inference_ip_blacklist()  # Compile IP blacklist text file of unresponsive Supernodes for inference tasks
        asyncio.create_task(schedule_generate_supernode_inference_ip_blacklist())  # Schedule the task
        asyncio.create_task(schedule_micro_benchmark_periodically())  # Schedule the task
    except Exception as e:
        logger.error(f"Error during startup: {e}")
        
@app.on_event("startup")
async def startup_event():
    await startup()

async def schedule_generate_supernode_inference_ip_blacklist():
    while True:
        jitter = random.randint(-180, 180)  # Jitter of up to 3 minutes (180 seconds)
        interval_seconds = 300 + jitter  # 300 seconds = 5 minutes
        await asyncio.sleep(interval_seconds)
        await generate_supernode_inference_ip_blacklist()
        
async def main():
    uvicorn_config = Config(
        "main:app",
        host="0.0.0.0",
        port=UVICORN_PORT,
        loop="uvloop",
    )
    server = Server(uvicorn_config)
    await server.serve()

if __name__ == "__main__":
    if USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
        ssh_thread = threading.Thread(target=establish_ssh_tunnel, daemon=True)
        ssh_thread.start()
    generate_or_load_encryption_key_sync()
    config = DecoupleConfig(RepositoryEnv('.env'))
    asyncio.run(main())


---
./endpoint_functions.py
---
import service_functions
import database_code as db
from logger_config import logger
from fastapi import APIRouter, Depends, Query, Request, Body
from fastapi.responses import HTMLResponse, JSONResponse, StreamingResponse, FileResponse
from fastapi.exceptions import HTTPException
from fastapi.security import HTTPBasic, HTTPBasicCredentials
from starlette.background import BackgroundTask
from starlette.status import HTTP_401_UNAUTHORIZED
from json import JSONEncoder
from pathlib import Path
import json
import os
import asyncio
import tempfile
import uuid
import traceback
import pickle
import secrets
import pandas as pd
import plotly.express as px
from datetime import datetime, timedelta, timezone
from typing import Optional, List, Union, Dict, Any
from pydantic import SecretStr, BaseModel
from decouple import Config as DecoupleConfig, RepositoryEnv

config = DecoupleConfig(RepositoryEnv('.env'))
TEMP_OVERRIDE_LOCALHOST_ONLY = config.get("TEMP_OVERRIDE_LOCALHOST_ONLY", default=0)
SN_PERFORMANCE_STATS_PASSWORD = config.get("SN_PERFORMANCE_STATS_PASSWORD", default="CHANGEME")
pickle_file_path = Path('performance_data_history.pkl')
security = HTTPBasic()

def verify_password(credentials: HTTPBasicCredentials = Depends(security)):
    correct_password = secrets.compare_digest(credentials.password, SN_PERFORMANCE_STATS_PASSWORD)
    if not correct_password:
        raise HTTPException(
            status_code=HTTP_401_UNAUTHORIZED,
            detail="Incorrect password",
            headers={"WWW-Authenticate": "Basic"},
        )
    return credentials.password

# RPC Client Dependency
async def get_rpc_connection():
    rpc_host, rpc_port, rpc_user, rpc_password, other_flags = service_functions.get_local_rpc_settings_func() 
    return service_functions.AsyncAuthServiceProxy(f"http://{rpc_user}:{rpc_password}@{rpc_host}:{rpc_port}")

router = APIRouter()

def localhost_only(request: Request):
    client_host = request.client.host
    if not TEMP_OVERRIDE_LOCALHOST_ONLY:
        if client_host != "127.0.0.1":
            raise HTTPException(status_code=401, detail="Unauthorized")
    
class DateTimeEncoder(JSONEncoder):
    def default(self, obj):
        if isinstance(obj, (datetime.date, datetime.datetime)):
            return obj.isoformat()
        
@router.get("/liveness_ping", response_model=dict)
async def liveness_ping_function():
    current_utc_timestamp = datetime.now(timezone.utc)
    if service_functions.benchmark_results_cache:
        latest_benchmark = service_functions.benchmark_results_cache[-1]
        raw_benchmark_score = latest_benchmark[1]
        performance_ratio_score = latest_benchmark[2]
        last_benchmark_time = datetime.fromisoformat(latest_benchmark[0])
        if last_benchmark_time.tzinfo is None:
            last_benchmark_time = last_benchmark_time.replace(tzinfo=timezone.utc)
        last_benchmark_n_seconds_ago = (current_utc_timestamp - last_benchmark_time).total_seconds()
    else:
        raw_benchmark_score = None
        performance_ratio_score = None
        last_benchmark_n_seconds_ago = None

    response_dict = {
        'status': 'alive', 
        'timestamp': current_utc_timestamp, 
        'raw_benchmark_score': raw_benchmark_score, 
        'performance_ratio_score': performance_ratio_score,
        'last_benchmark_n_seconds_ago': last_benchmark_n_seconds_ago
    }
    return response_dict

@router.get("/supernode_list_json", response_model=dict)
async def get_supernode_list_json(
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Retrieves the list of Supernodes as JSON data.

    Returns a JSON object containing the Supernode list data.

    Raises:
    - HTTPException (status_code=500): If an error occurs while retrieving the Supernode list.

    Example response:
    {
        "1234567890abcdef": {
            "supernode_status": "ENABLED",
            "protocol_version": "1.0",
            "supernode_psl_address": "tPmkbohSbiocyAhXJVBZkBsKJiuVyNRp2GJ",
            "lastseentime": "2024-03-22T12:34:56.789000",
            "activeseconds": 3600,
            "lastpaidtime": "2024-03-22T11:34:56.789000",
            "lastpaidblock": 12345,
            "ipaddress:port": "127.0.0.1:9999",
            "rank": 1,
            "pubkey": "0x1234567890abcdef",
            "extAddress": "127.0.0.1:9999",
            "extP2P": "127.0.0.1:9998",
            "extKey": "1234567890abcdef",
            "activedays": 1.0
        },
        ...
    }
    """
    try:
        _, supernode_list_json = await service_functions.check_supernode_list_func()
        return JSONResponse(content=json.loads(supernode_list_json))
    except Exception as e:
        logger.error(f"Error getting supernode list JSON: {e}")
        raise HTTPException(status_code=500, detail="Internal Server Error")


@router.get("/supernode_list_csv")
async def get_supernode_list_csv(
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Retrieves the list of Supernodes as a normalized CSV file.

    Returns a StreamingResponse containing the CSV file data.

    Raises:
    - HTTPException (status_code=500): If an error occurs while retrieving or processing the Supernode list.

    Example response:
    A CSV file named "supernode_list.csv" will be downloaded containing the normalized Supernode list data.

    CSV file structure:
    supernode_status,protocol_version,supernode_psl_address,lastseentime,activeseconds,lastpaidtime,lastpaidblock,ipaddress:port,rank,pubkey,extAddress,extP2P,extKey,activedays
    ENABLED,1.0,tPmkbohSbiocyAhXJVBZkBsKJiuVyNRp2GJ,2024-03-22T12:34:56.789000,3600,2024-03-22T11:34:56.789000,12345,127.0.0.1:9999,1,0x1234567890abcdef,127.0.0.1:9999,127.0.0.1:9998,1234567890abcdef,1.0
    ...
    """
    try:
        supernode_list_df, _ = await service_functions.check_supernode_list_func()
        normalized_df = pd.json_normalize(supernode_list_df.to_dict(orient='records'))
        # Convert the normalized DataFrame to CSV
        csv_data = normalized_df.to_csv(index=False)
        # Create a StreamingResponse with the CSV data
        response = StreamingResponse(iter([csv_data]), media_type="text/csv")
        response.headers["Content-Disposition"] = "attachment; filename=supernode_list.csv"
        return response
    except Exception as e:
        logger.error(f"Error getting supernode list CSV: {e}")
        raise HTTPException(status_code=500, detail="Internal Server Error")


@router.get("/get_local_machine_sn_info", response_model=db.LocalMachineSupernodeInfo, dependencies=[Depends(localhost_only)])
async def get_local_machine_sn_info(
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Note: Endpoint only available on localhost.
        
    Retrieves information about the local machine's Supernode status.

    Returns a LocalMachineSupernodeInfo object containing the following fields:
    - `local_machine_supernode_data`: DataFrame containing the local machine's Supernode data (if it is a Supernode).
    - `local_sn_rank`: The rank of the local machine's Supernode (if it is a Supernode).
    - `local_sn_pastelid`: The PastelID of the local machine's Supernode (if it is a Supernode).
    - `local_machine_ip_with_proper_port`: The IP address and port of the local machine's Supernode (if it is a Supernode).

    Raises:
    - HTTPException (status_code=404): If the local machine is not a Supernode.
    - HTTPException (status_code=500): If an error occurs while retrieving the local machine's Supernode information.

    Example response:
    {
        "local_machine_supernode_data": {
            "supernode_status": "ENABLED",
            "protocol_version": "1.0",
            ...
        },
        "local_sn_rank": 1,
        "local_sn_pastelid": "1234567890abcdef",
        "local_machine_ip_with_proper_port": "127.0.0.1:9999"
    }
    """
    try:
        local_machine_supernode_data, local_sn_rank, local_sn_pastelid, local_machine_ip_with_proper_port = await service_functions.get_local_machine_supernode_data_func()
        if len(local_machine_supernode_data) > 0:
            return db.LocalMachineSupernodeInfo(
                local_machine_supernode_data=local_machine_supernode_data.to_dict(orient='records')[0],
                local_sn_rank=local_sn_rank,
                local_sn_pastelid=local_sn_pastelid,
                local_machine_ip_with_proper_port=local_machine_ip_with_proper_port
            )
        else:
            raise HTTPException(status_code=404, detail="Local machine is not a Supernode")
    except Exception as e:
        logger.error(f"Error getting local machine Supernode info: {e}")
        raise HTTPException(status_code=500, detail="Internal Server Error")
    
    
@router.get("/get_sn_data_from_pastelid", response_model=db.SupernodeData, dependencies=[Depends(localhost_only)])
async def get_sn_data_from_pastelid(
    pastelid: str = Query(..., description="The PastelID of the Supernode"),
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Note: Endpoint only available on localhost.
        
    Retrieves Supernode data based on the specified PastelID.

    - `pastelid`: The PastelID of the Supernode.

    Returns a SupernodeData object containing the Supernode data.

    Raises:
    - HTTPException (status_code=404): If the specified machine is not a Supernode.
    - HTTPException (status_code=500): If an error occurs while retrieving the Supernode data.

    Example response:
    {
        "supernode_status": "ENABLED",
        "protocol_version": "1.0",
        "supernode_psl_address": "tPmkbohSbiocyAhXJVBZkBsKJiuVyNRp2GJ",
        ...
    }
    """
    try:
        supernode_data = await service_functions.get_sn_data_from_pastelid_func(pastelid)
        if not supernode_data.empty:
            return db.SupernodeData(**supernode_data.to_dict(orient='records')[0])
        else:
            raise HTTPException(status_code=404, detail="Specified machine is not a Supernode")
    except Exception as e:
        logger.error(f"Error getting Supernode data from PastelID: {e}")
        raise HTTPException(status_code=500, detail="Internal Server Error")    
    
    
@router.get("/get_sn_data_from_sn_pubkey", response_model=db.SupernodeData, dependencies=[Depends(localhost_only)])
async def get_sn_data_from_sn_pubkey(
    pubkey: str = Query(..., description="The public key of the Supernode"),
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Note: Endpoint only available on localhost.
        
    Retrieves Supernode data based on the specified Supernode public key.

    - `pubkey`: The public key of the Supernode.

    Returns a SupernodeData object containing the Supernode data.

    Raises:
    - HTTPException (status_code=404): If the specified machine is not a Supernode.
    - HTTPException (status_code=500): If an error occurs while retrieving the Supernode data.

    Example response:
    {
        "supernode_status": "ENABLED",
        "protocol_version": "1.0",
        "supernode_psl_address": "tPmkbohSbiocyAhXJVBZkBsKJiuVyNRp2GJ",
        ...
    }
    """
    try:
        supernode_data = await service_functions.get_sn_data_from_sn_pubkey_func(pubkey)
        if not supernode_data.empty:
            return db.SupernodeData(**supernode_data.to_dict(orient='records')[0])
        else:
            raise HTTPException(status_code=404, detail="Specified machine is not a Supernode")
    except Exception as e:
        logger.error(f"Error getting Supernode data from public key: {e}")
        raise HTTPException(status_code=500, detail="Internal Server Error")
        

@router.get("/get_messages", response_model=List[db.Message], dependencies=[Depends(localhost_only)])
async def get_messages(
    last_k_minutes: Optional[int] = Query(100, description="Number of minutes to retrieve messages from"),
    message_type: Optional[str] = Query("all", description="Type of messages to retrieve ('all' or specific type)"),
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Note: Endpoint only available on localhost.
        
    Retrieves Supernode messages from the last specified minutes.

    - `last_k_minutes`: Number of minutes to retrieve messages from (default: 100).
    - `message_type`: Type of messages to retrieve ('all' or specific type) (default: 'all').

    Returns a list of Message objects containing the message, message_type, sending_sn_pastelid, and timestamp.
    """
    try:
        messages = await service_functions.parse_sn_messages_from_last_k_minutes_func(last_k_minutes, message_type)
        return [
            db.Message(
                message=msg["message"],
                message_type=msg["message_type"],
                sending_sn_pastelid=msg["sending_sn_pastelid"],
                timestamp=msg["timestamp"]
            )
            for msg in messages
        ]
    except Exception as e:
        logger.error(f"Error retrieving messages: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error retrieving messages: {str(e)}")
    

@router.post("/broadcast_message_to_all_sns", response_model=db.SendMessageResponse, dependencies=[Depends(localhost_only)])
async def broadcast_message_to_all_sns(
    message: str = Query(..., description="Message to broadcast"),
    message_type: str = Query(..., description="Type of the message"),
    pastelid_passphrase: SecretStr = Query(..., description="Passphrase for the sending PastelID"),
    verbose: Optional[int] = Query(0, description="Verbose mode (0 or 1)"),
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Note: Endpoint only available on localhost.
    
    Broadcasts a message to a list of Supernodes.

    - `message`: Message to broadcast.
    - `message_type`: Type of the message.
    - `pastelid_passphrase`: Passphrase for the sending PastelID.
    - `verbose`: Verbose mode (0 or 1) (default: 0).

    Returns a SendMessageResponse object containing the status and message.
    """
    try:
        signed_message = await service_functions.broadcast_message_to_all_sns_using_pastelid_func(
            message, message_type, pastelid_passphrase.get_secret_value(), verbose
        )
        return db.SendMessageResponse(status="success", message=f"Message broadcasted: {signed_message}")
    except Exception as e:
        logger.error(f"Error broadcasting message: {str(e)}")
        return db.SendMessageResponse(status="error", message=f"Error broadcasting message: {str(e)}")


@router.get("/request_challenge/{pastelid}")
async def request_challenge(
    pastelid: str,
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Request a challenge string for authentication.

    - `pastelid`: The PastelID requesting the challenge.

    Returns a dictionary containing the challenge string and the challenge ID.
    """
    try:
        challenge, challenge_id = await service_functions.generate_challenge(pastelid)
        return {"challenge": challenge, "challenge_id": challenge_id}
    except Exception as e:
        logger.error(f"Error generating challenge: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error generating challenge: {str(e)}")


@router.post("/send_user_message", response_model=db.SupernodeUserMessage)
async def send_user_message(
    user_message: db.UserMessage = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Send a user message via Supernodes.

    This endpoint allows a user to send a message to another user via the Supernode network.
    The sender must provide a valid challenge signature to authenticate their identity.

    Parameters:
    - `user_message` (UserMessage): The user message to be sent, including:
        - `from_pastelid` (str): The PastelID of the sender.
        - `to_pastelid` (str): The PastelID of the recipient.
        - `message_body` (str): The content of the message.
        - `message_signature` (str): The signature of the message by the sender's PastelID.
    - `challenge` (str): The challenge string obtained from the `/request_challenge` endpoint.
    - `challenge_id` (str): The ID of the challenge string.
    - `challenge_signature` (str): The signature of the PastelID on the challenge string.

    Returns:
    - `SupernodeUserMessage`: The sent message details, including:
        - `message` (str): The content of the sent message.
        - `message_type` (str): The type of the message (always "user_message").
        - `sending_sn_pastelid` (str): The PastelID of the Supernode that sent the message.
        - `timestamp` (datetime): The timestamp of when the message was sent.
        - `id` (int): The unique identifier of the Supernode user message.
        - `user_message` (UserMessage): The details of the user message, including:
            - `from_pastelid` (str): The PastelID of the sender.
            - `to_pastelid` (str): The PastelID of the recipient.
            - `message_body` (str): The content of the message.
            - `message_signature` (str): The signature of the message by the sender's PastelID.
            - `id` (int): The unique identifier of the user message.
            - `timestamp` (datetime): The timestamp of when the user message was created.

    Raises:
    - HTTPException (status_code=401): If the provided challenge signature is invalid.
    - HTTPException (status_code=500): If an error occurs while sending the user message.
    """
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            user_message.from_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        supernode_user_message = await service_functions.send_user_message_via_supernodes(
            user_message.from_pastelid, user_message.to_pastelid, user_message.message_body, user_message.message_signature
        )
        return supernode_user_message 
    except Exception as e:
        logger.error(f"Error sending user message: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error sending user message: {str(e)}")


@router.get("/get_user_messages", response_model=List[db.UserMessage])
async def get_user_messages(
    pastelid: str = Query(..., description="The PastelID to retrieve messages for"),
    challenge: str = Query(..., description="The challenge string"),
    challenge_id: str = Query(..., description="The ID of the challenge string"),
    challenge_signature: str = Query(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    """
    Retrieve all user messages (sent and received) for a given PastelID.

    This endpoint allows a user to retrieve all messages associated with their PastelID.
    The user must provide a valid challenge signature to authenticate their identity.

    Parameters:
    - `pastelid` (str): The PastelID of the user to retrieve messages for.
    - `challenge` (str): The challenge string obtained from the `/request_challenge` endpoint.
    - `challenge_id` (str): The ID of the challenge string.
    - `challenge_signature` (str): The signature of the PastelID on the challenge string.

    Returns:
    - List[UserMessage]: A list of user messages associated with the provided PastelID, each including:
        - `from_pastelid` (str): The PastelID of the sender.
        - `to_pastelid` (str): The PastelID of the recipient.
        - `message_body` (str): The content of the message.
        - `message_signature` (str): The signature of the message by the sender's PastelID.
        - `id` (int): The unique identifier of the user message.
        - `timestamp` (datetime): The timestamp of when the user message was created.

    Raises:
    - HTTPException (status_code=401): If the provided challenge signature is invalid.
    - HTTPException (status_code=500): If an error occurs while retrieving the user messages.
    """
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        user_messages = await service_functions.get_user_messages_for_pastelid(pastelid)
        return [
            db.UserMessage(
                from_pastelid=message.from_pastelid,
                to_pastelid=message.to_pastelid,
                message_body=message.message_body,
                message_signature=message.message_signature,
                id=message.id,
                timestamp=message.timestamp
            )
            for message in user_messages
        ]
    except Exception as e:
        logger.error(f"Error retrieving user messages: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error retrieving user messages: {str(e)}")


#__________________________________________________________________________________________________________
# Endpoints related to Credit pack purchasing and provisioning:

class CreditPackTicketResponse(BaseModel):
    credit_pack_purchase_request_response: db.CreditPackPurchaseRequestResponse
    credit_pack_purchase_request_confirmation: db.CreditPackPurchaseRequestConfirmation
    
    
@router.get("/get_credit_pack_ticket_from_txid", response_model=CreditPackTicketResponse)
async def get_credit_pack_ticket_from_txid_endpoint(
    txid: str = Query(..., description="The transaction ID of the credit pack ticket"),
    pastelid: str = Query(..., description="The PastelID of the requesting party"),
    challenge: str = Query(..., description="The challenge string"),
    challenge_id: str = Query(..., description="The ID of the challenge string"),
    challenge_signature: str = Query(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        _, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation = await service_functions.retrieve_credit_pack_ticket_using_txid(txid)
        if not all((credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation)):
            raise HTTPException(status_code=404, detail=f"Credit pack ticket with TXID {txid} not found or was invalid!")
        credit_pack_ticket = CreditPackTicketResponse(
            credit_pack_purchase_request_response=credit_pack_purchase_request_response,
            credit_pack_purchase_request_confirmation=credit_pack_purchase_request_confirmation
        )        
        return credit_pack_ticket
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"Error retrieving credit pack ticket from txid: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error retrieving credit pack ticket from txid: {str(e)}")
    
    
@router.post("/credit_purchase_initial_request", response_model=Union[db.CreditPackPurchaseRequestPreliminaryPriceQuote, db.CreditPackPurchaseRequestRejection])
async def credit_purchase_initial_request_endpoint(
    credit_pack_request: db.CreditPackPurchaseRequest = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            credit_pack_request.requesting_end_user_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        preliminary_price_quote = await service_functions.process_credit_purchase_initial_request(credit_pack_request)
        return preliminary_price_quote
    except Exception as e:
        logger.error(f"Error encountered with credit purchase initial request: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error encountered with credit purchase initial request: {str(e)}")


@router.post("/credit_purchase_preliminary_price_quote_response", response_model=Union[db.CreditPackPurchaseRequestResponse, db.CreditPackPurchaseRequestResponseTermination])
async def credit_purchase_preliminary_price_quote_response_endpoint(
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    preliminary_price_quote_response: db.CreditPackPurchaseRequestPreliminaryPriceQuoteResponse = Body(...),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            preliminary_price_quote_response.requesting_end_user_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        result = await service_functions.process_credit_purchase_preliminary_price_quote_response(preliminary_price_quote_response)
        if isinstance(result, db.CreditPackPurchaseRequestResponse):
            result_dict = result.model_dump()
            result_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in result_dict.items()}
            service_functions.log_action_with_payload("processed", "credit purchase preliminary price quote response", result_dict)
            return result
        elif isinstance(result, db.CreditPackPurchaseRequestResponseTermination):
            result_dict = result.model_dump()
            result_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in result_dict.items()}
            logger.warning(f"Credit purchase preliminary price quote response terminated: {result_dict}")
            return result
        else:
            raise HTTPException(status_code=500, detail="Unexpected response type")
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"Error processing credit purchase preliminary price quote response: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing credit purchase preliminary price quote response: {str(e)}")


@router.post("/credit_pack_price_agreement_request", response_model=db.CreditPackPurchasePriceAgreementRequestResponse)
async def credit_pack_price_agreement_request_endpoint(
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    credit_pack_price_agreement_request: db.CreditPackPurchasePriceAgreementRequest = Body(...),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            credit_pack_price_agreement_request.supernode_requesting_price_agreement_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        response = await service_functions.process_credit_pack_price_agreement_request(credit_pack_price_agreement_request)
        if isinstance(response, db.CreditPackPurchasePriceAgreementRequestResponse):
            response_dict = response.model_dump()
            response_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in response_dict.items()}
            service_functions.log_action_with_payload("processed", "credit pack price agreement request", response_dict)
            return response
        else:
            raise HTTPException(status_code=400, detail=response)
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"Error processing credit pack price agreement request: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing credit pack price agreement request: {str(e)}")
    

@router.post("/check_status_of_credit_purchase_request", response_model=db.CreditPackPurchaseRequestStatus)
async def check_status_of_credit_purchase_request_endpoint(
    credit_pack_request_status_check: db.CreditPackRequestStatusCheck = Body(...), 
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            credit_pack_request_status_check.requesting_end_user_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        status = await service_functions.get_credit_purchase_request_status(credit_pack_request_status_check)
        status_dict = status.model_dump()
        status_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in status_dict.items()}
        service_functions.log_action_with_payload("checking status of", "credit purchase request", status_dict)
        return status
    except Exception as e:
        logger.error(f"Error checking status of credit purchase request: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error checking status of credit purchase request: {str(e)}")


@router.post("/confirm_credit_purchase_request", response_model=db.CreditPackPurchaseRequestConfirmationResponse)
async def confirm_credit_purchase_request_endpoint(
    confirmation: db.CreditPackPurchaseRequestConfirmation = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            confirmation.requesting_end_user_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        response = await service_functions.process_credit_purchase_request_confirmation(confirmation)
        response_dict = response.model_dump()
        response_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in response_dict.items()}
        service_functions.log_action_with_payload("processed", "credit purchase request confirmation", response_dict)
        return response
    except Exception as e:
        logger.error(f"Error processing credit purchase request confirmation: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing credit purchase request confirmation: {str(e)}")


@router.post("/credit_pack_purchase_request_final_response_announcement")
async def credit_pack_purchase_request_final_response_announcement_endpoint(
    response: db.CreditPackPurchaseRequestResponse = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            response.responding_supernode_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        await service_functions.process_credit_pack_purchase_request_final_response_announcement(response)
        service_functions.log_action_with_payload("processed", "credit pack purchase request final response announcement", response)
        return {"message": "Announcement processed successfully"}
    except Exception as e:
        logger.error(f"Error processing credit pack purchase request final response announcement: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing credit pack purchase request final response announcement: {str(e)}")


@router.post("/credit_pack_purchase_completion_announcement")
async def credit_pack_purchase_completion_announcement_endpoint(
    confirmation: db.CreditPackPurchaseRequestConfirmation = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            confirmation.requesting_end_user_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        await service_functions.process_credit_pack_purchase_completion_announcement(confirmation)
        service_functions.log_action_with_payload("processed", "credit pack purchase completion announcement", confirmation)
        return {"message": "Announcement processed successfully"}
    except Exception as e:
        logger.error(f"Error processing credit pack purchase completion announcement: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing credit pack purchase completion announcement: {str(e)}")


@router.post("/credit_pack_storage_completion_announcement")
async def credit_pack_storage_completion_announcement_endpoint(
    storage_completion_announcement: db.CreditPackPurchaseRequestConfirmationResponse = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            storage_completion_announcement.responding_supernode_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        await service_functions.process_credit_pack_storage_completion_announcement(storage_completion_announcement)
        service_functions.log_action_with_payload("processed", "credit pack storage completion announcement", storage_completion_announcement)
        return {"message": "Announcement processed successfully"}
    except Exception as e:
        logger.error(f"Error processing credit pack storage completion announcement: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing credit pack storage completion announcement: {str(e)}")


@router.post("/credit_pack_storage_retry_request", response_model=db.CreditPackStorageRetryRequestResponse)
async def credit_pack_storage_retry_request_endpoint(
    request: db.CreditPackStorageRetryRequest = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            request.requesting_end_user_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        response = await service_functions.process_credit_pack_storage_retry_request(request)
        response_dict = response.model_dump()
        response_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in response_dict.items()}
        service_functions.log_action_with_payload("processed", "credit pack storage retry request", response_dict)
        return response
    except Exception as e:
        logger.error(f"Error processing credit pack storage retry request: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing credit pack storage retry request: {str(e)}")
    
    
@router.post("/credit_pack_storage_retry_completion_announcement")
async def credit_pack_storage_retry_completion_announcement_endpoint(
    response: db.CreditPackStorageRetryRequestResponse = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            response.responding_supernode_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        await service_functions.process_credit_pack_storage_retry_completion_announcement(response)
        service_functions.log_action_with_payload("processed", "credit pack storage retry completion announcement", json.dumps(response))
        return {"message": "Announcement processed successfully"}
    except Exception as e:
        logger.error(f"Error processing credit pack storage retry completion announcement: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error processing credit pack storage retry completion announcement: {str(e)}")


@router.post("/get_valid_credit_pack_tickets_for_pastelid", response_model=List[dict])
async def get_valid_credit_pack_tickets_for_pastelid_endpoint(
    pastelid: str = Body(..., description="The PastelID to retrieve credit pack tickets for"),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(pastelid, challenge_signature, challenge_id)
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        valid_tickets = await service_functions.get_valid_credit_pack_tickets_for_pastelid(pastelid)
        return valid_tickets
    except Exception as e:
        logger.error(f"Error retrieving valid credit pack tickets for PastelID {pastelid}: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error retrieving valid credit pack tickets: {str(e)}")
    
    
@router.post("/check_credit_pack_balance", response_model=Dict[str, Any])
async def check_credit_pack_balance_endpoint(
    credit_pack_ticket_txid: str = Body(..., description="The transaction ID of the credit pack ticket"),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(credit_pack_ticket_txid, challenge_signature, challenge_id)
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        current_credit_balance, number_of_transactions = await service_functions.determine_current_credit_pack_balance_based_on_tracking_transactions_new(credit_pack_ticket_txid)
        balance_info = {
            "current_credit_balance": current_credit_balance,
            "number_of_confirmation_transactions": number_of_transactions
        }
        logger.info(f"Checked credit pack balance for txid {credit_pack_ticket_txid}: {balance_info}")
        return balance_info
    except Exception as e:
        logger.error(f"Error checking credit pack balance for txid {credit_pack_ticket_txid}: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error checking credit pack balance: {str(e)}")
    
    
@router.post("/retrieve_credit_pack_ticket_from_purchase_burn_txid", response_model=Dict[str, Any])
async def retrieve_credit_pack_ticket_endpoint(
    purchase_burn_txid: str = Body(..., description="The transaction ID of the credit pack purchase burn"),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(purchase_burn_txid, challenge_signature, challenge_id)
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation = await service_functions.retrieve_credit_pack_ticket_from_purchase_burn_txid(purchase_burn_txid)
        if credit_pack_purchase_request is None or credit_pack_purchase_request_response is None or credit_pack_purchase_request_confirmation is None:
            raise HTTPException(status_code=404, detail="Credit pack ticket not found")
        ticket_info = {
            "credit_pack_purchase_request": credit_pack_purchase_request.model_dump(),
            "credit_pack_purchase_request_response": credit_pack_purchase_request_response.model_dump(),
            "credit_pack_purchase_request_confirmation": credit_pack_purchase_request_confirmation.model_dump()
        }
        logger.info(f"Retrieved credit pack ticket for purchase burn txid {purchase_burn_txid}: {ticket_info}")
        return ticket_info
    except Exception as e:
        logger.error(f"Error retrieving credit pack ticket for purchase burn txid {purchase_burn_txid}: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error retrieving credit pack ticket: {str(e)}")
    
    
@router.post("/get_final_credit_pack_registration_txid_from_credit_purchase_burn_txid", response_model=Dict[str, Any])
async def get_final_credit_pack_registration_txid_endpoint(
    purchase_burn_txid: str = Body(..., description="The transaction ID of the credit pack purchase burn"),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(purchase_burn_txid, challenge_signature, challenge_id)
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        sha3_256_hash_of_credit_pack_purchase_request_fields = await service_functions.get_final_credit_pack_registration_txid_from_credit_purchase_burn_txid(purchase_burn_txid)
        if sha3_256_hash_of_credit_pack_purchase_request_fields is None:
            raise HTTPException(status_code=404, detail="Credit pack ticket not found")
        logger.info(f"Retrieved final credit pack registration txid for purchase burn txid {purchase_burn_txid}: {sha3_256_hash_of_credit_pack_purchase_request_fields}")
        return {"final_credit_pack_registration_txid": sha3_256_hash_of_credit_pack_purchase_request_fields}
    except Exception as e:
        logger.error(f"Error retrieving final credit pack registration txid for purchase burn txid {purchase_burn_txid}: {str(e)}")
        traceback.print_exc()        
        raise HTTPException(status_code=500, detail=f"Error retrieving final credit pack registration txid: {str(e)}")

    
#__________________________________________________________________________________________________________


@router.post("/make_inference_api_usage_request", response_model=db.InferenceAPIUsageResponse)
async def make_inference_api_usage_request_endpoint(
    inference_api_usage_request: db.InferenceAPIUsageRequest = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(
            inference_api_usage_request.requesting_pastelid, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        # Validate and process the inference API usage request
        inference_response = await service_functions.process_inference_api_usage_request(inference_api_usage_request)
        inference_request_dict = inference_api_usage_request.model_dump()
        inference_request_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in inference_request_dict.items()}
        # Abbreviate the 'model_input_data_json_b64' field to the first 32 characters
        inference_request_dict['model_input_data_json_b64'] = inference_request_dict['model_input_data_json_b64'][:32]        
        inference_response_dict = inference_response.model_dump()
        inference_response_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in inference_response_dict.items()}
        combined_message_dict = {**inference_request_dict, **inference_response_dict}
        # Broadcast message to nearest SNs to requester's pastelid containing inference request/response message 
        response_message_body = json.dumps(combined_message_dict)
        response_message_type = "inference_request_response_announcement_message"
        _ = await service_functions.broadcast_message_to_n_closest_supernodes_to_given_pastelid(inference_api_usage_request.requesting_pastelid, response_message_body, response_message_type) 
        # Return the InferenceAPIUsageResponse as the API response
        return inference_response
    except Exception as e:
        logger.error(f"Error encountered with inference API usage request: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error encountered with inference API usage request: {str(e)}")
    

@router.post("/confirm_inference_request", response_model=db.InferenceConfirmation)
async def confirm_inference_request_endpoint(
    inference_confirmation: db.InferenceConfirmation = Body(...),
    challenge: str = Body(..., description="The challenge string"),
    challenge_id: str = Body(..., description="The ID of the challenge string"),
    challenge_signature: str = Body(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature_from_inference_request_id(
            inference_confirmation.inference_request_id, challenge_signature, challenge_id
        )
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        # Process the inference confirmation
        is_processed = await service_functions.process_inference_confirmation(
            inference_confirmation.inference_request_id, inference_confirmation
        )
        if is_processed:
            logger.info(f"Inference request {inference_confirmation.inference_request_id} confirmed successfully")
        else:
            logger.error(f"Error confirming inference request {inference_confirmation.inference_request_id}")
        return inference_confirmation
    except Exception as e:
        logger.error(f"Error sending inference confirmation: {str(e)}")
        traceback.print_exc()       
        raise HTTPException(status_code=500, detail=f"Error sending inference confirmation: {str(e)}")


@router.get("/check_status_of_inference_request_results/{inference_response_id}")
async def check_status_of_inference_request_results_endpoint(inference_response_id: str):
    try:
        request_result_is_available = await service_functions.check_status_of_inference_request_results(inference_response_id)
        if request_result_is_available is None:
            raise HTTPException(status_code=404, detail="Inference request result not found")
        return request_result_is_available
    except ValueError as ve:
        logger.error(f"Validation error: {str(ve)}")
        raise HTTPException(status_code=400, detail=f"Invalid request: {str(ve)}")
    except Exception as e:
        logger.error(f"Error checking status of inference request results: {str(e)}")
        traceback.print_exc()        
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@router.post("/retrieve_inference_output_results", response_model=db.InferenceAPIOutputResult)
async def retrieve_inference_output_results_endpoint(
    inference_response_id: str = Query(..., description="The ResponseID Associated with the Inference Request"),
    pastelid: str = Query(..., description="The PastelID of the requesting party"),
    challenge: str = Query(..., description="The challenge string"),
    challenge_id: str = Query(..., description="The ID of the challenge string"),
    challenge_signature: str = Query(..., description="The signature of the PastelID on the challenge string"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        is_valid_signature = await service_functions.verify_challenge_signature(pastelid, challenge_signature, challenge_id)
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        inference_output_results = await service_functions.get_inference_output_results_and_verify_authorization(inference_response_id, pastelid)
        # Broadcast message to nearest SNs to requester's pastelid containing inference results
        inference_output_results_dict = inference_output_results.model_dump()
        inference_output_results_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in inference_output_results_dict.items()}
        # Retrieve the inference API usage request from the database
        inference_usage_request_object = await service_functions.get_inference_api_usage_request_for_audit(inference_output_results_dict['inference_request_id'])
        inference_usage_request_dict = inference_usage_request_object.model_dump()
        inference_usage_request_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in inference_usage_request_dict.items()}
        # Add model_parameters_json_b64 and other fields to the inference output results dict:
        inference_output_results_dict['model_parameters_json_b64'] = inference_usage_request_dict['model_parameters_json_b64']
        inference_output_results_dict['requested_model_canonical_string'] = inference_usage_request_dict['requested_model_canonical_string']
        inference_output_results_dict['model_inference_type_string'] = inference_usage_request_dict['model_inference_type_string']
        # Abbreviate the 'inference_result_json_base64' field to the first 32 characters
        inference_output_results_dict['inference_result_json_base64'] = inference_output_results_dict['inference_result_json_base64'][:32]
        result_message_body = json.dumps(inference_output_results_dict)
        result_message_type = "inference_request_result_announcement_message"
        _ = await service_functions.broadcast_message_to_n_closest_supernodes_to_given_pastelid(pastelid, result_message_body, result_message_type) 
        return inference_output_results
    except Exception as e:
        logger.error(f"Error retrieving inference output results: {str(e)}")
        traceback.print_exc()        
        raise HTTPException(status_code=500, detail=f"Error retrieving inference output results: {str(e)}")


@router.post("/audit_inference_request_response", response_model=db.InferenceAPIUsageResponse)
async def audit_inference_request_response_endpoint(
    inference_response_id: str = Body(..., description="The inference response ID"),
    pastel_id: str = Body(..., description="The PastelID of the requester"),
    signature: str = Body(..., description="The signature of the PastelID on the inference_response_id"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        # Retrieve the InferenceAPIOutputResult from the local database
        api_usage_response = await service_functions.get_inference_api_usage_response_for_audit(inference_response_id)
        if api_usage_response is None:
            raise HTTPException(status_code=404, detail="Inference result not found")
        # Verify the signature
        is_valid_signature = await service_functions.verify_message_with_pastelid_func(pastel_id, inference_response_id, signature)
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        api_usage_request = await service_functions.get_inference_api_usage_request_for_audit(api_usage_response.inference_request_id)
        # Verify that the PastelID matches the one in the response
        if api_usage_request.requesting_pastelid != pastel_id:
            raise HTTPException(status_code=403, detail="PastelID does not match the one in the inference request")
        # Return the InferenceAPIOutputResult as the API response
        return api_usage_response
    except Exception as e:
        logger.error(f"Error auditing inference request result: {str(e)}")
        traceback.print_exc()        
        raise HTTPException(status_code=500, detail=f"Error auditing inference request result: {str(e)}")
    
    
@router.post("/audit_inference_request_result", response_model=db.InferenceAPIOutputResult)
async def audit_inference_request_result_endpoint(
    inference_response_id: str = Body(..., description="The inference response ID"),
    pastel_id: str = Body(..., description="The PastelID of the requester"),
    signature: str = Body(..., description="The signature of the PastelID on the inference_response_id"),
    rpc_connection=Depends(get_rpc_connection),
):
    try:
        # Retrieve the InferenceAPIOutputResult from the local database
        api_usage_result = await service_functions.get_inference_api_usage_result_for_audit(inference_response_id)
        if api_usage_result is None:
            raise HTTPException(status_code=404, detail="Inference result not found")
        # Verify the signature
        is_valid_signature = await service_functions.verify_message_with_pastelid_func(pastel_id, inference_response_id, signature)
        if not is_valid_signature:
            raise HTTPException(status_code=401, detail="Invalid PastelID signature")
        api_usage_request = await service_functions.get_inference_api_usage_request_for_audit(api_usage_result.inference_request_id)
        # Verify that the PastelID matches the one in the response
        if api_usage_request.requesting_pastelid != pastel_id:
            raise HTTPException(status_code=403, detail="PastelID does not match the one in the inference request")
        # Return the InferenceAPIOutputResult as the API response
        return api_usage_result
    except Exception as e:
        logger.error(f"Error auditing inference request result: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error auditing inference request result: {str(e)}")
    
    
@router.get("/get_inference_model_menu")
async def get_inference_model_menu_endpoint(
    rpc_connection=Depends(get_rpc_connection),
):
    model_menu = await service_functions.get_inference_model_menu()
    return model_menu


@router.get("/favicon.ico", include_in_schema=False)
async def favicon():
    return FileResponse("favicon.ico")


@router.get("/download/{file_name}")
async def download_file(file_name: str):
    file_location = os.path.join(tempfile.gettempdir(), file_name)
    if file_location in service_functions.file_store and service_functions.file_store[file_location] > datetime.utcnow():
        return FileResponse(file_location, background=BackgroundTask(service_functions.remove_file, file_location))
    else:
        service_functions.remove_file(file_location)
        raise HTTPException(status_code=404, detail="File not found or expired")


@router.post("/update_inference_sn_reputation_score")
async def update_inference_sn_reputation_score_endpoint(
    reputation_score_data: db.ReputationScoreUpdate,
    rpc_connection=Depends(get_rpc_connection),
):
    is_updated = await service_functions.update_inference_sn_reputation_score(reputation_score_data.supernode_pastelid, reputation_score_data.reputation_score)
    return {"is_updated": is_updated}


@router.get("/show_logs/{minutes}", response_class=HTMLResponse)
async def show_logs(minutes: int = 5):
    logger.info("Reading logs for the last %d minutes", minutes)
    # Read the entire log file and generate HTML with logs up to `minutes` minutes from now
    with open("pastel_supernode_inference_layer.log", "r") as f:
        lines = f.readlines()
    logs = []
    now = datetime.now(timezone.utc)  # Get current time, make it timezone-aware
    for line in lines:
        if line.strip() == "":
            continue
        if line[0].isdigit():
            try:  # Try to parse the datetime
                log_datetime_str = line.split(" - ")[0]  # Assuming the datetime is at the start of each line
                log_datetime = datetime.strptime(log_datetime_str, "%Y-%m-%d %H:%M:%S,%f")  # Parse the datetime string to a datetime object
                log_datetime = log_datetime.replace(tzinfo=timezone.utc)  # Set the datetime object timezone to UTC to match `now`
                if now - log_datetime <= timedelta(minutes=minutes):  # If the log is within `minutes` minutes from now
                    logs.append(service_functions.highlight_rules_func(line.rstrip('\n')))  # Add the highlighted log to the list and strip any newline at the end
            except ValueError:
                pass  # If the line does not start with a datetime, ignore the ValueError and process the line anyway
    logs_as_string = "<br>".join(logs)  # Joining with <br> directly
    logs_as_string_newlines_rendered = logs_as_string.replace("\n", "<br>")
    logs_as_string_newlines_rendered_font_specified = f"""
    <html>
    <head>
    <link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" rel="stylesheet">
    <script>
    var logContainer;
    var lastLogs = `{logs_as_string_newlines_rendered}`;
    var shouldScroll = true;
    var userScroll = false;
    var lastPosition = 0;
    var minutes = {minutes};
    function fetchLogs() {{
        if (typeof minutes !== 'undefined' && typeof lastPosition !== 'undefined') {{
            fetch('/show_logs_incremental/' + minutes + '/' + lastPosition)
            .then(response => response.json())
            .then(data => {{
                if (logContainer) {{
                    var div = document.createElement('div');
                    div.innerHTML = data.logs;
                    if (div.innerHTML) {{
                        lastLogs += div.innerHTML;
                        lastPosition = data.last_position;
                    }}
                    logContainer.innerHTML = lastLogs;
                    if (shouldScroll) {{
                        logContainer.scrollTop = logContainer.scrollHeight;
                    }}
                }}
            }});
        }}
    }}
    function checkScroll() {{
        if(logContainer.scrollTop + logContainer.clientHeight < logContainer.scrollHeight) {{
            userScroll = true;
            shouldScroll = false;
        }} else {{
            userScroll = false;
        }}
        if (!userScroll) {{
            setTimeout(function(){{ shouldScroll = true; }}, 10000);
        }}
    }}
    window.onload = function() {{
        let p = document.getElementsByTagName('p');
        for(let i = 0; i < p.length; i++) {{
            let color = window.getComputedStyle(p[i]).getPropertyValue('color');
            p[i].style.textShadow = `0 0 5px ${{color}}, 0 0 10px ${{color}}, 0 0 15px ${{color}}, 0 0 20px ${{color}}`;
        }}
        document.querySelector('#copy-button').addEventListener('click', function() {{
            var text = document.querySelector('#log-container').innerText;
            navigator.clipboard.writeText(text).then(function() {{
                console.log('Copying to clipboard was successful!');
            }}, function(err) {{
                console.error('Could not copy text: ', err);
            }});
        }});
        document.querySelector('#download-button').addEventListener('click', function() {{
            var text = document.querySelector('#log-container').innerText;
            var element = document.createElement('a');
            element.setAttribute('href', 'data:text/plain;charset=utf-8,' + encodeURIComponent(text));
            element.setAttribute('download', 'pastel_supernode_inference_layer_log__' + new Date().toISOString() + '.txt');
            element.style.display = 'none';
            document.body.appendChild(element);
            element.click();
            document.body.removeChild(element);
        }});
    }}
    document.addEventListener('DOMContentLoaded', (event) => {{
        logContainer = document.getElementById('log-container');
        logContainer.innerHTML = lastLogs;
        logContainer.addEventListener('scroll', checkScroll);
        fetchLogs();
        setInterval(fetchLogs, 1000);  // Fetch the logs every 1 second
    }});
    </script>
    </head>        
    <style>
    .log-container {{
        scroll-behavior: smooth;
        background-color: #2b2b2b; /* dark gray */
        color: #d3d3d3; /* light gray */
        background-image: linear-gradient(rgba(0,0,0,0.1) 1px, transparent 1px), linear-gradient(90deg, rgba(0,0,0,0.1) 1px, transparent 1px);
        background-size: 100% 10px, 10px 100%;
        background-position: 0 0, 0 0;
        animation: scan 1s linear infinite;
        @keyframes scan {{
            0% {{
                background-position: 0 0, 0 0;
            }}
            100% {{
                background-position: -10px -10px, -10px -10px;
            }}
        }}
        font-size: 14px;
        font-family: monospace;
        padding: 10px;
        height: 100vh;
        margin: 0;
        box-sizing: border-box;
        overflow: auto;
    }}
    .icon-button {{
        position: fixed;
        right: 10px;
        margin: 10px;
        background-color: #555;
        color: white;
        border: none;
        cursor: pointer;
        border-radius: 50%;
        width: 60px;
        height: 60px;
        font-size: 30px;
        display: flex;
        align-items: center;
        justify-content: center;
        text-decoration: none;
    }}
    #copy-button {{
        bottom: 80px;  // Adjust this value as needed
    }}
    #download-button {{
        bottom: 10px;
    }}
    </style>
    <body>
    <pre id="log-container" class="log-container"></pre>
    <button id="copy-button" class="icon-button"><i class="fas fa-copy"></i></button>
    <button id="download-button" class="icon-button"><i class="fas fa-download"></i></button>
    </body>
    </html>"""
    return HTMLResponse(content=logs_as_string_newlines_rendered_font_specified)


@router.get("/show_logs_incremental/{minutes}/{last_position}")
def show_logs_incremental(minutes: int, last_position: int):
    new_logs = []
    now = datetime.now(timezone.utc)  # Get current time, make it timezone-aware
    try:
        with open("pastel_supernode_inference_layer.log", "r") as f:
            f.seek(last_position)  # Seek to `last_position`
            while True:
                line = f.readline()
                if line == "":  # If EOF
                    break
                if line.strip() == "":
                    continue
                try:  # Try to parse the datetime
                    log_datetime_str = line.split(" - ")[0]  # Assuming the datetime is at the start of each line
                    log_datetime = datetime.strptime(log_datetime_str, "%Y-%m-%d %H:%M:%S,%f")  # Parse the datetime string to a datetime object
                    log_datetime = log_datetime.replace(tzinfo=timezone.utc)  # Set the datetime object timezone to UTC to match `now`
                    if now - log_datetime > timedelta(minutes=minutes):  # If the log is older than `minutes` minutes from now
                        continue  # Ignore the log and continue with the next line
                except ValueError:
                    pass  # If the line does not start with a datetime, ignore the ValueError and process the line anyway
                new_logs.append(service_functions.highlight_rules_func(line.rstrip('\n')))  # Add the highlighted log to the list and strip any newline at the end
            last_position = f.tell()  # Get the last position
        new_logs_as_string = "<br>".join(new_logs)  # Joining with <br> directly
    except FileNotFoundError:
        new_logs_as_string = ""
        last_position = 0
    return {"logs": new_logs_as_string, "last_position": last_position}  # Also return the last position


@router.get("/show_logs", response_class=HTMLResponse)
async def show_logs_default():
    return await show_logs(5)


async def read_performance_data():
    retries = 3
    for _ in range(retries):
        try:
            if pickle_file_path.exists():
                with open(pickle_file_path, 'rb') as f:
                    performance_data_history = pickle.load(f)
                # Replace pd.NA with None to avoid serialization issues
                for df in performance_data_history.values():
                    df.replace({pd.NA: None}, inplace=True)
                return performance_data_history
        except (EOFError, pickle.UnpicklingError) as e:
            logger.error(f"Error reading pickle file: {e}", exc_info=True)
            await asyncio.sleep(2)
    raise HTTPException(status_code=500, detail="Could not read performance data.")


@router.post("/clear_out_known_bad_credit_pack_table")
async def clear_out_known_bad_credit_pack_table_endpoint(password: str = Depends(verify_password)):
    try:
        await service_functions.clear_out_all_credit_packs_from_known_bad_table()
        return {"message": "Successfully cleared all credit packs from the known bad table."}
    except Exception as e:
        logger.error(f"Error clearing known bad credit pack table: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error clearing known bad credit pack table: {str(e)}")


@router.get("/get_supernode_inference_server_benchmark_plots")
async def get_supernode_inference_server_benchmark_plots(password: str = Depends(verify_password)):
    performance_data_history = await read_performance_data()
    if not performance_data_history:
        raise HTTPException(status_code=404, detail="No performance data available.")
    data_frames = []
    for timestamp, df in performance_data_history.items():
        df['Timestamp'] = timestamp
        df['Smoothed Performance Ratio'] = df['Performance Ratio'].rolling(window=20, min_periods=5).mean()
        data_frames.append(df)
    if not data_frames:
        raise HTTPException(status_code=404, detail="No data available for plotting.")
    combined_df = pd.concat(data_frames)
    non_summary_df = combined_df[~combined_df['IP Address'].isin(['Min', 'Average', 'Median', 'Max'])]
    summary_df = combined_df[combined_df['IP Address'].isin(['Min', 'Average', 'Median', 'Max'])]

    # Generate the main plot with smoothed line charts for each supernode
    fig_main = px.line(non_summary_df, x='Timestamp', y='Smoothed Performance Ratio', color='IP Address',
                       title="Supernode Inference Server Benchmark Performance",
                       labels={'Smoothed Performance Ratio': 'Performance Ratio', 'Timestamp': 'Timestamp'},
                       template='plotly_white')

    # Set all traces except the first one to be initially invisible
    for i, trace in enumerate(fig_main.data):
        trace.visible = True if i == 0 else "legendonly"

    fig_main.update_layout(
        font=dict(family="Montserrat", size=14, color="black"),
        title=dict(font=dict(size=20)),
        xaxis=dict(showgrid=True, gridcolor='LightGray'),
        yaxis=dict(showgrid=True, gridcolor='LightGray'),
        height=750,
        hovermode="closest",
        hoverdistance=1000,
        spikedistance=1000,
        updatemenus=[
            dict(
                type="buttons",
                showactive=False,
                buttons=[
                    dict(
                        label="Reset",
                        method="relayout",
                        args=[{"updatemenus": [{"visible": False}]}]
                    )
                ]
            )
        ]
    )
    fig_main.update_traces(
        hovertemplate='<b>IP Address</b>: %{customdata}<br><b>Performance Ratio</b>: %{y:.2f}<br><b>Timestamp</b>: %{x}<extra></extra>',
        customdata=non_summary_df['IP Address']
    )

    # Generate the summary plot (unchanged)
    fig_summary = px.line(summary_df, x='Timestamp', y='Performance Ratio', color='IP Address', markers=True,
                          title="Summary Statistics (Min, Average, Median, Max)",
                          labels={'Performance Ratio': 'Performance Ratio', 'Timestamp': 'Timestamp'},
                          template='plotly_white')
    fig_summary.update_layout(
        font=dict(family="Montserrat", size=14, color="black"),
        title=dict(font=dict(size=20)),
        xaxis=dict(showgrid=True, gridcolor='LightGray'),
        yaxis=dict(showgrid=True, gridcolor='LightGray'),
        height=600
    )
    fig_summary.update_traces(
        hovertemplate='<b>Statistic</b>: %{customdata}<br><b>Performance Ratio</b>: %{y:.2f}<br><b>Timestamp</b>: %{x}<extra></extra>',
        customdata=summary_df['IP Address']
    )

    main_plot_html = fig_main.to_html(full_html=False, include_plotlyjs='cdn')
    summary_plot_html = fig_summary.to_html(full_html=False, include_plotlyjs=False)

    most_recent_df = non_summary_df.sort_values('Timestamp').groupby('IP Address').tail(1)
    table_html = most_recent_df.to_html(classes='display nowrap', index=False)

    html_content = f"""
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Supernode Inference Server Benchmark Performance</title>
        <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;500&display=swap" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.10.21/css/jquery.dataTables.css">
        <style>
            body {{
                font-family: 'Montserrat', sans-serif;
                background-color: #f5f5f5;
                color: #333;
                margin: 0;
                padding: 20px;
            }}
            h1 {{
                text-align: center;
                font-size: 2.5em;
                margin-bottom: 20px;
            }}
            .plot-container, .table-container {{
                width: 95%;
                margin: 0 auto;
                max-width: 1400px;
            }}
            hr {{
                margin: 40px 0;
            }}
            table.dataTable {{
                width: 100% !important;
                border-collapse: collapse;
            }}
            table.dataTable thead th {{
                background-color: #f2f2f2;
                text-align: left;
                padding: 8px;
            }}
            table.dataTable tbody tr:nth-child(even) {{
                background-color: #f9f9f9;
            }}
            table.dataTable tbody td {{
                padding: 8px;
            }}
            #toggleAllBtn {{
                margin: 10px 0;
                padding: 10px;
                background-color: #4CAF50;
                color: white;
                border: none;
                cursor: pointer;
            }}
        </style>
        <script src="https://code.jquery.com/jquery-3.5.1.js"></script>
        <script type="text/javascript" charset="utf8" src="https://cdn.datatables.net/1.10.21/js/jquery.dataTables.js"></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    </head>
    <body>
        <h1>Supernode Inference Server Benchmark Performance</h1>
        <div class="plot-container" id="main-plot">
            {main_plot_html}
        </div>
        <button id="toggleAllBtn">Toggle All Series</button>
        <hr>
        <div class="plot-container" id="summary-plot">
            {summary_plot_html}
        </div>
        <hr>
        <h2 style="text-align:center;">Benchmark Data Table</h2>
        <div class="table-container">
            {table_html}
        </div>
        <script>
            $(document).ready(function() {{
                $('table.display').DataTable({{
                    scrollX: true
                }});

                function setupPlotlyHoverEffects() {{
                    var mainPlotDiv = document.getElementById('main-plot');
                    if (!mainPlotDiv) {{
                        console.error('Main plot div not found');
                        return;
                    }}

                    var plotlyInstance = mainPlotDiv.getElementsByClassName('js-plotly-plot')[0];
                    if (!plotlyInstance) {{
                        console.error('Plotly instance not found');
                        return;
                    }}

                    var plotData = plotlyInstance.data;
                    if (!plotData || !Array.isArray(plotData)) {{
                        console.error('Plot data is not available or is not an array');
                        return;
                    }}

                    var originalColors = plotData.map(trace => trace.line ? trace.line.color : null);

                    plotlyInstance.on('plotly_hover', function(data) {{
                        if (!data.points || data.points.length === 0) return;
                        
                        var curveNumber = data.points[0].curveNumber;
                        var update = {{
                            'line.width': plotData.map((_, i) => i === curveNumber ? 4 : 1),
                            'line.color': plotData.map((_, i) => i === curveNumber ? 'black' : originalColors[i])
                        }};
                        Plotly.restyle(plotlyInstance, update);
                    }});

                    plotlyInstance.on('plotly_unhover', function() {{
                        var update = {{
                            'line.width': plotData.map(() => 1),
                            'line.color': originalColors
                        }};
                        Plotly.restyle(plotlyInstance, update);
                    }});

                    // Toggle all series visibility
                    $('#toggleAllBtn').click(function() {{
                        var newVisibility = plotData.map(trace => !trace.visible);
                        Plotly.restyle(plotlyInstance, {{'visible': newVisibility}});
                    }});
                }}

                // Wait for Plotly to be fully loaded
                var checkPlotlyReady = setInterval(function() {{
                    if (window.Plotly && document.getElementById('main-plot').getElementsByClassName('js-plotly-plot')[0]) {{
                        clearInterval(checkPlotlyReady);
                        setupPlotlyHoverEffects();
                    }}
                }}, 100);
            }});
        </script>
    </body>
    </html>
    """
    return HTMLResponse(content=html_content)

---
./service_functions.py
---
import asyncio
import base64
import decimal
import hashlib
import ipaddress
import json
import os
import io
import platform
import statistics
import time
import csv
import uuid
import socket
import subprocess
import random
import re
import sys
import traceback
import html
import tempfile
import warnings
import pickle
import pytz
from collections import defaultdict
from collections.abc import Iterable
from functools import wraps
from cachetools import TTLCache
from diskcache import Cache
from pathlib import Path
from urllib.parse import quote_plus, unquote_plus
from datetime import datetime, timedelta, date, timezone
import pandas as pd
import httpx
from httpx import Timeout
from urllib.parse import urlparse
from logger_config import logger
import zstandard as zstd
from sqlalchemy.exc import OperationalError, InvalidRequestError
from typing import List, Tuple, Dict, Union, Optional
from decouple import Config as DecoupleConfig, RepositoryEnv
from magika import Magika
import tiktoken
import anthropic
from groq import AsyncGroq
from mistralai.async_client import MistralAsyncClient
from mistralai.models.chat_completion import ChatMessage
from cryptography.fernet import Fernet
from fuzzywuzzy import process
from transformers import AutoTokenizer, GPT2TokenizerFast, WhisperTokenizer
import database_code as db_code
from sqlmodel import select, delete, func, SQLModel
from sqlalchemy.exc import IntegrityError
from sshtunnel import SSHTunnelForwarder, BaseSSHTunnelForwarderError
from mutagen import File as MutagenFile
from PIL import Image
import libpastelid

tracking_period_start = datetime.utcnow()
rpc_call_stats = defaultdict(lambda: {
    "count": 0,
    "cumulative_time": 0.0,
    "average_time": 0.0,
    "success_count": 0,
    "total_response_size": 0,
    "average_response_size": 0.0,
    "timeout_errors": 0,
    "connection_errors": 0,
    "other_errors": 0
})

pastel_keys_dir = os.path.expanduser("/home/ubuntu/.pastel/pastelkeys")
pastel_signer = libpastelid.PastelSigner(pastel_keys_dir)

encryption_key = None
magika = Magika()

SENSITIVE_ENV_FIELDS = ["LOCAL_PASTEL_ID_PASSPHRASE", "SWISS_ARMY_LLAMA_SECURITY_TOKEN", "OPENAI_API_KEY", "CLAUDE3_API_KEY", "GROQ_API_KEY", "MISTRAL_API_KEY", "STABILITY_API_KEY", "OPENROUTER_API_KEY"]
LOCAL_PASTEL_ID_PASSPHRASE = None
SWISS_ARMY_LLAMA_SECURITY_TOKEN = None
OPENAI_API_KEY = None
CLAUDE3_API_KEY = None
GROQ_API_KEY = None
MISTRAL_API_KEY = None
STABILITY_API_KEY = None
OPENROUTER_API_KEY = None

def get_local_ip():
    hostname = socket.gethostname()
    return socket.gethostbyname(hostname)

def get_env_value(key):
    current_dir = os.path.dirname(os.path.abspath(__file__))
    env_file_path = os.path.join(current_dir, '.env')    
    try:
        with open(env_file_path, 'r') as env_file:
            for line in env_file:
                if line.startswith(key + '='):
                    return line.split('=', 1)[1].strip() # Split on the first '=' to allow for '=' in the value
    except FileNotFoundError:
        print(f"Error: .env file at {env_file_path} not found.")
    return None

def generate_or_load_encryption_key_sync():
    key_file_path = os.path.expanduser('~/env_encryption_key_for_supernode_inference_app')
    key = None
    if os.path.exists(key_file_path): # Check if key file exists and load it
        with open(key_file_path, 'rb') as key_file:
            key = key_file.read()
        try:
            Fernet(key)  # Validate the key
            loaded_or_generated = "loaded"
        except ValueError:
            key = None
    if key is None: # If key is invalid or doesn't exist, generate a new one
        logger.info("Invalid or no encryption key found. Generating a new one.")
        loaded_or_generated = "generated"
        key = Fernet.generate_key()
        with open(key_file_path, 'wb') as key_file:
            key_file.write(key)
        print(f"Generated new encryption key for sensitive env fields: {key}")
        encrypt_sensitive_fields(key)  # Encrypt sensitive fields if generating key for the first time
    logger.info(f"Encryption key {loaded_or_generated} successfully.")        
    return key

def encrypt_sensitive_fields(key):
    current_dir = os.path.dirname(os.path.abspath(__file__))
    env_file_path = os.path.join(current_dir, '.env')    
    cipher_suite = Fernet(key)
    with open(env_file_path, 'r') as file: # Load existing .env file
        lines = file.readlines()
    updated_lines = [] # Encrypt and update sensitive fields
    for line in lines:
        if any(field in line for field in SENSITIVE_ENV_FIELDS):
            for field in SENSITIVE_ENV_FIELDS:
                if line.startswith(field):
                    value = line.strip().split('=')[1]
                    encrypted_data = cipher_suite.encrypt(value.encode()).decode()
                    url_encoded_encrypted_data = quote_plus(encrypted_data)
                    line = f"{field}={url_encoded_encrypted_data}\n"
                    print(f"Encrypted {field}: {url_encoded_encrypted_data}")
                    break
        updated_lines.append(line)
    with open(env_file_path, 'w') as file: # Write the updated lines back to the .env file
        file.writelines(updated_lines)
    logger.info(f"Updated {len(SENSITIVE_ENV_FIELDS)} sensitive fields in .env file with encrypted values!")

def decrypt_sensitive_data(url_encoded_encrypted_data, encryption_key):
    cipher_suite = Fernet(encryption_key)
    encrypted_data = unquote_plus(url_encoded_encrypted_data)  # URL-decode first
    decrypted_data = cipher_suite.decrypt(encrypted_data.encode()).decode()  # Ensure this is a bytes-like object
    return decrypted_data

def encrypt_sensitive_data(data, encryption_key):
    cipher_suite = Fernet(encryption_key)
    encrypted_data = cipher_suite.encrypt(data.encode()).decode()
    url_encoded_encrypted_data = quote_plus(encrypted_data)
    return url_encoded_encrypted_data

def decrypt_sensitive_fields():
    global LOCAL_PASTEL_ID_PASSPHRASE, SWISS_ARMY_LLAMA_SECURITY_TOKEN, OPENAI_API_KEY, CLAUDE3_API_KEY, GROQ_API_KEY, MISTRAL_API_KEY, STABILITY_API_KEY, OPENROUTER_API_KEY, encryption_key
    LOCAL_PASTEL_ID_PASSPHRASE = decrypt_sensitive_data(get_env_value("LOCAL_PASTEL_ID_PASSPHRASE"), encryption_key)
    SWISS_ARMY_LLAMA_SECURITY_TOKEN = decrypt_sensitive_data(get_env_value("SWISS_ARMY_LLAMA_SECURITY_TOKEN"), encryption_key)
    OPENAI_API_KEY = decrypt_sensitive_data(get_env_value("OPENAI_API_KEY"), encryption_key)
    CLAUDE3_API_KEY = decrypt_sensitive_data(get_env_value("CLAUDE3_API_KEY"), encryption_key)
    GROQ_API_KEY = decrypt_sensitive_data(get_env_value("GROQ_API_KEY"), encryption_key)
    MISTRAL_API_KEY = decrypt_sensitive_data(get_env_value("MISTRAL_API_KEY"), encryption_key)
    STABILITY_API_KEY = decrypt_sensitive_data(get_env_value("STABILITY_API_KEY"), encryption_key)
    OPENROUTER_API_KEY = decrypt_sensitive_data(get_env_value("OPENROUTER_API_KEY"), encryption_key)
        
number_of_cpus = os.cpu_count()
my_os = platform.system()
loop = asyncio.get_event_loop()
warnings.filterwarnings('ignore')
local_ip = get_local_ip()
benchmark_results_cache = [] # Global cache to store benchmark results in memory
performance_data_df = pd.DataFrame(columns=['IP Address', 'Performance Ratio', 'Actual Score', 'Seconds Since Last Updated'])
performance_data_history = {}
local_benchmark_csv_file_path = Path('local_sn_micro_benchmark_results.csv')
pickle_file_path = Path('performance_data_history.pkl')
use_libpastelid_for_pastelid_sign_verify = 1

# Configuration for diskcache
CACHE_DIR = './local_credit_pack_cache'
CREDIT_BALANCE_CACHE_INVALIDATION_PERIOD_IN_SECONDS = 5 * 60  # 5 minutes

# Initialize the cache
credit_pack_cache = Cache(CACHE_DIR)

use_purge_all_caches = 0
if use_purge_all_caches:
    logger.info("Purging all caches...")
    credit_pack_cache.clear()
    
config = DecoupleConfig(RepositoryEnv('.env'))
TEMP_OVERRIDE_LOCALHOST_ONLY = config.get("TEMP_OVERRIDE_LOCALHOST_ONLY", default=0, cast=int)
NUMBER_OF_DAYS_BEFORE_MESSAGES_ARE_CONSIDERED_OBSOLETE = config.get("NUMBER_OF_DAYS_BEFORE_MESSAGES_ARE_CONSIDERED_OBSOLETE", default=3, cast=int)
GITHUB_MODEL_MENU_URL = config.get("GITHUB_MODEL_MENU_URL")
CHALLENGE_EXPIRATION_TIME_IN_SECONDS = config.get("CHALLENGE_EXPIRATION_TIME_IN_SECONDS", default=300, cast=int)
SWISS_ARMY_LLAMA_PORT = config.get("SWISS_ARMY_LLAMA_PORT", default=8089, cast=int)
USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE = config.get("USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE", default=0, cast=int)
REMOTE_SWISS_ARMY_LLAMA_INSTANCE_SSH_KEY_PATH = config.get("REMOTE_SWISS_ARMY_LLAMA_INSTANCE_SSH_KEY_PATH", default="/home/ubuntu/vastai_privkey")
REMOTE_SWISS_ARMY_LLAMA_INSTANCE_IP_ADDRESS = config.get("REMOTE_SWISS_ARMY_LLAMA_INSTANCE_IP_ADDRESS", default="50.20.127.188")
REMOTE_SWISS_ARMY_LLAMA_INSTANCE_PORT = config.get("REMOTE_SWISS_ARMY_LLAMA_INSTANCE_PORT", default=40396, cast=int)
REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT = config.get("REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT", default=8087, cast=int)
REMOTE_SWISS_ARMY_LLAMA_EXPOSED_PORT = config.get("REMOTE_SWISS_ARMY_LLAMA_EXPOSED_PORT", default=8089, cast=int)
CREDIT_COST_MULTIPLIER_FACTOR = config.get("CREDIT_COST_MULTIPLIER_FACTOR", default=0.1, cast=float)
MESSAGING_TIMEOUT_IN_SECONDS = config.get("MESSAGING_TIMEOUT_IN_SECONDS", default=60, cast=int)
API_KEY_TESTS_FILE = "api_key_tests.json"
API_KEY_TEST_VALIDITY_HOURS = config.get("API_KEY_TEST_VALIDITY_HOURS", default=72, cast=int)
TARGET_VALUE_PER_CREDIT_IN_USD = config.get("TARGET_VALUE_PER_CREDIT_IN_USD", default=0.1, cast=float)
TARGET_PROFIT_MARGIN = config.get("TARGET_PROFIT_MARGIN", default=0.1, cast=float)
MINIMUM_COST_IN_CREDITS = config.get("MINIMUM_COST_IN_CREDITS", default=0.1, cast=float)
CREDIT_USAGE_TO_TRACKING_AMOUNT_MULTIPLIER = config.get("CREDIT_USAGE_TO_TRACKING_AMOUNT_MULTIPLIER", default=10, cast=int) # Since we always round inference credits to the nearest 0.1, this gives us enough resolution using Patoshis     
MAXIMUM_NUMBER_OF_PASTEL_BLOCKS_FOR_USER_TO_SEND_BURN_AMOUNT_FOR_CREDIT_TICKET = config.get("MAXIMUM_NUMBER_OF_PASTEL_BLOCKS_FOR_USER_TO_SEND_BURN_AMOUNT_FOR_CREDIT_TICKET", default=50, cast=int)
MAXIMUM_LOCAL_CREDIT_PRICE_DIFFERENCE_TO_ACCEPT_CREDIT_PRICING = config.get("MAXIMUM_LOCAL_CREDIT_PRICE_DIFFERENCE_TO_ACCEPT_CREDIT_PRICING", default=0.1, cast=float)
MAXIMUM_LOCAL_UTC_TIMESTAMP_DIFFERENCE_IN_SECONDS = config.get("MAXIMUM_LOCAL_UTC_TIMESTAMP_DIFFERENCE_IN_SECONDS", default=15.0, cast=float)
MAXIMUM_LOCAL_PASTEL_BLOCK_HEIGHT_DIFFERENCE_IN_BLOCKS = config.get("MAXIMUM_LOCAL_PASTEL_BLOCK_HEIGHT_DIFFERENCE_IN_BLOCKS", default=1, cast=int)
MINIMUM_NUMBER_OF_PASTEL_BLOCKS_BEFORE_TICKET_STORAGE_RETRY_ALLOWED = config.get("MINIMUM_NUMBER_OF_PASTEL_BLOCKS_BEFORE_TICKET_STORAGE_RETRY_ALLOWED", default=10, cast=int)
MINIMUM_CONFIRMATION_BLOCKS_FOR_CREDIT_PACK_BURN_TRANSACTION = config.get("MINIMUM_CONFIRMATION_BLOCKS_FOR_CREDIT_PACK_BURN_TRANSACTION", default=3, cast=int)
MINIMUM_NUMBER_OF_POTENTIALLY_AGREEING_SUPERNODES = config.get("MINIMUM_NUMBER_OF_POTENTIALLY_AGREEING_SUPERNODES", default=10, cast=int)
MAXIMUM_NUMBER_OF_CONCURRENT_RPC_REQUESTS = config.get("MAXIMUM_NUMBER_OF_CONCURRENT_RPC_REQUESTS", default=30, cast=int)
MICRO_BENCHMARK_PERFORMANCE_RATIO_THRESHOLD = config.get("MICRO_BENCHMARK_PERFORMANCE_RATIO_THRESHOLD", default=0.55, cast=float)
INDIVIDUAL_SUPERNODE_PRICE_AGREEMENT_REQUEST_TIMEOUT_PERIOD_IN_SECONDS = config.get("INDIVIDUAL_SUPERNODE_PRICE_AGREEMENT_REQUEST_TIMEOUT_PERIOD_IN_SECONDS", default=12, cast=int)
INDIVIDUAL_SUPERNODE_MODEL_MENU_REQUEST_TIMEOUT_PERIOD_IN_SECONDS = config.get("INDIVIDUAL_SUPERNODE_MODEL_MENU_REQUEST_TIMEOUT_PERIOD_IN_SECONDS", default=3, cast=int)
BURN_TRANSACTION_MAXIMUM_AGE_IN_DAYS = config.get("BURN_TRANSACTION_MAXIMUM_AGE_IN_DAYS", default=3, cast=float)
SUPERNODE_CREDIT_PRICE_AGREEMENT_QUORUM_PERCENTAGE = config.get("SUPERNODE_CREDIT_PRICE_AGREEMENT_QUORUM_PERCENTAGE", default=0.51, cast=float)
SUPERNODE_CREDIT_PRICE_AGREEMENT_MAJORITY_PERCENTAGE = config.get("SUPERNODE_CREDIT_PRICE_AGREEMENT_MAJORITY_PERCENTAGE", default=0.65, cast=float)
MINIMUM_CREDITS_PER_CREDIT_PACK = config.get("MINIMUM_CREDITS_PER_CREDIT_PACK", default=10, cast=int)
MAXIMUM_CREDITS_PER_CREDIT_PACK = config.get("MAXIMUM_CREDITS_PER_CREDIT_PACK", default=1000000, cast=int)
SUPERNODE_DATA_CACHE_EVICTION_TIME_IN_MINUTES = config.get("SUPERNODE_DATA_CACHE_EVICTION_TIME_IN_MINUTES", default=60, cast=int)
SKIP_BURN_TRANSACTION_BLOCK_CONFIRMATION_CHECK = 1
UVICORN_PORT = config.get("UVICORN_PORT", default=7123, cast=int)
COIN = 100000 # patoshis in 1 PSL
SUPERNODE_DATA_CACHE = TTLCache(maxsize=1, ttl=SUPERNODE_DATA_CACHE_EVICTION_TIME_IN_MINUTES * 60) # Define the cache with a TTL (time to live) in seconds
challenge_store = {}
file_store = {} # In-memory store for files with expiration times

def async_disk_cached(cache, ttl=None):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # Create a unique cache key based on the function name and arguments
            cache_key = f"{func.__name__}:{str(args)}:{str(kwargs)}"
            cached_result = cache.get(cache_key, default=None, expire_time=True)
            # Check if cached_result is a valid, non-None value
            if cached_result is not None and not (isinstance(cached_result, tuple) and all(v is None for v in cached_result)):
                if isinstance(cached_result, tuple) and len(cached_result) == 2:
                    value, expire_time = cached_result
                    if value is not None and (ttl is None or expire_time is None or expire_time > 0):
                        return value
                elif cached_result is not None:
                    return cached_result
            try:
                value = await func(*args, **kwargs)
            except Exception as e:
                logger.error(f"Exception in {func.__name__}: {str(e)}")
                logger.error(f"Exception type: {type(e).__name__}")
                logger.error(f"Exception details: {traceback.format_exc()}")
                raise
            if value is not None:
                cache.set(cache_key, value, expire=ttl)
            else:
                logger.warning(f"Not caching None value for {func.__name__}")
            return value
        return wrapper
    return decorator

# Initialize PastelSigner
pastel_keys_dir = os.path.expanduser("~/.pastel/pastelkeys")
pastel_signer = libpastelid.PastelSigner(pastel_keys_dir)

def parse_timestamp(timestamp_str):
    try:
        # Attempt to parse with fractional seconds
        return pd.to_datetime(timestamp_str, format='%Y-%m-%dT%H:%M:%S.%f')
    except ValueError:
        # Fall back to parsing without fractional seconds
        return pd.to_datetime(timestamp_str, format='%Y-%m-%dT%H:%M:%S')

def parse_and_format(value):
    try:
        # Check if the JSON string is already formatted
        if isinstance(value, str) and "\n" in value:
            return value
        # Unescape the JSON string if it's a string
        if isinstance(value, str):
            unescaped_value = json.loads(json.dumps(value))
            parsed_value = json.loads(unescaped_value)
        else:
            parsed_value = value
        return json.dumps(parsed_value, indent=4)
    except (json.JSONDecodeError, TypeError):
        return value

def normalize_data(data):
    if isinstance(data, dict):
        return {key: normalize_data(value) for key, value in data.items()}
    elif isinstance(data, list):
        return [normalize_data(item) for item in data]
    elif isinstance(data, uuid.UUID):
        return str(data)
    elif isinstance(data, datetime):
        if data.tzinfo is None:
            # Make the datetime offset-aware with UTC timezone
            return data.replace(tzinfo=pytz.UTC)
        else:
            # Convert to UTC timezone
            return data.astimezone(pytz.UTC)
    else:
        return data
    
def format_list(input_list):
    def json_serialize(item):
        if isinstance(item, uuid.UUID):
            return json.dumps(str(item), indent=4)
        elif isinstance(item, dict):
            return json.dumps(pretty_json_func(item), indent=4)
        elif isinstance(item, list):
            return format_list(item)
        else:
            return json.dumps(item, indent=4)
    formatted_list = "[\n" + ",\n".join("    " + json_serialize(item).replace("\n", "\n    ") for item in input_list) + "\n]"
    return formatted_list

def pretty_json_func(data):
    if isinstance(data, SQLModel):
        data = data.dict()  # Convert SQLModel instance to dictionary
    if isinstance(data, dict):
        formatted_data = {}
        for key, value in data.items():
            if isinstance(value, uuid.UUID):  # Convert UUIDs to string
                formatted_data[key] = str(value)
            elif isinstance(value, dict):  # Recursively handle dictionary values
                formatted_data[key] = pretty_json_func(value)
            elif isinstance(value, list):  # Special handling for lists
                formatted_data[key] = format_list(value)
            elif key.endswith("_json"):  # Handle keys that end with '_json'
                formatted_data[key] = parse_and_format(value)
            else:  # Handle other types of values
                formatted_data[key] = value
        return json.dumps(formatted_data, indent=4)
    elif isinstance(data, list):  # Top-level list handling
        return format_list(data)
    elif isinstance(data, str):  # Handle string type data separately
        return parse_and_format(data)
    else:
        return data  # Return data as is if not a dictionary or string
    
def abbreviated_pretty_json_func(data):
    max_payload_length_in_characters = 10000
    formatted_payload = pretty_json_func(data)
    if len(formatted_payload) > max_payload_length_in_characters:
        abbreviated_payload = formatted_payload[:max_payload_length_in_characters] + "..."
        closing_brackets = "]" * (formatted_payload.count("[") - formatted_payload[:max_payload_length_in_characters].count("["))
        closing_brackets += "}" * (formatted_payload.count("{") - formatted_payload[:max_payload_length_in_characters].count("{"))
        abbreviated_payload += closing_brackets
        formatted_payload = abbreviated_payload
    return formatted_payload    
    
def log_action_with_payload(action_string, payload_name, json_payload):
    formatted_payload = abbreviated_pretty_json_func(json_payload)
    logger.info(f"Now {action_string} {payload_name} with payload:\n{formatted_payload}")
    
def get_local_rpc_settings_func(directory_with_pastel_conf=os.path.expanduser("~/.pastel/")):
    with open(os.path.join(directory_with_pastel_conf, "pastel.conf"), 'r') as f:
        lines = f.readlines()
    other_flags = {}
    rpchost = '127.0.0.1'
    rpcport = '19932'
    rpcuser = None
    rpcpassword = None
    for line in lines:
        line = line.strip()
        if not line or line.startswith('#'):  # Ignore blank lines and comments
            continue
        if '=' in line:
            key, value = line.split('=', 1)  # Split only on the first '='
            key = key.strip()
            value = value.strip()
            if key == 'rpcport':
                rpcport = value
            elif key == 'rpcuser':
                rpcuser = value
            elif key == 'rpcpassword':
                rpcpassword = value
            elif key == 'rpchost':
                rpchost = value
            else:
                other_flags[key] = value
    return rpchost, rpcport, rpcuser, rpcpassword, other_flags

def get_network_info(rpc_port):
    if rpc_port == '9932':
        network = 'mainnet'
        burn_address = 'PtpasteLBurnAddressXXXXXXXXXXbJ5ndd'
    elif rpc_port == '19932':
        network = 'testnet'
        burn_address = 'tPpasteLBurnAddressXXXXXXXXXXX3wy7u'
    elif rpc_port == '29932':
        network = 'devnet'
        burn_address = '44oUgmZSL997veFEQDq569wv5tsT6KXf9QY7'
    else:
        raise ValueError(f"Unknown RPC port: {rpc_port}")
    return network, burn_address

def required_collateral(network):
    if network == 'mainnet':
        return 5000000  # 5 million PSL for mainnet
    else:
        return 1000000  # 1 million PSL for testnet/devnet

def write_rpc_settings_to_env_file_func(rpc_host, rpc_port, rpc_user, rpc_password, other_flags):
    with open('.env', 'w') as f:
        f.write(f"RPC_HOST={rpc_host}\n")
        f.write(f"RPC_PORT={rpc_port}\n")
        f.write(f"RPC_USER={rpc_user}\n")
        f.write(f"RPC_PASSWORD={rpc_password}\n")
        for current_flag in other_flags:
            current_value = other_flags[current_flag]
            try:
                f.write(f"{current_flag}={current_value}\n")
            except Exception as e:
                logger.error(f"Error writing to .env file: {e}")
                pass
    return

def is_base64_encoded(data):
    if not isinstance(data, str):
        return False
    if len(data) % 4 != 0:
        return False
    base64_pattern = re.compile(r'^[A-Za-z0-9+/]+={0,2}$')
    if not base64_pattern.match(data):
        return False
    try:
        base64.b64decode(data, validate=True)
        return True
    except Exception:
        return False

def kill_open_ssh_tunnels(local_port):
    try:
        lsof_command = [
            "lsof", "-i", f"TCP:{local_port}", "-sTCP:LISTEN"
        ]
        result = subprocess.run(lsof_command, capture_output=True, text=True)
        if result.stdout:
            lines = result.stdout.strip().split('\n')
            pids = []
            for line in lines[1:]:
                columns = line.split()
                if len(columns) > 1:
                    pids.append(columns[1])
            if pids:
                for pid in pids:
                    kill_command = ["kill", "-9", pid]
                    subprocess.run(kill_command)
                logger.info("Killed SSH tunnels with PIDs: {}".format(', '.join(pids)))
            else:
                logger.info("No SSH tunnels found to kill.")
        else:
            logger.info("No SSH tunnels found.")
    except Exception as e:
        logger.error("Error while killing SSH tunnels: {}".format(e))
        
def establish_ssh_tunnel():
    if USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
        kill_open_ssh_tunnels(REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT)
        key_path = REMOTE_SWISS_ARMY_LLAMA_INSTANCE_SSH_KEY_PATH
        if not os.access(key_path, os.R_OK):
            raise PermissionError("SSH key file at {} is not readable.".format(key_path))
        current_permissions = os.stat(key_path).st_mode & 0o777
        if current_permissions != 0o600:
            os.chmod(key_path, 0o600)
            logger.info("Permissions for SSH key file set to 600.")
        try:
            tunnel = SSHTunnelForwarder(
                (REMOTE_SWISS_ARMY_LLAMA_INSTANCE_IP_ADDRESS, REMOTE_SWISS_ARMY_LLAMA_INSTANCE_PORT),
                ssh_username="root",
                ssh_pkey=key_path,
                remote_bind_address=("localhost", REMOTE_SWISS_ARMY_LLAMA_EXPOSED_PORT),
                local_bind_address=("localhost", REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT),
                host_pkey_directories=[]  # Disable host key checking
            )
            tunnel.start()
            logger.info("SSH tunnel established: {}".format(tunnel.local_bind_address))
            while True:
                time.sleep(10)
        except BaseSSHTunnelForwarderError as e:
            logger.error("SSH tunnel error: {}".format(e))
        except Exception as e:
            logger.error("Error establishing SSH tunnel: {}".format(e))
                
def get_audio_length(audio_input) -> float:
    if isinstance(audio_input, bytes):
        audio_file = io.BytesIO(audio_input)
        audio = MutagenFile(audio_file)
    elif isinstance(audio_input, str):
        audio = MutagenFile(audio_input)
    else:
        raise ValueError("audio_input must be either bytes or a file path string.")
    if audio is None or not hasattr(audio.info, 'length'):
        raise ValueError("Could not determine the length of the audio file.")
    return audio.info.length
        
def convert_uuids_to_strings(data):
    if isinstance(data, dict):
        return {key: convert_uuids_to_strings(value) for key, value in data.items()}
    elif isinstance(data, list):
        return [convert_uuids_to_strings(item) for item in data]
    elif isinstance(data, uuid.UUID):
        return str(data)
    else:
        return data

def compute_sha3_256_hexdigest(input_str: str):
    """Compute the SHA3-256 hash of the input string and return the hexadecimal digest."""
    return hashlib.sha3_256(input_str.encode('utf-8')).hexdigest()

def compute_sha3_256_hexdigest_of_file(file_data: bytes):
    return hashlib.sha3_256(file_data).hexdigest()

def remove_file(path: str):
    if os.path.exists(path):
        os.remove(path)
    if path in file_store:
        del file_store[path]

async def save_file(file_content: bytes, filename: str):
    file_location = os.path.join(tempfile.gettempdir(), filename)
    with open(file_location, "wb") as buffer:
        buffer.write(file_content)
    file_hash = compute_sha3_256_hexdigest_of_file(file_content)
    file_size = os.path.getsize(file_location) # Calculate file size
    expire_at = datetime.utcnow() + timedelta(hours=24) # Set expiration time (24 hours)
    file_store[file_location] = expire_at
    return file_location, file_hash, file_size

async def upload_and_get_file_metadata(file_content: bytes, file_prefix: str = "document") -> Dict:
    file_name = f"{file_prefix}_{compute_sha3_256_hexdigest_of_file(file_content)[:8]}.{magika.identify_bytes(file_content).output.ct_label}"
    file_location, file_hash, file_size = await save_file(file_content, file_name)
    external_ip = get_external_ip_func()
    file_url = f"http://{external_ip}:{UVICORN_PORT}/download/{file_name}"    
    return {
        "file_location": file_location,
        "file_hash": file_hash,
        "file_size": file_size,
        "file_url": file_url
    }
    
async def calculate_xor_distance(pastelid1: str, pastelid2: str) -> int:
    hash1 = compute_sha3_256_hexdigest(pastelid1)
    hash2 = compute_sha3_256_hexdigest(pastelid2)
    xor_result = int(hash1, 16) ^ int(hash2, 16)
    return xor_result

async def get_supernode_url_from_pastelid_func(pastelid: str, supernode_list_df: pd.DataFrame) -> str:
    supernode_row = supernode_list_df[supernode_list_df['extKey'] == pastelid]
    if not supernode_row.empty:
        supernode_ipaddress_port = supernode_row['ipaddress:port'].values[0]
        ipaddress = supernode_ipaddress_port.split(':')[0]
        supernode_url = f"http://{ipaddress}:7123"
        return supernode_url
    else:
        raise ValueError(f"Supernode with PastelID {pastelid} not found in the supernode list")

async def get_closest_supernode_pastelid_from_list(local_pastelid: str, supernode_pastelids: List[str]) -> str:
    xor_distances = [(supernode_pastelid, await calculate_xor_distance(local_pastelid, supernode_pastelid)) for supernode_pastelid in supernode_pastelids]
    closest_supernode = min(xor_distances, key=lambda x: x[1])
    return closest_supernode[0]

async def get_closest_supernode_to_pastelid_url(input_pastelid: str, supernode_list_df: pd.DataFrame) -> Tuple[Optional[str], Optional[str]]:
    if not supernode_list_df.empty:
        list_of_supernode_pastelids = supernode_list_df['extKey'].tolist()
        closest_supernode_pastelid = await get_closest_supernode_pastelid_from_list(input_pastelid, list_of_supernode_pastelids)
        supernode_url = await get_supernode_url_from_pastelid_func(closest_supernode_pastelid, supernode_list_df)
        return supernode_url, closest_supernode_pastelid
    return None, None

async def get_n_closest_supernodes_to_pastelid_urls(n: int, input_pastelid: str, supernode_list_df: pd.DataFrame) -> List[Tuple[str, str]]:
    if not supernode_list_df.empty:
        list_of_supernode_pastelids = supernode_list_df['extKey'].tolist()
        xor_distances = [(supernode_pastelid, await calculate_xor_distance(input_pastelid, supernode_pastelid)) for supernode_pastelid in list_of_supernode_pastelids]
        sorted_xor_distances = sorted(xor_distances, key=lambda x: x[1])
        closest_supernodes = sorted_xor_distances[:n]
        supernode_urls_and_pastelids = [(await get_supernode_url_from_pastelid_func(pastelid, supernode_list_df), pastelid) for pastelid, _ in closest_supernodes]
        return supernode_urls_and_pastelids
    return []

class JSONRPCException(Exception):
    def __init__(self, rpc_error):
        parent_args = []
        try:
            parent_args.append(rpc_error['message'])
        except Exception as e:
            logger.error(f"Error occurred in JSONRPCException: {e}")
            pass
        Exception.__init__(self, *parent_args)
        self.error = rpc_error
        self.code = rpc_error['code'] if 'code' in rpc_error else None
        self.message = rpc_error['message'] if 'message' in rpc_error else None

    def __str__(self):
        return '%d: %s' % (self.code, self.message)

    def __repr__(self):
        return '<%s \'%s\'>' % (self.__class__.__name__, self)

def EncodeDecimal(o):
    if isinstance(o, decimal.Decimal):
        return float(round(o, 8))
    raise TypeError(repr(o) + " is not JSON serializable")

class AsyncAuthServiceProxy:
    _semaphore = asyncio.BoundedSemaphore(MAXIMUM_NUMBER_OF_CONCURRENT_RPC_REQUESTS)

    def __init__(self, service_url, service_name=None, reconnect_timeout=2, reconnect_amount=3, request_timeout=20):
        self.service_url = service_url
        self.service_name = service_name
        self.url = urlparse(service_url)
        self.id_count = 0
        user = self.url.username
        password = self.url.password
        authpair = f"{user}:{password}".encode('utf-8')
        self.auth_header = b'Basic ' + base64.b64encode(authpair)
        self.reconnect_timeout = reconnect_timeout
        self.reconnect_amount = reconnect_amount
        self.request_timeout = request_timeout
        self.client = httpx.AsyncClient(timeout=request_timeout, http2=True)

    def __getattr__(self, name):
        if name.startswith('__') and name.endswith('__'):
            raise AttributeError
        if self.service_name is not None:
            name = f"{self.service_name}.{name}"
        return AsyncAuthServiceProxy(self.service_url, name)

    async def __call__(self, *args):
        async with self._semaphore:
            self.id_count += 1
            postdata = json.dumps({
                'version': '2.0',
                'method': self.service_name,
                'params': args,
                'id': self.id_count
            })
            headers = {
                'Host': self.url.hostname,
                'User-Agent': "AuthServiceProxy/0.1",
                'Authorization': self.auth_header.decode(),
                'Content-type': 'application/json',
                'Connection': 'keep-alive'
            }
            for i in range(self.reconnect_amount):
                try:
                    if i > 0:
                        logger.warning(f"Reconnect try #{i+1}")
                        sleep_time = self.reconnect_timeout * (2 ** i)
                        logger.info(f"Waiting for {sleep_time} seconds before retrying.")
                        await asyncio.sleep(sleep_time)
                    response = await self.client.post(
                        self.service_url,
                        headers=headers,
                        content=postdata
                    )
                    response.raise_for_status()
                    response_json = response.json()
                    break
                except httpx.HTTPStatusError as e:
                    logger.error(f"HTTP error occurred in __call__: {e}")
                except httpx.RequestError as e:
                    logger.error(f"Request error occurred in __call__: {e}")
                except Exception as e:
                    logger.error(f"Unexpected error occurred in __call__: {e}")
            else:
                logger.error("Reconnect tries exceeded.")
                return
            if 'error' in response_json and response_json['error'] is not None:
                raise JSONRPCException(response_json['error'])
            elif 'result' not in response_json:
                raise JSONRPCException({
                    'code': -343, 'message': 'missing JSON-RPC result'})
            else:
                return response_json['result']
    async def close(self):
        await self.client.aclose()

async def save_stats_to_json():
    global rpc_call_stats, tracking_period_start  # Move both global declarations here
    while True:
        await asyncio.sleep(3600)  # Adjust this value for how often you want to save stats (e.g., every hour)
        tracking_period_end = datetime.utcnow()
        stats_snapshot = {
            "tracking_period_start": tracking_period_start.isoformat() + 'Z',
            "tracking_period_end": tracking_period_end.isoformat() + 'Z',
            "rpc_call_stats": dict(rpc_call_stats)
        }
        # Append the stats to the JSON file
        try:
            with open('rpc_call_stats.json', 'a') as f:
                f.write(json.dumps(stats_snapshot) + '\n')
        except Exception as e:
            print(f"Failed to save stats to JSON: {e}")
        # Reset tracking for the next period
        rpc_call_stats = defaultdict(lambda: {
            "count": 0,
            "cumulative_time": 0.0,
            "average_time": 0.0,
            "success_count": 0,
            "total_response_size": 0,
            "average_response_size": 0.0,
            "timeout_errors": 0,
            "connection_errors": 0,
            "other_errors": 0
        })
        tracking_period_start = tracking_period_end

def track_rpc_call(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        method_name = kwargs.get('method_name', func.__name__)
        
        def make_hashable(obj):
            if isinstance(obj, (list, tuple)):
                return tuple(make_hashable(e) for e in obj)
            elif isinstance(obj, dict):
                return tuple(sorted((k, make_hashable(v)) for k, v in obj.items()))
            elif isinstance(obj, set):
                return frozenset(make_hashable(e) for e in obj)
            return obj

        hashable_args = make_hashable(args)
        hashable_kwargs = make_hashable(kwargs)
        
        rpc_key = (method_name, hashable_args, hashable_kwargs)
        
        start_time = time.time()
        try:
            result = await func(*args, **kwargs)
            elapsed_time = time.time() - start_time
            response_size = len(str(result).encode('utf-8'))
            
            if rpc_key not in rpc_call_stats:
                rpc_call_stats[rpc_key] = {
                    "count": 0,
                    "cumulative_time": 0.0,
                    "average_time": 0.0,
                    "success_count": 0,
                    "total_response_size": 0,
                    "average_response_size": 0.0,
                    "timeout_errors": 0,
                    "connection_errors": 0,
                    "other_errors": 0
                }
            
            rpc_call_stats[rpc_key]["count"] += 1
            rpc_call_stats[rpc_key]["cumulative_time"] += elapsed_time
            rpc_call_stats[rpc_key]["average_time"] = (
                rpc_call_stats[rpc_key]["cumulative_time"] / rpc_call_stats[rpc_key]["count"]
            )
            rpc_call_stats[rpc_key]["total_response_size"] += response_size
            rpc_call_stats[rpc_key]["average_response_size"] = (
                rpc_call_stats[rpc_key]["total_response_size"] / rpc_call_stats[rpc_key]["count"]
            )
            rpc_call_stats[rpc_key]["success_count"] += 1
            return result
        except httpx.TimeoutException:
            if rpc_key not in rpc_call_stats:
                rpc_call_stats[rpc_key] = {"timeout_errors": 0}
            rpc_call_stats[rpc_key]["timeout_errors"] = rpc_call_stats[rpc_key].get("timeout_errors", 0) + 1
            raise
        except httpx.ConnectError:
            if rpc_key not in rpc_call_stats:
                rpc_call_stats[rpc_key] = {"connection_errors": 0}
            rpc_call_stats[rpc_key]["connection_errors"] = rpc_call_stats[rpc_key].get("connection_errors", 0) + 1
            raise
        except Exception as e:
            if rpc_key not in rpc_call_stats:
                rpc_call_stats[rpc_key] = {"other_errors": 0}
            rpc_call_stats[rpc_key]["other_errors"] = rpc_call_stats[rpc_key].get("other_errors", 0) + 1
            raise e
    return wrapper

#Wrapped RPC calls so we can track them and log their performance:
@track_rpc_call
async def getinfo(rpc_connection):
    return await rpc_connection.getinfo()

@track_rpc_call
async def getblockcount(rpc_connection):
    return await rpc_connection.getblockcount()

@track_rpc_call
async def getblockhash(rpc_connection, block_height):
    return await rpc_connection.getblockhash(block_height)

@track_rpc_call
async def getblock(rpc_connection, block_hash):
    return await rpc_connection.getblock(block_hash)

@track_rpc_call
async def listaddressamounts(rpc_connection):
    return await rpc_connection.listaddressamounts()

@track_rpc_call
async def z_getbalance(rpc_connection, address_to_check):
    return await rpc_connection.z_getbalance(address_to_check)

@track_rpc_call
async def getrawtransaction(rpc_connection, txid, verbose=1):
    return await rpc_connection.getrawtransaction(txid, verbose)

@track_rpc_call
async def masternode_top(rpc_connection):
    return await rpc_connection.masternode('top')

@track_rpc_call
async def masternodelist_full(rpc_connection):
    return await rpc_connection.masternodelist('full')

@track_rpc_call
async def masternodelist_rank(rpc_connection):
    return await rpc_connection.masternodelist('rank')

@track_rpc_call
async def masternodelist_pubkey(rpc_connection):
    return await rpc_connection.masternodelist('pubkey')

@track_rpc_call
async def masternodelist_extra(rpc_connection):
    return await rpc_connection.masternodelist('extra')

@track_rpc_call
async def masternode_message_list(rpc_connection):
    return await rpc_connection.masternode('message', 'list')

@track_rpc_call
async def pastelid_sign(rpc_connection, message_to_sign, pastelid, passphrase, algorithm='ed448'):
    return await rpc_connection.pastelid('sign', message_to_sign, pastelid, passphrase, algorithm)

@track_rpc_call
async def pastelid_verify(rpc_connection, message_to_verify, signature, pastelid, algorithm='ed448'):
    return await rpc_connection.pastelid('verify', message_to_verify, signature, pastelid, algorithm)

@track_rpc_call
async def masternode_message_send(rpc_connection, receiving_sn_pubkey, compressed_message_base64):
    return await rpc_connection.masternode('message', 'send', receiving_sn_pubkey, compressed_message_base64)

@track_rpc_call
async def tickets_register_contract(rpc_connection, ticket_json_b64, ticket_type_identifier, ticket_input_data_hash):
    return await rpc_connection.tickets('register', 'contract', ticket_json_b64, ticket_type_identifier, ticket_input_data_hash)

@track_rpc_call
async def tickets_get(rpc_connection, ticket_txid, verbose=1):
    return await rpc_connection.tickets('get', ticket_txid, verbose)

@track_rpc_call
async def generic_tickets_find(rpc_connection, credit_ticket_secondary_key):
    return await rpc_connection.tickets('find', 'contract', credit_ticket_secondary_key)

@track_rpc_call
async def tickets_list_contract(rpc_connection, ticket_type_identifier, starting_block_height):
    return await rpc_connection.tickets('list', 'contract', ticket_type_identifier, starting_block_height)

@track_rpc_call
async def listsinceblock(rpc_connection, start_block_hash, target_confirmations=1, include_watchonly=True):
    return await rpc_connection.listsinceblock(start_block_hash, target_confirmations, include_watchonly)

@track_rpc_call
async def tickets_list_id(rpc_connection, identifier):
    return await rpc_connection.tickets('list', 'id', identifier)

@track_rpc_call
async def getaddressutxosextra(rpc_connection, params):
    try:
        formatted_params = {
            "addresses": params.get('addresses', []),
            "simple": False,  # We want full info
            "minHeight": params.get('minHeight', 0),
            "sender": params.get('sender', ''),
            "mempool": params.get('mempool', True)
        }
        result = await rpc_connection.getaddressutxosextra(formatted_params)
        return result
    except Exception as e:
        logger.error(f"Error in getaddressutxosextra RPC call: {str(e)}")
        logger.error(f"Error type: {type(e)}")
        logger.error(f"Error details: {traceback.format_exc()}")
        return None  # Return None explicitly on error

@track_rpc_call
async def decoderawtransaction(rpc_connection, raw_tx_data):
    return await rpc_connection.decoderawtransaction(raw_tx_data)

@track_rpc_call
async def gettransaction(rpc_connection, txid, include_watchonly=False):
    return await rpc_connection.gettransaction(txid, include_watchonly)

async def micro_benchmarking_func():
    baseline_score = 16
    duration_of_benchmark_in_seconds = 4.0
    end_time = time.time() + duration_of_benchmark_in_seconds
    actual_score = 0
    while time.time() < end_time:
        try:
            info_results = await getinfo(rpc_connection)  # Await the coroutine
            if 'blocks' in info_results and isinstance(info_results['blocks'], int):
                actual_score += 1
        except Exception as e:
            logger.error(f"Error during benchmarking: {e}", exc_info=True)
            continue
    benchmark_performance_ratio = actual_score / baseline_score
    current_datetime_utc = datetime.utcnow().isoformat()
    logger.info(f"Benchmark performance ratio as of {current_datetime_utc}: {benchmark_performance_ratio}; Raw score: {actual_score}")
    benchmark_results_cache.append([current_datetime_utc, actual_score, benchmark_performance_ratio])
    cutoff_date = datetime.utcnow() - timedelta(weeks=2)
    benchmark_results_cache[:] = [row for row in benchmark_results_cache if datetime.fromisoformat(row[0]) >= cutoff_date]

async def write_benchmark_cache_to_csv():
    try:
        with open(local_benchmark_csv_file_path, mode='w', newline='') as file:
            writer = csv.writer(file)
            writer.writerows(benchmark_results_cache)
    except Exception as e:
        logger.error(f"Error writing to CSV: {e}", exc_info=True)

async def load_benchmark_cache_from_csv():
    global benchmark_results_cache
    try:
        if local_benchmark_csv_file_path.exists():
            with open(local_benchmark_csv_file_path, mode='r') as file:
                reader = csv.reader(file)
                benchmark_results_cache = list(reader)
    except Exception as e:
        logger.error(f"Error loading from CSV: {e}", exc_info=True)

async def schedule_micro_benchmark_periodically():
    await load_benchmark_cache_from_csv()
    while True:
        await micro_benchmarking_func()
        await asyncio.sleep(60)
        await write_benchmark_cache_to_csv()
        
async def get_current_pastel_block_height_func():
    curent_block_height = await getblockcount(rpc_connection)
    return curent_block_height

async def get_best_block_hash_and_merkle_root_func():
    best_block_height = await get_current_pastel_block_height_func()
    best_block_hash = await getblockhash(rpc_connection, best_block_height)
    best_block_details = await getblock(rpc_connection, best_block_hash)
    best_block_merkle_root = best_block_details['merkleroot']
    return best_block_hash, best_block_merkle_root, best_block_height

async def get_last_block_data_func():
    current_block_height = await get_current_pastel_block_height_func()
    block_data = await getblock(rpc_connection, str(current_block_height))
    return block_data

async def check_psl_address_balance_alternative_func(address_to_check):
    address_amounts_dict = await listaddressamounts(rpc_connection)
    # Convert the dictionary into a list of dictionaries, each representing a row
    data = [{'address': address, 'amount': amount} for address, amount in address_amounts_dict.items()]
    # Create the DataFrame from the list of dictionaries
    address_amounts_df = pd.DataFrame(data)
    # Filter the DataFrame for the specified address
    address_amounts_df_filtered = address_amounts_df[address_amounts_df['address'] == address_to_check]
    # Calculate the sum of the 'amount' column for the filtered DataFrame
    balance_at_address = address_amounts_df_filtered['amount'].sum()
    return balance_at_address

async def check_psl_address_balance_func(address_to_check):
    balance_at_address = await z_getbalance(rpc_connection, address_to_check)
    return balance_at_address

async def get_raw_transaction_func(txid):
    raw_transaction_data = await getrawtransaction(rpc_connection, txid, 1)
    return raw_transaction_data

async def generate_challenge(pastelid: str) -> Tuple[str, str]:
    """
    Generates a random challenge string and a unique challenge ID for a given PastelID.
    The challenge string is stored temporarily, associated with the challenge ID, and expires after a certain period of time.
    """
    challenge_string = str(uuid.uuid4())
    challenge_id = str(uuid.uuid4())
    expiration_time = time.time() + CHALLENGE_EXPIRATION_TIME_IN_SECONDS
    challenge_store[challenge_id] = {
        "pastelid": pastelid,
        "challenge_string": challenge_string,
        "expiration_time": expiration_time
    }
    return challenge_string, challenge_id

async def verify_challenge_signature(pastelid: str, signature: str, challenge_id: str) -> bool:
    """
    Verifies the signature of the PastelID on the challenge string associated with the provided challenge ID.
    If the signature is valid and the challenge ID exists and hasn't expired, it returns True. Otherwise, it returns False.
    """
    if challenge_id not in challenge_store:
        return False
    challenge_data = challenge_store[challenge_id]
    stored_pastelid = challenge_data["pastelid"]
    challenge_string = challenge_data["challenge_string"]
    expiration_time = challenge_data["expiration_time"]
    if pastelid != stored_pastelid:
        return False
    current_time = time.time()
    if current_time > expiration_time:
        del challenge_store[challenge_id]
        return False
    verification_result = await verify_message_with_pastelid_func(pastelid=pastelid, message_to_verify=challenge_string, pastelid_signature_on_message=signature) 
    is_valid_signature = verification_result == 'OK'
    if is_valid_signature:
        del challenge_store[challenge_id]
        return True
    else:
        return False

async def verify_challenge_signature_from_inference_request_id(inference_request_id: str, challenge_signature: str, challenge_id: str) -> bool:
    # Retrieve the inference API usage request from the database
    async with db_code.Session() as db:
        result = await db.exec(
            select(db_code.InferenceAPIUsageRequest).where(db_code.InferenceAPIUsageRequest.inference_request_id == inference_request_id)
        )
        inference_request = result.one_or_none()
    if inference_request:
        requesting_pastelid = inference_request.requesting_pastelid
        is_valid_signature = await verify_challenge_signature(requesting_pastelid, challenge_signature, challenge_id)
        return is_valid_signature
    else:
        return False

async def check_masternode_top_func():
    masternode_top_command_output = await masternode_top(rpc_connection)
    return masternode_top_command_output

async def check_inference_port(supernode, max_response_time_in_milliseconds, local_performance_data):
    ip_address_port = supernode.get('ipaddress:port')
    if not ip_address_port or ip_address_port.startswith(local_ip):
        return None
    ip_address = ip_address_port.split(":")[0]
    try:
        async with httpx.AsyncClient(timeout=max_response_time_in_milliseconds / 1000) as client:
            response = await client.get(f'http://{ip_address}:7123/liveness_ping')
            if response.status_code != 200:
                return None
            response_data = response.json()
            performance_ratio = response_data.get('performance_ratio_score')
            if performance_ratio is None:
                performance_ratio = float('nan')  # Assign NaN if the value is None
            actual_score = response_data.get('raw_benchmark_score', 'N/A')
            if not isinstance(performance_ratio, (int, float)) or performance_ratio < MICRO_BENCHMARK_PERFORMANCE_RATIO_THRESHOLD:
                performance_ratio = 'N/A'
                actual_score = 'N/A'
            timestamp_str = response_data.get('timestamp')
            timestamp = datetime.fromisoformat(timestamp_str)
            if timestamp.tzinfo is None:
                timestamp = timestamp.replace(tzinfo=timezone.utc)
            last_updated = (datetime.now(timezone.utc) - timestamp).total_seconds()
            local_performance_data.append({'IP Address': ip_address, 'Performance Ratio': performance_ratio, 'Actual Score': actual_score, 'Seconds Since Last Updated': last_updated})
            return supernode
    except (httpx.RequestError, httpx.ConnectTimeout, httpx.ConnectError, httpx.ReadTimeout, OSError) as e:  # noqa: F841
        return None

async def update_performance_data_df(local_performance_data):
    global performance_data_df
    local_performance_data_df = pd.DataFrame(local_performance_data)
    if not local_performance_data_df.empty and 'IP Address' in local_performance_data_df.columns:
        local_performance_data_df.sort_values(by='IP Address', inplace=True)
    # Replace "N/A" with NaN for numerical operations
    local_performance_data_df.replace("N/A", pd.NA, inplace=True)
    summary_statistics = {
        'IP Address': ['Min', 'Average', 'Median', 'Max'],
        'Performance Ratio': [
            pd.to_numeric(local_performance_data_df['Performance Ratio'], errors='coerce').min(),
            pd.to_numeric(local_performance_data_df['Performance Ratio'], errors='coerce').mean(),
            pd.to_numeric(local_performance_data_df['Performance Ratio'], errors='coerce').median(),
            pd.to_numeric(local_performance_data_df['Performance Ratio'], errors='coerce').max()
        ],
        'Actual Score': [
            pd.to_numeric(local_performance_data_df['Actual Score'], errors='coerce').min(),
            pd.to_numeric(local_performance_data_df['Actual Score'], errors='coerce').mean(),
            pd.to_numeric(local_performance_data_df['Actual Score'], errors='coerce').median(),
            pd.to_numeric(local_performance_data_df['Actual Score'], errors='coerce').max()
        ],
        'Seconds Since Last Updated': [
            pd.to_numeric(local_performance_data_df['Seconds Since Last Updated'], errors='coerce').min(),
            pd.to_numeric(local_performance_data_df['Seconds Since Last Updated'], errors='coerce').mean(),
            pd.to_numeric(local_performance_data_df['Seconds Since Last Updated'], errors='coerce').median(),
            pd.to_numeric(local_performance_data_df['Seconds Since Last Updated'], errors='coerce').max()
        ]
    }
    summary_df = pd.DataFrame(summary_statistics)
    local_performance_data_df = pd.concat([local_performance_data_df, summary_df], ignore_index=True)
    local_performance_data_df.to_csv('supernode_performance_data.csv', index=False)
    performance_data_df = pd.concat([performance_data_df, local_performance_data_df], ignore_index=True)
    return local_performance_data_df

async def save_performance_data_history(local_performance_data_df):
    global performance_data_history
    current_time_str = datetime.utcnow().isoformat()
    # Load existing data
    if pickle_file_path.exists():
        try:
            with open(pickle_file_path, 'rb') as f:
                existing_data = pickle.load(f)
        except (EOFError, pickle.UnpicklingError) as e:
            logger.error(f"Error reading existing pickle file: {e}", exc_info=True)
            existing_data = {}
    else:
        existing_data = {}
    # Update the global performance_data_history
    performance_data_history.update(existing_data)
    performance_data_history[current_time_str] = local_performance_data_df
    # Remove entries older than 3 days
    cutoff_date = datetime.utcnow() - timedelta(days=3)
    performance_data_history = {k: v for k, v in performance_data_history.items() if datetime.fromisoformat(k) >= cutoff_date}
    # Save updated data
    try:
        with open(pickle_file_path, 'wb') as f:
            pickle.dump(performance_data_history, f)
    except Exception as e:
        logger.error(f"Error saving pickle file: {e}", exc_info=True)

async def generate_supernode_inference_ip_blacklist(max_response_time_in_milliseconds=800):
    global performance_data_df, performance_data_history
    blacklist_path = Path('supernode_inference_ip_blacklist.txt')
    valid_supernode_list_path = Path('valid_supernode_list.txt')
    logger.info("Now compiling Supernode IP blacklist based on Supernode responses to port checks...")
    # Ensure the blacklist and valid supernode files exist
    if not blacklist_path.exists():
        blacklist_path.touch()
    if not valid_supernode_list_path.exists():
        valid_supernode_list_path.touch()
    # Perform the supernode checks
    full_supernode_list_df, _ = await check_supernode_list_func()
    local_performance_data = []
    check_results = await asyncio.gather(
        *(check_inference_port(supernode, max_response_time_in_milliseconds, local_performance_data) for supernode in full_supernode_list_df.to_dict(orient='records'))
    )
    logger.info(f"Gathered check results. Number of results: {len(check_results)}")
    logger.info(f"Local performance data entries: {len(local_performance_data)}")
    # Filter supernodes based on success/failure
    successful_nodes = {supernode['extKey'] for supernode in check_results if supernode is not None}
    successful_nodes_ip_addresses = {supernode['ipaddress:port']for supernode in check_results if supernode is not None}
    failed_nodes = {supernode['ipaddress:port'] for supernode in full_supernode_list_df.to_dict(orient='records') if supernode['extKey'] not in successful_nodes}
    logger.info(f"There were {len(failed_nodes)} failed Supernodes out of {len(full_supernode_list_df)} total Supernodes, a failure rate of {len(failed_nodes) / len(full_supernode_list_df) * 100:.2f}%")
    # Update performance data
    local_performance_data_df = await update_performance_data_df(local_performance_data)
    await save_performance_data_history(local_performance_data_df)
    # Write failed nodes to blacklist
    with blacklist_path.open('w') as blacklist_file:
        for failed_node in failed_nodes:
            ip_address = failed_node.split(':')[0]
            blacklist_file.write(f"{ip_address}\n")
    # Write successful nodes to the valid supernode list
    with valid_supernode_list_path.open('w') as valid_supernode_file:
        for successful_node in successful_nodes_ip_addresses:
            ip_address = successful_node.split(':')[0]
            valid_supernode_file.write(f"{ip_address}\n")
    return list(successful_nodes)

async def fetch_supernode_list_data():
    masternode_list_full_command_output = await masternodelist_full(rpc_connection)
    masternode_list_rank_command_output = await masternodelist_rank(rpc_connection)
    masternode_list_pubkey_command_output = await masternodelist_pubkey(rpc_connection)
    masternode_list_extra_command_output = await masternodelist_extra(rpc_connection)
    if masternode_list_full_command_output:
        masternode_list_full_df = pd.DataFrame([masternode_list_full_command_output[x].split() for x in masternode_list_full_command_output])
        masternode_list_full_df['txid_vout'] = [x for x in masternode_list_full_command_output]
        masternode_list_full_df.columns = ['supernode_status', 'protocol_version', 'supernode_psl_address', 'lastseentime', 'activeseconds', 'lastpaidtime', 'lastpaidblock', 'ipaddress:port', 'txid_vout']
        masternode_list_full_df.index = masternode_list_full_df['txid_vout']
        masternode_list_full_df.drop(columns=['txid_vout'], inplace=True)
        for txid_vout in masternode_list_full_df.index:
            rank = masternode_list_rank_command_output.get(txid_vout)
            pubkey = masternode_list_pubkey_command_output.get(txid_vout)
            extra = masternode_list_extra_command_output.get(txid_vout, {})
            masternode_list_full_df.at[txid_vout, 'rank'] = rank if rank is not None else 'Unknown'
            masternode_list_full_df.at[txid_vout, 'pubkey'] = pubkey if pubkey is not None else 'Unknown'
            masternode_list_full_df.at[txid_vout, 'extAddress'] = extra.get('extAddress', 'Unknown')
            masternode_list_full_df.at[txid_vout, 'extP2P'] = extra.get('extP2P', 'Unknown')
            masternode_list_full_df.at[txid_vout, 'extKey'] = extra.get('extKey', 'Unknown')
        masternode_list_full_df['lastseentime'] = pd.to_numeric(masternode_list_full_df['lastseentime'], downcast='integer')
        masternode_list_full_df['lastpaidtime'] = pd.to_numeric(masternode_list_full_df['lastpaidtime'], downcast='integer')
        masternode_list_full_df['lastseentime'] = pd.to_datetime(masternode_list_full_df['lastseentime'], unit='s')
        masternode_list_full_df['lastpaidtime'] = pd.to_datetime(masternode_list_full_df['lastpaidtime'], unit='s')
        masternode_list_full_df['activeseconds'] = masternode_list_full_df['activeseconds'].astype(int)
        masternode_list_full_df['lastpaidblock'] = masternode_list_full_df['lastpaidblock'].astype(int)
        masternode_list_full_df['activedays'] = masternode_list_full_df['activeseconds'].apply(lambda x: float(x) / 86400.0)
        masternode_list_full_df['rank'] = masternode_list_full_df['rank'].astype(int, errors='ignore')
        masternode_list_full_df = masternode_list_full_df[masternode_list_full_df['supernode_status'].isin(['ENABLED', 'PRE_ENABLED'])]
        masternode_list_full_df__json = masternode_list_full_df.to_json(orient='index')
        return masternode_list_full_df, masternode_list_full_df__json
    else:
        error_message = "Masternode list command returning nothing-- pasteld probably just started and hasn't yet finished the mnsync process!"
        logger.error(error_message)
        raise ValueError(error_message)

async def check_supernode_list_func():
    if 'supernode_data' not in SUPERNODE_DATA_CACHE:
        SUPERNODE_DATA_CACHE['supernode_data'] = await fetch_supernode_list_data()
    return SUPERNODE_DATA_CACHE['supernode_data']

async def get_local_machine_supernode_data_func():
    local_machine_ip = get_external_ip_func()
    supernode_list_full_df, _ = await check_supernode_list_func()
    proper_port_number = statistics.mode([x.split(':')[1] for x in supernode_list_full_df['ipaddress:port'].values.tolist()])
    local_machine_ip_with_proper_port = local_machine_ip + ':' + proper_port_number
    local_machine_supernode_data = supernode_list_full_df[supernode_list_full_df['ipaddress:port'] == local_machine_ip_with_proper_port]
    if len(local_machine_supernode_data) == 0:
        logger.error('Local machine is not a supernode!')
        return 0, 0, 0, 0
    else:
        local_sn_rank = local_machine_supernode_data['rank'].values[0]
        local_sn_pastelid = local_machine_supernode_data['extKey'].values[0]
    return local_machine_supernode_data, local_sn_rank, local_sn_pastelid, local_machine_ip_with_proper_port

async def get_my_local_pastelid_func():
    local_machine_supernode_data, _, _, _ = await get_local_machine_supernode_data_func()
    my_local_pastelid = local_machine_supernode_data['extKey'].values.tolist()[0]    
    return my_local_pastelid

async def get_sn_data_from_pastelid_func(specified_pastelid):
    supernode_list_full_df, _ = await check_supernode_list_func()
    specified_machine_supernode_data = supernode_list_full_df[supernode_list_full_df['extKey'] == specified_pastelid]
    if len(specified_machine_supernode_data) == 0:
        logger.error('Specified machine is not a supernode!')
        return pd.DataFrame()
    else:
        return specified_machine_supernode_data

async def get_sn_data_from_sn_pubkey_func(specified_sn_pubkey):
    supernode_list_full_df, _ = await check_supernode_list_func()
    specified_machine_supernode_data = supernode_list_full_df[supernode_list_full_df['pubkey'] == specified_sn_pubkey]
    if len(specified_machine_supernode_data) == 0:
        logger.error('Specified machine is not a supernode!')
        return pd.DataFrame()
    else:
        return specified_machine_supernode_data

async def compress_data_with_zstd_func(input_data):
    if isinstance(input_data, str):
        input_data = input_data.encode('utf-8')
    zstd_compression_level = 22
    zstandard_compressor = zstd.ZstdCompressor(level=zstd_compression_level, write_content_size=True, write_checksum=True)
    zstd_compressed_data = zstandard_compressor.compress(input_data)
    zstd_compressed_data__base64_encoded = base64.b64encode(zstd_compressed_data).decode('utf-8')
    return zstd_compressed_data, zstd_compressed_data__base64_encoded

async def decompress_data_with_zstd_func(compressed_input_data):
    zstd_decompressor = zstd.ZstdDecompressor()
    zstd_decompressed_data = zstd_decompressor.decompress(compressed_input_data)
    return zstd_decompressed_data

async def list_sn_messages_func():
    datetime_cutoff_to_ignore_obsolete_messages = pd.to_datetime(datetime.now(timezone.utc) - timedelta(days=NUMBER_OF_DAYS_BEFORE_MESSAGES_ARE_CONSIDERED_OBSOLETE)).isoformat()
    try:
        supernode_list_df, _ = await check_supernode_list_func()
    except Exception as e:  # noqa: F841
        return None
    txid_vout_to_pastelid_dict = dict(zip(supernode_list_df.index, supernode_list_df['extKey']))
    async with db_code.Session() as db:
        # Retrieve messages from the database that meet the timestamp criteria
        query = await db.exec(
            select(db_code.Message)
            .where(db_code.Message.timestamp >= datetime_cutoff_to_ignore_obsolete_messages)
            .order_by(db_code.Message.timestamp.desc())
        )
        db_messages = query.all()
        existing_messages = {(message.sending_sn_pastelid, message.receiving_sn_pastelid, message.timestamp) for message in db_messages}
    # Retrieve new messages from the RPC interface
    new_messages = await masternode_message_list(rpc_connection)
    new_messages_data = []
    for message in new_messages:
        message_key = list(message.keys())[0]
        message = message[message_key]
        sending_sn_txid_vout = message['From']
        receiving_sn_txid_vout = message['To']
        sending_pastelid = txid_vout_to_pastelid_dict.get(sending_sn_txid_vout)
        receiving_pastelid = txid_vout_to_pastelid_dict.get(receiving_sn_txid_vout)
        if sending_pastelid is None or receiving_pastelid is None:
            # logger.warning(f"Skipping message due to missing PastelID for txid_vout: {sending_sn_txid_vout} or {receiving_sn_txid_vout}")
            continue
        message_timestamp = pd.to_datetime(datetime.fromtimestamp(message['Timestamp']).isoformat(), utc=True)
        # Check if the message already exists in the database
        if (sending_pastelid, receiving_pastelid, message_timestamp) in existing_messages:
            logger.info("Message already exists in the database. Skipping...")
            continue
        message_body = base64.b64decode(message['Message'].encode('utf-8'))
        verification_status = await verify_received_message_using_pastelid_func(message_body, sending_pastelid)
        decompressed_message = await decompress_data_with_zstd_func(message_body)
        decompressed_message = decompressed_message.decode('utf-8')
        try:
            message_dict = json.loads(decompressed_message)
        except json.JSONDecodeError as e:
            logger.error(f"Error parsing JSON: {e}")
            logger.error(f"Decompressed message: {decompressed_message}")
            continue
        if verification_status == 'OK':
            new_message = {
                'sending_sn_pastelid': sending_pastelid,
                'receiving_sn_pastelid': receiving_pastelid,
                'message_type': message_dict['message_type'],
                'message_body': decompressed_message,
                'signature': message_dict['signature'],
                'timestamp': message_timestamp,
                'sending_sn_txid_vout': sending_sn_txid_vout,
                'receiving_sn_txid_vout': receiving_sn_txid_vout
            }
            new_messages_data.append(new_message)
    combined_messages_df = pd.DataFrame(new_messages_data)
    if not combined_messages_df.empty:
        combined_messages_df['timestamp'] = pd.to_datetime(combined_messages_df['timestamp'], utc=True)
        combined_messages_df = combined_messages_df[combined_messages_df['timestamp'] >= datetime_cutoff_to_ignore_obsolete_messages]
        combined_messages_df = combined_messages_df.sort_values('timestamp', ascending=False)
    return combined_messages_df

def get_oldest_pastelid_file_pubkey(pastel_keys_dir, pastelid_override=None):
    if pastelid_override:
        return pastelid_override
    pastel_key_files = [f for f in os.listdir(pastel_keys_dir) if os.path.isfile(os.path.join(pastel_keys_dir, f))]
    if not pastel_key_files:
        raise ValueError("No PastelID files found in the specified directory.")
    return min(pastel_key_files, key=lambda f: os.path.getctime(os.path.join(pastel_keys_dir, f)))

async def sign_message_with_libpastelid(message: str, pastelid: str, passphrase: str):
    signed_message = pastel_signer.sign_with_pastel_id(message, pastelid, passphrase)
    return signed_message

async def verify_message_with_libpastelid(pastelid: str, message: str, signature: str):
    is_valid = pastel_signer.verify_with_pastel_id(message, signature, pastelid)
    return "OK" if is_valid else "Failed"

async def sign_message_with_pastelid_func(pastelid: str, message_to_sign: str, passphrase: str) -> str:
    if use_libpastelid_for_pastelid_sign_verify:
        return await sign_message_with_libpastelid(message_to_sign, pastelid, passphrase)
    else:
        results_dict = await pastelid_sign(rpc_connection, message_to_sign, pastelid, passphrase, 'ed448')
        return results_dict['signature']

async def verify_message_with_pastelid_func(pastelid: str, message_to_verify: str, pastelid_signature_on_message: str) -> str:
    if use_libpastelid_for_pastelid_sign_verify:
        return await verify_message_with_libpastelid(pastelid, message_to_verify, pastelid_signature_on_message)
    else:
        verification_result = await pastelid_verify(rpc_connection, message_to_verify, pastelid_signature_on_message, pastelid, 'ed448')
        return verification_result['verification']
    
async def parse_sn_messages_from_last_k_minutes_func(k=10, message_type='all'):
    messages_list_df = await list_sn_messages_func()
    messages_list_df__recent = messages_list_df[messages_list_df['timestamp'] > (datetime.now(timezone.utc) - timedelta(minutes=k))]
    if message_type == 'all':
        list_of_message_dicts = messages_list_df__recent[['message_body', 'message_type', 'sending_sn_pastelid', 'timestamp']].to_dict(orient='records')
    else:
        list_of_message_dicts = messages_list_df__recent[messages_list_df__recent['message_type'] == message_type][['message_body', 'message_type', 'sending_sn_pastelid', 'timestamp']].to_dict(orient='records')
    if len(list_of_message_dicts) > 0:
        return [
            {
                'message': json.loads(msg['message_body'])['message'],  # Extract the 'message' field as a string
                'message_type': msg['message_type'],
                'sending_sn_pastelid': msg['sending_sn_pastelid'],
                'timestamp': msg['timestamp'].isoformat()  # Convert timestamp to ISO format
            }
            for msg in list_of_message_dicts
        ]
    else:
        return []

async def verify_received_message_using_pastelid_func(message_received, sending_sn_pastelid):
    try:
        decompressed_message = await decompress_data_with_zstd_func(message_received)
        message_received_dict = json.loads(decompressed_message)
        raw_message = message_received_dict['message']
        signature = message_received_dict['signature']
        verification_status = await verify_message_with_pastelid_func(sending_sn_pastelid, raw_message, signature)
    except Exception as e:
        logger.error(f"Error verifying message: {e}")
        verification_status = f"Message verification failed: {str(e)}"
    return verification_status

async def send_message_to_sn_using_pastelid_func(message_to_send, message_type, receiving_sn_pastelid, pastelid_passphrase):
    local_machine_supernode_data, _, _, _ = await get_local_machine_supernode_data_func()
    sending_sn_pastelid = local_machine_supernode_data['extKey'].values.tolist()[0]
    sending_sn_pubkey = local_machine_supernode_data['pubkey'].values.tolist()[0]
    pastelid_signature_on_message = await sign_message_with_pastelid_func(sending_sn_pastelid, message_to_send, pastelid_passphrase)
    signed_message_to_send = json.dumps({
        'message': message_to_send,
        'message_type': message_type,
        'signature': pastelid_signature_on_message,
        'sending_sn_pubkey': sending_sn_pubkey
    }, ensure_ascii=False)    
    compressed_message, _ = await compress_data_with_zstd_func(signed_message_to_send.encode('utf-8'))
    compressed_message_base64 = base64.b64encode(compressed_message).decode('utf-8')
    specified_machine_supernode_data = await get_sn_data_from_pastelid_func(receiving_sn_pastelid)
    receiving_sn_pubkey = specified_machine_supernode_data['pubkey'].values.tolist()[0]
    logger.info(f"Now sending message to SN with PastelID: {receiving_sn_pastelid} and SN pubkey: {receiving_sn_pubkey}: {message_to_send}")
    await masternode_message_send(rpc_connection, receiving_sn_pubkey, compressed_message_base64)
    return signed_message_to_send, pastelid_signature_on_message

async def broadcast_message_to_list_of_sns_using_pastelid_func(message_to_send, message_type, list_of_receiving_sn_pastelids, pastelid_passphrase, verbose=0):
    local_machine_supernode_data, _, _, _ = await get_local_machine_supernode_data_func()
    sending_sn_pastelid = local_machine_supernode_data['extKey'].values.tolist()[0]
    sending_sn_pubkey = local_machine_supernode_data['pubkey'].values.tolist()[0]
    pastelid_signature_on_message = await sign_message_with_pastelid_func(sending_sn_pastelid, message_to_send, pastelid_passphrase)
    signed_message_to_send = json.dumps({
        'message': message_to_send,
        'message_type': message_type,
        'signature': pastelid_signature_on_message,
        'sending_sn_pubkey': sending_sn_pubkey
    }, ensure_ascii=False)
    compressed_message, _ = await compress_data_with_zstd_func(signed_message_to_send.encode('utf-8'))
    compressed_message_base64 = base64.b64encode(compressed_message).decode('utf-8')
    if verbose:
        logger.info(f"Now sending message to list of {len(list_of_receiving_sn_pastelids)} SNs: `{message_to_send}`")        
    async def send_message(receiving_sn_pastelid):
        current_receiving_sn_pubkey = (await get_sn_data_from_pastelid_func(receiving_sn_pastelid))['pubkey'].values.tolist()[0]
        await masternode_message_send(rpc_connection, current_receiving_sn_pubkey, compressed_message_base64)
    await asyncio.gather(*[send_message(pastelid) for pastelid in list_of_receiving_sn_pastelids])
    return signed_message_to_send

async def broadcast_message_to_all_sns_using_pastelid_func(message_to_send, message_type, pastelid_passphrase, verbose=0):
    local_machine_supernode_data, _, _, _ = await get_local_machine_supernode_data_func()
    sending_sn_pastelid = local_machine_supernode_data['extKey'].values.tolist()[0]
    sending_sn_pubkey = local_machine_supernode_data['pubkey'].values.tolist()[0]
    pastelid_signature_on_message = await sign_message_with_pastelid_func(sending_sn_pastelid, message_to_send, pastelid_passphrase)
    signed_message_to_send = json.dumps({
        'message': message_to_send,
        'message_type': message_type,
        'signature': pastelid_signature_on_message,
        'sending_sn_pubkey': sending_sn_pubkey
    }, ensure_ascii=False)
    compressed_message, _ = await compress_data_with_zstd_func(signed_message_to_send.encode('utf-8'))
    compressed_message_base64 = base64.b64encode(compressed_message).decode('utf-8')
    list_of_receiving_sn_pastelids = (await check_supernode_list_func())[0]['extKey'].values.tolist()
    if verbose:
        logger.info(f"Now sending message to ALL {len(list_of_receiving_sn_pastelids)} SNs: `{message_to_send}`")        
    async def send_message(receiving_sn_pastelid):
        current_receiving_sn_pubkey = (await get_sn_data_from_pastelid_func(receiving_sn_pastelid))['pubkey'].values.tolist()[0]
        await masternode_message_send(rpc_connection, current_receiving_sn_pubkey, compressed_message_base64)
    await asyncio.gather(*[send_message(pastelid) for pastelid in list_of_receiving_sn_pastelids])
    return signed_message_to_send

async def get_supernode_model_menu(supernode_url):
    try:
        async with httpx.AsyncClient(timeout=Timeout(INDIVIDUAL_SUPERNODE_MODEL_MENU_REQUEST_TIMEOUT_PERIOD_IN_SECONDS)) as client:
            response = await client.get(f"{supernode_url}/get_inference_model_menu")
            response.raise_for_status()
            model_menu = response.json()
            return model_menu
    except Exception as e:
        logger.error(f"Error retrieving model menu from Supernode URL: {supernode_url}: {e}")
        return None

def is_model_supported(model_menu, desired_model_canonical_string, desired_model_inference_type_string, desired_model_parameters_json):
    if model_menu:
        desired_parameters = json.loads(desired_model_parameters_json)
        model_names = [model["model_name"] for model in model_menu["models"]]
        best_match = process.extractOne(desired_model_canonical_string, model_names)
        if best_match is not None and best_match[1] >= 95:
            matched_model = next(model for model in model_menu["models"] if model["model_name"] == best_match[0])
            if "supported_inference_type_strings" in matched_model and "model_parameters" in matched_model:
                if desired_model_inference_type_string in matched_model["supported_inference_type_strings"]:
                    for desired_param, desired_value in desired_parameters.items():
                        param_found = False
                        for param in matched_model["model_parameters"]:
                            if param["name"] == desired_param:
                                if "type" in param:
                                    if param["type"] == "int" and isinstance(int(desired_value), int):
                                        param_found = True
                                    elif param["type"] == "float" and isinstance(float(desired_value), float):
                                        param_found = True
                                    elif param["type"] == "string" and isinstance(str(desired_value), str):
                                        if "options" in param and desired_value in param["options"]:
                                            param_found = True
                                        elif "options" not in param:
                                            param_found = True
                                else:
                                    param_found = True
                                break
                        if not param_found:
                            return False
                    return True
            return False
    return False
        
async def broadcast_message_to_n_closest_supernodes_to_given_pastelid(input_pastelid, message_body, message_type):
    blacklist_path = Path('supernode_inference_ip_blacklist.txt')
    blacklisted_ips = set()
    if blacklist_path.exists():
        with blacklist_path.open('r') as blacklist_file:
            blacklisted_ips = {line.strip() for line in blacklist_file if line.strip()}
    else:
        logger.info("Blacklist file not found. Proceeding without blacklist filtering.")
    supernode_list_df, _ = await check_supernode_list_func()
    n = 4
    local_machine_supernode_data, _, _, _ = await get_local_machine_supernode_data_func()
    local_sn_pastelid = local_machine_supernode_data['extKey'].values.tolist()[0]
    message_body_dict = json.loads(message_body)
    desired_model_canonical_string = message_body_dict.get('requested_model_canonical_string')
    desired_model_inference_type_string = message_body_dict.get('model_inference_type_string')
    desired_model_parameters_json = base64.b64decode(message_body_dict.get('model_parameters_json_b64')).decode('utf-8')
    async def is_model_supported_async(supernode_ip_and_port, model_canonical_string, model_inference_type_string, model_parameters_json):
        supernode_ip = supernode_ip_and_port.split(':')[0]
        supernode_url = f"http://{supernode_ip}:7123"
        if supernode_ip in blacklisted_ips:
            return False
        model_menu = await get_supernode_model_menu(supernode_url)
        return is_model_supported(model_menu, model_canonical_string, model_inference_type_string, model_parameters_json)
    supported_supernodes_coroutines = [
        is_model_supported_async(row['ipaddress:port'], desired_model_canonical_string, desired_model_inference_type_string, desired_model_parameters_json)
        for _, row in supernode_list_df.iterrows()
    ]
    supported_supernodes_mask = await asyncio.gather(*supported_supernodes_coroutines)
    supported_supernodes = supernode_list_df[supported_supernodes_mask]
    supported_supernodes_minus_this_supernode = supported_supernodes[supported_supernodes['extKey'] != local_sn_pastelid]
    if len(supported_supernodes_minus_this_supernode) == 0:
        logger.error(f"No other supported supernodes found for the desired model: {desired_model_canonical_string} with inference type: {desired_model_inference_type_string} and parameters: {desired_model_parameters_json}")
        supported_supernodes_minus_this_supernode = supernode_list_df[supernode_list_df['extKey'] != local_sn_pastelid]
        logger.info("We had to choose audit supernodes which cannot process the request themselves if needed!")
    closest_supernodes = await get_n_closest_supernodes_to_pastelid_urls(n, input_pastelid, supported_supernodes_minus_this_supernode)
    list_of_supernode_pastelids = [x[1] for x in closest_supernodes]
    list_of_supernode_urls = [x[0] for x in closest_supernodes]
    list_of_supernode_ips = [x.split('//')[1].split(':')[0] for x in list_of_supernode_urls]
    signed_message = await broadcast_message_to_list_of_sns_using_pastelid_func(message_body, message_type, list_of_supernode_pastelids, LOCAL_PASTEL_ID_PASSPHRASE)
    logger.info(f"Broadcasted a {message_type} to {len(list_of_supernode_pastelids)} closest supernodes to PastelID: {input_pastelid} (with Supernode IPs of {list_of_supernode_ips}): {message_body}")
    return signed_message

async def retry_on_database_locked(func, *args, max_retries=5, initial_delay=1, backoff_factor=2, jitter_factor=0.1, **kwargs):
    delay = initial_delay
    for attempt in range(max_retries):
        try:
            result = func(*args, **kwargs)
            if asyncio.iscoroutine(result):
                return await result
            return result
        except OperationalError as e:
            if "database is locked" in str(e) and attempt < max_retries - 1:
                jitter = random.uniform(1 - jitter_factor, 1 + jitter_factor)
                delay *= backoff_factor * jitter
                logger.warning(f"Database locked. Retrying in {delay:.2f} second(s)...")
                await asyncio.sleep(delay)
            else:
                raise
        except InvalidRequestError as e:
            if "This Session's transaction has been rolled back due to a previous exception during flush" in str(e):
                logger.warning("Session transaction has been rolled back. Retrying...")
                db_session = args[0]  # Assuming the first argument is the db_session
                await db_session.rollback()
            else:
                raise
            
async def check_if_record_exists(db_session, model, **kwargs):
    existing_record = await db_session.execute(
        select(model).filter_by(**kwargs)
    )
    return existing_record.scalars().first()

async def process_broadcast_messages(message, db_session):
    try:
        message_body = json.loads(message.message_body)
        if message.message_type == 'inference_request_response_announcement_message':
            response_data = json.loads(message_body['message'])
            existing_request = await check_if_record_exists(
                db_session, db_code.InferenceAPIUsageRequest,
                sha3_256_hash_of_inference_request_fields=response_data['sha3_256_hash_of_inference_request_fields']
            )
            existing_response = await check_if_record_exists(
                db_session, db_code.InferenceAPIUsageResponse,
                sha3_256_hash_of_inference_request_response_fields=response_data['sha3_256_hash_of_inference_request_response_fields']
            )
            if not existing_request and not existing_response:
                usage_request = db_code.InferenceAPIUsageRequest(**response_data)
                usage_response = db_code.InferenceAPIUsageResponse(**response_data)
                await asyncio.sleep(random.uniform(0.1, 0.5))  # Random sleep before DB operations
                await retry_on_database_locked(db_session.add, usage_request)
                await retry_on_database_locked(db_session.add, usage_response)
                await retry_on_database_locked(db_session.commit)
                await retry_on_database_locked(db_session.refresh, usage_request)
                await retry_on_database_locked(db_session.refresh, usage_response)
            else:
                logger.info("Skipping insertion as the record already exists.")
        elif message.message_type == 'inference_request_result_announcement_message':
            result_data = json.loads(message_body['message'])
            existing_result = await check_if_record_exists(
                db_session, db_code.InferenceAPIOutputResult,
                sha3_256_hash_of_inference_result_fields=result_data['sha3_256_hash_of_inference_result_fields']
            )
            if not existing_result:
                output_result = db_code.InferenceAPIOutputResult(**result_data)
                await asyncio.sleep(random.uniform(0.1, 0.5))  # Random sleep before DB operations
                await retry_on_database_locked(db_session.add, output_result)
                await retry_on_database_locked(db_session.commit)
                await retry_on_database_locked(db_session.refresh, output_result)
            else:
                logger.info("Skipping insertion as the result record already exists.")
    except Exception as e:   # noqa: F841
        traceback.print_exc()
        
async def monitor_new_messages():
    last_processed_timestamp = None
    while True:
        try:
            async with db_code.Session() as db:
                if last_processed_timestamp is None:
                    query = await db.exec(select(db_code.Message.timestamp).order_by(db_code.Message.timestamp.desc()).limit(1))
                    last_processed_timestamp_raw = query.one_or_none()
                    if last_processed_timestamp_raw is None:
                        last_processed_timestamp = pd.Timestamp.min.tz_localize('UTC')
                    else:
                        last_processed_timestamp = pd.Timestamp(last_processed_timestamp_raw).tz_localize('UTC').tz_convert('UTC')
                try:
                    new_messages_df = await list_sn_messages_func()
                except Exception as e:   # noqa: F841
                    new_messages_df = None
                if new_messages_df is not None and not new_messages_df.empty:
                    new_messages_df['timestamp'] = pd.to_datetime(new_messages_df['timestamp'], utc=True)
                    new_messages_df = new_messages_df[new_messages_df['timestamp'] > last_processed_timestamp]
                    if not new_messages_df.empty:
                        for _, message in new_messages_df.iterrows():
                            query = await db.exec(
                                select(db_code.Message).where(
                                    db_code.Message.sending_sn_pastelid == message['sending_sn_pastelid'],
                                    db_code.Message.receiving_sn_pastelid == message['receiving_sn_pastelid'],
                                    db_code.Message.timestamp == message['timestamp']
                                )
                            )
                            existing_messages = query.all()
                            if len(existing_messages) > 1:
                                logger.error(f"Multiple rows found for message: {message}")
                                continue
                            elif len(existing_messages) == 1:
                                continue
                            log_action_with_payload("received new", "message", message)
                            last_processed_timestamp = message['timestamp']
                            sending_sn_pastelid = message['sending_sn_pastelid']
                            receiving_sn_pastelid = message['receiving_sn_pastelid']
                            message_size_bytes = len(message['message_body'].encode('utf-8'))
                            await asyncio.sleep(random.uniform(0.1, 0.5))
                            query = await db.exec(
                                select(db_code.MessageSenderMetadata).where(db_code.MessageSenderMetadata.sending_sn_pastelid == sending_sn_pastelid)
                            )
                            sender_metadata = query.one_or_none()
                            if sender_metadata:
                                sender_metadata.total_messages_sent += 1
                                sender_metadata.total_data_sent_bytes += message_size_bytes
                                sender_metadata.sending_sn_txid_vout = message['sending_sn_txid_vout']
                                sender_metadata.sending_sn_pubkey = message['signature']
                            else:
                                sender_metadata = db_code.MessageSenderMetadata(
                                    sending_sn_pastelid=sending_sn_pastelid,
                                    total_messages_sent=1,
                                    total_data_sent_bytes=message_size_bytes,
                                    sending_sn_txid_vout=message['sending_sn_txid_vout'],
                                    sending_sn_pubkey=message['signature']
                                )
                                db.add(sender_metadata)
                            query = await db.exec(
                                select(db_code.MessageReceiverMetadata).where(db_code.MessageReceiverMetadata.receiving_sn_pastelid == receiving_sn_pastelid)
                            )
                            receiver_metadata = query.one_or_none()
                            if receiver_metadata:
                                receiver_metadata.total_messages_received += 1
                                receiver_metadata.total_data_received_bytes += message_size_bytes
                                receiver_metadata.receiving_sn_txid_vout = message['receiving_sn_txid_vout']
                            else:
                                receiver_metadata = db_code.MessageReceiverMetadata(
                                    receiving_sn_pastelid=receiving_sn_pastelid,
                                    total_messages_received=1,
                                    total_data_received_bytes=message_size_bytes,
                                    receiving_sn_txid_vout=message['receiving_sn_txid_vout']
                                )
                                db.add(receiver_metadata)
                            query = await db.exec(
                                select(db_code.MessageSenderReceiverMetadata).where(
                                    db_code.MessageSenderReceiverMetadata.sending_sn_pastelid == sending_sn_pastelid,
                                    db_code.MessageSenderReceiverMetadata.receiving_sn_pastelid == receiving_sn_pastelid
                                )
                            )
                            sender_receiver_metadata = query.one_or_none()
                            if sender_receiver_metadata:
                                sender_receiver_metadata.total_messages += 1
                                sender_receiver_metadata.total_data_bytes += message_size_bytes
                            else:
                                sender_receiver_metadata = db_code.MessageSenderReceiverMetadata(
                                    sending_sn_pastelid=sending_sn_pastelid,
                                    receiving_sn_pastelid=receiving_sn_pastelid,
                                    total_messages=1,
                                    total_data_bytes=message_size_bytes
                                )
                                db.add(sender_receiver_metadata)
                            new_messages = [
                                db_code.Message(
                                    sending_sn_pastelid=row['sending_sn_pastelid'],
                                    receiving_sn_pastelid=row['receiving_sn_pastelid'],
                                    message_type=row['message_type'],
                                    message_body=row['message_body'],
                                    signature=row['signature'],
                                    timestamp=row['timestamp'],
                                    sending_sn_txid_vout=row['sending_sn_txid_vout'],
                                    receiving_sn_txid_vout=row['receiving_sn_txid_vout']
                                )
                                for _, row in new_messages_df.iterrows()
                            ]
                            await asyncio.sleep(random.uniform(0.1, 0.5))
                            await retry_on_database_locked(db.add_all, new_messages)
                            await retry_on_database_locked(db.commit)
                            query = await db.exec(
                                select(
                                    func.count(db_code.Message.id),
                                    func.count(func.distinct(db_code.Message.sending_sn_pastelid)),
                                    func.count(func.distinct(db_code.Message.receiving_sn_pastelid))
                                )
                            )
                            total_messages, total_senders, total_receivers = query.one()
                            query = await db.exec(select(db_code.MessageMetadata).order_by(db_code.MessageMetadata.timestamp.desc()).limit(1))
                            message_metadata = query.one_or_none()
                            if message_metadata:
                                message_metadata.total_messages = total_messages
                                message_metadata.total_senders = total_senders
                                message_metadata.total_receivers = total_receivers
                            else:
                                message_metadata = db_code.MessageMetadata(
                                    total_messages=total_messages,
                                    total_senders=total_senders,
                                    total_receivers=total_receivers
                                )
                                db.add(message_metadata)
                            await asyncio.sleep(random.uniform(0.1, 0.5))
                            await retry_on_database_locked(db.commit)
                            processing_tasks = [
                                process_broadcast_messages(message, db)
                                for message in new_messages
                            ]
                            await asyncio.gather(*processing_tasks)
                await asyncio.sleep(5)
        except Exception as e:
            logger.error(f"Error while monitoring new messages: {str(e)}")
            traceback.print_exc()
            await asyncio.sleep(5)
        finally:
            await asyncio.sleep(5)
            
async def get_list_of_credit_pack_ticket_txids_from_pastelid(pastelid: str) -> list:
    ticket_type_identifier = "INFERENCE_API_CREDIT_PACK_TICKET"
    starting_block_height = 700000
    list_of_ticket_data_dicts = await tickets_list_contract(rpc_connection, ticket_type_identifier, starting_block_height)
    list_of_ticket_internal_data_dicts = [x['ticket']['contract_ticket'] for x in list_of_ticket_data_dicts]
    list_of_ticket_input_data_json_strings_for_tickets_where_given_pastelid_is_in_list_of_allowed_users = [json.dumps(x['ticket_input_data_dict']) for x in list_of_ticket_internal_data_dicts if pastelid in x['ticket_input_data_dict']['credit_pack_purchase_request_dict']['list_of_authorized_pastelids_allowed_to_use_credit_pack']] 
    list_of_ticket_input_data_fully_parsed_sha3_256_hashes = [compute_fully_parsed_json_sha3_256_hash(x)[0] for x in list_of_ticket_input_data_json_strings_for_tickets_where_given_pastelid_is_in_list_of_allowed_users]
    list_of_credit_pack_ticket_data = [None] * len(list_of_ticket_input_data_fully_parsed_sha3_256_hashes)
    async def fetch_and_store_ticket_data(index, hash):
        list_of_credit_pack_ticket_data[index] = await generic_tickets_find(rpc_connection, hash)
    await asyncio.gather( *[fetch_and_store_ticket_data(i, hash) for i, hash in enumerate(list_of_ticket_input_data_fully_parsed_sha3_256_hashes)])
    list_of_credit_pack_ticket_registration_txids = [x['txid'] for x in list_of_credit_pack_ticket_data if x is not None]
    return list_of_credit_pack_ticket_registration_txids, list_of_credit_pack_ticket_data

async def create_user_message(from_pastelid: str, to_pastelid: str, message_body: str, message_signature: str) -> dict:
    async with db_code.Session() as db:
        user_message = db_code.UserMessage(from_pastelid=from_pastelid, to_pastelid=to_pastelid, message_body=message_body, message_signature=message_signature)
        db.add(user_message)
        db.commit()
        db.refresh(user_message)
        user_message_dict = user_message.model_dump()
        user_message_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in user_message_dict.items()}
        return user_message.model_dump()

async def create_supernode_user_message(sending_sn_pastelid: str, receiving_sn_pastelid: str, user_message_data: dict) -> dict:
    async with db_code.Session() as db:
        supernode_user_message = db_code.SupernodeUserMessage(
            message_body=user_message_data['message_body'],
            message_type="user_message",
            sending_sn_pastelid=sending_sn_pastelid,
            receiving_sn_pastelid=receiving_sn_pastelid,
            signature=user_message_data['message_signature'],
            timestamp=datetime.utcnow(),
            user_message_id=user_message_data['id']
        )
        db.add(supernode_user_message)
        db.commit()
        db.refresh(supernode_user_message)
        supernode_user_message_dict = supernode_user_message.model_dump()
        supernode_user_message_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in supernode_user_message_dict.items()}
        return supernode_user_message_dict

async def send_user_message_via_supernodes(from_pastelid: str, to_pastelid: str, message_body: str, message_signature: str) -> dict:
    user_message_data = await create_user_message(from_pastelid, to_pastelid, message_body, message_signature)
    local_machine_supernode_data, _, _, _ = await get_local_machine_supernode_data_func()
    sending_sn_pastelid = local_machine_supernode_data['extKey'][0]  # Assuming this is a list
    # Find the 3 closest Supernodes to the receiving end user's PastelID
    supernode_list_df, _ = await check_supernode_list_func()
    closest_supernodes = await get_n_closest_supernodes_to_pastelid_urls(3, to_pastelid, supernode_list_df)
    if not closest_supernodes:
        raise ValueError(f"No Supernodes found for PastelID: {to_pastelid}.")
    # Create a list to store the message_dicts for each Supernode
    message_dicts = []
    # Send the message to the 3 closest Supernodes in parallel using asyncio.gather
    send_tasks = []
    for closest_supernode_url, receiving_sn_pastelid in closest_supernodes:
        # Now that we have user_message_data and the receiving Supernode PastelID, let's create the supernode user message.
        supernode_user_message_data = await create_supernode_user_message(sending_sn_pastelid, receiving_sn_pastelid, user_message_data)
        # Preparing the message to be sent to the supernode.
        signed_message_to_send = json.dumps({
            'message': user_message_data['message_body'],
            'message_type': 'user_message',
            'signature': user_message_data['message_signature'],
            'from_pastelid': user_message_data['from_pastelid'],
            'to_pastelid': user_message_data['to_pastelid']
        }, ensure_ascii=False)
        # Send the message to the receiving Supernode.
        send_task = asyncio.create_task(send_message_to_sn_using_pastelid_func(signed_message_to_send, 'user_message', receiving_sn_pastelid, LOCAL_PASTEL_ID_PASSPHRASE))
        send_tasks.append(send_task)
    # Wait for all send tasks to complete
    send_results = await asyncio.gather(*send_tasks)
    # Create the message_dict for each Supernode
    for send_result in send_results:
        signed_message, pastelid_signature_on_message = send_result
        message_dict = {
            "message": user_message_data['message_body'],  # The content of the message
            "message_type": "user_message",  # Static type as per your design
            "sending_sn_pastelid": sending_sn_pastelid,  # From local machine supernode data
            "timestamp": datetime.now(timezone.utc).isoformat(),  # Current UTC timestamp
            "id": supernode_user_message_data['id'],  # ID from the supernode user message record
            "signature": pastelid_signature_on_message,
            "user_message": {
                # Details from the user_message_data
                "from_pastelid": from_pastelid,
                "to_pastelid": to_pastelid,
                "message_body": message_body,
                "message_signature": user_message_data['message_signature'],
                "id": user_message_data['id'],  # Assuming these fields are included in your dictionary
                "timestamp": user_message_data['timestamp']
            }
        }
        message_dicts.append(message_dict)
    return message_dicts

async def process_received_user_message(supernode_user_message: db_code.SupernodeUserMessage):
    async with db_code.Session() as db:
        query = await db.exec(select(db_code.UserMessage).where(db_code.UserMessage.id == supernode_user_message.user_message_id))
        user_message = query.one_or_none()
        if user_message:
            verification_status = await verify_message_with_pastelid_func(user_message.from_pastelid, user_message.message_body, user_message.message_signature)
            if verification_status == 'OK':
                # Process the user message (e.g., store it, forward it to the recipient, etc.)
                logger.info(f"Received and verified user message from {user_message.from_pastelid} to {user_message.to_pastelid}")
            else:
                logger.warning(f"Received user message from {user_message.from_pastelid} to {user_message.to_pastelid}, but verification failed")
        else:
            logger.warning(f"Received SupernodeUserMessage (id: {supernode_user_message.id}), but the associated UserMessage was not found")

async def get_user_messages_for_pastelid(pastelid: str) -> List[db_code.UserMessage]:
    async with db_code.Session() as db:
        query = await db.exec(select(db_code.UserMessage).where((db_code.UserMessage.from_pastelid == pastelid) | (db_code.UserMessage.to_pastelid == pastelid)))
        user_messages = query.all()
        return user_messages
            
#________________________________________________________________________________________________________________            
# Credit pack related service functions:

async def get_credit_pack_purchase_request(sha3_256_hash_of_credit_pack_purchase_request_fields: str) -> db_code.CreditPackPurchaseRequest:
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequest).where(db_code.CreditPackPurchaseRequest.sha3_256_hash_of_credit_pack_purchase_request_fields == sha3_256_hash_of_credit_pack_purchase_request_fields)
            )
            return result.one_or_none()
    except Exception as e:
        logger.error(f"Error getting credit pack purchase request: {str(e)}")
        raise

async def save_credit_pack_purchase_request(credit_pack_purchase_request: db_code.CreditPackPurchaseRequest) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_request)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase request: {str(e)}")
        raise

async def get_credit_pack_purchase_request_response(sha3_256_hash_of_credit_pack_purchase_request_response_fields: str) -> db_code.CreditPackPurchaseRequestResponse:
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestResponse).where(db_code.CreditPackPurchaseRequestResponse.sha3_256_hash_of_credit_pack_purchase_request_response_fields == sha3_256_hash_of_credit_pack_purchase_request_response_fields)
            )
            return result.one_or_none()
    except Exception as e:
        logger.error(f"Error getting credit pack purchase request response: {str(e)}")
        raise
    
async def get_credit_pack_purchase_request_response_from_request_hash(sha3_256_hash_of_credit_pack_purchase_request_fields: str) -> db_code.CreditPackPurchaseRequestResponse:
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestResponse).where(db_code.CreditPackPurchaseRequestResponse.sha3_256_hash_of_credit_pack_purchase_request_fields == sha3_256_hash_of_credit_pack_purchase_request_fields)
            )
            return result.one_or_none()
    except Exception as e:
        logger.error(f"Error getting credit pack purchase request response: {str(e)}")
        raise
    
async def get_credit_pack_purchase_request_from_response(response: db_code.CreditPackPurchaseRequestResponse) -> Optional[db_code.CreditPackPurchaseRequest]:
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequest).where(db_code.CreditPackPurchaseRequest.sha3_256_hash_of_credit_pack_purchase_request_fields == response.sha3_256_hash_of_credit_pack_purchase_request_fields)
            )
            return result.one_or_none()
    except Exception as e:
        logger.error(f"Error getting credit pack purchase request response: {str(e)}")
        raise    
    
async def save_credit_pack_purchase_request_response(credit_pack_purchase_request_response: db_code.CreditPackPurchaseRequestResponse) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_request_response)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase request response: {str(e)}")
        raise

async def get_credit_pack_purchase_request_rejection(sha3_256_hash_of_credit_pack_purchase_request_fields: str) -> db_code.CreditPackPurchaseRequestRejection:
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestRejection).where(db_code.CreditPackPurchaseRequestRejection.sha3_256_hash_of_credit_pack_purchase_request_fields == sha3_256_hash_of_credit_pack_purchase_request_fields)
            )
            return result.one_or_none()
    except Exception as e:
        logger.error(f"Error getting credit pack purchase request rejection: {str(e)}")
        raise

async def save_credit_pack_purchase_request_rejection(credit_pack_purchase_request_rejection: db_code.CreditPackPurchaseRequestRejection) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_request_rejection)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase request rejection: {str(e)}")
        raise

async def get_credit_pack_purchase_request_response_termination(sha3_256_hash_of_credit_pack_purchase_request_fields: str) -> db_code.CreditPackPurchaseRequestResponseTermination:
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestResponseTermination).where(db_code.CreditPackPurchaseRequestResponseTermination.sha3_256_hash_of_credit_pack_purchase_request_fields == sha3_256_hash_of_credit_pack_purchase_request_fields)
            )
            return result.one_or_none()
    except Exception as e:
        logger.error(f"Error getting credit pack purchase request response termination: {str(e)}")
        raise

async def save_credit_pack_purchase_request_response_termination(credit_pack_purchase_request_response_termination: db_code.CreditPackPurchaseRequestResponseTermination) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_request_response_termination)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase request response termination: {str(e)}")
        raise

async def get_credit_pack_purchase_request_confirmation(sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str) -> db_code.CreditPackPurchaseRequestConfirmation:
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestConfirmation).where(db_code.CreditPackPurchaseRequestConfirmation.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields == sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields)
            )
            return result.one_or_none()
    except Exception as e:
        logger.error(f"Error getting credit pack purchase request confirmation: {str(e)}")
        raise

async def get_credit_pack_purchase_request_confirmation_from_request_hash(sha3_256_hash_of_credit_pack_purchase_request_fields: str) -> db_code.CreditPackPurchaseRequestConfirmation:
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestConfirmation).where(db_code.CreditPackPurchaseRequestConfirmation.sha3_256_hash_of_credit_pack_purchase_request_fields == sha3_256_hash_of_credit_pack_purchase_request_fields)
            )
            return result.one_or_none()
    except Exception as e:
        logger.error(f"Error getting credit pack purchase request confirmation: {str(e)}")
        raise

async def save_credit_pack_purchase_request_confirmation(credit_pack_purchase_request_confirmation: db_code.CreditPackPurchaseRequestConfirmation) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_request_confirmation)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase request confirmation: {str(e)}")
        raise

async def get_credit_pack_purchase_request_confirmation_response(sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields: str) -> db_code.CreditPackPurchaseRequestConfirmationResponse:
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestConfirmationResponse).where(db_code.CreditPackPurchaseRequestConfirmationResponse.sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields == sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields)
            )
            return result.one_or_none()
    except Exception as e:
        logger.error(f"Error getting credit pack purchase request confirmation response: {str(e)}")
        raise
            
async def save_credit_pack_purchase_request_confirmation_response(credit_pack_purchase_request_confirmation_response: db_code.CreditPackPurchaseRequestConfirmationResponse) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_request_confirmation_response)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase request confirmation response: {str(e)}")
        raise
    
async def save_credit_pack_purchase_preliminary_price_quote(credit_pack_purchase_preliminary_price_quote: db_code.CreditPackPurchaseRequestPreliminaryPriceQuote) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_preliminary_price_quote)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase preliminary price quote: {str(e)}")
        raise

async def save_credit_pack_purchase_price_agreement_request(credit_pack_purchase_price_agreement_request: db_code.CreditPackPurchasePriceAgreementRequest) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_price_agreement_request)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase price agreement request: {str(e)}")
        raise

async def save_credit_pack_purchase_price_agreement_request_response(credit_pack_purchase_price_agreement_request_response: db_code.CreditPackPurchasePriceAgreementRequestResponse) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_price_agreement_request_response)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase price agreement request response: {str(e)}")
        raise    
    
async def save_credit_pack_purchase_request_status_check(credit_pack_purchase_request_status_check: db_code.CreditPackPurchaseRequestStatus) -> None:
    try:
        async with db_code.Session() as db_session:
            db_session.add(credit_pack_purchase_request_status_check)
            await db_session.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase request status check: {str(e)}")
        raise    

async def save_credit_pack_purchase_request_final_response(response: db_code.CreditPackPurchaseRequestResponse) -> None:
    try:
        async with db_code.Session() as db:
            db.add(response)
            await db.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack purchase request final response: {str(e)}")
        raise

async def save_credit_pack_storage_completion_announcement(response: db_code.CreditPackPurchaseRequestConfirmationResponse) -> None:
    try:
        async with db_code.Session() as db:
            db.add(response)
            await db.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack storage completion announcement: {str(e)}")
        raise
        
async def save_credit_pack_purchase_completion_announcement(confirmation: db_code.CreditPackPurchaseRequestConfirmation) -> None:
    try:
        async with db_code.Session() as db:
            db.add(confirmation)
            await db.commit()
    except Exception as e:
        logger.error(f"Error storing credit pack purchase completion announcement: {str(e)}")
        raise
    
async def save_credit_pack_storage_retry_request(response: db_code.CreditPackPurchaseRequestConfirmationResponse) -> None:
    try:
        async with db_code.Session() as db:
            db.add(response)
            await db.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack storage retry request: {str(e)}")
        raise
    
async def save_credit_pack_storage_retry_request_response(response: db_code.CreditPackPurchaseRequestConfirmationResponse) -> None:
    try:
        async with db_code.Session() as db:
            db.add(response)
            await db.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack storage retry request response: {str(e)}")
        raise

async def save_credit_pack_storage_retry_completion_announcement(response: db_code.CreditPackPurchaseRequestConfirmationResponse) -> None:
    try:
        async with db_code.Session() as db:
            db.add(response)
            await db.commit()
    except Exception as e:
        logger.error(f"Error saving credit pack storage retry completion announcement: {str(e)}")
        raise
            
async def check_credit_pack_purchase_request_status(credit_pack_purchase_request: db_code.CreditPackPurchaseRequest) -> str:
    async with db_code.Session() as db:
        response = await db.exec(
            select(db_code.CreditPackPurchaseRequestResponse).where(db_code.CreditPackPurchaseRequestResponse.sha3_256_hash_of_credit_pack_purchase_request_fields == credit_pack_purchase_request.sha3_256_hash_of_credit_pack_purchase_request_fields)
        )
        response = response.one_or_none()
        if response is None:
            return "pending"
        else:
            confirmation = await db.exec(
                select(db_code.CreditPackPurchaseRequestConfirmation).where(db_code.CreditPackPurchaseRequestConfirmation.sha3_256_hash_of_credit_pack_purchase_request_response_fields == response.sha3_256_hash_of_credit_pack_purchase_request_response_fields)
            )
            confirmation = confirmation.one_or_none()
            if confirmation is None:
                return "approved"
            else:
                confirmation_response = await db.exec(
                    select(db_code.CreditPackPurchaseRequestConfirmationResponse).where(db_code.CreditPackPurchaseRequestConfirmationResponse.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields == confirmation.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields)
                )
                confirmation_response = confirmation_response.one_or_none()
                if confirmation_response is None:
                    return "confirmed"
                else:
                    if confirmation_response.credit_pack_confirmation_outcome_string == "success":
                        return "completed"
                    else:
                        return "failed"
                    
async def calculate_preliminary_psl_price_per_credit():
    try:
        # Get the current PSL market price in USD
        psl_price_usd = await fetch_current_psl_market_price()
        # Calculate the cost per credit in USD, considering the profit margin
        cost_per_credit_usd = TARGET_VALUE_PER_CREDIT_IN_USD / (1 - TARGET_PROFIT_MARGIN)
        # Convert the cost per credit from USD to PSL
        cost_per_credit_psl = cost_per_credit_usd / psl_price_usd
        # Round the cost per credit to the nearest 0.1
        rounded_cost_per_credit_psl = round(cost_per_credit_psl, 1)
        logger.info(f"Calculated preliminary price per credit: {rounded_cost_per_credit_psl:,.1f} PSL")
        return rounded_cost_per_credit_psl
    except (ValueError, ZeroDivisionError) as e:
        logger.error(f"Error calculating preliminary price per credit: {str(e)}")
        traceback.print_exc()
        raise

async def determine_agreement_with_proposed_price(proposed_psl_price_per_credit: float) -> bool:
    try:
        # Calculate the local preliminary price per credit
        local_price_per_credit = await calculate_preliminary_psl_price_per_credit()
        # Calculate the acceptable price range (within 10% of the local price)
        min_acceptable_price = local_price_per_credit * (1.0 - MAXIMUM_LOCAL_CREDIT_PRICE_DIFFERENCE_TO_ACCEPT_CREDIT_PRICING)
        max_acceptable_price = local_price_per_credit * (1.0 + MAXIMUM_LOCAL_CREDIT_PRICE_DIFFERENCE_TO_ACCEPT_CREDIT_PRICING)
        # Determine if the proposed price is within the acceptable range
        agree_with_proposed_price = min_acceptable_price <= proposed_psl_price_per_credit <= max_acceptable_price
        logger.info(f"Proposed price per credit: {proposed_psl_price_per_credit:,.1f} PSL")
        logger.info(f"Local price per credit: {local_price_per_credit:,.1f} PSL")
        logger.info(f"Acceptable price range: [{min_acceptable_price:,.1f}, {max_acceptable_price:,.1f}] PSL")
        logger.info(f"Agreement with proposed price: {agree_with_proposed_price}")
        return agree_with_proposed_price
    except Exception as e:
        logger.error(f"Error determining agreement with proposed price: {str(e)}")
        traceback.print_exc()        
        raise
    
async def check_burn_transaction(txid: str, credit_usage_tracking_psl_address: str, total_cost_in_psl: float, request_response_pastel_block_height: int) -> Tuple[bool, int]:
    try:
        max_block_height = request_response_pastel_block_height + MAXIMUM_NUMBER_OF_PASTEL_BLOCKS_FOR_USER_TO_SEND_BURN_AMOUNT_FOR_CREDIT_TICKET
        max_retries = 30
        initial_retry_delay_in_seconds = 30
        total_cost_in_psl = round(total_cost_in_psl,5)
        matching_transaction_found, exceeding_transaction_found, transaction_block_height, num_confirmations, amount_received_at_burn_address = await check_burn_address_for_tracking_transaction(
            credit_usage_tracking_psl_address, 
            total_cost_in_psl,
            txid,
            max_block_height,
            max_retries,
            initial_retry_delay_in_seconds
        )
        return matching_transaction_found, exceeding_transaction_found, transaction_block_height, num_confirmations, amount_received_at_burn_address
    except Exception as e:
        logger.error(f"Error checking burn transaction: {str(e)}")
        traceback.print_exc()
        raise

def turn_lists_and_dicts_into_strings_func(data: bytes) -> str:
    # Decode the bytes to a string
    data_str = data.decode('utf-8')
    # Parse the string as JSON
    data_dict = json.loads(data_str)
    def replace_lists_and_dicts_with_strings(obj):
        if isinstance(obj, dict):
            # Convert the dictionary to a JSON string if it's not the top-level object
            return json.dumps({k: replace_lists_and_dicts_with_strings(v) for k, v in obj.items()})
        elif isinstance(obj, list):
            # Convert the list to a JSON string
            return json.dumps(obj)
        else:
            return obj
    # Process the input data
    processed_data = replace_lists_and_dicts_with_strings(data_dict)
    # Return the processed data
    return processed_data

def transform_sqlmodel_list_and_dict_fields_into_strings_func(model_instance: SQLModel):
    for field_name, value in model_instance.__dict__.items():
        if isinstance(value, (list, dict)):
            # Convert the list or dictionary to a JSON string
            setattr(model_instance, field_name, json.dumps(value))
    return model_instance    
        
def parse_sqlmodel_strings_into_lists_and_dicts_func(model_instance: SQLModel) -> SQLModel:
    for field_name, value in model_instance.__dict__.items():
        if isinstance(value, str):  # Check if the field value is a string
            try:
                parsed_value = json.loads(value)  # Try to parse the string as JSON
                # Ensure the parsed value is a list or dictionary before setting it
                if isinstance(parsed_value, (list, dict)):
                    setattr(model_instance, field_name, parsed_value)  # Replace the string with a list or dictionary
            except json.JSONDecodeError:
                # If parsing fails, skip this field and continue
                continue
    return model_instance
        
async def retrieve_credit_pack_ticket_from_blockchain_using_txid(txid: str) -> db_code.CreditPackPurchaseRequestResponse:
    try:
        if not txid:
            logger.error("Error retrieving credit pack ticket from blockchain: No TXID provided")
            return None, None, None
        credit_pack_combined_blockchain_ticket_data_json = await retrieve_generic_ticket_data_from_blockchain(txid)
        if credit_pack_combined_blockchain_ticket_data_json:
            credit_pack_combined_blockchain_ticket_data_dict = json.loads(credit_pack_combined_blockchain_ticket_data_json)
        else:
            logger.error(f"Error retrieving credit pack ticket from blockchain for txid {txid}: No data found")
            return None, None, None
        if credit_pack_combined_blockchain_ticket_data_dict is None:
            return None, None, None
        credit_pack_purchase_request_dict = credit_pack_combined_blockchain_ticket_data_dict['credit_pack_purchase_request_dict']
        credit_pack_purchase_request_response_dict = credit_pack_combined_blockchain_ticket_data_dict['credit_pack_purchase_request_response_dict']
        credit_pack_purchase_request_confirmation_dict = credit_pack_combined_blockchain_ticket_data_dict['credit_pack_purchase_request_confirmation_dict']
        credit_pack_purchase_request = db_code.CreditPackPurchaseRequest(**credit_pack_purchase_request_dict)
        credit_pack_purchase_request_response = db_code.CreditPackPurchaseRequestResponse(**credit_pack_purchase_request_response_dict)
        credit_pack_purchase_request_confirmation = db_code.CreditPackPurchaseRequestConfirmation(**credit_pack_purchase_request_confirmation_dict)
        return credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation
    except Exception as e:
        logger.error(f"Error retrieving credit pack ticket from blockchain: {str(e)}")
        raise

async def retrieve_credit_pack_ticket_from_purchase_burn_txid(purchase_burn_txid: str):
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestConfirmation).where(db_code.CreditPackPurchaseRequestConfirmation.txid_of_credit_purchase_burn_transaction == purchase_burn_txid)
            )
            credit_pack_request_confirmation = result.one_or_none()
            if credit_pack_request_confirmation is None:
                return None, None, None
            sha3_256_hash_of_credit_pack_purchase_request_fields = credit_pack_request_confirmation.sha3_256_hash_of_credit_pack_purchase_request_fields
        # Try to retrieve the credit pack ticket from the local database using the txid mapping
        async with db_code.Session() as db_session:
            mapping = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestResponseTxidMapping).where(db_code.CreditPackPurchaseRequestResponseTxidMapping.sha3_256_hash_of_credit_pack_purchase_request_fields == sha3_256_hash_of_credit_pack_purchase_request_fields)
            )
            mapping_result = mapping.one_or_none()
            if mapping_result is not None:
                logger.info(f"Found credit pack data for purchase burn TXID {purchase_burn_txid} in the local database, so returning it immediately!")
                credit_pack_purchase_request = await get_credit_pack_purchase_request(mapping_result.sha3_256_hash_of_credit_pack_purchase_request_fields)
                credit_pack_purchase_request_response = await get_credit_pack_purchase_request_response_from_request_hash(mapping_result.sha3_256_hash_of_credit_pack_purchase_request_fields)
                credit_pack_purchase_request_confirmation = await get_credit_pack_purchase_request_confirmation_from_request_hash(mapping_result.sha3_256_hash_of_credit_pack_purchase_request_fields)
                return credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation            
    except Exception as e:
        logger.error(f"Error occurred while retrieving credit pack ticket data: {e}")
        traceback.print_exc()
        return None, None, None          
    
async def retrieve_credit_pack_ticket_using_txid(txid: str) -> Tuple[Optional[db_code.CreditPackPurchaseRequest], Optional[db_code.CreditPackPurchaseRequestResponse], Optional[db_code.CreditPackPurchaseRequestConfirmation]]:
    logger.info(f"Attempting to retrieve credit pack ticket for TXID: {txid}")
    max_retries = 10  # Maximum retries with backoff
    base_delay = 1  # Starting delay in seconds
    max_delay = 60  # Maximum delay in seconds for backoff
    total_wait_time = 0  # Total wait time across retries
    jitter_max = 1  # Max jitter in seconds
    if txid is None:
        logger.error("Error: TXID is None")
        return None, None, None
    try:
        async with db_code.Session() as db_session:
            mapping = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestResponseTxidMapping).where(db_code.CreditPackPurchaseRequestResponseTxidMapping.pastel_api_credit_pack_ticket_registration_txid == txid)
            )
            mapping_result = mapping.one_or_none()
            if mapping_result is not None:
                logger.info(f"Found mapping in local database for ticket registration TXID: {txid}")
                credit_pack_purchase_request = await get_credit_pack_purchase_request(mapping_result.sha3_256_hash_of_credit_pack_purchase_request_fields)
                credit_pack_purchase_request_response = await get_credit_pack_purchase_request_response_from_request_hash(mapping_result.sha3_256_hash_of_credit_pack_purchase_request_fields)
                credit_pack_purchase_request_confirmation = await get_credit_pack_purchase_request_confirmation_from_request_hash(mapping_result.sha3_256_hash_of_credit_pack_purchase_request_fields)
                logger.info(f"Successfully retrieved credit pack ticket data from local database for TXID: {txid}")
                return credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation
        logger.info(f"Ticket not found in local database for TXID {txid}, attempting retrieval from blockchain")
        credit_pack_purchase_request = credit_pack_purchase_request_response = credit_pack_purchase_request_confirmation = None
        for attempt in range(max_retries):
            try:
                logger.info(f"Attempting to retrieve ticket from blockchain using registration TXID: {txid}")
                credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation = await retrieve_credit_pack_ticket_from_blockchain_using_txid(txid)
                if all((credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation)):
                    logger.info(f"Successfully retrieved credit pack ticket from blockchain using registration TXID: {txid}")
                    break
            except Exception as e:
                logger.error(f"Error retrieving ticket using registration TXID {txid} on attempt {attempt + 1}: {str(e)}")
                delay = min(base_delay * 2 ** attempt + random.uniform(0, jitter_max), max_delay)
                total_wait_time += delay
                if total_wait_time >= 300:  # 5 minutes
                    logger.error(f"Failed to retrieve credit pack ticket from blockchain after {attempt + 1} attempts in 5 minutes")
                    break
                logger.info(f"Waiting for {delay:.2f} seconds before retrying...")
                await asyncio.sleep(delay)
        # If not found, try to retrieve the ticket assuming txid is a purchase burn transaction TXID
        if not all((credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation)):
            try:
                logger.info(f"Attempting to retrieve ticket from blockchain using burn transaction TXID: {txid}")
                credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation = await retrieve_credit_pack_ticket_from_purchase_burn_txid(txid)
                if all((credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation)):
                    logger.info(f"Successfully retrieved credit pack ticket from blockchain using burn transaction TXID: {txid}")
                else:
                    logger.error(f"Failed to retrieve credit pack ticket from blockchain for burn transaction TXID: {txid}")
            except Exception as e:
                logger.error(f"Error retrieving ticket using burn transaction TXID {txid}: {str(e)}")
        if all((credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation)):
            logger.info(f"Saving retrieved credit pack ticket to local database for TXID: {txid}")
            await save_or_update_credit_pack_ticket(credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation, txid)
        else:
            logger.error(f"Failed to retrieve credit pack ticket from blockchain for TXID: {txid}")
            return None, None, None
        return credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation
    except Exception as e:
        logger.error(f"Error retrieving credit pack ticket for TXID {txid}: {str(e)}")
        traceback.print_exc()
        raise

async def save_or_update_credit_pack_ticket(credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation, txid):
    logger.info(f"Attempting to save or update credit pack ticket for TXID: {txid}")
    async with db_code.Session() as db_session:
        try:
            async with db_session.begin():
                logger.info(f"Checking for existing credit pack purchase request response for TXID: {txid}")
                existing_response = await db_session.exec(
                    select(db_code.CreditPackPurchaseRequestResponse).where(
                        db_code.CreditPackPurchaseRequestResponse.sha3_256_hash_of_credit_pack_purchase_request_response_fields == credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_response_fields
                    )
                )
                existing_response = existing_response.one_or_none()
                if existing_response:
                    logger.info(f"Updating existing credit pack purchase request response for TXID: {txid}")
                    for key, value in credit_pack_purchase_request_response.dict().items():
                        setattr(existing_response, key, value)
                else:
                    logger.info(f"Adding new credit pack purchase request response for TXID: {txid}")
                    db_session.add(credit_pack_purchase_request_response)
                logger.info(f"Checking for existing credit pack purchase request confirmation for TXID: {txid}")
                existing_confirmation = await db_session.exec(
                    select(db_code.CreditPackPurchaseRequestConfirmation).where(
                        db_code.CreditPackPurchaseRequestConfirmation.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields == credit_pack_purchase_request_confirmation.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields
                    )
                )
                existing_confirmation = existing_confirmation.one_or_none()
                if existing_confirmation:
                    logger.info(f"Updating existing credit pack purchase request confirmation for TXID: {txid}")
                    for key, value in credit_pack_purchase_request_confirmation.dict().items():
                        setattr(existing_confirmation, key, value)
                else:
                    logger.info(f"Adding new credit pack purchase request confirmation for TXID: {txid}")
                    db_session.add(credit_pack_purchase_request_confirmation)
                logger.info(f"Updating TXID mapping for credit pack ticket with TXID: {txid}")
                await save_credit_pack_purchase_request_response_txid_mapping(credit_pack_purchase_request_response, txid)
            logger.info(f"Successfully saved or updated credit pack ticket data for TXID: {txid}")
        except Exception as e:
            logger.error(f"Error saving or updating credit pack ticket data for TXID {txid}: {str(e)}")
            raise
    logger.info(f"Consolidating WAL data for credit pack ticket with TXID: {txid}")
    await db_code.consolidate_wal_data()

async def save_credit_pack_purchase_request_response_txid_mapping(credit_pack_purchase_request_response: db_code.CreditPackPurchaseRequestResponse, txid: str) -> None:
    logger.info(f"Attempting to save credit pack purchase request response TXID mapping for TXID: {txid}")
    try:
        async with db_code.Session() as db_session:        
            existing_mapping = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestResponseTxidMapping).where(
                    db_code.CreditPackPurchaseRequestResponseTxidMapping.sha3_256_hash_of_credit_pack_purchase_request_fields == credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_fields
                )
            )
            existing_mapping = existing_mapping.one_or_none()
            if existing_mapping is None:
                logger.info(f"Creating new TXID mapping for credit pack ticket with TXID: {txid}")
                mapping = db_code.CreditPackPurchaseRequestResponseTxidMapping(
                    sha3_256_hash_of_credit_pack_purchase_request_fields=credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_fields,
                    pastel_api_credit_pack_ticket_registration_txid=txid
                )
                db_session.add(mapping)
            else:
                logger.info(f"Updating existing TXID mapping for credit pack ticket with TXID: {txid}")
                existing_mapping.pastel_api_credit_pack_ticket_registration_txid = txid
            logger.info(f"Successfully saved credit pack purchase request response TXID mapping for TXID: {txid}")
    except Exception as e:
        logger.error(f"Error saving credit pack purchase request response TXID mapping for TXID {txid}: {str(e)}")
        raise
    
async def get_final_credit_pack_registration_txid_from_credit_purchase_burn_txid(purchase_burn_txid: str):
    try:
        async with db_code.Session() as db_session:
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestConfirmation).where(db_code.CreditPackPurchaseRequestConfirmation.txid_of_credit_purchase_burn_transaction == purchase_burn_txid)
            )
            credit_pack_request_confirmation = result.one_or_none()
            sha3_256_hash_of_credit_pack_purchase_request_fields = credit_pack_request_confirmation.sha3_256_hash_of_credit_pack_purchase_request_fields    
            return sha3_256_hash_of_credit_pack_purchase_request_fields
    except Exception as e:
        logger.error(f"Error occurred while retrieving credit pack ticket data: {e}")
        traceback.print_exc()
        return None      
    
async def get_credit_pack_ticket_registration_txid_from_corresponding_burn_transaction_txid(purchase_burn_txid: str) -> str:
    try:
        async with db_code.Session() as db_session:
            # Retrieve the corresponding confirmation from the purchase burn txid
            result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestConfirmation).where(
                    db_code.CreditPackPurchaseRequestConfirmation.txid_of_credit_purchase_burn_transaction == purchase_burn_txid
                )
            )
            credit_pack_request_confirmation = result.one_or_none()
            if credit_pack_request_confirmation is None:
                raise ValueError(f"No confirmation found for purchase burn txid: {purchase_burn_txid}")
            # Retrieve the mapping for the sha3_256 hash from the confirmation
            sha3_256_hash_of_credit_pack_purchase_request_fields = credit_pack_request_confirmation.sha3_256_hash_of_credit_pack_purchase_request_fields
            mapping_result = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestResponseTxidMapping).where(
                    db_code.CreditPackPurchaseRequestResponseTxidMapping.sha3_256_hash_of_credit_pack_purchase_request_fields == sha3_256_hash_of_credit_pack_purchase_request_fields
                )
            )
            mapping = mapping_result.one_or_none()
            if mapping is None:
                raise ValueError(f"No TXID mapping found for purchase burn txid: {purchase_burn_txid}")
            # Return the TXID of the credit pack ticket registration
            return mapping.pastel_api_credit_pack_ticket_registration_txid
    except Exception as e:
        logger.error(f"Error occurred while retrieving credit pack ticket registration TXID: {e}")
        traceback.print_exc()
        return None

def recursively_parse_json(data):
    # Helper function to handle recursive parsing
    if isinstance(data, str):
        try:
            parsed_data = json.loads(data)
            # After parsing, check if parsed_data is a dict or list, otherwise return the original string
            if isinstance(parsed_data, (dict, list)):
                return recursively_parse_json(parsed_data)
            else:
                return data  # Do not convert numbers, booleans, or nulls
        except json.JSONDecodeError:
            return data  # Return the original string if it's not JSON
    elif isinstance(data, dict):
        # Recursively process each key-value pair in the dictionary with sorted keys
        return {k: recursively_parse_json(data[k]) for k in sorted(data.keys())}
    elif isinstance(data, list):
        # Recursively process each element in the list
        return [recursively_parse_json(item) for item in data]
    else:
        # Return the data as is if it's neither a string, dict, nor list
        return data

def collect_leaf_nodes(data, parent_key=''):
    # Helper function to collect leaf nodes, concatenating key-value pairs
    if isinstance(data, dict):
        return '|'.join(f"{k} : {collect_leaf_nodes(v, k)}" for k, v in sorted(data.items()))
    elif isinstance(data, list):
        # Use parent key for elements of a list to keep the context
        return '|'.join(f"{parent_key} : {collect_leaf_nodes(item, parent_key)}" for item in data)
    elif isinstance(data, str) or not isinstance(data, Iterable):
        return str(data)
    return ''

def compute_fully_parsed_json_sha3_256_hash(input_json):
    # Parse the input JSON string
    parsed_data = json.loads(input_json)
    # Recursively parse nested JSON and other data structures
    fully_parsed_data = recursively_parse_json(parsed_data)
    # Collect all leaf nodes and concatenate them with key names
    concatenated_data = collect_leaf_nodes(fully_parsed_data)
    # Compute SHA3-256 hash
    sha3_hash = hashlib.sha3_256(concatenated_data.encode('utf-8')).hexdigest()
    return sha3_hash, concatenated_data

async def store_generic_ticket_data_in_blockchain(ticket_input_data_json: str, ticket_type_identifier: str):
    try:    
        if not isinstance(ticket_input_data_json, str):
            error_message = "Ticket data must be a valid string!"
            logger.error(error_message)
            raise ValueError(error_message)
        if not isinstance(ticket_type_identifier, str):
            error_message = "Ticket type identifier data must be a valid string!"
            logger.error(error_message)
            raise ValueError(error_message)
        ticket_type_identifier = ticket_type_identifier.upper()
        ticket_input_data_fully_parsed_sha3_256_hash, concatenated_data = compute_fully_parsed_json_sha3_256_hash(ticket_input_data_json)
        ticket_input_data_dict = recursively_parse_json(ticket_input_data_json)
        ticket_uncompressed_size_in_bytes = len(ticket_input_data_json.encode('utf-8'))
        ticket_dict = {"ticket_identifier_string": ticket_type_identifier,
                        "ticket_input_data_fully_parsed_sha3_256_hash": ticket_input_data_fully_parsed_sha3_256_hash,
                        "ticket_uncompressed_size_in_bytes": ticket_uncompressed_size_in_bytes,
                        "ticket_input_data_dict": ticket_input_data_dict}
        ticket_json = json.dumps(ticket_dict, ensure_ascii=False)
        ticket_json_b64 = base64.b64encode(ticket_json.encode('utf-8')).decode('utf-8')
        ticket_txid = ""
        logger.info("Now attempting to write data to blockchain using 'tickets register contract' command...")
        ticket_register_command_response = await tickets_register_contract(
            rpc_connection, ticket_json_b64, ticket_type_identifier, ticket_input_data_fully_parsed_sha3_256_hash)
        logger.info("Done with 'tickets register contract' command!")
        asyncio.sleep(2)
        if len(ticket_register_command_response) > 0:
            if 'txid' in ticket_register_command_response.keys():
                ticket_txid = ticket_register_command_response['txid']
                ticket_get_command_response = await tickets_get(rpc_connection, ticket_txid , 1)
                retrieved_ticket_data = ticket_get_command_response['ticket']['contract_ticket']
                ticket_tx_info = ticket_get_command_response['tx_info']
                if ticket_tx_info is None:
                    logger.error(f"Ticket was not processed correctly for registration txid {ticket_txid}")
                    return None
                uncompressed_ticket_size_in_bytes = ticket_tx_info['uncompressed_size']
                compressed_ticket_size_in_bytes = ticket_tx_info['compressed_size']
                retrieved_ticket_input_data_fully_parsed_sha3_256_hash = retrieved_ticket_data['ticket_input_data_fully_parsed_sha3_256_hash']
                retrieved_ticket_input_data_dict = retrieved_ticket_data['ticket_input_data_dict']
                retrieved_ticket_input_data_dict_json = json.dumps(retrieved_ticket_input_data_dict)
                computed_fully_parsed_json_sha3_256_hash, concatenated_data = compute_fully_parsed_json_sha3_256_hash(retrieved_ticket_input_data_dict_json)
                assert(computed_fully_parsed_json_sha3_256_hash==retrieved_ticket_input_data_fully_parsed_sha3_256_hash)
                assert(computed_fully_parsed_json_sha3_256_hash==ticket_input_data_fully_parsed_sha3_256_hash)
                logger.info(f"Generic blockchain ticket of sub-type {ticket_type_identifier} was successfully stored in the blockchain with TXID {ticket_txid}")
                logger.info(f"Original Data length: {uncompressed_ticket_size_in_bytes:,} bytes; Compressed data length: {compressed_ticket_size_in_bytes:,} bytes;") #Number of multisig outputs: {len(txouts):,}; Total size of multisig outputs in bytes: {sum(len(txout[1]) for txout in txouts):,}") 
        else:
            logger.error("Error storing ticket data in the blockchain! Could not retrieve and perfectly reconstruct original ticket data.")
        return ticket_txid, ticket_dict, ticket_json_b64
    except Exception as e:
        logger.error(f"Error occurred while storing ticket data in the blockchain: {e}")
        traceback.print_exc()
        return None

async def retrieve_generic_ticket_data_from_blockchain(ticket_txid: str):
    try:
        if ticket_txid is None:
            logger.error("No ticket TXID provided!")
            return None
        ticket_get_command_response = await tickets_get(rpc_connection, ticket_txid, 1)
        retrieved_ticket_data = ticket_get_command_response['ticket']['contract_ticket']
        if retrieved_ticket_data is None:
            logger.error(f"Error: no ticket data returned for TXID {ticket_txid}")
            return None
        retrieved_ticket_input_data_fully_parsed_sha3_256_hash = retrieved_ticket_data['ticket_input_data_fully_parsed_sha3_256_hash']
        retrieved_ticket_input_data_dict = retrieved_ticket_data['ticket_input_data_dict']
        retrieved_ticket_input_data_dict_json = json.dumps(retrieved_ticket_input_data_dict)
        computed_fully_parsed_json_sha3_256_hash, concatenated_data = compute_fully_parsed_json_sha3_256_hash(retrieved_ticket_input_data_dict_json)
        assert(computed_fully_parsed_json_sha3_256_hash == retrieved_ticket_input_data_fully_parsed_sha3_256_hash)
        credit_pack_combined_blockchain_ticket_data_json = json.dumps(retrieved_ticket_input_data_dict)
        return credit_pack_combined_blockchain_ticket_data_json
    except Exception as e:
        logger.error(f"Error occurred while retrieving ticket data in the blockchain: {e}")
        traceback.print_exc()
        return None
    
async def get_list_of_credit_pack_ticket_txids_already_in_db():
    async with db_code.Session() as db_session:
        mappings = await db_session.exec(select(db_code.CreditPackPurchaseRequestResponseTxidMapping))
        mapping_results = mappings.all()
        if mapping_results is not None:
            logger.info(f"Found {len(mapping_results):,} credit pack TXIDs already stored in database...")
            list_of_already_stored_credit_pack_txids = list(set([x.pastel_api_credit_pack_ticket_registration_txid for x in mapping_results]))          
            return list_of_already_stored_credit_pack_txids
        else:
            return []

async def list_generic_tickets_in_blockchain_and_parse_and_validate_and_store_them(ticket_type_identifier: str = "INFERENCE_API_CREDIT_PACK_TICKET", starting_block_height: int = 700000, force_revalidate_all_tickets: int = 0):
    try:
        if not isinstance(ticket_type_identifier, str):
            error_message = "Ticket type identifier data must be a valid string!"
            logger.error(error_message)
            raise ValueError(error_message)
        ticket_type_identifier = ticket_type_identifier.upper()
        logger.info(f"Getting all blockchain tickets of type {ticket_type_identifier} starting from block height {starting_block_height:,}...")
        list_of_ticket_data_dicts = await tickets_list_contract(rpc_connection, ticket_type_identifier, starting_block_height)
        list_of_ticket_internal_data_dicts = [x['ticket']['contract_ticket'] for x in list_of_ticket_data_dicts]
        logger.info(f"Found {len(list_of_ticket_internal_data_dicts):,} total tickets in the blockchain since block height {starting_block_height:,} of type {ticket_type_identifier}; Now checking if they are internally consistent...")
        list_of_retrieved_ticket_input_data_fully_parsed_sha3_256_hashes = [x['ticket_input_data_fully_parsed_sha3_256_hash'] for x in list_of_ticket_internal_data_dicts]
        list_of_retrieved_ticket_input_data_dicts = [x['ticket_input_data_dict'] for x in list_of_ticket_internal_data_dicts]
        list_of_retrieved_ticket_input_data_dicts_json = [json.dumps(x) for x in list_of_retrieved_ticket_input_data_dicts]
        list_of_computed_fully_parsed_json_sha3_256_hashes = [compute_fully_parsed_json_sha3_256_hash(x)[0] for x in list_of_retrieved_ticket_input_data_dicts_json]
        list_of_computed_fully_parsed_json_sha3_256_hashes_sorted = sorted(list_of_computed_fully_parsed_json_sha3_256_hashes)
        list_of_retrieved_ticket_input_data_fully_parsed_sha3_256_hashes_sorted = sorted(list_of_retrieved_ticket_input_data_fully_parsed_sha3_256_hashes)
        mismatches = [(i, x) for i, x in enumerate(list_of_retrieved_ticket_input_data_fully_parsed_sha3_256_hashes_sorted) if list_of_computed_fully_parsed_json_sha3_256_hashes_sorted[i] != x]
        if mismatches:
            logger.error(f"Mismatches found in hashes: {mismatches}")
        else:
            logger.info("All retrieved tickets were internally consistent and match the computed SHA3-256 hash of all fields!")
        list_of_already_stored_credit_pack_txids = await get_list_of_credit_pack_ticket_txids_already_in_db()
        list_of_known_bad_credit_pack_txids = await get_list_of_all_known_bad_credit_pack_ticket_txids_from_db()
        list_of_ticket_txids = [x['txid'] for x in list_of_ticket_data_dicts]
        if force_revalidate_all_tickets:
            list_of_ticket_txids_not_already_stored = list_of_ticket_txids
            if len(list_of_ticket_txids) > 0:
                logger.info(f"Now attempting to perform in-depth validation of all aspects of the {len(list_of_ticket_txids):,} retrieved {ticket_type_identifier} tickets (Forcing full revalidation of ALL tickets, even those already stored in the local database or which are in the known bad list of TXIDs!)...")
        else:
            list_of_ticket_txids_not_already_stored = [x for x in list_of_ticket_txids if x not in list_of_already_stored_credit_pack_txids and x not in list_of_known_bad_credit_pack_txids]
            if len(list_of_ticket_txids_not_already_stored) > 0:
                logger.info(f"Now attempting to perform in-depth validation of all aspects of the {len(list_of_ticket_txids_not_already_stored):,} retrieved {ticket_type_identifier} tickets that are not already in the database or in the known bad list of TXIDs...")
        list_of_fully_validated_ticket_txids = []
        for idx, current_txid in enumerate(list_of_ticket_txids_not_already_stored):
            logger.info(f"Validating ticket {idx+1}/{len(list_of_ticket_txids_not_already_stored)}: {current_txid}")
            try:
                validation_results = await validate_existing_credit_pack_ticket(current_txid)
                if validation_results is None:
                    logger.error(f"[Ticket {idx+1}/{len(list_of_ticket_txids_not_already_stored)}] Validation for TXID {current_txid} returned None. Skipping this ticket.")
                    continue
                asyncio.sleep(0.5)
                if not validation_results["credit_pack_ticket_is_valid"]:
                    logger.warning(f"[Ticket {idx+1}/{len(list_of_ticket_txids_not_already_stored)}] Blockchain ticket of type {ticket_type_identifier} and TXID {current_txid} was unable to be fully validated, so skipping it! Reasons for validation failure:\n{validation_results['validation_failure_reasons_list']}")
                else:
                    logger.info(f"[Ticket {idx+1}/{len(list_of_ticket_txids_not_already_stored)}] Blockchain ticket of type {ticket_type_identifier} and TXID {current_txid} was fully validated! Now saving to database...")
                    list_of_fully_validated_ticket_txids.append(current_txid)
                    credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation = await retrieve_credit_pack_ticket_from_blockchain_using_txid(current_txid)
                    if all((credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation)):
                        await save_credit_pack_ticket_to_database(credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation, current_txid)
                        logger.info(f"Successfully saved credit pack ticket with TXID {current_txid} to database.")
                    else:
                        logger.error(f"Failed to retrieve complete credit pack ticket data for TXID {current_txid} from blockchain.")
            except Exception as e:
                logger.error(f"Exception occurred while validating TXID {current_txid}: {str(e)}")
                logger.error(f"Exception type: {type(e).__name__}")
                logger.error(f"Exception details: {traceback.format_exc()}")
                continue                    
        if len(list_of_ticket_txids_not_already_stored) > 0:
            logger.info(f"We were able to fully validate {len(list_of_fully_validated_ticket_txids):,} of the {len(list_of_ticket_txids_not_already_stored):,} retrieved {ticket_type_identifier} tickets!")
        return list_of_retrieved_ticket_input_data_dicts_json, list_of_fully_validated_ticket_txids
    except Exception as e:
        logger.error(f"Error occurred while listing generic blockchain tickets of type {ticket_type_identifier}: {e}")
        traceback.print_exc()
        return None
    
async def periodic_ticket_listing_and_validation():
    while True:
        try:
            starting_block_height = 700000
            await list_generic_tickets_in_blockchain_and_parse_and_validate_and_store_them(starting_block_height=starting_block_height)
            await asyncio.sleep(5*60)  # Sleep for 5 minutes
        except Exception as e:
            logger.error(f"Error in periodic ticket listing and validation: {str(e)}")
            traceback.print_exc()

async def store_credit_pack_ticket_in_blockchain(credit_pack_combined_blockchain_ticket_data_json: str) -> str:
    try:
        logger.info("Now attempting to write the ticket data to the blockchain...")
        ticket_type_identifier = "INFERENCE_API_CREDIT_PACK_TICKET"
        credit_pack_ticket_txid, ticket_dict, ticket_json_b64 = await store_generic_ticket_data_in_blockchain(credit_pack_combined_blockchain_ticket_data_json, ticket_type_identifier)
        total_bytes_used = 0
        logger.info(f"Received back pastel txid of {credit_pack_ticket_txid} for the stored blockchain ticket data; total bytes used to store the data in the blockchain was {total_bytes_used:,}; now waiting for the transaction to be confirmed...")
        max_retries = 20
        retry_delay = 20
        try_count = 0
        num_confirmations = 0
        storage_validation_error_string = ""
        while try_count < max_retries and num_confirmations == 0:
            # Retrieve the transaction details using the wrapped gettransaction function
            tx_info = await gettransaction(rpc_connection, credit_pack_ticket_txid)
            if tx_info:
                num_confirmations = tx_info.get("confirmations", 0)
                if (num_confirmations > 0) or SKIP_BURN_TRANSACTION_BLOCK_CONFIRMATION_CHECK:
                    if num_confirmations > 0:
                        logger.info(f"Transaction {credit_pack_ticket_txid} has been confirmed with {num_confirmations} confirmations.")
                    else:
                        logger.info(f"Transaction {credit_pack_ticket_txid} has not yet been confirmed, but we are skipping confirmation check to speed things up.")
                    break
                else:
                    logger.info(f"Transaction {credit_pack_ticket_txid} is not yet confirmed. Waiting for {retry_delay:.2f} seconds before checking again.")
                    await asyncio.sleep(retry_delay)
                    try_count += 1
                    retry_delay *= 1.15  # Optional: increase delay between retries
            else:
                logger.warning(f"Transaction {credit_pack_ticket_txid} not found. Waiting for {retry_delay} seconds before checking again.")
                await asyncio.sleep(retry_delay)
                try_count += 1
                retry_delay *= 1.15  # Optional: increase delay between retries
        if (num_confirmations > 0) or SKIP_BURN_TRANSACTION_BLOCK_CONFIRMATION_CHECK:
            logger.info("Now verifying that we can reconstruct the original file written exactly...")
            reconstructed_file_data = await retrieve_generic_ticket_data_from_blockchain(credit_pack_ticket_txid)
            retrieved_data_fully_parsed_sha3_256_hash, _ = compute_fully_parsed_json_sha3_256_hash(reconstructed_file_data)
            original_data_fully_parsed_sha3_256_hash, _ = compute_fully_parsed_json_sha3_256_hash(credit_pack_combined_blockchain_ticket_data_json)
            if retrieved_data_fully_parsed_sha3_256_hash == original_data_fully_parsed_sha3_256_hash:
                logger.info("Successfully verified that the stored blockchain ticket data can be reconstructed exactly!")
                use_test_reconstruction_of_object_from_json = 1
                if use_test_reconstruction_of_object_from_json:
                    credit_pack_combined_blockchain_ticket_data_dict = json.loads(reconstructed_file_data)
                    credit_pack_purchase_request_dict = credit_pack_combined_blockchain_ticket_data_dict['credit_pack_purchase_request_dict']
                    credit_pack_purchase_request_response_dict = credit_pack_combined_blockchain_ticket_data_dict['credit_pack_purchase_request_response_dict']
                    credit_pack_purchase_request_confirmation_dict = credit_pack_combined_blockchain_ticket_data_dict['credit_pack_purchase_request_confirmation_dict']
                    credit_pack_purchase_request = db_code.CreditPackPurchaseRequest(**credit_pack_purchase_request_dict)  # noqa: F841
                    credit_pack_purchase_request_response = db_code.CreditPackPurchaseRequestResponse(**credit_pack_purchase_request_response_dict)  # noqa: F841
                    credit_pack_purchase_request_confirmation = db_code.CreditPackPurchaseRequestConfirmation(**credit_pack_purchase_request_confirmation_dict)  # noqa: F841
                    logger.info(f"Reconstructed credit pack ticket data:\n Purchase Request: {abbreviated_pretty_json_func(credit_pack_purchase_request_dict)}\nPurchase Request Response: {abbreviated_pretty_json_func(credit_pack_purchase_request_response_dict)}\nPurchase Request Confirmation: {abbreviated_pretty_json_func(credit_pack_purchase_request_confirmation_dict)}")
            else:
                logger.error("Failed to verify that the stored blockchain ticket data can be reconstructed exactly!")
                storage_validation_error_string = "Failed to verify that the stored blockchain ticket data can be reconstructed exactly! Difference: " + str(set(reconstructed_file_data).symmetric_difference(set(credit_pack_combined_blockchain_ticket_data_json)))
                logger.error(storage_validation_error_string)
                return credit_pack_ticket_txid, storage_validation_error_string
        else:
            storage_validation_error_string = f"Transaction {credit_pack_ticket_txid} was not confirmed after {max_retries} attempts."
            logger.error(storage_validation_error_string)
            raise TimeoutError(f"Transaction {credit_pack_ticket_txid} was not confirmed after {max_retries} attempts.")
        return credit_pack_ticket_txid, storage_validation_error_string
    except Exception as e:
        storage_validation_error_string = f"Error storing credit pack ticket: {str(e)}"
        logger.error(storage_validation_error_string)
        traceback.print_exc()
        return credit_pack_ticket_txid, storage_validation_error_string
    
async def check_original_supernode_storage_confirmation(sha3_256_hash_of_credit_pack_purchase_request_response_fields: str) -> bool:
    async with db_code.Session() as db:
        result = await db.exec(
            select(db_code.CreditPackPurchaseRequestConfirmationResponse).where(db_code.CreditPackPurchaseRequestConfirmationResponse.sha3_256_hash_of_credit_pack_purchase_request_response_fields == sha3_256_hash_of_credit_pack_purchase_request_response_fields)
        )
        return result.one_or_none() is not None
    
async def check_if_credit_usage_tracking_psl_address_has_already_been_used_for_a_credit_pack(credit_usage_tracking_psl_address: str):
    async with db_code.Session() as db:
        result = await db.exec(
            select(db_code.CreditPackPurchaseRequestResponse).where(db_code.CreditPackPurchaseRequestResponse.credit_usage_tracking_psl_address == credit_usage_tracking_psl_address)
        )
        credit_pack_purchase_request_response = result.one_or_none()
        if credit_pack_purchase_request_response is not None:
            credit_tracking_address_already_used = True
        else:
            credit_tracking_address_already_used = False
        return credit_tracking_address_already_used, credit_pack_purchase_request_response

async def process_credit_purchase_initial_request(credit_pack_purchase_request: db_code.CreditPackPurchaseRequest) -> db_code.CreditPackPurchaseRequestPreliminaryPriceQuote:
    try:
        # Validate the request fields
        request_validation_errors = await validate_credit_pack_ticket_message_data_func(credit_pack_purchase_request)
        if request_validation_errors:
            rejection_message = await generate_credit_pack_request_rejection_message(credit_pack_purchase_request, request_validation_errors)
            logger.error(f"Invalid credit purchase request: {', '.join(request_validation_errors)}")
            return rejection_message
        if credit_pack_purchase_request.requested_initial_credits_in_credit_pack > MAXIMUM_CREDITS_PER_CREDIT_PACK:
            rejection_message = f"Requested initial credits in credit pack exceeds the maximum of {MAXIMUM_CREDITS_PER_CREDIT_PACK} credits allowed in a single credit pack!"
            logger.error(rejection_message)
            return rejection_message
        if credit_pack_purchase_request.requested_initial_credits_in_credit_pack < MINIMUM_CREDITS_PER_CREDIT_PACK:
            rejection_message = f"Requested initial credits in credit pack must be greater than or equal to {MINIMUM_CREDITS_PER_CREDIT_PACK} credits!"
            logger.error(rejection_message)
            return rejection_message
        # Check if credit_usage_tracking_psl_address has already been used for an existing credit pack at any time:
        credit_tracking_address_already_used, credit_pack_purchase_request_response = await check_if_credit_usage_tracking_psl_address_has_already_been_used_for_a_credit_pack(credit_pack_purchase_request.credit_usage_tracking_psl_address)
        if credit_tracking_address_already_used:
            rejection_message = f"The specified credit tracking address of {credit_pack_purchase_request.credit_usage_tracking_psl_address} has already been used for a credit pack purchase (with sha3-256 hash of credit request fields of {credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_fields})"
            logger.error(rejection_message)
            return rejection_message
        # Determine the preliminary price quote
        preliminary_quoted_price_per_credit_in_psl = await calculate_preliminary_psl_price_per_credit()
        preliminary_total_cost_of_credit_pack_in_psl = round(preliminary_quoted_price_per_credit_in_psl * credit_pack_purchase_request.requested_initial_credits_in_credit_pack, 5)
        # Create the response without the hash and signature fields
        credit_pack_purchase_request_fields_json = await extract_response_fields_from_credit_pack_ticket_message_data_as_json_func(credit_pack_purchase_request)
        credit_pack_purchase_request_response = db_code.CreditPackPurchaseRequestPreliminaryPriceQuote(
            sha3_256_hash_of_credit_pack_purchase_request_fields=credit_pack_purchase_request.sha3_256_hash_of_credit_pack_purchase_request_fields,
            credit_usage_tracking_psl_address=credit_pack_purchase_request.credit_usage_tracking_psl_address,
            credit_pack_purchase_request_fields_json_b64=base64.b64encode(credit_pack_purchase_request_fields_json.encode('utf-8')).decode('utf-8'),
            preliminary_quoted_price_per_credit_in_psl=preliminary_quoted_price_per_credit_in_psl,
            preliminary_total_cost_of_credit_pack_in_psl=preliminary_total_cost_of_credit_pack_in_psl,
            preliminary_price_quote_timestamp_utc_iso_string = datetime.now(timezone.utc).isoformat(),
            preliminary_price_quote_pastel_block_height=await get_current_pastel_block_height_func(),
            preliminary_price_quote_message_version_string="1.0",
            responding_supernode_pastelid=MY_PASTELID,
            sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields="",
            responding_supernode_signature_on_credit_pack_purchase_request_preliminary_price_quote_hash=""
        )
        # Generate the hash and signature fields
        credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(credit_pack_purchase_request_response)
        credit_pack_purchase_request_response.responding_supernode_signature_on_credit_pack_purchase_request_preliminary_price_quote_hash = await sign_message_with_pastelid_func(
            MY_PASTELID,
            credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields,
            LOCAL_PASTEL_ID_PASSPHRASE
        )
        # Validate the response
        response_validation_errors = await validate_credit_pack_ticket_message_data_func(credit_pack_purchase_request_response)
        if response_validation_errors:
            logger.error(f"Invalid credit purchase request preliminary price quote: {', '.join(response_validation_errors)}")
            raise ValueError(f"Invalid credit purchase request preliminary price quote: {', '.join(response_validation_errors)}")
        await save_credit_pack_purchase_request(credit_pack_purchase_request)
        await save_credit_pack_purchase_request_response(credit_pack_purchase_request_response)        
        return credit_pack_purchase_request_response
    except Exception as e:
        logger.error(f"Error processing credit purchase initial request: {str(e)}")
        traceback.print_exc()
        raise
    
async def generate_credit_pack_request_rejection_message(credit_pack_request: db_code.CreditPackPurchaseRequest, validation_errors: List[str]) -> db_code.CreditPackPurchaseRequestRejection:
    credit_pack_purchase_request_fields_json = await extract_response_fields_from_credit_pack_ticket_message_data_as_json_func(credit_pack_request)
    rejection_message = db_code.CreditPackPurchaseRequestRejection(
        sha3_256_hash_of_credit_pack_purchase_request_fields=credit_pack_request.sha3_256_hash_of_credit_pack_purchase_request_fields,
        credit_pack_purchase_request_fields_json_b64=base64.b64encode(credit_pack_purchase_request_fields_json.encode('utf-8')).decode('utf-8'),
        rejection_reason_string=", ".join(validation_errors),
        rejection_timestamp_utc_iso_string=datetime.now(timezone.utc).isoformat(),
        rejection_pastel_block_height=await get_current_pastel_block_height_func(),
        credit_purchase_request_rejection_message_version_string="1.0",
        responding_supernode_pastelid=MY_PASTELID,
        sha3_256_hash_of_credit_pack_purchase_request_rejection_fields="",
        responding_supernode_signature_on_credit_pack_purchase_request_rejection_hash=""
    )
    rejection_message.sha3_256_hash_of_credit_pack_purchase_request_rejection_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(rejection_message)
    rejection_message.responding_supernode_signature_on_credit_pack_purchase_request_rejection_hash = await sign_message_with_pastelid_func(
        MY_PASTELID,
        rejection_message.sha3_256_hash_of_credit_pack_purchase_request_rejection_fields,
        LOCAL_PASTEL_ID_PASSPHRASE
    )
    rejection_validation_errors = await validate_credit_pack_ticket_message_data_func(rejection_message)
    if rejection_validation_errors:
        logger.error(f"Invalid credit purchase request rejection message: {', '.join(rejection_validation_errors)}")
        raise ValueError(f"Invalid credit purchase request rejection message: {', '.join(rejection_validation_errors)}")
    await save_credit_pack_purchase_request_rejection(rejection_message)
    return rejection_message

async def select_potentially_agreeing_supernodes() -> List[str]:
    try:
        blacklist_path = Path('supernode_inference_ip_blacklist.txt')
        blacklisted_ips = set()
        if blacklist_path.exists(): # Read the blacklist file if it exists
            with blacklist_path.open('r') as blacklist_file:
                blacklisted_ips = {line.strip() for line in blacklist_file if line.strip()}
        else:
            logger.info("Blacklist file not found. Proceeding without blacklist filtering.")
        total_number_of_blacklisted_sns = len(blacklisted_ips)
        best_block_hash, best_block_merkle_root, _ = await get_best_block_hash_and_merkle_root_func() # Get the best block hash and merkle root
        supernode_list_df, _ = await check_supernode_list_func() # Get the list of all supernodes
        number_of_supernodes_found = len(supernode_list_df) - 1
        if number_of_supernodes_found < MINIMUM_NUMBER_OF_POTENTIALLY_AGREEING_SUPERNODES:
            logger.warning(f"Fewer than {MINIMUM_NUMBER_OF_POTENTIALLY_AGREEING_SUPERNODES} supernodes available. Using all {number_of_supernodes_found} available supernodes.")
            all_other_supernode_pastelids = [x for x in supernode_list_df['extKey'].tolist() if x != MY_PASTELID]
            return all_other_supernode_pastelids
        # Compute the XOR distance between each supernode's hash(pastelid) and the best block's merkle root
        xor_distances = []
        list_of_blacklisted_supernode_pastelids = []
        for _, row in supernode_list_df.iterrows():
            ip_address_port = row['ipaddress:port']
            ip_address = ip_address_port.split(":")[0]
            if ip_address in blacklisted_ips:
                current_supernode_pastelid = row['extKey']
                list_of_blacklisted_supernode_pastelids.append(current_supernode_pastelid)
                continue
            supernode_pastelid = row['extKey']
            supernode_pastelid_hash = get_sha256_hash_of_input_data_func(supernode_pastelid)
            supernode_pastelid_int = int(supernode_pastelid_hash, 16)
            merkle_root_int = int(best_block_merkle_root, 16)
            xor_distance = supernode_pastelid_int ^ merkle_root_int
            xor_distances.append((supernode_pastelid, xor_distance))
        # Sort the supernodes based on their XOR distances in ascending order
        sorted_supernodes = sorted(xor_distances, key=lambda x: x[1])
        # Select the supernodes with the closest XOR distances
        potentially_agreeing_supernodes = [supernode[0] for supernode in sorted_supernodes if supernode[0] != MY_PASTELID]
        return potentially_agreeing_supernodes, total_number_of_blacklisted_sns, list_of_blacklisted_supernode_pastelids
    except Exception as e:
        logger.error(f"Error selecting potentially agreeing supernodes: {str(e)}")
        traceback.print_exc()
        raise    

async def select_top_n_closest_supernodes_to_best_block_merkle_root(supernode_pastelids: List[str], n: int = 10, best_block_merkle_root: Optional[str] = None) -> List[str]:
    try:
        if best_block_merkle_root is None:
            _, best_block_merkle_root, _ = await get_best_block_hash_and_merkle_root_func()  # Get the best block hash and merkle root if not supplied
        best_block_merkle_root_hash = get_sha256_hash_of_input_data_func(best_block_merkle_root)
        merkle_root_hash_int = int(best_block_merkle_root_hash, 16)
        # Compute the XOR distance between each supernode's hash(pastelid) and the best block's merkle root
        xor_distances = []
        for pastelid in supernode_pastelids:
            supernode_pastelid_hash = get_sha256_hash_of_input_data_func(pastelid)
            supernode_pastelid_int = int(supernode_pastelid_hash, 16)
            xor_distance = supernode_pastelid_int ^ merkle_root_hash_int
            xor_distances.append((pastelid, xor_distance))
        # Sort the supernodes based on their XOR distances in ascending order
        sorted_supernodes = sorted(xor_distances, key=lambda x: x[1])
        # Select the top N supernodes with the closest XOR distances
        top_n_supernodes = [supernode[0] for supernode in sorted_supernodes[:n]]
        return top_n_supernodes
    except Exception as e:
        logger.error(f"Error selecting top N closest supernodes to best block merkle root: {str(e)}")
        traceback.print_exc()
        raise

async def check_liveness(supernode_base_url: str) -> bool:
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{supernode_base_url}/liveness_ping", timeout=2)
            return response.status_code == 200
    except httpx.RequestError:
        return False
    
async def get_supernode_url_and_check_liveness(supernode_pastelid, supernode_list_df, client):
    try:
        supernode_base_url = await get_supernode_url_from_pastelid_func(supernode_pastelid, supernode_list_df)
        is_alive = await check_liveness(supernode_base_url)
        return supernode_base_url, is_alive
    except Exception as e:
        logger.warning(f"Error getting supernode URL or checking liveness for supernode {supernode_pastelid}: {str(e)}")
        return None, False
    
async def send_price_agreement_request_to_supernodes(request: db_code.CreditPackPurchasePriceAgreementRequest, supernodes: List[str]) -> List[db_code.CreditPackPurchasePriceAgreementRequestResponse]:
    blacklist_path = Path('supernode_inference_ip_blacklist.txt')
    blacklisted_ips = set()
    if blacklist_path.exists():
        with blacklist_path.open('r') as blacklist_file:
            blacklisted_ips = {line.strip() for line in blacklist_file if line.strip()}
    else:
        logger.info("Blacklist file not found. Proceeding without blacklist filtering.")
    async with httpx.AsyncClient(timeout=Timeout(None)) as client:
        supernode_list_df, _ = await check_supernode_list_func()
        # Prepare tasks for supernode liveness check
        supernode_liveness_tasks = [
            asyncio.create_task(get_supernode_url_and_check_liveness(supernode_pastelid, supernode_list_df, client))
            for supernode_pastelid in supernodes
        ]
        supernode_urls_and_statuses = await asyncio.gather(*supernode_liveness_tasks)
        price_agreement_semaphore = asyncio.Semaphore(MAXIMUM_NUMBER_OF_CONCURRENT_RPC_REQUESTS*5)
        async def send_request(supernode_base_url, payload, timeout):
            async with price_agreement_semaphore:
                try:
                    response = await client.post(supernode_base_url, json=payload, timeout=timeout)
                    return response
                except httpx.RequestError as e:
                    logger.error(f"Error sending request to {supernode_base_url}: {str(e)}")
                    return None
        async def process_supernode(supernode_base_url, is_alive):
            if is_alive:
                async with price_agreement_semaphore:
                    challenge_dict = await request_and_sign_challenge(supernode_base_url)
                challenge = challenge_dict["challenge"]
                challenge_id = challenge_dict["challenge_id"]
                challenge_signature = challenge_dict["challenge_signature"]
                request_dict = request.model_dump()
                request_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in request_dict.items()}
                payload = {
                    "credit_pack_price_agreement_request": request_dict,
                    "challenge": challenge,
                    "challenge_id": challenge_id,
                    "challenge_signature": challenge_signature
                }
                url = f"{supernode_base_url}/credit_pack_price_agreement_request"
                timeout = Timeout(INDIVIDUAL_SUPERNODE_PRICE_AGREEMENT_REQUEST_TIMEOUT_PERIOD_IN_SECONDS)  # Timeout for individual supernode request
                return await send_request(url, payload, timeout)
            else:
                logger.warning(f"Supernode {supernode_base_url} is not responding on port 7123")
                return None
        request_tasks = [
            process_supernode(supernode_base_url, is_alive)
            for supernode_base_url, is_alive in supernode_urls_and_statuses
            if supernode_base_url.split(":")[1].replace("//", "").split("/")[0] not in blacklisted_ips
        ]
        logger.info(f"Now sending out {len(request_tasks):,} price agreement requests to potentially agreeing supernodes...")
        datetime_start = datetime.now(timezone.utc)
        responses = await asyncio.gather(*request_tasks, return_exceptions=True)
        datetime_end = datetime.now(timezone.utc)
        duration = datetime_end - datetime_start
        logger.info(f"Finished sending price agreement requests to supernodes in {duration.total_seconds():.2f} seconds!")
        price_agreement_request_responses = [
            db_code.CreditPackPurchasePriceAgreementRequestResponse(**response.json())
            for response in responses
            if isinstance(response, httpx.Response) and response.status_code == 200
        ]
        logger.info(f"Received a total of {len(price_agreement_request_responses):,} valid price agreement responses from supernodes out of {len(request_tasks):,} total requests sent, a success rate of {len(price_agreement_request_responses)/len(request_tasks):.2%}")
        return price_agreement_request_responses

async def request_and_sign_challenge(supernode_url: str) -> Dict[str, str]:
    async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS/5)) as client:
        response = await client.get(f"{supernode_url}/request_challenge/{MY_PASTELID}")
        response.raise_for_status()
        result = response.json()
        challenge = result["challenge"]
        challenge_id = result["challenge_id"]
        # Sign the challenge string using the local RPC client
        challenge_signature = await sign_message_with_pastelid_func(MY_PASTELID, challenge, LOCAL_PASTEL_ID_PASSPHRASE)
        return {
            "challenge": challenge,
            "challenge_id": challenge_id,
            "challenge_signature": challenge_signature
        }    

async def send_credit_pack_purchase_request_final_response_to_supernodes(response: db_code.CreditPackPurchaseRequestResponse, supernodes: List[str]) -> List[httpx.Response]:
    try:
        # Read the blacklist file if it exists
        blacklist_path = Path('supernode_inference_ip_blacklist.txt')
        blacklisted_ips = set()
        if blacklist_path.exists():
            with blacklist_path.open('r') as blacklist_file:
                blacklisted_ips = {line.strip() for line in blacklist_file if line.strip()}
        else:
            logger.info("Blacklist file not found. Proceeding without blacklist filtering.")
        async with httpx.AsyncClient() as client:
            tasks = []
            supernode_list_df, _ = await check_supernode_list_func()
            for supernode_pastelid in supernodes:
                payload = {}
                try:
                    supernode_base_url = await get_supernode_url_from_pastelid_func(supernode_pastelid, supernode_list_df)
                    ip_address_port = supernode_base_url.split("//")[1].split(":")[0]
                    if ip_address_port in blacklisted_ips:
                        logger.info(f"Skipping blacklisted supernode {supernode_pastelid}")
                        continue
                    url = f"{supernode_base_url}/credit_pack_purchase_request_final_response_announcement"
                    challenge_dict = await request_and_sign_challenge(supernode_base_url)
                    challenge = challenge_dict["challenge"]
                    challenge_id = challenge_dict["challenge_id"]
                    challenge_signature = challenge_dict["challenge_signature"]
                    response_dict = response.model_dump()
                    response_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in response_dict.items()}
                    payload = {
                        "response": response_dict,
                        "challenge": challenge,
                        "challenge_id": challenge_id,
                        "challenge_signature": challenge_signature
                    }
                except Exception as e:
                    logger.warning(f"Error getting challenge from supernode {supernode_pastelid}: {str(e)}")
                if len(payload) > 0:
                    task = asyncio.create_task(client.post(url, json=payload))
                    tasks.append(task)
            responses = await asyncio.gather(*tasks, return_exceptions=True)
            valid_responses = []
            for response in responses:
                if isinstance(response, httpx.Response):
                    if response.status_code == 200:
                        valid_responses.append(response)
                    else:
                        logger.warning(f"Error sending final response announcement to supernode {response.url}: {response.text}")
                else:
                    logger.error(f"Error sending final response announcement to supernode: {str(response)}")
            return valid_responses
    except Exception as e:
        logger.error(f"Error sending final response announcement to supernodes: {str(e)}")
        traceback.print_exc()
        raise    

def transform_json(input_string):
    # Parse the JSON string into a Python dictionary
    data = json.loads(input_string)
    # Serialize the dictionary back to a JSON string
    # Ensure that special characters are escaped correctly
    transformed_string = json.dumps(data, ensure_ascii=False)
    return transformed_string

async def process_credit_pack_price_agreement_request(price_agreement_request: db_code.CreditPackPurchasePriceAgreementRequest) -> Union[db_code.CreditPackPurchasePriceAgreementRequestResponse, str]:
    try:
        # Validate the request fields
        request_validation_errors = await validate_credit_pack_ticket_message_data_func(price_agreement_request)
        if request_validation_errors:
            logger.error(f"Invalid price agreement request: {', '.join(request_validation_errors)}")
            return f"Invalid price agreement request: {', '.join(request_validation_errors)}"
        # Determine if the supernode agrees with the proposed price
        agree_with_proposed_price = await determine_agreement_with_proposed_price(price_agreement_request.proposed_psl_price_per_credit)
        # Create the response without the hash and signature fields
        response = db_code.CreditPackPurchasePriceAgreementRequestResponse(
            sha3_256_hash_of_price_agreement_request_fields=price_agreement_request.sha3_256_hash_of_price_agreement_request_fields,
            credit_pack_purchase_request_fields_json_b64=price_agreement_request.credit_pack_purchase_request_fields_json_b64,
            agree_with_proposed_price=agree_with_proposed_price,
            credit_usage_tracking_psl_address=price_agreement_request.credit_usage_tracking_psl_address,
            proposed_psl_price_per_credit=price_agreement_request.proposed_psl_price_per_credit,
            proposed_price_agreement_response_timestamp_utc_iso_string=datetime.now(timezone.utc).isoformat(),
            proposed_price_agreement_response_pastel_block_height=await get_current_pastel_block_height_func(),
            proposed_price_agreement_response_message_version_string="1.0",
            responding_supernode_signature_on_credit_pack_purchase_request_fields_json_b64=await sign_message_with_pastelid_func(MY_PASTELID, price_agreement_request.credit_pack_purchase_request_fields_json_b64, LOCAL_PASTEL_ID_PASSPHRASE),
            responding_supernode_pastelid=MY_PASTELID,
            sha3_256_hash_of_price_agreement_request_response_fields="",
            responding_supernode_signature_on_price_agreement_request_response_hash=""
        )
        # Generate the hash and signature fields
        response.sha3_256_hash_of_price_agreement_request_response_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(response)
        response.responding_supernode_signature_on_price_agreement_request_response_hash = await sign_message_with_pastelid_func(
            MY_PASTELID,
            response.sha3_256_hash_of_price_agreement_request_response_fields,
            LOCAL_PASTEL_ID_PASSPHRASE
        )
        # Validate the response
        response_validation_errors = await validate_credit_pack_ticket_message_data_func(response)
        if response_validation_errors:
            logger.error(f"Invalid price agreement request response: {', '.join(response_validation_errors)}")
            return f"Invalid price agreement request response: {', '.join(response_validation_errors)}"
        await save_credit_pack_purchase_price_agreement_request_response(response)        
        return response
    except Exception as e:
        logger.error(f"Error processing credit pack price agreement request: {str(e)}")
        traceback.print_exc()
        raise
            
async def process_credit_purchase_preliminary_price_quote_response(preliminary_price_quote_response: db_code.CreditPackPurchaseRequestPreliminaryPriceQuoteResponse) -> Union[db_code.CreditPackPurchaseRequestResponse, db_code.CreditPackPurchaseRequestResponseTermination]:
    try:
        # Validate the response fields
        if not preliminary_price_quote_response.agree_with_preliminary_price_quote:
            logger.warning("End user does not agree with preliminary price quote! Unable to proceed with credit pack purchase.")
            raise ValueError("End user does not agree with preliminary price quote")
        response_validation_errors = await validate_credit_pack_ticket_message_data_func(preliminary_price_quote_response)
        if response_validation_errors:
            logger.error(f"Invalid preliminary price quote response: {', '.join(response_validation_errors)}")
            raise ValueError(f"Invalid preliminary price quote response: {', '.join(response_validation_errors)}")
        # Select the potentially agreeing supernodes
        logger.info("Now selecting potentially agreeing supernodes to sign off on the proposed credit pricing for the credit pack purchase request...")
        potentially_agreeing_supernodes, total_number_of_blacklisted_sns, list_of_blacklisted_supernode_pastelids = await select_potentially_agreeing_supernodes()
        logger.info(f"Selected {len(potentially_agreeing_supernodes)} potentially agreeing supernodes: {potentially_agreeing_supernodes}")
        # Create the price agreement request without the hash and signature fields
        price_agreement_request = db_code.CreditPackPurchasePriceAgreementRequest(
            sha3_256_hash_of_credit_pack_purchase_request_response_fields=preliminary_price_quote_response.sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields,
            supernode_requesting_price_agreement_pastelid=MY_PASTELID,
            credit_pack_purchase_request_fields_json_b64=preliminary_price_quote_response.credit_pack_purchase_request_fields_json_b64,
            credit_usage_tracking_psl_address=preliminary_price_quote_response.credit_usage_tracking_psl_address,
            proposed_psl_price_per_credit=preliminary_price_quote_response.preliminary_quoted_price_per_credit_in_psl,
            price_agreement_request_timestamp_utc_iso_string=datetime.now(timezone.utc).isoformat(),
            price_agreement_request_pastel_block_height=await get_current_pastel_block_height_func(),
            price_agreement_request_message_version_string="1.0",
            sha3_256_hash_of_price_agreement_request_fields="",
            supernode_requesting_price_agreement_pastelid_signature_on_request_hash=""
        )
        # Generate the hash and signature fields
        price_agreement_request.sha3_256_hash_of_price_agreement_request_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(price_agreement_request)
        price_agreement_request.supernode_requesting_price_agreement_pastelid_signature_on_request_hash = await sign_message_with_pastelid_func(
            MY_PASTELID,
            price_agreement_request.sha3_256_hash_of_price_agreement_request_fields,
            LOCAL_PASTEL_ID_PASSPHRASE
        )
        # Validate the price_agreement_request
        request_validation_errors = await validate_credit_pack_ticket_message_data_func(price_agreement_request)
        if request_validation_errors:
            logger.error(f"Invalid price agreement request: {', '.join(request_validation_errors)}")
            raise ValueError(f"Invalid price agreement request: {', '.join(request_validation_errors)}") 
        await save_credit_pack_purchase_price_agreement_request(price_agreement_request)
        # Send the price agreement request to the potentially agreeing supernodes
        logger.info(f"Now sending price agreement request to {len(potentially_agreeing_supernodes)} potentially agreeing supernodes...")
        price_agreement_request_responses = await send_price_agreement_request_to_supernodes(price_agreement_request, potentially_agreeing_supernodes)
        # Process the price agreement request responses
        valid_price_agreement_request_responses = []
        use_manual_signature_validation = 0
        for current_price_agreement_response in price_agreement_request_responses:
            if use_manual_signature_validation:
                agreeing_supernode_pastelid = current_price_agreement_response.responding_supernode_pastelid
                sha3_256_hash_of_price_agreement_request_response_fields = current_price_agreement_response.sha3_256_hash_of_price_agreement_request_response_fields
                responding_supernode_signature_on_price_agreement_request_response_hash = current_price_agreement_response.responding_supernode_signature_on_price_agreement_request_response_hash
                is_signature_valid = await verify_message_with_pastelid_func(agreeing_supernode_pastelid, sha3_256_hash_of_price_agreement_request_response_fields, responding_supernode_signature_on_price_agreement_request_response_hash)
                if not is_signature_valid:
                    logger.warning(f"Error validating price agreement request response signature from supernode {current_price_agreement_response.responding_supernode_pastelid}")
                    continue
            response_validation_errors = await validate_credit_pack_ticket_message_data_func(current_price_agreement_response)
            if not response_validation_errors:
                valid_price_agreement_request_responses.append(current_price_agreement_response)
        supernode_price_agreement_response_percentage_achieved = len(valid_price_agreement_request_responses) / len(potentially_agreeing_supernodes)
        logger.info(f"Received {len(valid_price_agreement_request_responses)} valid price agreement responses from potentially agreeing supernodes out of {len(potentially_agreeing_supernodes)} asked (excluding {total_number_of_blacklisted_sns} blacklisted supernodes), for a quorum percentage of {supernode_price_agreement_response_percentage_achieved:.2%} (Required minimum quorum percentage is {SUPERNODE_CREDIT_PRICE_AGREEMENT_QUORUM_PERCENTAGE:.2%})")              
        # Check if enough supernodes responded with valid responses
        if supernode_price_agreement_response_percentage_achieved <= SUPERNODE_CREDIT_PRICE_AGREEMENT_QUORUM_PERCENTAGE:
            logger.warning(f"Not enough supernodes responded with valid price agreement responses; only {supernode_price_agreement_response_percentage_achieved:.2%} of the supernodes responded, less than the required quorum percentage of {SUPERNODE_CREDIT_PRICE_AGREEMENT_QUORUM_PERCENTAGE:.2%}")
            logger.info("Responding to end user with termination message...")
            termination_message = db_code.CreditPackPurchaseRequestResponseTermination(
                sha3_256_hash_of_credit_pack_purchase_request_fields=preliminary_price_quote_response.sha3_256_hash_of_credit_pack_purchase_request_fields,
                credit_pack_purchase_request_fields_json_b64=preliminary_price_quote_response.credit_pack_purchase_request_fields_json_b64,
                termination_reason_string="Not enough supernodes responded with valid price agreement responses",
                termination_timestamp_utc_iso_string=datetime.now(timezone.utc).isoformat(),
                termination_pastel_block_height=await get_current_pastel_block_height_func(),
                credit_purchase_request_termination_message_version_string="1.0",
                responding_supernode_pastelid=MY_PASTELID,
                sha3_256_hash_of_credit_pack_purchase_request_termination_fields="",
                responding_supernode_signature_on_credit_pack_purchase_request_termination_hash="",
            )
            termination_message.sha3_256_hash_of_credit_pack_purchase_request_termination_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(termination_message)
            termination_message.responding_supernode_signature_on_credit_pack_purchase_request_termination_hash = await sign_message_with_pastelid_func(
                MY_PASTELID,
                termination_message.sha3_256_hash_of_credit_pack_purchase_request_termination_fields,
                LOCAL_PASTEL_ID_PASSPHRASE
            )
            termination_validation_errors = await validate_credit_pack_ticket_message_data_func(termination_message)
            if termination_validation_errors:
                logger.error(f"Invalid credit purchase request termination message: {', '.join(termination_validation_errors)}")
                raise ValueError(f"Invalid credit purchase request termination message: {', '.join(termination_validation_errors)}")
            return termination_message
        # Tally the agreeing supernodes
        list_of_agreeing_supernodes = [response.responding_supernode_pastelid for response in valid_price_agreement_request_responses if response.agree_with_proposed_price]
        supernode_price_agreement_voting_percentage = len(list_of_agreeing_supernodes) / len(valid_price_agreement_request_responses)
        logger.info(f"Of the {len(valid_price_agreement_request_responses)} valid price agreement responses, {len(list_of_agreeing_supernodes)} supernodes agreed to the proposed pricing, achieving a voting percentage of {supernode_price_agreement_voting_percentage:.2%}")
        # Check if enough supernodes agreed to the proposed pricing
        if supernode_price_agreement_voting_percentage <= SUPERNODE_CREDIT_PRICE_AGREEMENT_MAJORITY_PERCENTAGE:
            logger.warning(f"Not enough supernodes agreed to the proposed pricing; only {supernode_price_agreement_voting_percentage:.2%} of the supernodes agreed, less than the required majority percentage of {SUPERNODE_CREDIT_PRICE_AGREEMENT_MAJORITY_PERCENTAGE:.2%}")
            logger.info("Responding to end user with termination message...")
            termination_message = db_code.CreditPackPurchaseRequestResponseTermination(
                sha3_256_hash_of_credit_pack_purchase_request_fields=preliminary_price_quote_response.sha3_256_hash_of_credit_pack_purchase_request_fields,
                credit_pack_purchase_request_fields_json_b64=preliminary_price_quote_response.credit_pack_purchase_request_fields_json_b64,
                termination_reason_string="Not enough supernodes agreed to the proposed pricing",
                termination_timestamp_utc_iso_string=datetime.now(timezone.utc).isoformat(),
                termination_pastel_block_height=await get_current_pastel_block_height_func(),
                credit_purchase_request_termination_message_version_string="1.0",
                responding_supernode_pastelid=MY_PASTELID,
                sha3_256_hash_of_credit_pack_purchase_request_termination_fields="",
                responding_supernode_signature_on_credit_pack_purchase_request_termination_hash=""
            )
            termination_message.sha3_256_hash_of_credit_pack_purchase_request_termination_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(termination_message)
            termination_message.responding_supernode_signature_on_credit_pack_purchase_request_termination_hash = await sign_message_with_pastelid_func(
                MY_PASTELID,
                termination_message.sha3_256_hash_of_credit_pack_purchase_request_termination_fields,
                LOCAL_PASTEL_ID_PASSPHRASE
            )
            termination_validation_errors = await validate_credit_pack_ticket_message_data_func(termination_message)
            if termination_validation_errors:
                logger.error(f"Invalid credit purchase request termination message: {', '.join(termination_validation_errors)}")
                raise ValueError(f"Invalid credit purchase request termination message: {', '.join(termination_validation_errors)}")            
            return termination_message
        logger.info(f"Enough supernodes agreed to the proposed pricing; {len(list_of_agreeing_supernodes)} supernodes agreed to the proposed pricing, achieving a voting percentage of {supernode_price_agreement_voting_percentage:.2%}, more than the required minimum percentage of {SUPERNODE_CREDIT_PRICE_AGREEMENT_MAJORITY_PERCENTAGE:.2%}")
        # Select top N closest supernodes to the best block merkle root for inclusion in the response
        _, best_block_merkle_root, best_block_height = await get_best_block_hash_and_merkle_root_func()
        list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion = await select_top_n_closest_supernodes_to_best_block_merkle_root(list_of_agreeing_supernodes, n=10, best_block_merkle_root=best_block_merkle_root)
        # Aggregate the signatures from the selected agreeing supernodes
        selected_agreeing_supernodes_signatures_dict = {}
        for response in valid_price_agreement_request_responses:
            if response.responding_supernode_pastelid in list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion:
                selected_agreeing_supernodes_signatures_dict[response.responding_supernode_pastelid] = {
                    "price_agreement_request_response_hash_signature": response.responding_supernode_signature_on_price_agreement_request_response_hash,
                    "credit_pack_purchase_request_fields_json_b64_signature": response.responding_supernode_signature_on_credit_pack_purchase_request_fields_json_b64
                }
        if isinstance(preliminary_price_quote_response.credit_pack_purchase_request_fields_json_b64, str):
            credit_pack_purchase_request_fields_json = base64.b64decode(preliminary_price_quote_response.credit_pack_purchase_request_fields_json_b64).decode('utf-8')
            credit_request_response_dict = json.loads(credit_pack_purchase_request_fields_json)
        requested_initial_credits_in_credit_pack = credit_request_response_dict['requested_initial_credits_in_credit_pack']
        # Create the credit pack purchase request response
        credit_pack_purchase_request_response = db_code.CreditPackPurchaseRequestResponse(
            sha3_256_hash_of_credit_pack_purchase_request_fields=preliminary_price_quote_response.sha3_256_hash_of_credit_pack_purchase_request_fields,
            credit_pack_purchase_request_fields_json_b64=preliminary_price_quote_response.credit_pack_purchase_request_fields_json_b64,
            psl_cost_per_credit=preliminary_price_quote_response.preliminary_quoted_price_per_credit_in_psl,
            proposed_total_cost_of_credit_pack_in_psl=requested_initial_credits_in_credit_pack*preliminary_price_quote_response.preliminary_quoted_price_per_credit_in_psl,
            credit_usage_tracking_psl_address=preliminary_price_quote_response.credit_usage_tracking_psl_address,
            request_response_timestamp_utc_iso_string=datetime.now(timezone.utc).isoformat(),
            request_response_pastel_block_height=await get_current_pastel_block_height_func(),
            best_block_merkle_root=best_block_merkle_root,
            best_block_height=best_block_height,
            credit_purchase_request_response_message_version_string="1.0",
            responding_supernode_pastelid=MY_PASTELID,
            list_of_blacklisted_supernode_pastelids=list_of_blacklisted_supernode_pastelids,
            list_of_potentially_agreeing_supernodes=potentially_agreeing_supernodes,
            list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms=list_of_agreeing_supernodes,
            list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion=list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion,
            selected_agreeing_supernodes_signatures_dict=selected_agreeing_supernodes_signatures_dict,
            sha3_256_hash_of_credit_pack_purchase_request_response_fields="",
            responding_supernode_signature_on_credit_pack_purchase_request_response_hash=""
        )
        logger.info(f"Now generating the final credit pack purchase request response and assembling the {len(list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion)} selected agreeing supernode signatures...")
        # Generate the hash and signature fields
        response_fields_that_are_hashed = await extract_response_fields_from_credit_pack_ticket_message_data_as_json_func(credit_pack_purchase_request_response)  # noqa: F841
        credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_response_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(credit_pack_purchase_request_response)
        credit_pack_purchase_request_response.responding_supernode_signature_on_credit_pack_purchase_request_response_hash = await sign_message_with_pastelid_func(
            MY_PASTELID,
            credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_response_fields,
            LOCAL_PASTEL_ID_PASSPHRASE
        )
        # Validate the credit_pack_purchase_request_response
        request_response_validation_errors = await validate_credit_pack_ticket_message_data_func(credit_pack_purchase_request_response)
        if request_response_validation_errors:
            logger.error(f"Invalid credit pack purchase request response: {', '.join(request_response_validation_errors)}")
            raise ValueError(f"Invalid credit pack purchase request response: {', '.join(request_response_validation_errors)}")          
        await save_credit_pack_purchase_request_response(credit_pack_purchase_request_response)
        # Send the final credit pack purchase request response to the agreeing supernodes
        logger.info(f"Now sending the final credit pack purchase request response to the list of {len(list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion)} selected agreeing supernodes: {list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion}")
        announcement_responses = await send_credit_pack_purchase_request_final_response_to_supernodes(credit_pack_purchase_request_response, list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion)
        logger.info(f"Received {len(announcement_responses)} responses to the final credit pack purchase request response announcement, of which {len([response for response in announcement_responses if response.status_code == 200])} were successful")
        return credit_pack_purchase_request_response
    except Exception as e:
        logger.error(f"Error processing credit purchase preliminary price quote response: {str(e)}")
        traceback.print_exc()
        raise

async def get_credit_purchase_request_status(status_request: db_code.CreditPackRequestStatusCheck) -> db_code.CreditPackPurchaseRequestStatus:
    try:
        # Validate the request fields
        if not status_request.sha3_256_hash_of_credit_pack_purchase_request_fields or not status_request.requesting_end_user_pastelid:
            raise ValueError("Invalid status check request")
        await save_credit_pack_purchase_request_status_check(status_request)
        # Retrieve the credit pack purchase request
        credit_pack_purchase_request = await get_credit_pack_purchase_request(status_request.sha3_256_hash_of_credit_pack_purchase_request_fields)
        # Retrieve the credit pack purchase request response
        credit_pack_purchase_request_response = await get_credit_pack_purchase_request_response_from_request_hash(status_request.sha3_256_hash_of_credit_pack_purchase_request_fields)
        # Check the status of the credit pack purchase request
        status = await check_credit_pack_purchase_request_status(credit_pack_purchase_request)
        # Determine the status details based on the status
        if status == "pending":
            status_details = "Waiting for the credit pack purchase request to be processed"
        elif status == "approved":
            status_details = "Credit pack purchase request has been approved, waiting for confirmation"
        elif status == "confirmed":
            status_details = "Credit pack purchase has been confirmed, waiting for completion"
        elif status == "completed":
            status_details = "Credit pack purchase has been completed successfully"
        elif status == "failed":
            status_details = "Credit pack purchase has failed"
        else:
            status_details = "Unknown status"
        # Create the response
        response = db_code.CreditPackPurchaseRequestStatus(
            sha3_256_hash_of_credit_pack_purchase_request_fields=status_request.sha3_256_hash_of_credit_pack_purchase_request_fields,
            sha3_256_hash_of_credit_pack_purchase_request_response_fields=credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_response_fields if credit_pack_purchase_request_response else "",
            status=status,
            status_details=status_details,
            status_update_timestamp_utc_iso_string=datetime.now(timezone.utc).isoformat(),
            status_update_pastel_block_height=await get_current_pastel_block_height_func(),
            credit_purchase_request_status_message_version_string="1.0",
            responding_supernode_pastelid=credit_pack_purchase_request_response.responding_supernode_pastelid if credit_pack_purchase_request_response else MY_PASTELID,
            sha3_256_hash_of_credit_pack_purchase_request_status_fields="",
            responding_supernode_signature_on_credit_pack_purchase_request_status_hash=""
        )
        # Generate the hash and signature fields
        response.sha3_256_hash_of_credit_pack_purchase_request_status_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(response)
        response.responding_supernode_signature_on_credit_pack_purchase_request_status_hash = await sign_message_with_pastelid_func(
            MY_PASTELID,
            response.sha3_256_hash_of_credit_pack_purchase_request_status_fields,
            LOCAL_PASTEL_ID_PASSPHRASE
        )
        await save_credit_pack_purchase_request_status_check(response)
        return response
    except Exception as e:
        logger.error(f"Error getting credit purchase request status: {str(e)}")
        traceback.print_exc()
        raise
    
async def process_credit_pack_purchase_request_final_response_announcement(response: db_code.CreditPackPurchaseRequestResponse) -> None:
    try:
        # Validate the response fields
        if not response.sha3_256_hash_of_credit_pack_purchase_request_fields or not response.credit_pack_purchase_request_fields_json_b64:
            raise ValueError("Invalid final response announcement")
        # Save the final response to the db
        await save_credit_pack_purchase_request_final_response(response)
    except Exception as e:
        logger.error(f"Error processing credit pack purchase request final response announcement: {str(e)}")
        raise
    
async def get_block_height_for_credit_pack_purchase_request_confirmation(sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str) -> int:
    try:
        # Retrieve the CreditPackPurchaseRequestConfirmation from the database using the hash
        confirmation = await db_code.CreditPackPurchaseRequestConfirmation.get(sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields=sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields)
        return confirmation.credit_purchase_request_confirmation_pastel_block_height
    except Exception as e:
        logger.error(f"Error retrieving block height for CreditPackPurchaseRequestConfirmation: {str(e)}")
        raise
    
async def get_block_height_from_block_hash(pastel_block_hash: str):
    if not pastel_block_hash:
        raise ValueError("Invalid block hash provided.")
    try:
        block_details = await getblock(rpc_connection, pastel_block_hash)
        block_height = block_details.get('height')
        if block_height is not None:
            return block_height
        else:
            raise ValueError("Block height could not be retrieved.")
    except Exception as e:
        raise Exception(f"Error retrieving block height: {str(e)}")

async def get_closest_agreeing_supernode_pastelid(end_user_pastelid: str, agreeing_supernode_pastelids: List[str]) -> str:
    try:
        end_user_pastelid_hash = compute_sha3_256_hexdigest(end_user_pastelid)
        end_user_pastelid_int = int(end_user_pastelid_hash, 16)
        xor_distances = []
        for supernode_pastelid in agreeing_supernode_pastelids:
            supernode_pastelid_hash = compute_sha3_256_hexdigest(supernode_pastelid)
            supernode_pastelid_int = int(supernode_pastelid_hash, 16)
            distance = end_user_pastelid_int ^ supernode_pastelid_int
            xor_distances.append((supernode_pastelid, distance))
        closest_supernode_pastelid = min(xor_distances, key=lambda x: x[1])[0]
        return closest_supernode_pastelid
    except Exception as e:
        logger.error(f"Error getting closest agreeing supernode pastelid: {str(e)}")
        traceback.print_exc()
        raise    
    
async def send_credit_pack_storage_completion_announcement_to_supernodes(response: db_code.CreditPackPurchaseRequestConfirmationResponse, agreeing_supernode_pastelids: List[str]) -> List[httpx.Response]:
    try:
        # Read the blacklist file if it exists
        blacklist_path = Path('supernode_inference_ip_blacklist.txt')
        blacklisted_ips = set()
        if blacklist_path.exists():
            with blacklist_path.open('r') as blacklist_file:
                blacklisted_ips = {line.strip() for line in blacklist_file if line.strip()}
        else:
            logger.info("Blacklist file not found. Proceeding without blacklist filtering.")
        
        async with httpx.AsyncClient() as client:
            tasks = []
            supernode_list_df, _ = await check_supernode_list_func()
            for supernode_pastelid in agreeing_supernode_pastelids:
                payload = {}
                try:
                    supernode_base_url = await get_supernode_url_from_pastelid_func(supernode_pastelid, supernode_list_df)
                    ip_address_port = supernode_base_url.split("//")[1].split(":")[0]
                    if ip_address_port in blacklisted_ips:
                        logger.info(f"Skipping blacklisted supernode {supernode_pastelid}")
                        continue
                    url = f"{supernode_base_url}/credit_pack_storage_completion_announcement"
                    challenge_dict = await request_and_sign_challenge(supernode_base_url)
                    challenge = challenge_dict["challenge"]
                    challenge_id = challenge_dict["challenge_id"]
                    challenge_signature = challenge_dict["challenge_signature"]
                    response_dict = response.model_dump()
                    response_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in response_dict.items()}
                    payload = {
                        "storage_completion_announcement": response_dict,
                        "challenge": challenge,
                        "challenge_id": challenge_id,
                        "challenge_signature": challenge_signature
                    }
                except Exception as e:
                    logger.warning(f"Error getting challenge from supernode {supernode_pastelid}: {str(e)}")
                if len(payload) > 0:
                    task = asyncio.create_task(client.post(url, json=payload))
                    tasks.append(task)
            responses = await asyncio.gather(*tasks, return_exceptions=True)
            valid_responses = []
            for response in responses:
                if isinstance(response, httpx.Response):
                    if response.status_code == 200:
                        valid_responses.append(response)
                    else:
                        logger.warning(f"Error sending storage completion announcement to supernode {response.url}: {response.text}")
                else:
                    logger.error(f"Error sending storage completion announcement to supernode: {str(response)}")
            return valid_responses
    except Exception as e:
        logger.error(f"Error sending storage completion announcement to supernodes: {str(e)}")
        traceback.print_exc()
        raise
    
async def process_credit_purchase_request_confirmation(confirmation: db_code.CreditPackPurchaseRequestConfirmation) -> db_code.CreditPackPurchaseRequestConfirmationResponse:
    try:
        # Validate the confirmation fields
        validation_errors = await validate_credit_pack_ticket_message_data_func(confirmation)
        if validation_errors:
            logger.error(f"Invalid credit purchase request confirmation: {', '.join(validation_errors)}")
            raise ValueError(f"Invalid credit purchase request confirmation: {', '.join(validation_errors)}")
        await save_credit_pack_purchase_request_confirmation(confirmation)
        # Retrieve the credit pack purchase request response
        credit_pack_purchase_request_response = await get_credit_pack_purchase_request_response(confirmation.sha3_256_hash_of_credit_pack_purchase_request_response_fields)
        # Check the burn transaction
        logger.info("Now checking the burn transaction for the credit pack purchase request...")
        matching_transaction_found = False
        exceeding_transaction_found = False
        num_confirmations = 0
        current_block_height = await get_current_pastel_block_height_func()
        initial_transaction_check_sleep_time_in_seconds = 30 
        current_transaction_check_sleep_time_in_seconds = initial_transaction_check_sleep_time_in_seconds
        while current_block_height <= credit_pack_purchase_request_response.request_response_pastel_block_height + MAXIMUM_NUMBER_OF_PASTEL_BLOCKS_FOR_USER_TO_SEND_BURN_AMOUNT_FOR_CREDIT_TICKET:
            matching_transaction_found, exceeding_transaction_found, transaction_block_height, num_confirmations, amount_received_at_burn_address = await check_burn_transaction(
                confirmation.txid_of_credit_purchase_burn_transaction,
                credit_pack_purchase_request_response.credit_usage_tracking_psl_address,
                credit_pack_purchase_request_response.proposed_total_cost_of_credit_pack_in_psl,
                credit_pack_purchase_request_response.request_response_pastel_block_height
            )
            if matching_transaction_found or exceeding_transaction_found:
                if (num_confirmations >= MINIMUM_CONFIRMATION_BLOCKS_FOR_CREDIT_PACK_BURN_TRANSACTION) or SKIP_BURN_TRANSACTION_BLOCK_CONFIRMATION_CHECK:
                    break
                else:
                    logger.info(f"Waiting for {current_transaction_check_sleep_time_in_seconds} seconds to check again if burn transaction is confirmed...")
                    await asyncio.sleep(current_transaction_check_sleep_time_in_seconds) 
            else:
                logger.info(f"Waiting for {current_transaction_check_sleep_time_in_seconds} seconds to check again if burn transaction is confirmed...")
                await asyncio.sleep(current_transaction_check_sleep_time_in_seconds)
            current_transaction_check_sleep_time_in_seconds = round(current_transaction_check_sleep_time_in_seconds*1.05, 2)
            current_block_height = await get_current_pastel_block_height_func()
        if matching_transaction_found or exceeding_transaction_found:
            # Store the credit pack ticket on the blockchain
            credit_pack_purchase_request_json = base64.b64decode(credit_pack_purchase_request_response.credit_pack_purchase_request_fields_json_b64).decode('utf-8')
            credit_pack_purchase_request_dict = json.loads(credit_pack_purchase_request_json)
            credit_pack_purchase_request_response_dict = credit_pack_purchase_request_response.model_dump()
            credit_pack_purchase_request_response_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in credit_pack_purchase_request_response_dict.items()}
            credit_pack_purchase_request_confirmation_dict = confirmation.model_dump()
            credit_pack_purchase_request_confirmation_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in credit_pack_purchase_request_confirmation_dict.items()}
            credit_pack_combined_blockchain_ticket_data = {"credit_pack_purchase_request_dict": credit_pack_purchase_request_dict, "credit_pack_purchase_request_response_dict": credit_pack_purchase_request_response_dict, "credit_pack_purchase_request_confirmation_dict": credit_pack_purchase_request_confirmation_dict}
            credit_pack_combined_blockchain_ticket_data_json = json.dumps(credit_pack_combined_blockchain_ticket_data)
            credit_pack_ticket_bytes_before_compression = sys.getsizeof(credit_pack_combined_blockchain_ticket_data_json)
            compressed_credit_pack_ticket, _ = await compress_data_with_zstd_func(credit_pack_combined_blockchain_ticket_data_json)
            credit_pack_ticket_bytes_after_compression = sys.getsizeof(compressed_credit_pack_ticket)
            compression_ratio = credit_pack_ticket_bytes_before_compression / credit_pack_ticket_bytes_after_compression
            logger.info(f"Achieved a compression ratio of {compression_ratio:.2f} on credit pack ticket data!")
            logger.info(f"Required burn transaction confirmed with {num_confirmations} confirmations; now attempting to write the credit pack ticket to the blockchain (a total of {credit_pack_ticket_bytes_before_compression:,} bytes before compression and {credit_pack_ticket_bytes_after_compression:,} bytes after compression)...")
            log_action_with_payload("Writing", "the credit pack ticket to the blockchain", credit_pack_combined_blockchain_ticket_data_json)
            pastel_api_credit_pack_ticket_registration_txid, storage_validation_error_string = await store_credit_pack_ticket_in_blockchain(credit_pack_combined_blockchain_ticket_data_json)
            if storage_validation_error_string=="":
                credit_pack_confirmation_outcome_string = "success"
                await save_credit_pack_purchase_request_response_txid_mapping(credit_pack_purchase_request_response, pastel_api_credit_pack_ticket_registration_txid)
            else:
                credit_pack_confirmation_outcome_string = "failed"
            # Create the confirmation response without the hash and signature fields
            confirmation_response = db_code.CreditPackPurchaseRequestConfirmationResponse(
                sha3_256_hash_of_credit_pack_purchase_request_fields=confirmation.sha3_256_hash_of_credit_pack_purchase_request_fields,
                sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields=confirmation.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields,
                credit_pack_confirmation_outcome_string=credit_pack_confirmation_outcome_string,
                pastel_api_credit_pack_ticket_registration_txid=pastel_api_credit_pack_ticket_registration_txid,
                credit_pack_confirmation_failure_reason_if_applicable="",
                credit_purchase_request_confirmation_response_utc_iso_string=datetime.now(timezone.utc).isoformat(),
                credit_purchase_request_confirmation_response_pastel_block_height=await get_current_pastel_block_height_func(),
                credit_purchase_request_confirmation_response_message_version_string="1.0",
                responding_supernode_pastelid=MY_PASTELID,
                sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields="",
                responding_supernode_signature_on_credit_pack_purchase_request_confirmation_response_hash=""
            )
            # Generate the hash and signature fields
            confirmation_response.sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(confirmation_response)
            confirmation_response.responding_supernode_signature_on_credit_pack_purchase_request_confirmation_response_hash = await sign_message_with_pastelid_func(
                MY_PASTELID,
                confirmation_response.sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields,
                LOCAL_PASTEL_ID_PASSPHRASE
            )
            confirmation_response_validation_errors = await validate_credit_pack_ticket_message_data_func(confirmation_response)
            # Send the CreditPackPurchaseRequestConfirmationResponse to the agreeing supernodes
            logger.info(f"Now attempting to send the credit pack storage completion announcement to the {len(credit_pack_purchase_request_response.list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms)} agreeing Supernodes...")
            announcement_responses = await send_credit_pack_storage_completion_announcement_to_supernodes(confirmation_response, credit_pack_purchase_request_response.list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms)
            logger.info(f"Received {len(announcement_responses)} responses to the credit pack storage completion announcement, of which {len([response for response in announcement_responses if response.status_code == 200])} were successful")
        else:
            # Create the confirmation response with failure without the hash and signature fields
            confirmation_response = db_code.CreditPackPurchaseRequestConfirmationResponse(
                sha3_256_hash_of_credit_pack_purchase_request_fields=confirmation.sha3_256_hash_of_credit_pack_purchase_request_fields,
                sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields=confirmation.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields,
                credit_pack_confirmation_outcome_string="failure",
                pastel_api_credit_pack_ticket_registration_txid="",
                credit_pack_confirmation_failure_reason_if_applicable="Burn transaction not confirmed within the required number of blocks",
                credit_purchase_request_confirmation_response_utc_iso_string=datetime.now(timezone.utc).isoformat(),
                credit_purchase_request_confirmation_response_pastel_block_height=await get_current_pastel_block_height_func(),
                credit_purchase_request_confirmation_response_message_version_string="1.0",
                responding_supernode_pastelid=MY_PASTELID,
                sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields="",
                responding_supernode_signature_on_credit_pack_purchase_request_confirmation_response_hash=""
            )
            # Generate the hash and signature fields
            confirmation_response.sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(confirmation_response)
            confirmation_response.responding_supernode_signature_on_credit_pack_purchase_request_confirmation_response_hash = await sign_message_with_pastelid_func(
                MY_PASTELID,
                confirmation_response.sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields,
                LOCAL_PASTEL_ID_PASSPHRASE
            )
        confirmation_response_validation_errors = await validate_credit_pack_ticket_message_data_func(confirmation_response)
        if confirmation_response_validation_errors:
            logger.error(f"Invalid credit purchase request confirmation response: {', '.join(confirmation_response_validation_errors)}")
            raise ValueError(f"Invalid credit purchase request confirmation response: {', '.join(confirmation_response_validation_errors)}")
        await save_credit_pack_purchase_request_confirmation_response(confirmation_response)            
        # Validate the response
        confirmation_response_validation_errors = await validate_credit_pack_ticket_message_data_func(confirmation_response)
        if confirmation_response_validation_errors:
            logger.error(f"Invalid credit pack purchase completion announcement: {', '.join(confirmation_response_validation_errors)}")
            raise ValueError(f"Invalid credit purchase request confirmation response: {', '.join(validation_errors)}")
        await save_credit_pack_purchase_request_confirmation_response(confirmation_response)
        return confirmation_response
    except Exception as e:
        logger.error(f"Error processing credit purchase request confirmation: {str(e)}")
        traceback.print_exc()
        raise    
    
async def process_credit_pack_purchase_completion_announcement(confirmation: db_code.CreditPackPurchaseRequestConfirmation) -> None:
    try:
        # Validate the confirmation fields
        if not confirmation.sha3_256_hash_of_credit_pack_purchase_request_fields or not confirmation.sha3_256_hash_of_credit_pack_purchase_request_response_fields:
            raise ValueError("Invalid credit pack purchase completion announcement")
        confirmation_validation_errors = await validate_credit_pack_ticket_message_data_func(confirmation)
        if confirmation_validation_errors:
            logger.error(f"Invalid credit pack purchase completion announcement: {', '.join(confirmation_validation_errors)}")
            raise ValueError(f"Invalid credit pack purchase completion announcement: {', '.join(confirmation_validation_errors)}")
        # Store the completion announcement
        await save_credit_pack_purchase_completion_announcement(confirmation)
    except Exception as e:
        logger.error(f"Error processing credit pack purchase completion announcement: {str(e)}")
        traceback.print_exc()
        raise

async def process_credit_pack_storage_completion_announcement(completion_response: db_code.CreditPackPurchaseRequestConfirmationResponse) -> None:
    try:
        # Validate the response fields
        if not completion_response.sha3_256_hash_of_credit_pack_purchase_request_fields or not completion_response.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields:
            raise ValueError("Invalid credit pack storage completion announcement")
        completion_response_validation_errors = await validate_credit_pack_ticket_message_data_func(completion_response)
        if completion_response_validation_errors:
            logger.error(f"Invalid credit pack storage completion announcement: {', '.join(completion_response_validation_errors)}")
            raise ValueError(f"Invalid credit pack storage completion announcement: {', '.join(completion_response_validation_errors)}")        
        # Store the storage completion announcement
        await save_credit_pack_storage_completion_announcement(completion_response)
    except Exception as e:
        logger.error(f"Error processing credit pack storage completion announcement: {str(e)}")
        traceback.print_exc()
        raise

async def process_credit_pack_storage_retry_request(storage_retry_request: db_code.CreditPackStorageRetryRequest) -> db_code.CreditPackStorageRetryRequestResponse:
    try:
        # Validate the request fields
        storage_retry_request_validation_errors = await validate_credit_pack_ticket_message_data_func(storage_retry_request)
        if storage_retry_request_validation_errors:
            logger.error(f"Invalid credit pack storage retry request: {', '.join(storage_retry_request_validation_errors)}")
            raise ValueError(f"Invalid credit pack storage retry request: {', '.join(storage_retry_request_validation_errors)}")
        await save_credit_pack_storage_retry_request(storage_retry_request)
        # Check if the supernode receiving the retry request is the closest agreeing supernode to the end user's pastelid
        closest_agreeing_supernode_pastelid = await get_closest_agreeing_supernode_pastelid(storage_retry_request.requesting_end_user_pastelid, json.loads(storage_retry_request.credit_pack_purchase_request_response_json)["list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms"])
        if closest_agreeing_supernode_pastelid != MY_PASTELID:
            error_message = f"The supernode receiving the retry request is not the closest agreeing supernode to the end user's pastelid! Closest agreeing supernode to end user's pastelid ({storage_retry_request.requesting_end_user_pastelid}) is {closest_agreeing_supernode_pastelid}, whereas our local pastelid is {MY_PASTELID}"
            logger.error(error_message)
            raise ValueError(error_message)
        # Check if more than MINIMUM_NUMBER_OF_PASTEL_BLOCKS_BEFORE_TICKET_STORAGE_RETRY_ALLOWED Pastel blocks have elapsed since the CreditPackPurchaseRequestConfirmation was sent
        confirmation_block_height = await get_block_height_for_credit_pack_purchase_request_confirmation(storage_retry_request.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields)
        current_block_height = await get_current_pastel_block_height_func()
        if current_block_height - confirmation_block_height <= MINIMUM_NUMBER_OF_PASTEL_BLOCKS_BEFORE_TICKET_STORAGE_RETRY_ALLOWED:
            error_message = f"Insufficient time has elapsed since the CreditPackPurchaseRequestConfirmation was sent (current block height: {current_block_height}, confirmation block height: {confirmation_block_height}, so the elapsed block count of {current_block_height - confirmation_block_height} is less than the minimum required block count of {MINIMUM_NUMBER_OF_PASTEL_BLOCKS_BEFORE_TICKET_STORAGE_RETRY_ALLOWED})"
            logger.error(error_message)
            raise ValueError(error_message)
        # Check if the original responding supernode has confirmed the storage
        original_supernode_confirmed_storage = await check_original_supernode_storage_confirmation(storage_retry_request.sha3_256_hash_of_credit_pack_purchase_request_response_fields)
        if not original_supernode_confirmed_storage:
            # Store the credit pack ticket on the blockchain
            credit_pack_purchase_request_response = await get_credit_pack_purchase_request_response(storage_retry_request.sha3_256_hash_of_credit_pack_purchase_request_response_fields)
            credit_pack_purchase_request_response_dict = credit_pack_purchase_request_response.model_dump()
            credit_pack_purchase_request_json = base64.b64decode(credit_pack_purchase_request_response_dict['credit_pack_purchase_request_fields_json_b64']).decode('utf-8')
            credit_pack_purchase_request_dict = json.loads(credit_pack_purchase_request_json)
            credit_pack_purchase_request_confirmation = await get_credit_pack_purchase_request_confirmation_from_request_hash(credit_pack_purchase_request_response_dict['sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields'])
            credit_pack_purchase_request_confirmation_dict = credit_pack_purchase_request_confirmation.model_dump()
            credit_pack_purchase_request_confirmation_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in credit_pack_purchase_request_confirmation_dict.items()}
            credit_pack_combined_blockchain_ticket_data = {"credit_pack_purchase_request_dict": credit_pack_purchase_request_dict, "credit_pack_purchase_request_response_dict": credit_pack_purchase_request_response_dict, "credit_pack_purchase_request_confirmation_dict": credit_pack_purchase_request_confirmation_dict}
            credit_pack_combined_blockchain_ticket_data_json = json.dumps(credit_pack_combined_blockchain_ticket_data)
            pastel_api_credit_pack_ticket_registration_txid, storage_validation_error_string = await store_credit_pack_ticket_in_blockchain(credit_pack_combined_blockchain_ticket_data_json)
            if storage_validation_error_string=="":
                credit_pack_confirmation_outcome_string = "success"
            else:
                credit_pack_confirmation_outcome_string = "failed"            
            # Create the retry request response without the hash and signature fields
            retry_request_response = db_code.CreditPackStorageRetryRequestResponse(
                sha3_256_hash_of_credit_pack_purchase_request_fields=storage_retry_request.sha3_256_hash_of_credit_pack_purchase_request_response_fields,
                sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields=storage_retry_request.sha3_256_hash_of_credit_pack_purchase_request_response_fields,
                credit_pack_storage_retry_confirmation_outcome_string=credit_pack_confirmation_outcome_string,
                pastel_api_credit_pack_ticket_registration_txid=pastel_api_credit_pack_ticket_registration_txid,
                credit_pack_storage_retry_confirmation_failure_reason_if_applicable="",
                credit_pack_storage_retry_confirmation_response_utc_iso_string=datetime.now(timezone.utc).isoformat(),
                credit_pack_storage_retry_confirmation_response_pastel_block_height=await get_current_pastel_block_height_func(),
                credit_pack_storage_retry_confirmation_response_message_version_string="1.0",
                closest_agreeing_supernode_to_retry_storage_pastelid=MY_PASTELID,
                sha3_256_hash_of_credit_pack_storage_retry_confirmation_response_fields="",
                closest_agreeing_supernode_to_retry_storage_pastelid_signature_on_credit_pack_storage_retry_confirmation_response_hash=""
            )
            # Generate the hash and signature fields
            retry_request_response.sha3_256_hash_of_credit_pack_storage_retry_confirmation_response_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(retry_request_response)
            retry_request_response.closest_agreeing_supernode_to_retry_storage_pastelid_signature_on_credit_pack_storage_retry_confirmation_response_hash = await sign_message_with_pastelid_func(
                MY_PASTELID,
                retry_request_response.sha3_256_hash_of_credit_pack_storage_retry_confirmation_response_fields,
                LOCAL_PASTEL_ID_PASSPHRASE
            )
        else:
            # Create the retry request response with failure without the hash and signature fields
            retry_request_response = db_code.CreditPackStorageRetryRequestResponse(
                sha3_256_hash_of_credit_pack_purchase_request_fields=storage_retry_request.sha3_256_hash_of_credit_pack_purchase_request_response_fields,
                sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields=storage_retry_request.sha3_256_hash_of_credit_pack_purchase_request_response_fields,
                credit_pack_storage_retry_confirmation_outcome_string="failure",
                pastel_api_credit_pack_ticket_registration_txid="",
                credit_pack_storage_retry_confirmation_failure_reason_if_applicable="Original responding supernode already confirmed storage",
                credit_pack_storage_retry_confirmation_response_utc_iso_string=datetime.now(timezone.utc).isoformat(),
                credit_pack_storage_retry_confirmation_response_pastel_block_height=await get_current_pastel_block_height_func(),
                credit_pack_storage_retry_confirmation_response_message_version_string="1.0",
                closest_agreeing_supernode_to_retry_storage_pastelid=MY_PASTELID,
                sha3_256_hash_of_credit_pack_storage_retry_confirmation_response_fields="",
                closest_agreeing_supernode_to_retry_storage_pastelid_signature_on_credit_pack_storage_retry_confirmation_response_hash=""
            )
            # Generate the hash and signature fields
            retry_request_response.sha3_256_hash_of_credit_pack_storage_retry_confirmation_response_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(retry_request_response)
            retry_request_response.closest_agreeing_supernode_to_retry_storage_pastelid_signature_on_credit_pack_storage_retry_confirmation_response_hash = await sign_message_with_pastelid_func(
                MY_PASTELID,
                retry_request_response.sha3_256_hash_of_credit_pack_storage_retry_confirmation_response_fields,
                LOCAL_PASTEL_ID_PASSPHRASE
            )
        # Validate the response
        retry_request_response_validation_errors = await validate_credit_pack_ticket_message_data_func(retry_request_response)
        if retry_request_response_validation_errors:
            logger.error(f"Invalid credit pack storage retry request response: {', '.join(retry_request_response_validation_errors)}")
            raise ValueError(f"Invalid credit pack storage retry request response: {', '.join(retry_request_response_validation_errors)}")
        await save_credit_pack_storage_retry_request_response(retry_request_response)
        return retry_request_response_validation_errors
    except Exception as e:
        logger.error(f"Error processing credit pack storage retry request: {str(e)}")
        traceback.print_exc()
        raise
                    
async def process_credit_pack_storage_retry_completion_announcement(retry_completion_response: db_code.CreditPackStorageRetryRequestResponse) -> None:
    try:
        # Validate the response fields
        if not retry_completion_response.sha3_256_hash_of_credit_pack_purchase_request_fields or not retry_completion_response.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields:
            raise ValueError("Invalid credit pack storage retry completion announcement")
        retry_completion_response_validation_errors = await validate_credit_pack_ticket_message_data_func(retry_completion_response)
        if retry_completion_response_validation_errors:
            logger.error(f"Invalid credit pack storage retry completion announcement: {', '.join(retry_completion_response_validation_errors)}")
            raise ValueError(f"Invalid credit pack storage retry completion announcement: {', '.join(retry_completion_response_validation_errors)}")        
        # Store the storage retry completion announcement
        await save_credit_pack_storage_retry_completion_announcement(retry_completion_response)
    except Exception as e:
        logger.error(f"Error processing credit pack storage retry completion announcement: {str(e)}")
        traceback.print_exc()
        raise
    
async def insert_credit_pack_ticket_txid_into_known_bad_table_in_db(credit_pack_ticket_txid: str, list_of_reasons_it_is_known_bad: str):
    known_bad_txid_object = db_code.CreditPackKnownBadTXID(credit_pack_ticket_txid=credit_pack_ticket_txid, list_of_reasons_it_is_known_bad=list_of_reasons_it_is_known_bad)
    async with db_code.Session() as db_session:
        db_session.add(known_bad_txid_object)
        await db_session.commit()
        await db_session.refresh(known_bad_txid_object)
    return known_bad_txid_object

async def clear_out_all_credit_packs_from_known_bad_table():
    try:
        async with db_code.Session() as db_session:
            # Delete all entries from the CreditPackKnownBadTXID table
            await db_session.exec(delete(db_code.CreditPackKnownBadTXID))
            await db_session.commit()
            logger.info("Successfully cleared all credit packs from the known bad table.")
    except Exception as e:
        logger.error(f"Error clearing credit packs from known bad table: {str(e)}")
        traceback.print_exc()
        raise

async def get_list_of_all_known_bad_credit_pack_ticket_txids_from_db():
    async with db_code.Session() as db_session:
        known_bad_txids = await db_session.exec(select(db_code.CreditPackKnownBadTXID))
        known_bad_txid_results = known_bad_txids.all()
        if known_bad_txid_results is not None:
            list_of_all_known_bad_credit_pack_txids = list(set([x.credit_pack_ticket_txid for x in known_bad_txid_results]))          
            return list_of_all_known_bad_credit_pack_txids
        else:
            return []        

async def check_if_credit_pack_ticket_txid_in_list_of_known_bad_txids_in_db(credit_pack_ticket_txid: str):
    async with db_code.Session() as db_session:
        # Query to check if the TXID exists in the known bad TXIDs table
        result = await db_session.execute(
            select(db_code.CreditPackKnownBadTXID)
            .where(db_code.CreditPackKnownBadTXID.credit_pack_ticket_txid == credit_pack_ticket_txid)
        )
        known_bad_txid = result.scalar_one_or_none()
        # Return True if the TXID is found in the known bad list, False otherwise
        return known_bad_txid is not None
    
async def validate_merkle_root_at_block_height(merkle_root_to_check: str, block_height: int) -> bool:
    try:
        if block_height is None:
            logger.error("Block height is None")
            return False
        block_hash = await getblockhash(rpc_connection, block_height)
        if not block_hash:
            logger.error(f"Failed to get block hash for height {block_height}")
            return False
        block_details = await getblock(rpc_connection, block_hash)
        if not block_details:
            logger.error(f"Failed to get block details for hash {block_hash}")
            return False
        if 'merkleroot' not in block_details:
            logger.error(f"Merkle root not found in block details for hash {block_hash}")
            return False
        return block_details['merkleroot'] == merkle_root_to_check
    except Exception as e:
        logger.error(f"Error validating merkle root: {str(e)}")
        return False

@async_disk_cached(credit_pack_cache)
async def get_credit_pack_ticket_data(credit_pack_ticket_registration_txid: str, list_of_credit_pack_ticket_data: list, idx: int) -> dict:
    try:
        current_credit_pack_ticket_data = list_of_credit_pack_ticket_data[idx]
        current_credit_pack_ticket_data_json = base64.b64decode(current_credit_pack_ticket_data['ticket']['contract_ticket']).decode('utf-8')
        current_credit_pack_ticket_data_dict = json.loads(current_credit_pack_ticket_data_json)
        return {
            'credit_pack_registration_txid': credit_pack_ticket_registration_txid,
            'credit_purchase_request_confirmation_pastel_block_height': current_credit_pack_ticket_data_dict['ticket_input_data_dict']['credit_pack_purchase_request_confirmation_dict']['credit_purchase_request_confirmation_pastel_block_height'],
            'requesting_end_user_pastelid': current_credit_pack_ticket_data_dict['ticket_input_data_dict']['credit_pack_purchase_request_confirmation_dict']['requesting_end_user_pastelid'],
            'ticket_input_data_fully_parsed_sha3_256_hash': current_credit_pack_ticket_data_dict['ticket_input_data_fully_parsed_sha3_256_hash'],
            'txid_of_credit_purchase_burn_transaction': current_credit_pack_ticket_data_dict['ticket_input_data_dict']['credit_pack_purchase_request_confirmation_dict']['txid_of_credit_purchase_burn_transaction'],
            'credit_usage_tracking_psl_address': current_credit_pack_ticket_data_dict['ticket_input_data_dict']['credit_pack_purchase_request_response_dict']['credit_usage_tracking_psl_address'],
            'psl_cost_per_credit': current_credit_pack_ticket_data_dict['ticket_input_data_dict']['credit_pack_purchase_request_response_dict']['psl_cost_per_credit'],
            'requested_initial_credits_in_credit_pack': current_credit_pack_ticket_data_dict['ticket_input_data_dict']['credit_pack_purchase_request_dict']['requested_initial_credits_in_credit_pack'],
            'complete_credit_pack_data_json': current_credit_pack_ticket_data_json,
        }
    except Exception as e:
        logger.error(f"Error retrieving credit pack ticket data for TXID {credit_pack_ticket_registration_txid}: {str(e)}")
        traceback.print_exc()
        raise
    
@async_disk_cached(credit_pack_cache)
async def validate_existing_credit_pack_ticket(credit_pack_ticket_txid: str) -> dict:
    try:
        use_verbose_validation = 0
        logger.info(f"Validating credit pack ticket with TXID: {credit_pack_ticket_txid}")
        # Retrieve the credit pack ticket data from the blockchain
        credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation = await retrieve_credit_pack_ticket_from_blockchain_using_txid(credit_pack_ticket_txid)
        if use_verbose_validation:
            logger.info(f"Credit pack ticket data for credit pack with TXID {credit_pack_ticket_txid}:\n\nTicket Request Response:\n\n {abbreviated_pretty_json_func(credit_pack_purchase_request_response.model_dump())} \n\nTicket Request Confirmation:\n\n {abbreviated_pretty_json_func(credit_pack_purchase_request_confirmation.model_dump())}")
        validation_results = {
            "credit_pack_ticket_is_valid": True,
            "validation_checks": [],
            "validation_failure_reasons_list": []
        }
        # Define validation tasks
        validation_tasks = [
            validate_payment(credit_pack_ticket_txid, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation, validation_results),
            validate_supernode_lists(credit_pack_ticket_txid, credit_pack_purchase_request_response, validation_results),
            validate_hashes(credit_pack_ticket_txid, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation, validation_results),
            validate_signatures(credit_pack_ticket_txid, credit_pack_purchase_request_response, use_verbose_validation, validation_results),
            validate_selected_agreeing_supernodes(credit_pack_ticket_txid, credit_pack_purchase_request_response, validation_results)
        ]
        # Run validation tasks in parallel
        await asyncio.gather(*validation_tasks)
        if validation_results["credit_pack_ticket_is_valid"] and not validation_results["validation_failure_reasons_list"]:
            logger.info(f"All validation checks passed for credit pack ticket with TXID {credit_pack_ticket_txid}")
        else:
            logger.info(f"Validation failures for credit pack ticket with TXID {credit_pack_ticket_txid}: {validation_results['validation_failure_reasons_list']}")
            list_of_reasons_it_is_known_bad = json.dumps(validation_results['validation_failure_reasons_list'])
            known_bad_txid_object = await insert_credit_pack_ticket_txid_into_known_bad_table_in_db(credit_pack_ticket_txid, list_of_reasons_it_is_known_bad)
            if known_bad_txid_object:
                logger.info(f"Added invalid credit pack ticket TXID {credit_pack_ticket_txid} to known bad list in database!")
        # Sort validation_failure_reasons_list alphabetically
        validation_results["validation_failure_reasons_list"].sort()
        logger.info(f"Validation completed successfully for TXID: {credit_pack_ticket_txid}")
        return validation_results
    except Exception as e:
        logger.error(f"Error validating credit pack ticket with TXID {credit_pack_ticket_txid}: {str(e)}")
        logger.error(f"Exception type: {type(e).__name__}")
        logger.error(f"Exception details: {traceback.format_exc()}")
        return {
            "credit_pack_ticket_is_valid": False,
            "validation_checks": [],
            "validation_failure_reasons_list": [f"Exception during validation: {str(e)}"]
        }
        
async def validate_payment(credit_pack_ticket_txid, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation, validation_results):
    matching_transaction_found, exceeding_transaction_found, transaction_block_height, num_confirmations, amount_received_at_burn_address = await check_burn_transaction(
        credit_pack_purchase_request_confirmation.txid_of_credit_purchase_burn_transaction,
        credit_pack_purchase_request_response.credit_usage_tracking_psl_address,
        credit_pack_purchase_request_response.proposed_total_cost_of_credit_pack_in_psl,
        credit_pack_purchase_request_response.request_response_pastel_block_height
    )
    validation_results["validation_checks"].append({
        "check_name": f"Ticket Payment Burn Transaction validation (Burn payment TXID: {credit_pack_purchase_request_confirmation.txid_of_credit_purchase_burn_transaction} sent with {amount_received_at_burn_address} PSL",
        "is_valid": matching_transaction_found or exceeding_transaction_found
    })
    if not (matching_transaction_found or exceeding_transaction_found):
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append(f"Invalid burn transaction for credit pack ticket with TXID: {credit_pack_ticket_txid}")

async def validate_supernode_lists(credit_pack_ticket_txid, credit_pack_purchase_request_response, validation_results):
    active_supernodes_count_at_the_time, active_supernodes_at_the_time = await fetch_active_supernodes_count_and_details(credit_pack_purchase_request_response.request_response_pastel_block_height)
    list_of_active_supernode_pastelids_at_the_time = [x["pastel_id"] for x in active_supernodes_at_the_time]
    list_of_potentially_agreeing_supernodes = credit_pack_purchase_request_response.list_of_potentially_agreeing_supernodes
    list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms = credit_pack_purchase_request_response.list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms
    list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion = credit_pack_purchase_request_response.list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion
    if list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion is None:
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append("Required field 'list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion' is missing in credit pack purchase request response")
        return
    for potentially_agreeing_supernode_pastelid in list_of_potentially_agreeing_supernodes:
        potentially_agreeing_supernode_pastelid_in_list_of_active_supernodes_at_block_height = potentially_agreeing_supernode_pastelid in list_of_active_supernode_pastelids_at_the_time
        validation_results["validation_checks"].append({
            "check_name": f"Potentially agreeing supernode with pastelid {potentially_agreeing_supernode_pastelid} was in the list of active supernodes as of block height {credit_pack_purchase_request_response.request_response_pastel_block_height:,}",
            "is_valid": potentially_agreeing_supernode_pastelid_in_list_of_active_supernodes_at_block_height
        })
        if not potentially_agreeing_supernode_pastelid_in_list_of_active_supernodes_at_block_height:
            validation_results["credit_pack_ticket_is_valid"] = False
            validation_results["validation_failure_reasons_list"].append(f"Potentially agreeing supernode with pastelid {potentially_agreeing_supernode_pastelid} was NOT in the list of active supernodes as of block height {credit_pack_purchase_request_response.request_response_pastel_block_height:,}")
    for agreeing_supernode_pastelid in list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms:
        agreeing_supernode_pastelid_in_list_of_active_supernodes_at_block_height = agreeing_supernode_pastelid in list_of_active_supernode_pastelids_at_the_time
        validation_results["validation_checks"].append({
            "check_name": f"Agreeing supernode with pastelid {agreeing_supernode_pastelid} was in the list of active supernodes as of block height {credit_pack_purchase_request_response.request_response_pastel_block_height:,}",
            "is_valid": agreeing_supernode_pastelid_in_list_of_active_supernodes_at_block_height
        })
        if not agreeing_supernode_pastelid_in_list_of_active_supernodes_at_block_height:
            validation_results["credit_pack_ticket_is_valid"] = False
            validation_results["validation_failure_reasons_list"].append(f"Agreeing supernode with pastelid {agreeing_supernode_pastelid} was NOT in the list of active supernodes as of block height {credit_pack_purchase_request_response.request_response_pastel_block_height:,}")

async def validate_hashes(credit_pack_ticket_txid, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation, validation_results):
    credit_pack_purchase_request_response_transformed = parse_sqlmodel_strings_into_lists_and_dicts_func(credit_pack_purchase_request_response)
    credit_pack_purchase_request_confirmation_transformed = parse_sqlmodel_strings_into_lists_and_dicts_func(credit_pack_purchase_request_confirmation)
    validation_errors_in_credit_pack_purchase_request_response = await validate_credit_pack_blockchain_ticket_data_field_hashes(credit_pack_purchase_request_response_transformed)
    validation_errors_in_credit_pack_purchase_request_confirmation = await validate_credit_pack_blockchain_ticket_data_field_hashes(credit_pack_purchase_request_confirmation_transformed)
    if len(validation_errors_in_credit_pack_purchase_request_response) > 0:
        logger.warning(f"Warning! Computed hash does not match for ticket request response object for credit pack ticket with txid {credit_pack_ticket_txid}; Validation errors detected:\n{validation_errors_in_credit_pack_purchase_request_response}")
        validation_results["validation_checks"].append({
            "check_name": f"Computed hash does not match for ticket request response object for credit pack ticket with txid: {validation_errors_in_credit_pack_purchase_request_response}",
            "is_valid": False
        })
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append("Hash of credit pack request response object stored in blockchain does not match the hash included in the object.")
    if len(validation_errors_in_credit_pack_purchase_request_confirmation) > 0:
        logger.warning(f"Warning! Computed hash does not match for ticket request confirmation object for credit pack ticket with txid {credit_pack_ticket_txid}; Validation errors detected:\n{validation_errors_in_credit_pack_purchase_request_confirmation}")
        validation_results["validation_checks"].append({
            "check_name": f"Computed hash does not match for ticket request confirmation object for credit pack ticket with txid: {validation_errors_in_credit_pack_purchase_request_confirmation}",
            "is_valid": False
        })
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append("Hash of credit pack request confirmation object stored in blockchain does not match the hash included in the object.")

async def validate_signatures(credit_pack_ticket_txid, credit_pack_purchase_request_response, use_verbose_validation, validation_results):
    list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion = credit_pack_purchase_request_response.list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion
    if list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion:
        selected_agreeing_supernodes_signatures_dict = credit_pack_purchase_request_response.selected_agreeing_supernodes_signatures_dict
        for agreeing_supernode_pastelid in list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion:
            signatures = selected_agreeing_supernodes_signatures_dict[agreeing_supernode_pastelid]
            if use_verbose_validation:
                logger.info(f"Verifying signature for selected agreeing supernode {agreeing_supernode_pastelid}")
                logger.info(f"Message to verify (decoded from b64): {credit_pack_purchase_request_response.credit_pack_purchase_request_fields_json_b64}")
                logger.info(f"Signature: {signatures['credit_pack_purchase_request_fields_json_b64_signature']}")
            is_fields_json_b64_signature_valid = await verify_message_with_pastelid_func(
                agreeing_supernode_pastelid,
                credit_pack_purchase_request_response.credit_pack_purchase_request_fields_json_b64,
                signatures['credit_pack_purchase_request_fields_json_b64_signature']
            )
            if not is_fields_json_b64_signature_valid:
                logger.warning(f"Warning! Signature failed for SN {agreeing_supernode_pastelid} for credit pack with txid {credit_pack_ticket_txid}")
            if use_verbose_validation:
                logger.info(f"Signature validation result: {is_fields_json_b64_signature_valid}")
            validation_results["validation_checks"].append({
                "check_name": f"Signature validation for selected agreeing supernode {agreeing_supernode_pastelid} on credit pack purchase request fields json",
                "is_valid": is_fields_json_b64_signature_valid
            })
            if not is_fields_json_b64_signature_valid:
                validation_results["credit_pack_ticket_is_valid"] = False
                validation_results["validation_failure_reasons_list"].append(f"Signature failed for SN {agreeing_supernode_pastelid} for credit pack with txid {credit_pack_ticket_txid}")
    else:
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append("Required field 'list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion' is missing in credit pack purchase request response")

async def validate_selected_agreeing_supernodes(credit_pack_ticket_txid, credit_pack_purchase_request_response, validation_results):
    list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms = credit_pack_purchase_request_response.list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms
    best_block_merkle_root = credit_pack_purchase_request_response.best_block_merkle_root
    best_block_height = credit_pack_purchase_request_response.best_block_height
    if not best_block_merkle_root or not best_block_height:
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append(f"Missing best block merkle root or height for credit pack with txid {credit_pack_ticket_txid}")
        return
    selected_agreeing_supernodes = await select_top_n_closest_supernodes_to_best_block_merkle_root(
        list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms,
        n=10,
        best_block_merkle_root=best_block_merkle_root
    )
    best_block_merkle_root_matches = await validate_merkle_root_at_block_height(best_block_merkle_root, best_block_height)
    if not best_block_merkle_root_matches:
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append(f"Best block merkle root {best_block_merkle_root} does not match actual merkle root as of block height {best_block_height} for credit pack with txid {credit_pack_ticket_txid}")
    max_block_height_difference_between_best_block_height_and_request_response_pastel_block_height = 10
    actual_block_height_difference_between_best_block_height_and_request_response_pastel_block_height = abs(credit_pack_purchase_request_response.request_response_pastel_block_height - best_block_height)
    if actual_block_height_difference_between_best_block_height_and_request_response_pastel_block_height > max_block_height_difference_between_best_block_height_and_request_response_pastel_block_height:
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append(f"Block height difference between specified best block height {best_block_height} and request response pastel block height {credit_pack_purchase_request_response.request_response_pastel_block_height} exceeds the maximum allowed difference of {max_block_height_difference_between_best_block_height_and_request_response_pastel_block_height} for credit pack with txid {credit_pack_ticket_txid}")
    selected_agreeing_supernodes_set = set(credit_pack_purchase_request_response.list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion)
    computed_selected_agreeing_supernodes_set = set(selected_agreeing_supernodes)
    is_selected_agreeing_supernodes_valid = selected_agreeing_supernodes_set == computed_selected_agreeing_supernodes_set
    validation_results["validation_checks"].append({
        "check_name": "Validation of selected agreeing supernodes for signature inclusion",
        "is_valid": is_selected_agreeing_supernodes_valid,
        "expected_selected_agreeing_supernodes": list(computed_selected_agreeing_supernodes_set),
        "actual_selected_agreeing_supernodes": list(selected_agreeing_supernodes_set)
    })
    if not is_selected_agreeing_supernodes_valid:
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append("Selected agreeing supernodes for signature inclusion do not match the expected set based on XOR distance to the best block merkle root.")
    num_potentially_agreeing_supernodes = len(credit_pack_purchase_request_response.list_of_potentially_agreeing_supernodes)
    num_agreeing_supernodes = len(list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms)
    number_of_blacklisted_supernodes_at_the_time = len(credit_pack_purchase_request_response.list_of_blacklisted_supernode_pastelids)
    quorum_percentage = num_potentially_agreeing_supernodes / (num_potentially_agreeing_supernodes + number_of_blacklisted_supernodes_at_the_time)
    agreeing_percentage = num_agreeing_supernodes / num_potentially_agreeing_supernodes
    is_quorum_valid = quorum_percentage >= SUPERNODE_CREDIT_PRICE_AGREEMENT_QUORUM_PERCENTAGE
    is_agreeing_percentage_valid = agreeing_percentage >= SUPERNODE_CREDIT_PRICE_AGREEMENT_MAJORITY_PERCENTAGE
    validation_results["validation_checks"].append({
        "check_name": "Agreeing supernodes percentage validation",
        "is_valid": is_agreeing_percentage_valid and is_quorum_valid,
        "quorum_percentage": round(quorum_percentage, 5),
        "agreeing_percentage": round(agreeing_percentage, 5)
    })
    if not is_quorum_valid:
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append(f"Agreeing supernodes quorum percentage validation failed for credit pack with txid {credit_pack_ticket_txid}; quorum % was only {quorum_percentage:.3f} (i.e., {num_potentially_agreeing_supernodes} out of {(num_potentially_agreeing_supernodes + number_of_blacklisted_supernodes_at_the_time)}) and the minimum required % is {SUPERNODE_CREDIT_PRICE_AGREEMENT_QUORUM_PERCENTAGE:.3f}")
    if not is_agreeing_percentage_valid:
        validation_results["credit_pack_ticket_is_valid"] = False
        validation_results["validation_failure_reasons_list"].append(f"Agreeing supernodes agreement percentage validation failed for credit pack with txid {credit_pack_ticket_txid}; agreement % was only {agreeing_percentage:.3f} (i.e., {num_agreeing_supernodes} out of {num_potentially_agreeing_supernodes}) and the minimum required % is {SUPERNODE_CREDIT_PRICE_AGREEMENT_MAJORITY_PERCENTAGE:.3f}")

async def get_valid_credit_pack_tickets_for_pastelid_old(pastelid: str) -> List[dict]:
    async def process_request_confirmation(request_confirmation):
        txid = request_confirmation.txid_of_credit_purchase_burn_transaction
        logger.info(f'Getting credit pack registration txid for ticket with burn txid of {txid}...')
        registration_txid = await get_credit_pack_ticket_registration_txid_from_corresponding_burn_transaction_txid(txid)
        logger.info(f'Got credit pack registration txid for ticket with burn txid of {txid}: {registration_txid}')        
        credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation = await retrieve_credit_pack_ticket_from_blockchain_using_txid(registration_txid)
        if not all((credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation)):
            logger.warning(f"Incomplete credit pack ticket data in database for TXID {registration_txid}. Fetching from blockchain.")
            credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation = await retrieve_credit_pack_ticket_from_blockchain_using_txid(registration_txid)
            if all((credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation)):
                await save_credit_pack_ticket_to_database(credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation, registration_txid)
            else:
                logger.error(f"Failed to retrieve complete credit pack ticket data for TXID {txid} from blockchain")
                return None
        current_credit_balance, number_of_confirmation_transactions = await determine_current_credit_pack_balance_based_on_tracking_transactions(txid)
        complete_ticket = {
            "credit_pack_purchase_request": json.loads(base64.b64decode(credit_pack_purchase_request_response.credit_pack_purchase_request_fields_json_b64).decode('utf-8')),
            "credit_pack_purchase_request_response": credit_pack_purchase_request_response.model_dump(),
            "credit_pack_purchase_request_confirmation": credit_pack_purchase_request_confirmation.model_dump(),
            "credit_pack_registration_txid": registration_txid,
            "credit_pack_current_credit_balance": current_credit_balance,
            "balance_as_of_datetime": datetime.now(timezone.utc).isoformat()
        }
        complete_ticket = convert_uuids_to_strings(complete_ticket)
        complete_ticket = normalize_data(complete_ticket)
        async with db_code.Session() as db_session:
            existing_data = await db_session.exec(
                select(db_code.CreditPackCompleteTicketWithBalance)
                .where(db_code.CreditPackCompleteTicketWithBalance.credit_pack_ticket_registration_txid == registration_txid)
            )
            existing_data = existing_data.first()
            if existing_data:
                existing_data.complete_credit_pack_data_json = json.dumps(complete_ticket)
                existing_data.datetime_last_updated = datetime.now(timezone.utc)
                db_session.add(existing_data)
            else:
                new_complete_ticket = db_code.CreditPackCompleteTicketWithBalance(
                    credit_pack_ticket_registration_txid=registration_txid,
                    complete_credit_pack_data_json=json.dumps(complete_ticket),
                    datetime_last_updated=datetime.now(timezone.utc)
                )
                db_session.add(new_complete_ticket)
            await db_session.commit()
        return complete_ticket
    try:
        async with db_code.Session() as db_session:
            credit_pack_request_confirmations_results = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestConfirmation)
                .where(db_code.CreditPackPurchaseRequestConfirmation.requesting_end_user_pastelid == pastelid)
            )
            credit_pack_request_confirmations = credit_pack_request_confirmations_results.all()
        tasks = [process_request_confirmation(request_confirmation) for request_confirmation in credit_pack_request_confirmations]
        complete_tickets = await asyncio.gather(*tasks)
        complete_tickets = [ticket for ticket in complete_tickets if ticket]
        return complete_tickets
    except Exception as e:
        logger.error(f"Error retrieving credit pack tickets for PastelID {pastelid}: {str(e)}")
        traceback.print_exc()
        raise

@async_disk_cached(credit_pack_cache, ttl=CREDIT_BALANCE_CACHE_INVALIDATION_PERIOD_IN_SECONDS)
async def get_valid_credit_pack_tickets_for_pastelid_cacheable(pastelid: str) -> List[dict]:
    try:
        list_of_credit_pack_ticket_registration_txids, list_of_credit_pack_ticket_data = await get_list_of_credit_pack_ticket_txids_from_pastelid(pastelid)
        complete_validated_tickets_list = [None] * len(list_of_credit_pack_ticket_registration_txids)
        async def process_ticket(idx, current_credit_pack_ticket_registration_txid):
            logger.info(f"Attempting to validate credit pack ticket {idx+1} of {len(list_of_credit_pack_ticket_registration_txids)} for PastelID {pastelid}...")
            validation_results = await validate_existing_credit_pack_ticket(current_credit_pack_ticket_registration_txid)
            if validation_results and len(validation_results['validation_failure_reasons_list']) == 0:
                try:
                    ticket_data = await get_credit_pack_ticket_data(current_credit_pack_ticket_registration_txid, list_of_credit_pack_ticket_data, idx)
                    logger.info(f"Finished validating and parsing credit pack ticket data for PastelID {pastelid} with txid {current_credit_pack_ticket_registration_txid}!")
                    return idx, ticket_data
                except Exception as e:
                    logger.error(f"Error processing credit pack ticket data for PastelID {pastelid}: {str(e)}")
                    traceback.print_exc()
            return idx, None
        tasks = [process_ticket(idx, txid) for idx, txid in enumerate(list_of_credit_pack_ticket_registration_txids)]
        results = await asyncio.gather(*tasks)
        for idx, ticket_data in results:
            if ticket_data is not None:
                complete_validated_tickets_list[idx] = ticket_data
        # Remove None values while preserving order
        complete_validated_tickets_list = [ticket for ticket in complete_validated_tickets_list if ticket is not None]
        return complete_validated_tickets_list
    except Exception as e:
        logger.error(f"Error retrieving credit pack tickets for PastelID {pastelid}: {str(e)}")
        traceback.print_exc()
        raise

@async_disk_cached(credit_pack_cache, ttl=CREDIT_BALANCE_CACHE_INVALIDATION_PERIOD_IN_SECONDS)
async def get_credit_pack_current_balance(credit_pack_ticket_registration_txid: str) -> Tuple[float, int]:
    async def get_balance():
        return await determine_current_credit_pack_balance_based_on_tracking_transactions(credit_pack_ticket_registration_txid)
    try:
        # Pass the function `get_balance` directly without any arguments
        current_credit_balance, number_of_confirmation_transactions = await retry_on_database_locked(get_balance)
        return current_credit_balance, number_of_confirmation_transactions
    except Exception as e:
        logger.error(f"Error getting current balance for credit pack ticket with TXID {credit_pack_ticket_registration_txid}: {str(e)}")
        traceback.print_exc()
        raise

@async_disk_cached(credit_pack_cache, ttl=CREDIT_BALANCE_CACHE_INVALIDATION_PERIOD_IN_SECONDS)
async def get_valid_credit_pack_tickets_for_pastelid(pastelid: str) -> List[dict]:
    try:
        complete_validated_tickets_list = await get_valid_credit_pack_tickets_for_pastelid_cacheable(pastelid)
        async def update_ticket_balance(current_credit_pack_ticket):
            current_credit_pack_ticket_registration_txid = current_credit_pack_ticket['credit_pack_registration_txid']
            logger.info(f"Determining current credit balance for credit pack ticket with txid {current_credit_pack_ticket_registration_txid} for PastelID {pastelid} (results will be cached for {round(CREDIT_BALANCE_CACHE_INVALIDATION_PERIOD_IN_SECONDS/60.0,2)} minutes)...")
            try:
                current_credit_balance, number_of_confirmation_transactions = await get_credit_pack_current_balance(current_credit_pack_ticket_registration_txid)
                current_credit_pack_ticket['credit_pack_current_credit_balance'] = current_credit_balance
                current_credit_pack_ticket['number_of_confirmation_transactions'] = number_of_confirmation_transactions
                current_credit_pack_ticket['balance_as_of_datetime'] = datetime.now(timezone.utc).isoformat()
            except Exception as e:
                logger.error(f"Error updating balance for ticket {current_credit_pack_ticket_registration_txid}: {str(e)}")
                current_credit_pack_ticket['credit_pack_current_credit_balance'] = None
                current_credit_pack_ticket['number_of_confirmation_transactions'] = None
                current_credit_pack_ticket['balance_as_of_datetime'] = None
            return current_credit_pack_ticket
        updated_tickets = await asyncio.gather(*[update_ticket_balance(ticket) for ticket in complete_validated_tickets_list])
        return updated_tickets
    except Exception as e:
        logger.error(f"Error determining credit balance for credit pack tickets for PastelID {pastelid}: {str(e)}")
        traceback.print_exc()
        raise
        
async def save_credit_pack_ticket_to_database(credit_pack_purchase_request, credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation, txid):
    async with db_code.Session() as db_session:
        try:
            # First, check if we already have this ticket in our database
            existing_mapping = await db_session.exec(
                select(db_code.CreditPackPurchaseRequestResponseTxidMapping)
                .where(db_code.CreditPackPurchaseRequestResponseTxidMapping.pastel_api_credit_pack_ticket_registration_txid == txid)
            )
            existing_mapping = existing_mapping.one_or_none()

            if existing_mapping:
                # If we have this ticket, update the existing data
                existing_data = await db_session.exec(
                    select(db_code.CreditPackCompleteTicketWithBalance)
                    .where(db_code.CreditPackCompleteTicketWithBalance.credit_pack_ticket_registration_txid == txid)
                )
                existing_data = existing_data.one_or_none()

                if existing_data:
                    complete_ticket = json.loads(existing_data.complete_credit_pack_data_json)
                    current_credit_balance, number_of_confirmation_transactions = await determine_current_credit_pack_balance_based_on_tracking_transactions(txid)
                    complete_ticket['credit_pack_current_credit_balance'] = current_credit_balance
                    complete_ticket['balance_as_of_datetime'] = datetime.now(timezone.utc).isoformat()
                    complete_ticket_json = json.dumps(complete_ticket)

                    existing_data.complete_credit_pack_data_json = complete_ticket_json
                    existing_data.datetime_last_updated = datetime.now(timezone.utc)
                    db_session.add(existing_data)
            else:
                # If we don't have this ticket, create new entries
                current_credit_balance, number_of_confirmation_transactions = await determine_current_credit_pack_balance_based_on_tracking_transactions(txid)
                
                complete_ticket = {
                    "credit_pack_purchase_request": json.loads(base64.b64decode(credit_pack_purchase_request_response.credit_pack_purchase_request_fields_json_b64).decode('utf-8')),
                    "credit_pack_purchase_request_response": credit_pack_purchase_request_response.model_dump(),
                    "credit_pack_purchase_request_confirmation": credit_pack_purchase_request_confirmation.model_dump(),
                    "credit_pack_registration_txid": txid,
                    "credit_pack_current_credit_balance": current_credit_balance,
                    "balance_as_of_datetime": datetime.now(timezone.utc).isoformat()
                }
                complete_ticket = convert_uuids_to_strings(complete_ticket)
                complete_ticket = normalize_data(complete_ticket)
                complete_ticket_json = json.dumps(complete_ticket)

                new_complete_ticket = db_code.CreditPackCompleteTicketWithBalance(
                    credit_pack_ticket_registration_txid=txid,
                    complete_credit_pack_data_json=complete_ticket_json,
                    datetime_last_updated=datetime.now(timezone.utc)
                )
                db_session.add(new_complete_ticket)

                # Create new mapping
                new_mapping = db_code.CreditPackPurchaseRequestResponseTxidMapping(
                    sha3_256_hash_of_credit_pack_purchase_request_fields=credit_pack_purchase_request_response.sha3_256_hash_of_credit_pack_purchase_request_fields,
                    pastel_api_credit_pack_ticket_registration_txid=txid
                )
                db_session.add(new_mapping)

            # Commit the changes
            await db_session.commit()
            logger.info(f"Successfully saved/updated credit pack ticket data for TXID {txid} to database.")

        except Exception as e:
            logger.error(f"Error saving/updating retrieved credit pack ticket to the local database: {str(e)}")
            await db_session.rollback()
            raise

    await db_code.consolidate_wal_data()

        
#________________________________________________________________________________________________________________            
                
# Inference request related service functions:
                
async def get_inference_model_menu(use_verbose=0):
    try:
        # Load the API key test results from the file
        api_key_tests = load_api_key_tests()
        # Fetch the latest model menu from GitHub
        async with httpx.AsyncClient() as client:
            response = await client.get(GITHUB_MODEL_MENU_URL)
            response.raise_for_status()
            github_model_menu = response.json()
        # Test API keys and filter out models based on the availability of valid API keys
        filtered_model_menu = {"models": []}
        for model in github_model_menu["models"]:
            if model["model_name"].startswith("stability-"):
                if STABILITY_API_KEY and await is_api_key_valid("stability", api_key_tests):
                    filtered_model_menu["models"].append(model)
                    if use_verbose:
                        logger.info(f"Added Stability model: {model['model_name']} to the filtered model menu.")
            elif model["model_name"].startswith("openai-"):
                if OPENAI_API_KEY and await is_api_key_valid("openai", api_key_tests):
                    filtered_model_menu["models"].append(model)
                    if use_verbose:
                        logger.info(f"Added OpenAImodel: {model['model_name']} to the filtered model menu.")
            elif model["model_name"].startswith("mistralapi-"):
                if MISTRAL_API_KEY and await is_api_key_valid("mistral", api_key_tests):
                    filtered_model_menu["models"].append(model)
                    if use_verbose:
                        logger.info(f"Added MistralAPI model: {model['model_name']} to the filtered model menu.")
            elif model["model_name"].startswith("groq-"):
                if GROQ_API_KEY and await is_api_key_valid("groq", api_key_tests):
                    filtered_model_menu["models"].append(model)
                    if use_verbose:
                        logger.info(f"Added Groq API model: {model['model_name']} to the filtered model menu.")
            elif "claude" in model["model_name"].lower():
                if CLAUDE3_API_KEY and await is_api_key_valid("claude", api_key_tests):
                    filtered_model_menu["models"].append(model)
                    if use_verbose:
                        logger.info(f"Added Anthropic API model: {model['model_name']} to the filtered model menu.")
            elif "openrouter" in model["model_name"].lower():
                if OPENROUTER_API_KEY and await is_api_key_valid("openrouter", api_key_tests):
                    filtered_model_menu["models"].append(model)
                    if use_verbose:
                        logger.info(f"Added OpenRouter model: {model['model_name']} to the filtered model menu.")
            else:
                # Models that don't require API keys can be automatically included
                filtered_model_menu["models"].append(model)
        # Save the filtered model menu locally
        with open("model_menu.json", "w") as file:
            json.dump(filtered_model_menu, file, indent=2)
        save_api_key_tests(api_key_tests) # Save the updated API key test results to file
        return filtered_model_menu
    except Exception as e:
        logger.error(f"Error retrieving inference model menu: {str(e)}")
        traceback.print_exc()
        raise

def load_api_key_tests():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    api_key_test_json_file_path = os.path.join(current_dir, API_KEY_TESTS_FILE) 
    try:
        with open(api_key_test_json_file_path, "r") as file:
            return json.load(file)
    except FileNotFoundError:
        return {}

def save_api_key_tests(api_key_tests):
    current_dir = os.path.dirname(os.path.abspath(__file__))
    api_key_test_json_file_path = os.path.join(current_dir, API_KEY_TESTS_FILE)     
    with open(api_key_test_json_file_path, "w") as file:
        json.dump(api_key_tests, file, indent=2)

async def is_api_key_valid(api_name, api_key_tests):
    if api_name not in api_key_tests or not is_test_result_valid(api_key_tests[api_name]["timestamp"]):
        test_passed = await run_api_key_test(api_name)
        api_key_tests[api_name] = {
            "passed": test_passed,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        return test_passed
    else:
        return api_key_tests[api_name]["passed"]

def is_test_result_valid(test_timestamp):
    test_datetime = datetime.fromisoformat(test_timestamp).replace(tzinfo=timezone.utc)
    return (datetime.now(timezone.utc) - test_datetime) < timedelta(hours=API_KEY_TEST_VALIDITY_HOURS)

async def run_api_key_test(api_name):
    if api_name == "stability":
        return await test_stability_api_key()
    elif api_name == "openai":
        return await test_openai_api_key()
    elif api_name == "mistral":
        return await test_mistral_api_key()
    elif api_name == "groq":
        return await test_groq_api_key()
    elif api_name == "claude":
        return await test_claude_api_key()
    elif api_name == "openrouter":
        return await test_openrouter_api_key()    
    else:
        return False

async def test_openai_api_key():
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {OPENAI_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "gpt-3.5-turbo",
                    "messages": [{"role": "user", "content": "Test; just reply with the word yes if you're working!"}]
                }
            )
            return response.status_code == 200
    except Exception as e:
        logger.warning(f"OpenAI API key test failed: {str(e)}")
        return False
    
async def test_stability_api_key():
    try:
        engine_id = "stable-diffusion-v1-6"
        api_host = "https://api.stability.ai"
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{api_host}/v1/generation/{engine_id}/text-to-image",
                headers={
                    "Content-Type": "application/json",
                    "Accept": "application/json",
                    "Authorization": f"Bearer {STABILITY_API_KEY}"
                },
                json={
                    "text_prompts": [
                        {
                            "text": "A lighthouse on a cliff"
                        }
                    ],
                    "cfg_scale": 7,
                    "height": 512,
                    "width": 512,
                    "samples": 1,
                    "steps": 10,
                },
            )
            if response.status_code != 200:
                raise Exception("Non-200 response: " + str(response.text))
            data = response.json()
            if "artifacts" in data and len(data["artifacts"]) > 0:
                for artifact in data["artifacts"]:  # Directly iterating over the list of dictionaries
                    if artifact.get("finishReason") == "SUCCESS":
                        logger.info("Stability API test passed.")
                        return True
            logger.info("Stability API test failed!")                    
            return False
    except Exception as e:
        logger.warning(f"Stability API key test failed: {str(e)}")
        return False
    
async def test_openrouter_api_key():
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "openai/gpt-3.5-turbo",
                    "messages": [{"role": "user", "content": "Test; just reply with the word yes if you're working!"}]
                }
            )
            return response.status_code == 200
    except Exception as e:
        logger.warning(f"OpenRouter API key test failed: {str(e)}")
        return False    

async def test_mistral_api_key():
    try:
        client = MistralAsyncClient(api_key=MISTRAL_API_KEY)
        async_response = client.chat_stream(
            model="mistral-small-latest",
            messages=[ChatMessage(role="user", content="Test; just reply with the word yes if you're working!")],
            max_tokens=10,
            temperature=0.7,
        )
        completion_text = ""
        async for chunk in async_response:
            if chunk.choices[0].delta.content:
                completion_text += chunk.choices[0].delta.content
        logger.info(f"Mistral API test response: {completion_text}")                
        return len(completion_text) > 0
    except Exception as e:
        logger.warning(f"Mistral API key test failed: {str(e)}")
        return False

async def test_groq_api_key():
    try:
        client = AsyncGroq(api_key=GROQ_API_KEY)
        chat_completion = await client.chat.completions.create(
            messages=[{"role": "user", "content": "Test; just reply with the word yes if you're working!"}],
            model="mixtral-8x7b-32768",
            max_tokens=10,
            temperature=0.7,
        )
        response_string = chat_completion.choices[0].message.content.strip()
        logger.info(f"Groq API test response: {response_string}")
        if response_string is not None:
            test_passed = len(response_string) > 0
        else:
            test_passed = False
        return test_passed
    except Exception as e:
        logger.warning(f"Groq API key test failed: {str(e)}")
        return False

async def test_claude_api_key():
    try:
        client = anthropic.AsyncAnthropic(api_key=CLAUDE3_API_KEY)
        async with client.messages.stream(
            model="claude-3-haiku-20240307",
            max_tokens=10,
            temperature=0.7,
            messages=[{"role": "user", "content": "Test; just reply with the word yes if you're working!"}],
        ) as stream:
            message = await stream.get_final_message()
            response_string = message.content[0].text.strip()
            logger.info(f"Anthropic API test response: {response_string}")
            if response_string is not None:
                test_passed = len(response_string) > 0
            else:
                test_passed = False
            return test_passed
    except Exception as e:
        logger.warning(f"Claude API key test failed: {str(e)}")
        return False

async def save_inference_api_usage_request(inference_request_model: db_code.InferenceAPIUsageRequest) -> db_code.InferenceAPIUsageRequest:
    async with db_code.Session() as db_session:
        db_session.add(inference_request_model)
        await db_session.commit()
        await db_session.refresh(inference_request_model)
    return inference_request_model

def get_fallback_tokenizer():
    return "gpt2"  # Default to "gpt2" tokenizer as a fallback

def get_tokenizer(model_name: str):
    model_name = model_name.replace('swiss_army_llama-', '')
    model_to_tokenizer_mapping = {
        "claude3": "Xenova/claude-tokenizer",
        "phi": "TheBloke/phi-2-GGUF",
        "openai": "cl100k_base",
        "groq-llama3": "gradientai/Llama-3-70B-Instruct-Gradient-1048k",
        "groq-mixtral": "EleutherAI/gpt-neox-20b",
        "groq-gemma": "google/flan-ul2",
        "bge-m3": "Shitao/bge-m3",
        "mistralapi": "mistralai/Mistral-7B-Instruct-v0.2",
        "stability": "openai/clip-vit-large-patch14",
        "Lexi-Llama-3-8B-Uncensored_Q5_K_M": "gradientai/Llama-3-70B-Instruct-Gradient-1048k",
        "Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M": "gradientai/Llama-3-70B-Instruct-Gradient-1048k",
        "Meta-Llama-3-8B-Instruct.Q3_K_S": "gradientai/Llama-3-70B-Instruct-Gradient-1048k",
        "whisper": "openai/whisper-large-v2",
        "clip-interrogator": "openai/clip-vit-large-patch14",
        "videocap-transformer": "ArhanK005/videocap-transformer",
        "openrouter/google": "google/flan-t5-xl",
        "openrouter/anthropic": "Xenova/claude-tokenizer",
        "openrouter/meta-llama": "huggyllama/llama-7b",
        "openrouter/distilgpt2": "distilgpt2",
        "openrouter/bigscience/bloom": "bigscience/bloom-1b7",
        "openrouter/databricks/dolly-v2-12b": "databricks/dolly-v2-12b",
        "openrouter/EleutherAI/gpt-j-6b": "EleutherAI/gpt-j-6B",
        "openrouter/stabilityai/stablelm-tuned-alpha-7b": "stabilityai/stablelm-base-alpha-7b",
        "openrouter/togethercomputer/GPT-JT-6B-v1": "togethercomputer/GPT-JT-6B-v1",
        "openrouter/tiiuae/falcon-7b-instruct": "tiiuae/falcon-7b-instruct"
    }
    best_match = process.extractOne(model_name.lower(), model_to_tokenizer_mapping.keys())
    return model_to_tokenizer_mapping.get(best_match[0], get_fallback_tokenizer())  # Default to fallback tokenizer if no match found

def count_tokens(model_name: str, input_data: str) -> int:
    tokenizer_name = get_tokenizer(model_name)
    logger.info(f"Selected tokenizer {tokenizer_name} for model {model_name}")
    try:
        if 'claude' in model_name.lower():
            tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer_name)
        elif 'whisper' in model_name.lower():
            tokenizer = WhisperTokenizer.from_pretrained(tokenizer_name)
        elif 'clip-interrogator' in model_name.lower() or 'stability' in model_name.lower():
            return 0
        elif 'videocap-transformer' in model_name.lower():
            tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        elif 'openai' in model_name.lower():
            encoding = tiktoken.get_encoding(tokenizer_name)
            input_tokens = encoding.encode(input_data)
            return len(input_tokens)
        else:
            tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        if hasattr(tokenizer, "encode"):
            input_tokens = tokenizer.encode(input_data)
        else:
            input_tokens = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(input_data))
        return len(input_tokens)
    except Exception as e:
        logger.error(f"Failed to load tokenizer {tokenizer_name} for model {model_name}: {e}")
        fallback_tokenizer_name = get_fallback_tokenizer()
        logger.info(f"Falling back to tokenizer {fallback_tokenizer_name}")
        if 'openai' in fallback_tokenizer_name.lower():
            encoding = tiktoken.get_encoding(fallback_tokenizer_name)
            input_tokens = encoding.encode(input_data)
        else:
            tokenizer = AutoTokenizer.from_pretrained(fallback_tokenizer_name)
            if hasattr(tokenizer, "encode"):
                input_tokens = tokenizer.encode(input_data)
            else:
                input_tokens = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(input_data))
        return len(input_tokens)

def calculate_api_cost(model_name: str, input_data: str, model_parameters: Dict) -> float:
    # Define the pricing data for each API service and model
    logger.info(f"Evaluating API cost for model: {model_name}")
    pricing_data = {
        "claude-2.1": {"input_cost": 0.008, "output_cost": 0.024, "per_call_cost": 0.0128},
        "claude3-haiku": {"input_cost": 0.00025, "output_cost": 0.00125, "per_call_cost": 0.0006},
        "claude3.5-sonnet": {"input_cost": 0.0003, "output_cost": 0.0015, "per_call_cost": 0.0},
        "claude3-opus": {"input_cost": 0.0015, "output_cost": 0.0075, "per_call_cost": 0.0},
        "mistralapi-mistral-small-latest": {"input_cost": 0.002, "output_cost": 0.006, "per_call_cost": 0.0032},
        "mistralapi-mistral-medium-latest": {"input_cost": 0.0027, "output_cost": 0.0081, "per_call_cost": 0},
        "mistralapi-mistral-large-latest": {"input_cost": 0.008, "output_cost": 0.024, "per_call_cost": 0.0128},
        "mistralapi-mistral-7b": {"input_cost": 0.00025, "output_cost": 0.00025, "per_call_cost": 0},
        "mistralapi-open-mistral-7b": {"input_cost": 0.00025, "output_cost": 0.00025, "per_call_cost": 0},
        "mistralapi-open-mixtral-8x7b": {"input_cost": 0.0007, "output_cost": 0.0007, "per_call_cost": 0},
        "mistralapi-mistral-embed": {"input_cost": 0.0001, "output_cost": 0, "per_call_cost": 0},
        "openai-gpt-4o": {"input_cost": 0.005, "output_cost": 0.015, "per_call_cost": 0.0},
        "openai-gpt-4o-2024-05-13": {"input_cost": 0.005, "output_cost": 0.015, "per_call_cost": 0.0},
        "openai-gpt-4o-vision": {"input_cost": 0.005, "output_cost": 0.015, "per_call_cost": 0.0},
        "openai-gpt-4-turbo": {"input_cost": 0.01, "output_cost": 0.03, "per_call_cost": 0.0},
        "openai-gpt-4-turbo-2024-04-09": {"input_cost": 0.01, "output_cost": 0.03, "per_call_cost": 0.0},
        "openai-gpt-4": {"input_cost": 0.03, "output_cost": 0.06, "per_call_cost": 0.0},
        "openai-gpt-4-32k": {"input_cost": 0.06, "output_cost": 0.12, "per_call_cost": 0.0},
        "openai-gpt-3.5-turbo-0125": {"input_cost": 0.0005, "output_cost": 0.0015, "per_call_cost": 0.0},
        "openai-gpt-3.5-turbo-instruct": {"input_cost": 0.0015, "output_cost": 0.002, "per_call_cost": 0.0},
        "openai-gpt-3.5-turbo-16k": {"input_cost": 0.003, "output_cost": 0.004, "per_call_cost": 0.0},
        "openai-gpt-4-0125-preview": {"input_cost": 0.01, "output_cost": 0.03, "per_call_cost": 0.0},
        "openai-gpt-4-1106-preview": {"input_cost": 0.01, "output_cost": 0.03, "per_call_cost": 0.0},
        "openai-gpt-4-vision-preview": {"input_cost": 0.01, "output_cost": 0.03, "per_call_cost": 0.0},
        "openai-text-embedding-ada-002": {"input_cost": 0.0004, "output_cost": 0.0, "per_call_cost": 0.0},
        "groq-llama3-70b-8192": {"input_cost": 0.0007, "output_cost": 0.0008, "per_call_cost": 0},
        "groq-llama3-8b-8192": {"input_cost": 0.0001, "output_cost": 0.0001, "per_call_cost": 0},
        "groq-mixtral-8x7b-32768": {"input_cost": 0.00027, "output_cost": 0.00027, "per_call_cost": 0},
        "groq-gemma-7b-it": {"input_cost": 0.0001, "output_cost": 0.0001, "per_call_cost": 0},
        "stability-core": {"credits_per_call": 3},
        "stability-ultra": {"credits_per_call": 8},
        "stability-sd3-medium": {"credits_per_call": 3.5},
        "stability-sd3-large": {"credits_per_call": 6.5},
        "stability-sd3-large-turbo": {"credits_per_call": 4},
        "stability-sdxl-1.0": {"credits_per_call": 0.4},  # Average of 0.2-0.6
        "stability-sd-1.6": {"credits_per_call": 0.6},  # Average of 0.2-1.0
        "stability-creative-upscaler": {"credits_per_call": 25},
        "stability-conservative-upscaler": {"credits_per_call": 25},
        "stability-esrgan": {"credits_per_call": 0.2},
        "stability-search-and-replace": {"credits_per_call": 4},
        "stability-inpaint": {"credits_per_call": 3},
        "stability-erase": {"credits_per_call": 3},
        "stability-outpaint": {"credits_per_call": 4},
        "stability-remove-background": {"credits_per_call": 2},
        "stability-search-and-recolor": {"credits_per_call": 5},
        "stability-structure": {"credits_per_call": 3},
        "stability-sketch": {"credits_per_call": 3},
        "stability-style": {"credits_per_call": 4},
        "stability-video": {"credits_per_call": 20},
        "stability-fast-3d": {"credits_per_call": 2},
        "openrouter/auto": {"input_cost": 0.0005, "output_cost": 0.0015, "per_call_cost": 0},
        "openrouter/nousresearch/nous-capybara-7b:free": {"input_cost": 0, "output_cost": 0, "per_call_cost": 0},
        "openrouter/mistralai/mistral-7b-instruct:free": {"input_cost": 0, "output_cost": 0, "per_call_cost": 0},
        "openrouter/gryphe/mythomist-7b:free": {"input_cost": 0, "output_cost": 0, "per_call_cost": 0},
        "openrouter/undi95/toppy-m-7b:free": {"input_cost": 0, "output_cost": 0, "per_call_cost": 0},
        "openrouter/cinematika-7b:free": {"input_cost": 0, "output_cost": 0, "per_call_cost": 0},
        "openrouter/google/gemma-7b-it:free": {"input_cost": 0, "output_cost": 0, "per_call_cost": 0},
        "openrouter/jebcarter/psyfighter-13b": {"input_cost": 0.001, "output_cost": 0.001, "per_call_cost": 0},
        "openrouter/koboldai/psyfighter-13b-2": {"input_cost": 0.001, "output_cost": 0.001, "per_call_cost": 0},
        "openrouter/intel/neural-chat-7b": {"input_cost": 0.005, "output_cost": 0.005, "per_call_cost": 0},
        "openrouter/haotian-liu/llava-13b": {"input_cost": 0.005, "output_cost": 0.005, "per_call_cost": 0},
        "openrouter/nousresearch/nous-hermes-2-vision-7b": {"input_cost": 0.005, "output_cost": 0.005, "per_call_cost": 0},
        "openrouter/meta-llama/llama-2-13b-chat": {"input_cost": 0.0001474, "output_cost": 0.0001474, "per_call_cost": 0},
        "openrouter/migtissera/synthia-70b": {"input_cost": 0.00375, "output_cost": 0.00375, "per_call_cost": 0},
        "openrouter/pygmalionai/mythalion-13b": {"input_cost": 0.001125, "output_cost": 0.001125, "per_call_cost": 0},
        "openrouter/xwin-lm/xwin-lm-70b": {"input_cost": 0.00375, "output_cost": 0.00375, "per_call_cost": 0},
        "openrouter/alpindale/goliath-120b": {"input_cost": 0.009375, "output_cost": 0.009375, "per_call_cost": 0},
        "openrouter/neversleep/noromaid-20b": {"input_cost": 0.00225, "output_cost": 0.00225, "per_call_cost": 0},
        "openrouter/gryphe/mythomist-7b": {"input_cost": 0.000375, "output_cost": 0.000375, "per_call_cost": 0},
        "openrouter/sophosympatheia/midnight-rose-70b": {"input_cost": 0.009, "output_cost": 0.009, "per_call_cost": 0},
        "openrouter/undi95/remm-slerp-l2-13b:extended": {"input_cost": 0.001125, "output_cost": 0.001125, "per_call_cost": 0},
        "openrouter/gryphe/mythomax-l2-13b:extended": {"input_cost": 0.001125, "output_cost": 0.001125, "per_call_cost": 0},
        "openrouter/mancer/weaver": {"input_cost": 0.003375, "output_cost": 0.003375, "per_call_cost": 0},
        "openrouter/nousresearch/nous-hermes-llama2-13b": {"input_cost": 0.00027, "output_cost": 0.00027, "per_call_cost": 0},
        "openrouter/nousresearch/nous-capybara-7b": {"input_cost": 0.00018, "output_cost": 0.00018, "per_call_cost": 0},
        "openrouter/meta-llama/codellama-34b-instruct": {"input_cost": 0.00072, "output_cost": 0.00072, "per_call_cost": 0},
        "openrouter/codellama/codellama-70b-instruct": {"input_cost": 0.00081, "output_cost": 0.00081, "per_call_cost": 0},
        "openrouter/phind/phind-codellama-34b": {"input_cost": 0.00072, "output_cost": 0.00072, "per_call_cost": 0},
        "openrouter/teknium/openhermes-2-mistral-7b": {"input_cost": 0.00018, "output_cost": 0.00018, "per_call_cost": 0},
        "openrouter/teknium/openhermes-2.5-mistral-7b": {"input_cost": 0.00018, "output_cost": 0.00018, "per_call_cost": 0},
        "openrouter/undi95/remm-slerp-l2-13b": {"input_cost": 0.00027, "output_cost": 0.00027, "per_call_cost": 0},
        "openrouter/undi95/toppy-m-7b": {"input_cost": 0.00018, "output_cost": 0.00018, "per_call_cost": 0},
        "openrouter/cinematika-7b": {"input_cost": 0.00018, "output_cost": 0.00018, "per_call_cost": 0},
        "openrouter/01-ai/yi-34b-chat": {"input_cost": 0.00072, "output_cost": 0.00072, "per_call_cost": 0},
        "openrouter/01-ai/yi-34b": {"input_cost": 0.00072, "output_cost": 0.00072, "per_call_cost": 0},
        "openrouter/01-ai/yi-6b": {"input_cost": 0.000126, "output_cost": 0.000126, "per_call_cost": 0},
        "openrouter/togethercomputer/stripedhyena-nous-7b": {"input_cost": 0.00018, "output_cost": 0.00018, "per_call_cost": 0},
        "openrouter/togethercomputer/stripedhyena-hessian-7b": {"input_cost": 0.00018, "output_cost": 0.00018, "per_call_cost": 0},
        "openrouter/mistralai/mixtral-8x7b": {"input_cost": 0.00054, "output_cost": 0.00054, "per_call_cost": 0},
        "openrouter/nousresearch/nous-hermes-yi-34b": {"input_cost": 0.00072, "output_cost": 0.00072, "per_call_cost": 0},
        "openrouter/nousresearch/nous-hermes-2-mixtral-8x7b-sft": {"input_cost": 0.00054, "output_cost": 0.00054, "per_call_cost": 0},
        "openrouter/nousresearch/nous-hermes-2-mistral-7b-dpo": {"input_cost": 0.00018, "output_cost": 0.00018, "per_call_cost": 0},
        "openrouter/mistralai/mixtral-8x22b": {"input_cost": 0.00108, "output_cost": 0.00108, "per_call_cost": 0},
        "openrouter/open-orca/mistral-7b-openorca": {"input_cost": 0.0001425, "output_cost": 0.0001425, "per_call_cost": 0},
        "openrouter/huggingfaceh4/zephyr-7b-beta": {"input_cost": 0.0001425, "output_cost": 0.0001425, "per_call_cost": 0},
        "openrouter/openai/gpt-3.5-turbo": {"input_cost": 0.0005, "output_cost": 0.0015, "per_call_cost": 0},
        "openrouter/openai/gpt-3.5-turbo-0125": {"input_cost": 0.0005, "output_cost": 0.0015, "per_call_cost": 0},
        "openrouter/openai/gpt-3.5-turbo-16k": {"input_cost": 0.003, "output_cost": 0.004, "per_call_cost": 0},
        "openrouter/openai/gpt-4-turbo": {"input_cost": 0.01, "output_cost": 0.03, "per_call_cost": 0},
        "openrouter/openai/gpt-4-turbo-preview": {"input_cost": 0.01, "output_cost": 0.03, "per_call_cost": 0},
        "openrouter/openai/gpt-4": {"input_cost": 0.03, "output_cost": 0.06, "per_call_cost": 0},
        "openrouter/openai/gpt-4-32k": {"input_cost": 0.06, "output_cost": 0.12, "per_call_cost": 0},
        "openrouter/openai/gpt-4-vision-preview": {"input_cost": 0.01, "output_cost": 0.03, "per_call_cost": 0},
        "openrouter/openai/gpt-3.5-turbo-instruct": {"input_cost": 0.0015, "output_cost": 0.002, "per_call_cost": 0},
        "openrouter/google/palm-2-chat-bison": {"input_cost": 0.00025, "output_cost": 0.0005, "per_call_cost": 0},
        "openrouter/google/palm-2-codechat-bison": {"input_cost": 0.00025, "output_cost": 0.0005, "per_call_cost": 0},
        "openrouter/google/palm-2-chat-bison-32k": {"input_cost": 0.00025, "output_cost": 0.0005, "per_call_cost": 0},
        "openrouter/google/palm-2-codechat-bison-32k": {"input_cost": 0.00025, "output_cost": 0.0005, "per_call_cost": 0},
        "openrouter/google/gemini-pro": {"input_cost": 0.000125, "output_cost": 0.000375, "per_call_cost": 0},
        "openrouter/google/gemini-pro-vision": {"input_cost": 0.000125, "output_cost": 0.000375, "per_call_cost": 0},
        "openrouter/google/gemini-pro-1.5": {"input_cost": 0.0025, "output_cost": 0.0075, "per_call_cost": 0},
        "openrouter/anthropic/claude-3-opus": {"input_cost": 0.015, "output_cost": 0.075, "per_call_cost": 0},
        "openrouter/anthropic/claude-3-sonnet": {"input_cost": 0.003, "output_cost": 0.015, "per_call_cost": 0},
        "openrouter/anthropic/claude-3-haiku": {"input_cost": 0.00025, "output_cost": 0.00125, "per_call_cost": 0},
        "openrouter/anthropic/claude-3-opus:beta": {"input_cost": 0.015, "output_cost": 0.075, "per_call_cost": 0},
        "openrouter/anthropic/claude-3-sonnet:beta": {"input_cost": 0.003, "output_cost": 0.015, "per_call_cost": 0},
        "openrouter/anthropic/claude-3-haiku:beta": {"input_cost": 0.00025, "output_cost": 0.00125, "per_call_cost": 0},
        "openrouter/meta-llama/llama-2-70b-chat": {"input_cost": 0.0007, "output_cost": 0.0009, "per_call_cost": 0},
        "openrouter/nousresearch/nous-capybara-34b": {"input_cost": 0.0009, "output_cost": 0.0009, "per_call_cost": 0},
        "openrouter/jondurbin/airoboros-l2-70b": {"input_cost": 0.0007, "output_cost": 0.0009, "per_call_cost": 0},
        "openrouter/jondurbin/bagel-34b": {"input_cost": 0.00575, "output_cost": 0.00575, "per_call_cost": 0},
        "openrouter/austism/chronos-hermes-13b": {"input_cost": 0.00022, "output_cost": 0.00022, "per_call_cost": 0},
        "openrouter/mistralai/mistral-7b-instruct": {"input_cost": 0.00013, "output_cost": 0.00013, "per_call_cost": 0},
        "openrouter/gryphe/mythomax-l2-13b": {"input_cost": 0.0002, "output_cost": 0.0002, "per_call_cost": 0},
        "openrouter/openchat/openchat-7b": {"input_cost": 0.00013, "output_cost": 0.00013, "per_call_cost": 0},
        "openrouter/lizpreciatior/lzlv-70b-fp16-hf": {"input_cost": 0.0007, "output_cost": 0.0008, "per_call_cost": 0},
        "openrouter/mistralai/mixtral-8x7b-instruct": {"input_cost": 0.00027, "output_cost": 0.00027, "per_call_cost": 0},
        "openrouter/cognitivecomputations/dolphin-mixtral-8x7b": {"input_cost": 0.0005, "output_cost": 0.0005, "per_call_cost": 0},
        "openrouter/neversleep/noromaid-mixtral-8x7b-instruct": {"input_cost": 0.008, "output_cost": 0.008, "per_call_cost": 0},        
    }
    # Find the best match for the model name using fuzzy string matching
    best_match = process.extractOne(model_name.lower(), pricing_data.keys())
    if best_match is None or best_match[1] < 60:  # Adjust the threshold as needed
        logger.warning(f"No pricing data found for model: {model_name}")
        return 0.0
    model_pricing = pricing_data[best_match[0]]
    if model_name.startswith("stability-"):
        # For Stability models, calculate the cost based on the credits per call
        number_of_completions_to_generate = model_parameters.get("number_of_completions_to_generate", 1)
        credits_cost = model_pricing["credits_per_call"] * number_of_completions_to_generate
        estimated_cost = credits_cost * 10 / 1000  # Convert credits to dollars ($10 per 1,000 credits)
    elif model_name.endswith("4o-vision"):
        if input_data is None:
            logger.error("Input data is empty!")
        input_data_dict = json.loads(input_data)
        image_data_binary = base64.b64decode(input_data_dict["image"]) # Decode image data and question from input_data
        question = input_data_dict["question"]
        # Calculate image resolution
        image = Image.open(io.BytesIO(image_data_binary))
        width, height = image.size
        image_file_size_in_mb = len(image_data_binary) / (1024 * 1024)  # Calculate image file size in MB
        logger.info(f"Submitted image file is {image_file_size_in_mb:.2f}MB and has resolution of {width} x {height}")
        # Resize logic to fit the larger dimension to 1286 pixels while maintaining aspect ratio
        target_larger_dimension = 1286
        aspect_ratio = width / height
        if width > height:
            resized_width = target_larger_dimension
            resized_height = int(target_larger_dimension / aspect_ratio)
        else:
            resized_height = target_larger_dimension
            resized_width = int(target_larger_dimension * aspect_ratio)
        logger.info(f"Image will be resized automatically by OpenAI to a resolution of {resized_width} x {resized_height}")
        # Calculate number of tiles and tokens
        tiles_x = -(-resized_width // 512)
        tiles_y = -(-resized_height // 512)
        total_tiles = tiles_x * tiles_y
        base_tokens = 85
        tile_tokens = 170 * total_tiles
        total_tokens = base_tokens + tile_tokens
        # Count tokens in the question
        question_tokens = count_tokens(model_name, question)
        input_data_tokens = total_tokens + question_tokens
        logger.info(f"Total input data tokens: {input_data_tokens}")
        number_of_tokens_to_generate = model_parameters.get("number_of_tokens_to_generate", 1000)
        number_of_completions_to_generate = model_parameters.get("number_of_completions_to_generate", 1)
        input_cost = float(model_pricing["input_cost"]) * float(input_data_tokens) / 1000.0
        output_cost = float(model_pricing["output_cost"]) * float(number_of_tokens_to_generate) / 1000.0
        per_call_cost = float(model_pricing["per_call_cost"]) * float(number_of_completions_to_generate)
        estimated_cost = input_cost + output_cost + per_call_cost
    else:
        # For other models, calculate the cost based on input/output tokens and per-call cost
        input_data_tokens = count_tokens(model_name, input_data)
        logger.info(f"Total input data tokens: {input_data_tokens}")
        number_of_tokens_to_generate = model_parameters.get("number_of_tokens_to_generate", 1000)
        number_of_completions_to_generate = model_parameters.get("number_of_completions_to_generate", 1)
        input_cost = float(model_pricing["input_cost"]) * float(input_data_tokens) / 1000.0
        output_cost = float(model_pricing["output_cost"]) * float(number_of_tokens_to_generate) / 1000.0
        per_call_cost = float(model_pricing["per_call_cost"]) * float(number_of_completions_to_generate)
        estimated_cost = input_cost + output_cost + per_call_cost
    logger.info(f"Estimated cost: ${estimated_cost:.4f}")
    return estimated_cost

async def convert_document_to_sentences(file_content: bytes, tried_local=False) -> Dict:
    logger.info("Now calling Swiss Army Llama to convert document to sentences.")
    local_swiss_army_llama_responding = is_swiss_army_llama_responding(local=True)
    if USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
        remote_swiss_army_llama_responding = is_swiss_army_llama_responding(local=False)
    else:
        remote_swiss_army_llama_responding = 0
    if remote_swiss_army_llama_responding and USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
        port = REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT
    elif local_swiss_army_llama_responding:
        port = SWISS_ARMY_LLAMA_PORT
    else:
        logger.error(f"Neither the local Swiss Army Llama (supposed to be running on port {SWISS_ARMY_LLAMA_PORT}) nor the remote Swiss Army Llama (supposed to be running, if enabled, on mapped port {REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT}) is responding!")
        raise ValueError("Swiss Army Llama is not responding.")
    metadata = await upload_and_get_file_metadata(file_content, "document")
    file_url = metadata["file_url"]
    file_hash = metadata["file_hash"]
    file_size = metadata["file_size"]
    url = f"http://localhost:{port}/convert_document_to_sentences/"
    async with httpx.AsyncClient(timeout=60) as client:
        try:
            response = await client.post(url, data={
                "url": file_url,
                "hash": file_hash,
                "size": file_size,
                "token": SWISS_ARMY_LLAMA_SECURITY_TOKEN
            })
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"Failed to convert document to sentences: {e}")
            if port == REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT and not tried_local:
                logger.info("Falling back to local Swiss Army Llama.")
                return await convert_document_to_sentences(file_content, tried_local=True)
            raise ValueError("Error converting document to sentences")
            
async def calculate_proposed_inference_cost_in_credits(requested_model_data: Dict, model_parameters: Dict, model_inference_type_string: str, input_data: str) -> float:
    model_name = requested_model_data["model_name"]
    if 'swiss_army_llama-' not in model_name:
        api_cost = calculate_api_cost(model_name, input_data, model_parameters)
        if api_cost > 0.0:
            target_value_per_credit = TARGET_VALUE_PER_CREDIT_IN_USD
            target_profit_margin = TARGET_PROFIT_MARGIN
            proposed_cost_in_credits = api_cost / (target_value_per_credit * (1 - target_profit_margin))
            final_proposed_cost_in_credits = max([MINIMUM_COST_IN_CREDITS, round(proposed_cost_in_credits, 1)])
            logger.info(f"Proposed cost in credits (API-based): {final_proposed_cost_in_credits}")
            return final_proposed_cost_in_credits
    else:
        credit_costs = requested_model_data["credit_costs"][model_inference_type_string]
        if model_inference_type_string == "ask_question_about_an_image":
            input_tokens = 3000
        else:
            input_tokens = count_tokens(model_name, input_data) if model_inference_type_string != "embedding_document" else 0
        compute_cost = float(credit_costs["compute_cost"])
        memory_cost = float(credit_costs["memory_cost"])
        if model_inference_type_string == "text_completion" or model_inference_type_string == "ask_question_about_an_image":
            output_token_cost = float(credit_costs["output_tokens"])
            number_of_tokens_to_generate = int(model_parameters.get("number_of_tokens_to_generate", 1000))
            number_of_completions_to_generate = int(model_parameters.get("number_of_completions_to_generate", 1))
            estimated_output_tokens = number_of_tokens_to_generate
            proposed_cost_in_credits = number_of_completions_to_generate * (
                (input_tokens * float(credit_costs["input_tokens"])) +
                (estimated_output_tokens * output_token_cost) +
                compute_cost
            ) + memory_cost
        elif model_inference_type_string == "embedding_document":
            if is_base64_encoded(input_data):
                input_data = base64.b64decode(input_data)
                input_data = input_data.decode('utf-8')
            try:
                input_data_dict = json.loads(input_data)
                document_file_data = input_data_dict['document']
                if is_base64_encoded(document_file_data):
                    document_file_data = base64.b64decode(document_file_data)
                document_stats = await convert_document_to_sentences(document_file_data)
                sentences = document_stats["individual_sentences"]
                total_sentences = document_stats["total_number_of_sentences"]
                concatenated_sentences = " ".join(sentences)
                total_tokens = count_tokens(model_name, concatenated_sentences)
                proposed_cost_in_credits = (
                    (total_tokens * float(credit_costs["average_tokens_per_sentence"])) +
                    (total_sentences * float(credit_costs["total_sentences"])) +
                    (1 if model_parameters.get("query_string") else 0) * float(credit_costs["query_string_included"]) +
                    compute_cost +
                    memory_cost
                )
            except Exception as e:
                logger.error(f"Error parsing document data from input: {str(e)}")
                traceback.print_exc()
                raise
        elif model_inference_type_string == "embedding_audio":
            if is_base64_encoded(input_data):
                input_data = base64.b64decode(input_data)
                input_data = input_data.decode('utf-8')
            try:
                input_data_dict = json.loads(input_data)
                audio_file_data = input_data_dict['audio']
                if is_base64_encoded(audio_file_data):
                    audio_file_data = base64.b64decode(audio_file_data)            
                audio_length_seconds = get_audio_length(audio_file_data)
                proposed_cost_in_credits = (
                    (1 if model_parameters.get("query_string") else 0) * float(credit_costs["query_string_included"]) +
                    (audio_length_seconds * float(credit_costs["audio_file_length_in_seconds"])) +
                    compute_cost +
                    memory_cost
                )
            except Exception as e:
                logger.error(f"Error parsing document data from input: {str(e)}")
                traceback.print_exc()
                raise
        final_proposed_cost_in_credits = round(proposed_cost_in_credits * CREDIT_COST_MULTIPLIER_FACTOR, 1)
        final_proposed_cost_in_credits = max([MINIMUM_COST_IN_CREDITS, final_proposed_cost_in_credits])
        logger.info(f"Proposed cost in credits (local LLM): {final_proposed_cost_in_credits}")
        return final_proposed_cost_in_credits

async def fetch_current_psl_market_price():
    async def check_prices():
        async with httpx.AsyncClient() as client:
            try:
                # Fetch data from CoinMarketCap
                response_cmc = await client.get("https://coinmarketcap.com/currencies/pastel/")
                price_cmc = float(re.search(r'price today is \$([0-9\.]+) USD', response_cmc.text).group(1))
            except (httpx.RequestError, AttributeError, ValueError):
                price_cmc = None
            try:
                # Fetch data from CoinGecko
                response_cg = await client.get("https://api.coingecko.com/api/v3/simple/price?ids=pastel&vs_currencies=usd")
                if response_cg.status_code == 200:
                    data = response_cg.json()
                    price_cg = data.get("pastel", {}).get("usd")
                else:
                    price_cg = None
            except (httpx.RequestError, AttributeError, ValueError):
                price_cg = None
        return price_cmc, price_cg
    price_cmc, price_cg = await check_prices()
    if price_cmc is None and price_cg is None:
        #Sleep for a couple seconds and try again:
        await asyncio.sleep(2)
        price_cmc, price_cg = await check_prices()
    # Calculate the average price
    prices = [price for price in [price_cmc, price_cg] if price is not None]
    if not prices:
        raise ValueError("Could not retrieve PSL price from any source.")
    average_price = sum(prices) / len(prices)
    # Validate the price
    if not 0.0000001 < average_price < 0.02:
        raise ValueError(f"Invalid PSL price: {average_price}")
    logger.info(f"The current Average PSL price is: ${average_price:.8f} based on {len(prices)} sources")
    return average_price

async def estimated_market_price_of_inference_credits_in_psl_terms() -> float:
    try:
        psl_price_usd = await fetch_current_psl_market_price()
        target_value_per_credit_usd = TARGET_VALUE_PER_CREDIT_IN_USD
        target_profit_margin = TARGET_PROFIT_MARGIN
        # Calculate the cost per credit in USD, considering the profit margin
        cost_per_credit_usd = target_value_per_credit_usd / (1 - target_profit_margin)
        # Convert the cost per credit from USD to PSL
        cost_per_credit_psl = cost_per_credit_usd / psl_price_usd
        logger.info(f"Estimated market price of 1.0 inference credit: {cost_per_credit_psl:.4f} PSL")
        return cost_per_credit_psl
    except (ValueError, ZeroDivisionError) as e:
        logger.error(f"Error calculating estimated market price of inference credits: {str(e)}")
        raise

def normalize_string(s):
    """Remove non-alphanumeric characters and convert to lowercase."""
    return re.sub(r'\W+', '', s).lower()

def validate_pastel_txid_string(input_string: str):
    # Sample txid: 625694b632a05f5df8d70904b9b3ff03d144aec0352b2290a275379586daf8db
    return re.match(r'^[0-9a-fA-F]{64}$', input_string) is not None

def is_swiss_army_llama_responding(local=True):
    port = SWISS_ARMY_LLAMA_PORT if local else REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT
    try:
        url = f"http://localhost:{port}/get_list_of_available_model_names/"
        params = {'token': SWISS_ARMY_LLAMA_SECURITY_TOKEN}
        response = httpx.get(url, params=params)
        return response.status_code == 200
    except Exception as e:
        logger.error("Error: {}".format(e))
        return False

async def check_if_input_text_would_get_rejected_from_api_services(input_text: str) -> bool:
    inference_request_allowed = True
    local_swiss_army_llama_responding = is_swiss_army_llama_responding(local=True)
    if USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
        remote_swiss_army_llama_responding = is_swiss_army_llama_responding(local=False)
    else:
        remote_swiss_army_llama_responding = 0
    if remote_swiss_army_llama_responding and USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
        port = REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT
    elif local_swiss_army_llama_responding:
        port = SWISS_ARMY_LLAMA_PORT
    else:
        logger.error(f"Neither the local Swiss Army Llama (supposed to be running on port {SWISS_ARMY_LLAMA_PORT}) nor the remote Swiss Army Llama (supposed to be running, if enabled, on mapped port {REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT}) is responding! Unable to check if request is allowed, so just saying request status simply marked as 'ACCEPT'!")
        return None
    start_time = time.time()
    logger.info("Now checking suitability of requested inference for API services to avoid banning (using Swiss Army Llama with model TinyLlama-1.1B-Chat)...")
    input_prompt = "A user wants to do inference using the following prompt text using the OpenAI API; is the content or nature of the request problematic or offensive in any way that makes it likely that OpenAI will reject the request for safety reasons and potentially ban the API key for submitting it? Respond ONLY with a one word answer-- `REJECT` if the request will cause problems with OpenAI, or `ACCEPT` if it's unlikely to pose a problem. Here is the inference request submitted by the user:\n\n{}".format(input_text)
    url = f"http://localhost:{port}/get_text_completions_from_input_prompt/"
    try:
        async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS * 3)) as client:
            payload = {
                "input_prompt": input_prompt,
                "llm_model_name": "Meta-Llama-3-8B-Instruct.Q3_K_S",
                "temperature": 0.7,
                "number_of_tokens_to_generate": 3,
                "number_of_completions_to_generate": 1,
                "grammar_file_string": "accept_or_reject",
            }
            response = await client.post(
                url,
                json=payload,
                params={"token": SWISS_ARMY_LLAMA_SECURITY_TOKEN}
            )
            response.raise_for_status()
            output_results = response.json()
            output_text = output_results[0]["generated_text"]
            end_time = time.time()
            duration_in_seconds = (end_time - start_time)
            logger.info("Total time taken for inference request content check: {} seconds".format(round(duration_in_seconds, 2)))
            if output_text == "ACCEPT":
                logger.info("Inference request is not problematic, passing on to API service now...")
                return inference_request_allowed
            if output_text == "REJECT":
                logger.error("Error! Inference request was determined to be problematic and likely to result in a rejection or ban from the API service!")
                inference_request_allowed = False
                return inference_request_allowed
            logger.warning("Warning! Inference check was supposed to result in either 'ACCEPT' or 'REJECT', but it instead returned: '{}'".format(output_text))
            return inference_request_allowed
    except Exception as e:
        logger.error("Failed to execute inference check request with Swiss Army Llama: {}".format(e))
        if port == REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT:
            logger.info("Falling back to local Swiss Army Llama.")
            return await check_if_input_text_would_get_rejected_from_api_services(input_text)
    return inference_request_allowed

async def validate_inference_api_usage_request(inference_api_usage_request: db_code.InferenceAPIUsageRequest) -> Tuple[bool, float, float]:
    try:
        validation_errors = await validate_inference_request_message_data_func(inference_api_usage_request)
        if validation_errors:
            raise ValueError(f"Invalid inference request message: {', '.join(validation_errors)}")
        requesting_pastelid = inference_api_usage_request.requesting_pastelid
        credit_pack_ticket_pastel_txid = inference_api_usage_request.credit_pack_ticket_pastel_txid
        requested_model = inference_api_usage_request.requested_model_canonical_string
        model_inference_type_string = inference_api_usage_request.model_inference_type_string
        model_parameters = base64.b64decode(inference_api_usage_request.model_parameters_json_b64).decode('utf-8')
        input_data = inference_api_usage_request.model_input_data_json_b64
        if not validate_pastel_txid_string(credit_pack_ticket_pastel_txid):
            logger.error(f"Invalid Pastel TXID: {credit_pack_ticket_pastel_txid}")
            return False, 0, 0
        _, credit_pack_purchase_request_response, _ = await retrieve_credit_pack_ticket_using_txid(credit_pack_ticket_pastel_txid)
        credit_pack_purchase_request_object = await get_credit_pack_purchase_request_from_response(credit_pack_purchase_request_response)
        if credit_pack_purchase_request_object:
            list_of_authorized_pastelids_allowed_to_use_credit_pack = json.dumps(credit_pack_purchase_request_object.list_of_authorized_pastelids_allowed_to_use_credit_pack)
            if requesting_pastelid not in list_of_authorized_pastelids_allowed_to_use_credit_pack:
                logger.warning(f"Unauthorized PastelID: {requesting_pastelid}")
                return False, 0, 0
        model_menu = await get_inference_model_menu()
        requested_model_data = next((model for model in model_menu["models"] if normalize_string(model["model_name"]) == normalize_string(requested_model)), None)
        if requested_model_data is None:
            logger.warning(f"Invalid model requested: {requested_model}")
            return False, 0, 0
        if "api_based_pricing" in requested_model_data['credit_costs']:
            is_api_based_model = 1
        else:
            is_api_based_model = 0
        if model_inference_type_string not in requested_model_data["supported_inference_type_strings"]:
            logger.warning(f"Unsupported inference type '{model_inference_type_string}' for model '{requested_model}'")
            return False, 0, 0
        if not is_api_based_model:
            logger.info("Inference request is for Swiss Army Llama model, so checking if the service is available...")
            local_swiss_army_llama_responding = is_swiss_army_llama_responding(local=True)
            if USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
                remote_swiss_army_llama_responding = is_swiss_army_llama_responding(local=False)
            else:
                remote_swiss_army_llama_responding = 0
            if remote_swiss_army_llama_responding and USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
                port = REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT
            elif local_swiss_army_llama_responding:
                port = SWISS_ARMY_LLAMA_PORT
            else:
                logger.error(f"Neither the local Swiss Army Llama (supposed to be running on port {SWISS_ARMY_LLAMA_PORT}) nor the remote Swiss Army Llama (supposed to be running, if enabled, on mapped port {REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT}) is responding!")
                return False, 0, 0                        
            async with httpx.AsyncClient() as client:
                params = {"token": SWISS_ARMY_LLAMA_SECURITY_TOKEN}
                response = await client.get(f"http://localhost:{port}/get_list_of_available_model_names/", params=params)
                if response.status_code == 200:
                    available_models = response.json()["model_names"]
                    if requested_model not in available_models:
                        add_model_response = await client.post(f"http://localhost:{port}/add_new_model/", params={"token": SWISS_ARMY_LLAMA_SECURITY_TOKEN, "model_url": requested_model_data["model_url"]})
                        if add_model_response.status_code != 200:
                            logger.warning(f"Failed to add new model to Swiss Army Llama: {requested_model}")
                            return False, 0, 0
                else:
                    logger.warning(f"Failed to retrieve available models from Swiss Army Llama API on port {port}")
                    return False, 0, 0
        model_parameters_dict = json.loads(model_parameters)
        input_data_binary = base64.b64decode(input_data)
        result = magika.identify_bytes(input_data_binary)
        detected_data_type = result.output.ct_label
        use_check_inference_requests_locally_before_sending_to_api_service = 0
        if detected_data_type == "txt":
            input_data = input_data_binary.decode("utf-8")
            if is_api_based_model and use_check_inference_requests_locally_before_sending_to_api_service:
                inference_request_allowed = await check_if_input_text_would_get_rejected_from_api_services(input_data)
                if not inference_request_allowed:
                    logger.error(f"Cannot proceed with inference request to model {requested_model} because of risk that it will be rejected and lead to banning!")
                    return False, 0, 0
        if is_base64_encoded(input_data):
            input_data = base64.b64decode(input_data)
            input_data = input_data.decode('utf-8')     
        proposed_cost_in_credits = await calculate_proposed_inference_cost_in_credits(requested_model_data, model_parameters_dict, model_inference_type_string, input_data)
        validation_results = await validate_existing_credit_pack_ticket(credit_pack_ticket_pastel_txid)
        if not validation_results["credit_pack_ticket_is_valid"]:
            logger.warning(f"Invalid credit pack ticket: {validation_results['validation_failure_reasons_list']}")
            return False, 0, 0
        else:
            logger.info(f"Credit pack ticket with txid {credit_pack_ticket_pastel_txid} passed all validation checks: {abbreviated_pretty_json_func(validation_results['validation_checks'])}")
        current_credit_balance, number_of_confirmation_transactions_from_tracking_address_to_burn_address = await determine_current_credit_pack_balance_based_on_tracking_transactions(credit_pack_ticket_pastel_txid)
        if proposed_cost_in_credits >= current_credit_balance:
            logger.warning(f"Insufficient credits for the request. Required: {proposed_cost_in_credits:,}, Available: {current_credit_balance:,}")
            return False, proposed_cost_in_credits, current_credit_balance
        else:
            logger.info(f"Credit pack ticket has sufficient credits for the request. Required: {proposed_cost_in_credits:,}, Available: {current_credit_balance:,}")
        remaining_credits_after_request = current_credit_balance - proposed_cost_in_credits
        return True, proposed_cost_in_credits, remaining_credits_after_request
    except Exception as e:
        logger.error(f"Error validating inference API usage request: {str(e)}")
        traceback.print_exc()
        raise
    
async def process_inference_api_usage_request(inference_api_usage_request: db_code.InferenceAPIUsageRequest) -> db_code.InferenceAPIUsageResponse: 
    # Validate the inference API usage request
    is_valid_request, proposed_cost_in_credits, remaining_credits_after_request = await validate_inference_api_usage_request(inference_api_usage_request) 
    inference_api_usage_request_dict = inference_api_usage_request.model_dump()
    inference_api_usage_request_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in inference_api_usage_request_dict.items()}
    if not is_valid_request:
        logger.error("Invalid inference API usage request received!")
        raise ValueError(f"Error! Received invalid inference API usage request: {inference_api_usage_request_dict}")
    else:
        log_action_with_payload("Received", "inference API usage request", inference_api_usage_request_dict)
    # Save the inference API usage request
    saved_request = await save_inference_api_usage_request(inference_api_usage_request)
    credit_pack_ticket_pastel_txid = inference_api_usage_request.credit_pack_ticket_pastel_txid
    _, credit_pack_purchase_request_response, _ = await retrieve_credit_pack_ticket_using_txid(credit_pack_ticket_pastel_txid)
    credit_usage_tracking_psl_address = credit_pack_purchase_request_response.credit_usage_tracking_psl_address
    # Create and save the InferenceAPIUsageResponse
    inference_response = await create_and_save_inference_api_usage_response(saved_request, proposed_cost_in_credits, remaining_credits_after_request, credit_usage_tracking_psl_address)
    return inference_response

async def create_and_save_inference_api_usage_response(saved_request: db_code.InferenceAPIUsageRequest, proposed_cost_in_credits: float, remaining_credits_after_request: float, credit_usage_tracking_psl_address: str) -> db_code.InferenceAPIUsageResponse:
    # Generate a unique identifier for the inference response
    inference_response_id = str(uuid.uuid4())
    # Create an InferenceAPIUsageResponse instance without the hash and signature fields
    _, _, local_supernode_pastelid, _ = await get_local_machine_supernode_data_func()
    inference_response = db_code.InferenceAPIUsageResponse(
        inference_response_id=inference_response_id,
        inference_request_id=saved_request.inference_request_id,
        proposed_cost_of_request_in_inference_credits=proposed_cost_in_credits,
        remaining_credits_in_pack_after_request_processed=round(remaining_credits_after_request,1),
        credit_usage_tracking_psl_address=credit_usage_tracking_psl_address,
        request_confirmation_message_amount_in_patoshis=int(proposed_cost_in_credits * CREDIT_USAGE_TO_TRACKING_AMOUNT_MULTIPLIER),
        max_block_height_to_include_confirmation_transaction=await get_current_pastel_block_height_func() + 10,  # Adjust as needed
        inference_request_response_utc_iso_string=datetime.now(timezone.utc).isoformat(),
        inference_request_response_pastel_block_height=await get_current_pastel_block_height_func(),
        inference_request_response_message_version_string="1.0"
    )
    # Generate the hash and signature fields
    inference_response.sha3_256_hash_of_inference_request_response_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(inference_response)
    confirmation_signature = await sign_message_with_pastelid_func(local_supernode_pastelid, inference_response.sha3_256_hash_of_inference_request_response_fields, LOCAL_PASTEL_ID_PASSPHRASE)
    inference_response.supernode_pastelid_and_signature_on_inference_request_response_hash = json.dumps({'signing_sn_pastelid': local_supernode_pastelid, 'sn_signature_on_response_hash': confirmation_signature})
    # Save the InferenceAPIUsageResponse to the database
    async with db_code.Session() as db_session:
        db_session.add(inference_response)
        await db_session.commit()
        await db_session.refresh(inference_response)
    return inference_response

async def check_burn_address_for_tracking_transaction(
    tracking_address: str,
    expected_amount: float,
    txid: Optional[str],
    max_block_height: int,
    max_retries: int = 10,
    initial_retry_delay: int = 25
) -> Tuple[bool, int, int]:
    global burn_address
    try_count = 0
    retry_delay = initial_retry_delay
    total_amount_to_burn_address = 0.0
    while try_count < max_retries:
        if txid is None:  # If txid is not provided, search for transactions using listsinceblock RPC method
            start_block_hash = await getblockhash(rpc_connection, 0)
            listsinceblock_output = await listsinceblock(rpc_connection, start_block_hash, 1, True)
            all_transactions = listsinceblock_output["transactions"]
            all_burn_transactions = [
                tx for tx in all_transactions
                if tx.get("address") == burn_address and tx.get("category") == "receive"
            ]
            all_burn_transactions_df = pd.DataFrame.from_records(all_burn_transactions)
            all_burn_transactions_df_filtered = all_burn_transactions_df[all_burn_transactions_df['amount'] == expected_amount]
            if len(all_burn_transactions_df_filtered) == 1:
                txid = all_burn_transactions_df_filtered['txid'].values[0]
            else:
                latest_block_height = await get_current_pastel_block_height_func()
                min_confirmations = latest_block_height - max_block_height
                max_confirmations = latest_block_height - (max_block_height - MAXIMUM_NUMBER_OF_PASTEL_BLOCKS_FOR_USER_TO_SEND_BURN_AMOUNT_FOR_CREDIT_TICKET)
                all_burn_transactions_df_filtered2 = all_burn_transactions_df_filtered[all_burn_transactions_df_filtered['confirmations'] <= max_confirmations]
                all_burn_transactions_df_filtered3 = all_burn_transactions_df_filtered2[all_burn_transactions_df_filtered2['confirmations'] >= min_confirmations]
                if len(all_burn_transactions_df_filtered3) > 1:
                    logger.warning(f"Multiple transactions found with the same amount and confirmations, but the most recent one is {all_burn_transactions_df_filtered3['txid'].values[0]}")
                txid = all_burn_transactions_df_filtered3['txid'].values[0]
        decoded_tx_data = await get_and_decode_raw_transaction(txid)
        if decoded_tx_data:
            # Check if the transaction matches the specified criteria
            if any(vout["scriptPubKey"].get("addresses", [None])[0] == burn_address for vout in decoded_tx_data["vout"]):
                # Calculate the total amount sent to the burn address in the transaction
                total_amount_to_burn_address = sum(
                    vout["value"] for vout in decoded_tx_data["vout"]
                    if vout["scriptPubKey"].get("addresses", [None])[0] == burn_address
                )
                if total_amount_to_burn_address == expected_amount:
                    # Retrieve the transaction details using gettransaction RPC method
                    tx_info = await gettransaction(rpc_connection, txid)
                    if tx_info:
                        num_confirmations = tx_info.get("confirmations", 0)
                        transaction_block_hash = tx_info.get("blockhash", None)
                        if transaction_block_hash:
                            transaction_block_height = await get_block_height_from_block_hash(transaction_block_hash)
                        else:
                            transaction_block_height = 0
                        if ((num_confirmations >= MINIMUM_CONFIRMATION_BLOCKS_FOR_CREDIT_PACK_BURN_TRANSACTION) or SKIP_BURN_TRANSACTION_BLOCK_CONFIRMATION_CHECK) and transaction_block_height <= max_block_height:
                            if SKIP_BURN_TRANSACTION_BLOCK_CONFIRMATION_CHECK:
                                logger.info(f"Matching confirmed transaction found with {num_confirmations:,} confirmation blocks, which is acceptable because the 'SKIP_BURN_TRANSACTION_BLOCK_CONFIRMATION_CHECK' flag is set to TRUE...")
                            else:
                                logger.info(f"Matching confirmed transaction found with {num_confirmations:,} confirmation blocks, greater than or equal to the required confirmation blocks of {MINIMUM_CONFIRMATION_BLOCKS_FOR_CREDIT_PACK_BURN_TRANSACTION}!")
                            return True, False, transaction_block_height, num_confirmations, total_amount_to_burn_address
                        else:
                            logger.info(f"Matching unconfirmed transaction found! Waiting for it to be mined in a block with at least {MINIMUM_CONFIRMATION_BLOCKS_FOR_CREDIT_PACK_BURN_TRANSACTION} confirmation blocks! (Currently it has only {num_confirmations} confirmation blocks)")
                            return True, False, transaction_block_height, num_confirmations, total_amount_to_burn_address
                elif total_amount_to_burn_address >= expected_amount:
                    # Retrieve the transaction details using gettransaction RPC method
                    tx_info = await gettransaction(rpc_connection, txid)
                    if tx_info:
                        num_confirmations = tx_info.get("confirmations", 0)
                        transaction_block_hash = tx_info.get("blockhash", None)
                        if transaction_block_hash:
                            transaction_block_height = await get_block_height_from_block_hash(transaction_block_hash)
                        else:
                            transaction_block_height = 0
                        if ((num_confirmations >= MINIMUM_CONFIRMATION_BLOCKS_FOR_CREDIT_PACK_BURN_TRANSACTION) or SKIP_BURN_TRANSACTION_BLOCK_CONFIRMATION_CHECK) and transaction_block_height <= max_block_height:
                            logger.info(f"Matching confirmed transaction was not found, but we did find a confirmed (with {num_confirmations} confirmation blocks) burn transaction with more than the expected amount ({total_amount_to_burn_address} sent versus the expected amount of {expected_amount})")
                            return False, True, transaction_block_height, num_confirmations, total_amount_to_burn_address
                        else:
                            logger.info(f"Matching unconfirmed transaction was not found, but we did find an unconfirmed burn transaction with more than the expected amount ({total_amount_to_burn_address} sent versus the expected amount of {expected_amount})")
                            return False, True, transaction_block_height, num_confirmations, total_amount_to_burn_address
                else:
                    logger.warning(f"Transaction {txid} found, but the amount sent to the burn address ({total_amount_to_burn_address}) is less than the expected amount ({expected_amount})")
            else:
                logger.warning(f"Transaction {txid} does not send funds to the specified burn address")
        else:
            logger.warning(f"Transaction {txid} not found.")
        # If the transaction is not found or does not match the criteria, wait before retrying
        logger.info(f"WAITING {retry_delay} seconds before checking transaction status again...")
        await asyncio.sleep(retry_delay)
        try_count += 1
        retry_delay *= 1.15  # Optional: increase delay between retries
    logger.info(f"Transaction not found or did not match the criteria after {max_retries} attempts.")
    return False, False, None, None, total_amount_to_burn_address

async def process_inference_confirmation(inference_request_id: str, inference_confirmation: db_code.InferenceConfirmation) -> bool:
    try:
        # Retrieve the inference API usage request from the database
        async with db_code.Session() as db:
            inference_request = await db.exec(
                select(db_code.InferenceAPIUsageRequest).where(db_code.InferenceAPIUsageRequest.inference_request_id == inference_request_id)
            )
            inference_request = inference_request.one_or_none()
        if inference_request is None:
            logger.warning(f"Invalid inference request ID: {inference_request_id}")
            return False
        # Retrieve the inference API usage request response from the database
        async with db_code.Session() as db:
            inference_response = await db.exec(
                select(db_code.InferenceAPIUsageResponse).where(db_code.InferenceAPIUsageResponse.inference_request_id == inference_request_id)
            )
            inference_response = inference_response.one_or_none()
        # Ensure burn address is tracked by local wallet:
        burn_address_already_imported = await check_if_address_is_already_imported_in_local_wallet(burn_address)
        if not burn_address_already_imported:
            await import_address_func(burn_address, "burn_address", True)        
        # Check burn address for tracking transaction:
        confirmation_transaction_txid = inference_confirmation.confirmation_transaction['txid']
        credit_usage_tracking_amount_in_psl = float(inference_response.request_confirmation_message_amount_in_patoshis)/(10**5) # Divide by number of Patoshis per PSL
        matching_transaction_found, exceeding_transaction_found, transaction_block_height, num_confirmations, amount_received_at_burn_address = await check_burn_address_for_tracking_transaction(inference_response.credit_usage_tracking_psl_address, credit_usage_tracking_amount_in_psl, confirmation_transaction_txid, inference_response.max_block_height_to_include_confirmation_transaction)
        if matching_transaction_found:
            logger.info(f"Found correct inference request confirmation tracking transaction in burn address (with {num_confirmations} confirmation blocks so far)! TXID: {confirmation_transaction_txid}; Tracking Amount in PSL: {credit_usage_tracking_amount_in_psl};")
            computed_current_credit_pack_balance, number_of_confirmation_transactions_from_tracking_address_to_burn_address = await determine_current_credit_pack_balance_based_on_tracking_transactions(inference_request.credit_pack_ticket_pastel_txid)
            logger.info(f"Computed current credit pack balance: {computed_current_credit_pack_balance:,.1f} based on {number_of_confirmation_transactions_from_tracking_address_to_burn_address:,} tracking transactions from tracking address to burn address.")       
            # Update the inference request status to "confirmed"
            inference_request.status = "confirmed"
            async with db_code.Session() as db:
                db.add(inference_request)
                await db.commit()
                await db.refresh(inference_request)
            # Trigger the inference request processing
            asyncio.create_task(execute_inference_request(inference_request_id))
            return True
        else:
            logger.error(f"Did not find correct inference request confirmation tracking transaction in burn address! TXID: {confirmation_transaction_txid}; Tracking Amount in PSL: {credit_usage_tracking_amount_in_psl};") 
    except Exception as e:
        logger.error(f"Error processing inference confirmation: {str(e)}")
        traceback.print_exc()
        raise

async def save_inference_output_results(inference_request_id: str, inference_response_id: str, output_results: dict, output_results_file_type_strings: dict) -> None:
    try:
        _, _, local_supernode_pastelid, _ = await get_local_machine_supernode_data_func()
        # Generate a unique identifier for the inference result
        inference_result_id = str(uuid.uuid4())
        # Create an inference output result record without the hash and signature fields
        inference_output_result = db_code.InferenceAPIOutputResult(
            inference_result_id=inference_result_id,
            inference_request_id=inference_request_id,
            inference_response_id=inference_response_id,
            responding_supernode_pastelid=local_supernode_pastelid,
            inference_result_json_base64=base64.b64encode(json.dumps(output_results).encode("utf-8")).decode("utf-8"),
            inference_result_file_type_strings=json.dumps(output_results_file_type_strings),
            inference_result_utc_iso_string=datetime.now(timezone.utc).isoformat(),
            inference_result_pastel_block_height=await get_current_pastel_block_height_func(),
            inference_result_message_version_string="1.0"
        )
        # Generate the hash and signature fields
        inference_output_result.sha3_256_hash_of_inference_result_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(inference_output_result)
        result_id_signature = await sign_message_with_pastelid_func(local_supernode_pastelid, inference_output_result.sha3_256_hash_of_inference_result_fields, LOCAL_PASTEL_ID_PASSPHRASE)
        inference_output_result.responding_supernode_signature_on_inference_result_id = result_id_signature
        # Save the inference output result to the database
        async with db_code.Session() as db:
            db.add(inference_output_result)
            await db.commit()
            await db.refresh(inference_output_result)
    except Exception as e:
        logger.error(f"Error saving inference output results: {str(e)}")
        traceback.print_exc()
        raise

def get_claude3_model_name(model_name: str) -> str:
    model_mapping = {
        "claude3-haiku": "claude-3-haiku-20240307",
        "claude3-opus": "claude-3-opus-20240229",
        "claude3.5-sonnet": "claude-3-5-sonnet-20240620"
    }
    return model_mapping.get(model_name, "")

async def submit_inference_request_to_stability_api(inference_request):
    model_parameters = json.loads(base64.b64decode(inference_request.model_parameters_json_b64).decode("utf-8"))
    if inference_request.model_inference_type_string == "text_to_image":
        prompt = base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")
        if "stability-core" in inference_request.requested_model_canonical_string:
            return await _handle_stability_core_request(prompt, model_parameters)
        elif "sd3" in inference_request.requested_model_canonical_string:
            return await _handle_sd3_request(prompt, model_parameters, inference_request.requested_model_canonical_string)
        else:
            logger.error(f"Unsupported model for text-to-image: {inference_request.requested_model_canonical_string}")
            return None, None
    elif inference_request.model_inference_type_string in ["conservative_upscale", "creative_upscale"]:
        input_image = base64.b64decode(inference_request.model_input_data_json_b64)
        prompt = model_parameters.get("prompt", "")
        if inference_request.model_inference_type_string == "conservative_upscale":
            return await _handle_conservative_upscale_request(input_image, prompt, model_parameters)
        else:
            return await _handle_creative_upscale_request(input_image, prompt, model_parameters)
    else:
        logger.error(f"Unsupported inference type: {inference_request.model_inference_type_string}")
        return None, None

async def _handle_stability_core_request(prompt, model_parameters):
    async with httpx.AsyncClient(timeout=httpx.Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
        data = {
            "prompt": prompt,
            "aspect_ratio": model_parameters.get("aspect_ratio", "1:1"),
            "output_format": model_parameters.get("output_format", "png")
        }
        if model_parameters.get("negative_prompt"):
            data["negative_prompt"] = model_parameters["negative_prompt"]
        if model_parameters.get("seed") is not None:
            data["seed"] = model_parameters["seed"]
        if model_parameters.get("style_preset"):
            data["style_preset"] = model_parameters["style_preset"]
        response = await client.post(
            "https://api.stability.ai/v2beta/stable-image/generate/core",
            headers={
                "Authorization": f"Bearer {STABILITY_API_KEY}",
                "Accept": "application/json"
            },
            files={"none": ''},
            data=data,
        )
        if response.status_code == 200:
            output_results = response.json()
            output_results_file_type_strings = {
                "output_text": "base64_image",
                "output_files": ["NA"]
            }
            return output_results, output_results_file_type_strings
        else:
            logger.error(f"Error generating image from Stability Core API: {response.text}")
            return None, None

async def _handle_sd3_request(prompt, model_parameters, model_name):
    async with httpx.AsyncClient(timeout=httpx.Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
        data = {
            "prompt": prompt,
            "output_format": model_parameters.get("output_format", "png"),
            "model": model_name.replace("stability-", "")
        }
        if model_parameters.get("aspect_ratio"):
            data["aspect_ratio"] = model_parameters["aspect_ratio"]
        if model_parameters.get("seed") is not None:
            data["seed"] = model_parameters["seed"]
        if model_parameters.get("negative_prompt") and "turbo" not in model_name:
            data["negative_prompt"] = model_parameters["negative_prompt"]
        response = await client.post(
            "https://api.stability.ai/v2beta/stable-image/generate/sd3",
            headers={
                "Authorization": f"Bearer {STABILITY_API_KEY}",
                "Accept": "application/json"
            },
            files={"none": ''},
            data=data,
        )
        if response.status_code == 200:
            output_results = response.json()
            output_results_file_type_strings = {
                "output_text": "base64_image",
                "output_files": ["NA"]
            }
            return output_results, output_results_file_type_strings
        else:
            logger.error(f"Error generating image from SD3 API: {response.text}")
            return None, None

async def _handle_conservative_upscale_request(input_image, prompt, model_parameters):
    async with httpx.AsyncClient(timeout=httpx.Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
        data = {
            "prompt": prompt,
            "output_format": model_parameters.get("output_format", "png"),
        }
        if model_parameters.get("negative_prompt"):
            data["negative_prompt"] = model_parameters["negative_prompt"]
        if model_parameters.get("seed") is not None:
            data["seed"] = model_parameters["seed"]
        response = await client.post(
            "https://api.stability.ai/v2beta/stable-image/upscale/conservative",
            headers={
                "Authorization": f"Bearer {STABILITY_API_KEY}",
                "Accept": "application/json"
            },
            files={"image": input_image},
            data=data,
        )
        if response.status_code == 200:
            output_results = base64.b64encode(response.content).decode("utf-8")
            output_results_file_type_strings = {
                "output_text": "base64_image",
                "output_files": ["NA"]
            }
            return output_results, output_results_file_type_strings
        else:
            logger.error(f"Error upscaling image with conservative method: {response.text}")
            return None, None

async def _handle_creative_upscale_request(input_image, prompt, model_parameters):
    async with httpx.AsyncClient(timeout=httpx.Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
        data = {
            "prompt": prompt,
            "output_format": model_parameters.get("output_format", "png"),
            "creativity": model_parameters.get("creativity", 0.3),
        }
        if model_parameters.get("negative_prompt"):
            data["negative_prompt"] = model_parameters["negative_prompt"]
        if model_parameters.get("seed") is not None:
            data["seed"] = model_parameters["seed"]
        response = await client.post(
            "https://api.stability.ai/v2beta/stable-image/upscale/creative",
            headers={
                "Authorization": f"Bearer {STABILITY_API_KEY}",
                "Accept": "application/json"
            },
            files={"image": input_image},
            data=data,
        )
        if response.status_code == 200:
            generation_id = response.json().get("id")
            while True:
                await asyncio.sleep(10)  # Wait for 10 seconds before polling for result
                result_response = await client.get(
                    f"https://api.stability.ai/v2beta/stable-image/upscale/creative/result/{generation_id}",
                    headers={
                        "Authorization": f"Bearer {STABILITY_API_KEY}",
                        "Accept": "image/*"
                    },
                )
                if result_response.status_code == 200:
                    output_results = base64.b64encode(result_response.content).decode("utf-8")
                    output_results_file_type_strings = {
                        "output_text": "base64_image",
                        "output_files": ["NA"]
                    }
                    return output_results, output_results_file_type_strings
                elif result_response.status_code != 202:
                    logger.error(f"Error retrieving creatively upscaled image: {result_response.text}")
                    return None, None
        else:
            logger.error(f"Error initiating creative upscale request: {response.text}")
            return None, None

async def submit_inference_request_to_openai_api(inference_request):
    logger.info("Now accessing OpenAI API...")
    if inference_request.model_inference_type_string == "text_completion":
        model_parameters = json.loads(base64.b64decode(inference_request.model_parameters_json_b64).decode("utf-8"))
        input_prompt = base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")
        num_completions = int(model_parameters.get("number_of_completions_to_generate", 1))
        output_results = []
        total_input_tokens = 0
        total_output_tokens = 0
        openai_text_completion_response_timeout_seconds = 60
        async with httpx.AsyncClient(timeout=openai_text_completion_response_timeout_seconds) as client:
            for i in range(num_completions):
                response = await client.post(
                    "https://api.openai.com/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {OPENAI_API_KEY}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": inference_request.requested_model_canonical_string.replace("openai-", ""),
                        "messages": [{"role": "user", "content": input_prompt}],
                        "max_tokens": int(model_parameters.get("number_of_tokens_to_generate", 1000)),
                        "temperature": float(model_parameters.get("temperature", 0.7)),
                        "n": 1
                    }
                )
                if response.status_code == 200:
                    response_json = response.json()
                    output_results.append(response_json["choices"][0]["message"]["content"])
                    total_input_tokens += response_json["usage"]["prompt_tokens"]
                    total_output_tokens += response_json["usage"]["completion_tokens"]
                else:
                    logger.error(f"Error generating text from OpenAI API: {response.text}")
                    return None, None
        logger.info(f"Total input tokens used with {inference_request.requested_model_canonical_string} model: {total_input_tokens}")
        logger.info(f"Total output tokens used with {inference_request.requested_model_canonical_string} model: {total_output_tokens}")
        if num_completions == 1:
            output_text = output_results[0]
        else:
            output_text = json.dumps({f"completion_{i+1:02}": result for i, result in enumerate(output_results)})
        logger.info(f"Generated the following output text using {inference_request.requested_model_canonical_string}: {output_text[:100]} <abbreviated>...")
        result = magika.identify_bytes(output_text.encode("utf-8"))
        detected_data_type = result.output.ct_label
        output_results_file_type_strings = {
            "output_text": detected_data_type,
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings
    elif inference_request.model_inference_type_string == "embedding":
        input_text = base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://api.openai.com/v1/embeddings",
                headers={
                    "Authorization": f"Bearer {OPENAI_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": inference_request.requested_model_canonical_string.replace("openai-", ""),
                    "input": input_text
                }
            )
            if response.status_code == 200:
                output_results = response.json()["data"][0]["embedding"]
                output_results_file_type_strings = {
                    "output_text": "embedding",
                    "output_files": ["NA"]
                }
                return output_results, output_results_file_type_strings
            else:
                logger.error(f"Error generating embedding from OpenAI API: {response.text}")
                return None, None
    elif inference_request.model_inference_type_string == "ask_question_about_an_image":
        model_parameters = json.loads(base64.b64decode(inference_request.model_parameters_json_b64).decode("utf-8"))
        num_completions = int(model_parameters.get("number_of_completions_to_generate", 1))
        input_data = json.loads(base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8"))
        image_data_binary = base64.b64decode(input_data["image"])
        question = input_data["question"]
        base64_image = base64.b64encode(image_data_binary).decode('utf-8')
        output_results = []
        total_input_tokens = 0
        total_output_tokens = 0        
        openai_vision_timeout_seconds = 90
        async with httpx.AsyncClient(timeout=openai_vision_timeout_seconds) as client:
            for i in range(num_completions):
                response = await client.post(
                    "https://api.openai.com/v1/chat/completions",
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {OPENAI_API_KEY}"
                    },
                    json={
                        "model": inference_request.requested_model_canonical_string.replace("openai-", "").replace("-vision", ""),
                        "messages": [
                            {
                                "role": "user",
                                "content": [
                                    {
                                        "type": "text",
                                        "text": question
                                    },
                                    {
                                        "type": "image_url",
                                        "image_url": {
                                            "url": f"data:image/jpeg;base64,{base64_image}"
                                        }
                                    }
                                ]
                            }
                        ],
                        "max_tokens": int(json.loads(base64.b64decode(inference_request.model_parameters_json_b64).decode("utf-8")).get("number_of_tokens_to_generate", 300))
                    }
                )
                if response.status_code == 200:
                    response_json = response.json()
                    output_results.append(response_json["choices"][0]["message"]["content"])
                    total_input_tokens += response_json["usage"]["prompt_tokens"]
                    total_output_tokens += response_json["usage"]["completion_tokens"]                    
                else:
                    logger.error(f"Error generating response from OpenAI API: {response.text}")
                    return None, None
        logger.info(f"Total input tokens used with {inference_request.requested_model_canonical_string} model: {total_input_tokens}")
        logger.info(f"Total output tokens used with {inference_request.requested_model_canonical_string} model: {total_output_tokens}")
        if num_completions == 1:
            output_text = output_results[0]
        else:
            output_text = json.dumps({f"completion_{i+1:02}": result for i, result in enumerate(output_results)})             
        logger.info(f"Generated the following output text using {inference_request.requested_model_canonical_string}: {output_text[:100]} <abbreviated>...")
        result = magika.identify_bytes(output_text.encode("utf-8"))
        detected_data_type = result.output.ct_label
        output_results_file_type_strings = {
            "output_text": detected_data_type,
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings               
    else:
        logger.warning(f"Unsupported inference type for OpenAI model: {inference_request.model_inference_type_string}")
        return None, None
    
async def submit_inference_request_to_openrouter(inference_request):
    logger.info("Now accessing OpenRouter...")
    if inference_request.model_inference_type_string == "text_completion":
        model_parameters = json.loads(base64.b64decode(inference_request.model_parameters_json_b64).decode("utf-8"))       
        messages = [{"role": "user", "content": base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")}]
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": inference_request.requested_model_canonical_string,
                    "messages": messages,
                    "max_tokens": int(model_parameters.get("number_of_tokens_to_generate", 1000)),
                    "temperature": float(model_parameters.get("temperature", 0.7)),
                }
            )
            if response.status_code == 200:
                output_results = response.json()["choices"][0]["message"]["content"]
                result = magika.identify_bytes(output_results.encode("utf-8")) 
                detected_data_type = result.output.ct_label
                output_results_file_type_strings = {
                    "output_text": detected_data_type,
                    "output_files": ["NA"]
                }
                return output_results, output_results_file_type_strings
            else:
                logger.error(f"Error generating text from OpenRouter: {response.text}")
                return None, None
    else:
        logger.warning(f"Unsupported inference type for OpenRouter model: {inference_request.model_inference_type_string}")
        return None, None

async def submit_inference_request_to_mistral_api(inference_request):
    # Integrate with the Mistral API to perform the inference task
    logger.info("Now accessing Mistral API...")
    client = MistralAsyncClient(api_key=MISTRAL_API_KEY)
    if inference_request.model_inference_type_string == "text_completion":
        model_parameters = json.loads(base64.b64decode(inference_request.model_parameters_json_b64).decode("utf-8"))        
        input_prompt = base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")
        num_completions = int(model_parameters.get("number_of_completions_to_generate", 1))
        output_results = []
        total_input_tokens = 0
        total_output_tokens = 0
        for i in range(num_completions):
            messages = [ChatMessage(role="user", content=input_prompt)]
            async_response = client.chat_stream(
                model=inference_request.requested_model_canonical_string.replace("mistralapi-",""),
                messages=messages,
                max_tokens=int(model_parameters.get("number_of_tokens_to_generate", 1000)),
                temperature=float(model_parameters.get("temperature", 0.7)),
            )
            completion_text = ""
            prompt_tokens = 0
            completion_tokens = 0
            async for chunk in async_response:
                if chunk.choices[0].delta.content:
                    completion_text += chunk.choices[0].delta.content
                    completion_tokens += 1
                else:
                    prompt_tokens += 1
            output_results.append(completion_text)
            total_input_tokens += prompt_tokens
            total_output_tokens += completion_tokens
        logger.info(f"Total input tokens used with {inference_request.requested_model_canonical_string} model: {total_input_tokens}")
        logger.info(f"Total output tokens used with {inference_request.requested_model_canonical_string} model: {total_output_tokens}")
        if num_completions == 1:
            output_text = output_results[0]
        else:
            output_text = json.dumps({f"completion_{i+1:02}": result for i, result in enumerate(output_results)})
        logger.info(f"Generated the following output text using {inference_request.requested_model_canonical_string}: {output_text[:100]} <abbreviated>...")
        result = magika.identify_bytes(output_text.encode("utf-8"))
        detected_data_type = result.output.ct_label
        output_results_file_type_strings = {
            "output_text": detected_data_type,
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings
    elif inference_request.model_inference_type_string == "embedding":
        input_text = base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")
        embeddings_batch_response = client.embeddings(
            model=inference_request.requested_model_canonical_string,
            input=[input_text],
        )
        output_results = embeddings_batch_response.data[0].embedding
        output_results_file_type_strings = {
            "output_text": "embedding",
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings
    else:
        logger.warning(f"Unsupported inference type for Mistral model: {inference_request.model_inference_type_string}")
        return None, None
    
async def submit_inference_request_to_groq_api(inference_request):
    # Integrate with the Groq API to perform the inference task
    logger.info("Now accessing Groq API...")
    client = AsyncGroq(api_key=GROQ_API_KEY)
    if inference_request.model_inference_type_string == "text_completion":
        model_parameters = json.loads(base64.b64decode(inference_request.model_parameters_json_b64).decode("utf-8"))
        input_prompt = base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")
        num_completions = int(model_parameters.get("number_of_completions_to_generate", 1))
        output_results = []
        total_input_tokens = 0
        total_output_tokens = 0
        for i in range(num_completions):
            chat_completion = await client.chat.completions.create(
                messages=[{"role": "user", "content": input_prompt}],
                model=inference_request.requested_model_canonical_string.replace("groq-",""),
                max_tokens=int(model_parameters.get("number_of_tokens_to_generate", 1000)),
                temperature=float(model_parameters.get("temperature", 0.7)),
            )
            output_results.append(chat_completion.choices[0].message.content)
            total_input_tokens += chat_completion.usage.prompt_tokens
            total_output_tokens += chat_completion.usage.completion_tokens
        logger.info(f"Total input tokens used with {inference_request.requested_model_canonical_string} model: {total_input_tokens}")
        logger.info(f"Total output tokens used with {inference_request.requested_model_canonical_string} model: {total_output_tokens}")
        if num_completions == 1:
            output_text = output_results[0]
        else:
            output_text = json.dumps({f"completion_{i+1:02}": result for i, result in enumerate(output_results)})
        logger.info(f"Generated the following output text using {inference_request.requested_model_canonical_string}: {output_text[:100]} <abbreviated>...")
        result = magika.identify_bytes(output_text.encode("utf-8"))
        detected_data_type = result.output.ct_label
        output_results_file_type_strings = {
            "output_text": detected_data_type,
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings
    elif inference_request.model_inference_type_string == "embedding":
        input_text = base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")
        response = await client.embed(
            input=input_text,
            model=inference_request.requested_model_canonical_string[5:],
        )
        output_results = response.embedding
        output_results_file_type_strings = {
            "output_text": "embedding",
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings
    else:
        logger.warning(f"Unsupported inference type for Groq model: {inference_request.model_inference_type_string}")
        return None, None

async def submit_inference_request_to_claude_api(inference_request):
    # Integrate with the Claude API to perform the inference task
    logger.info("Now accessing Claude (Anthropic) API...")
    client = anthropic.AsyncAnthropic(api_key=CLAUDE3_API_KEY)
    claude3_model_id_string = get_claude3_model_name(inference_request.requested_model_canonical_string)
    if inference_request.model_inference_type_string == "text_completion":
        model_parameters = json.loads(base64.b64decode(inference_request.model_parameters_json_b64).decode("utf-8"))
        input_prompt = base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")
        num_completions = int(model_parameters.get("number_of_completions_to_generate", 1))
        output_results = []
        total_input_tokens = 0
        total_output_tokens = 0
        for i in range(num_completions):
            async with client.messages.stream(
                model=claude3_model_id_string,
                max_tokens=int(model_parameters.get("number_of_tokens_to_generate", 1000)),
                temperature=float(model_parameters.get("temperature", 0.7)),
                messages=[{"role": "user", "content": input_prompt}],
            ) as stream:
                message = await stream.get_final_message()
                output_results.append(message.content[0].text)
                total_input_tokens += message.usage.input_tokens
                total_output_tokens += message.usage.output_tokens
        logger.info(f"Total input tokens used with {claude3_model_id_string} model: {total_input_tokens}")
        logger.info(f"Total output tokens used with {claude3_model_id_string} model: {total_output_tokens}")
        if num_completions == 1:
            output_text = output_results[0]
        else:
            output_text = json.dumps({f"completion_{i+1:02}": result for i, result in enumerate(output_results)})
        logger.info(f"Generated the following output text using {claude3_model_id_string}: {output_text[:100]} <abbreviated>...")
        result = magika.identify_bytes(output_text.encode("utf-8"))
        detected_data_type = result.output.ct_label
        output_results_file_type_strings = {
            "output_text": detected_data_type,
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings
    elif inference_request.model_inference_type_string == "embedding":
        input_text = base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8")
        response = await client.embed(
            text=input_text,
            model=claude3_model_id_string
        )
        output_results = response.embedding
        output_results_file_type_strings = {
            "output_text": "embedding",
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings
    else:
        logger.warning(f"Unsupported inference type for Claude3 Haiku: {inference_request.model_inference_type_string}")
        return None, None

# Swiss Army Llama related functions:

def determine_swiss_army_llama_port():
    local_swiss_army_llama_responding = is_swiss_army_llama_responding(local=True)
    if USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
        remote_swiss_army_llama_responding = is_swiss_army_llama_responding(local=False)
    else:
        remote_swiss_army_llama_responding = 0
    
    if remote_swiss_army_llama_responding and USE_REMOTE_SWISS_ARMY_LLAMA_IF_AVAILABLE:
        return REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT
    elif local_swiss_army_llama_responding:
        return SWISS_ARMY_LLAMA_PORT
    return None

async def handle_swiss_army_llama_exception(e, client, inference_request, model_parameters, port, is_fallback, handler_function):
    logger.error("Failed to execute inference request: {}".format(e))
    if port == REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT and not is_fallback:
        logger.info("Falling back to local Swiss Army Llama.")
        return await handler_function(client, inference_request, model_parameters, SWISS_ARMY_LLAMA_PORT, True)
    else:
        return None, None
    
async def handle_swiss_army_llama_text_completion(client, inference_request, model_parameters, port, is_fallback):
    payload = {
        "input_prompt": base64.b64decode(inference_request.model_input_data_json_b64).decode("utf-8"),
        "llm_model_name": inference_request.requested_model_canonical_string.replace("swiss_army_llama-", ""),
        "temperature": model_parameters.get("temperature", 0.7),
        "number_of_tokens_to_generate": model_parameters.get("number_of_tokens_to_generate", 1000),
        "number_of_completions_to_generate": model_parameters.get("number_of_completions_to_generate", 1),
        "grammar_file_string": model_parameters.get("grammar_file_string", ""),
    }
    try:
        response = await client.post(
            f"http://localhost:{port}/get_text_completions_from_input_prompt/",
            json=payload,
            params={"token": SWISS_ARMY_LLAMA_SECURITY_TOKEN}
        )
        response.raise_for_status()
        output_results = response.json()
        output_text = output_results[0]["generated_text"]
        result = magika.identify_bytes(output_text.encode("utf-8"))
        detected_data_type = result.output.ct_label
        output_results_file_type_strings = {
            "output_text": detected_data_type,
            "output_files": ["NA"]
        }
        return output_text, output_results_file_type_strings
    except Exception as e:
        return await handle_swiss_army_llama_exception(e, client, inference_request, model_parameters, port, is_fallback, handle_swiss_army_llama_text_completion)

async def handle_swiss_army_llama_image_question(client, inference_request, model_parameters, port, is_fallback):
    input_data = json.loads(base64.b64decode(inference_request.model_input_data_json_b64).decode())
    image_data_binary = base64.b64decode(input_data["image"])
    question = input_data["question"]
    payload = {
        "question": question,
        "llm_model_name": inference_request.requested_model_canonical_string.replace("swiss_army_llama-", ""),
        "temperature": model_parameters.get("temperature", 0.7),
        "number_of_tokens_to_generate": model_parameters.get("number_of_tokens_to_generate", 256),
        "number_of_completions_to_generate": model_parameters.get("number_of_completions_to_generate", 1)
    }
    files = {"image": ("image.png", image_data_binary, "image/png")}
    try:
        response = await client.post(
            f"http://localhost:{port}/ask_question_about_image/",
            data=payload,
            files=files,
            params={"token": SWISS_ARMY_LLAMA_SECURITY_TOKEN}
        )
        response.raise_for_status()
        output_results = response.json()
        output_text = output_results[0]["generated_text"]
        result = magika.identify_bytes(output_text.encode("utf-8"))
        detected_data_type = result.output.ct_label
        output_results_file_type_strings = {
            "output_text": detected_data_type,
            "output_files": ["NA"]
        }
        return output_text, output_results_file_type_strings
    except Exception as e:
        return await handle_swiss_army_llama_exception(e, client, inference_request, model_parameters, port, is_fallback, handle_swiss_army_llama_image_question)

async def handle_swiss_army_llama_embedding_document(client, inference_request, model_parameters, port, is_fallback):
    input_data = inference_request.model_input_data_json_b64
    if is_base64_encoded(input_data):
        input_data = base64.b64decode(input_data)
        input_data = input_data.decode('utf-8')
    try:
        input_data_dict = json.loads(input_data)
        document_file_data = input_data_dict['document']
        if is_base64_encoded(document_file_data):
            document_file_data = base64.b64decode(document_file_data)
    except Exception as e:
        logger.error(f"Error decoding audio data: {str(e)}")
        traceback.print_exc()
        raise("Error decoding audio data")
    try:
        file_metadata = await upload_and_get_file_metadata(document_file_data, file_prefix="document")
        file_url = file_metadata["file_url"]
        file_hash = file_metadata["file_hash"]
        file_size = file_metadata["file_size"]
        query_text = model_parameters.get("query_text", None)
    except Exception as e:
        logger.error(f"Error parsing document data from input: {str(e)}")
        traceback.print_exc()
        raise
    params = {
        "llm_model_name": inference_request.requested_model_canonical_string.replace("swiss_army_llama-", ""),
        "embedding_pooling_method": model_parameters.get("embedding_pooling_method", "svd"),
        "corpus_identifier_string": model_parameters.get("corpus_identifier_string", ""),
        "json_format": model_parameters.get("json_format", "records"),
        "send_back_json_or_zip_file": model_parameters.get("send_back_json_or_zip_file", "zip"),
        "query_text": query_text
    }
    files = {
        'url': (None, file_url),
        'hash': (None, file_hash),
        'size': (None, str(file_size))
    }
    try:
        response = await client.post(
            f"http://localhost:{port}/get_all_embedding_vectors_for_document/",
            params=params,
            files=files,
            headers={"accept": "application/json"}
        )
        response.raise_for_status()
        if model_parameters.get("send_back_json_or_zip_file", "zip") == "json":
            output_results = response.json()
            output_results_file_type_strings = {
                "output_text": "embedding_document",
                "output_files": ["NA"]
            }            
        else:
            zip_file_content = response.read()
            output_results = base64.b64encode(zip_file_content).decode('utf-8')
            output_results_file_type_strings = {
                "output_text": "NA",
                "output_files": ["zip"]
            }
        return output_results, output_results_file_type_strings
    except Exception as e:
        return await handle_swiss_army_llama_exception(e, client, inference_request, model_parameters, port, is_fallback, handle_swiss_army_llama_embedding_document)
    
async def handle_swiss_army_llama_embedding_audio(client, inference_request, model_parameters, port, is_fallback):
    input_data = inference_request.model_input_data_json_b64
    if is_base64_encoded(input_data):
        input_data = base64.b64decode(input_data)
        input_data = input_data.decode('utf-8')
    try:
        input_data_dict = json.loads(input_data)
        audio_file_data = input_data_dict['audio']
        if is_base64_encoded(audio_file_data):
            audio_file_data = base64.b64decode(audio_file_data) 
    except Exception as e:
        logger.error(f"Error decoding audio data: {str(e)}")
        traceback.print_exc()
        raise("Error decoding audio data")
    try:
        file_metadata = await upload_and_get_file_metadata(audio_file_data, file_prefix="audio")
        file_url = file_metadata["file_url"]
        file_hash = file_metadata["file_hash"]
        file_size = file_metadata["file_size"]
        query_text = model_parameters.get("query_text", "")
        corpus_identifier_string = model_parameters.get("corpus_identifier_string", "")
    except Exception as e:
        logger.error(f"Error parsing audio data from input: {str(e)}")
        traceback.print_exc()
        raise
    params = {
        "compute_embeddings_for_resulting_transcript_document": model_parameters.get("compute_embeddings_for_resulting_transcript_document", True),
        "llm_model_name": inference_request.requested_model_canonical_string.replace("swiss_army_llama-", ""),
        "embedding_pooling_method": model_parameters.get("embedding_pooling_method", "svd"),
        "corpus_identifier_string": corpus_identifier_string
    }
    files = {
        'url': (None, file_url),
        'hash': (None, file_hash),
        'size': (None, str(file_size))
    }
    try:
        response = await client.post(
            f"http://localhost:{port}/compute_transcript_with_whisper_from_audio/",
            params=params,
            files=files,
            headers={"accept": "application/json"}
        )
        response.raise_for_status()
        output_results = response.json()
        output_results_file_type_strings = {
            "output_text": "embedding_audio",
            "output_files": ["NA"]
        }
        if query_text:
            search_payload = {
                "query_text": query_text,
                "number_of_most_similar_strings_to_return": model_parameters.get("number_of_most_similar_strings_to_return", 10),
                "llm_model_name": inference_request.requested_model_canonical_string.replace("swiss_army_llama-", ""),
                "embedding_pooling_method": model_parameters.get("embedding_pooling_method", "svd"),
                "corpus_identifier_string": corpus_identifier_string
            }
            search_response = await client.post(
                f"http://localhost:{port}/search_stored_embeddings_with_query_string_for_semantic_similarity/",
                json=search_payload,
                headers={"accept": "application/json", "Content-Type": "application/json"}
            )
            search_response.raise_for_status()
            search_results = search_response.json()
            output_results["search_results"] = search_results
        return output_results, output_results_file_type_strings
    except Exception as e:
        return await handle_swiss_army_llama_exception(e, client, inference_request, model_parameters, port, is_fallback, handle_swiss_army_llama_embedding_audio)

async def handle_swiss_army_llama_semantic_search(client, inference_request, model_parameters, port, is_fallback):
    payload = {
        "query_text": model_parameters.get("query_text", ""),
        "number_of_most_similar_strings_to_return": model_parameters.get("number_of_most_similar_strings_to_return", 10),
        "llm_model_name": inference_request.requested_model_canonical_string.replace("swiss_army_llama-", ""),
        "embedding_pooling_method": model_parameters.get("embedding_pooling_method", "svd_first_four"),
        "corpus_identifier_string": model_parameters.get("corpus_identifier_string", "")
    }
    try:
        response = await client.post(
            f"http://localhost:{port}/search_stored_embeddings_with_query_string_for_semantic_similarity/",
            json=payload,
            params={"token": SWISS_ARMY_LLAMA_SECURITY_TOKEN}
        )
        response.raise_for_status()
        output_results = response.json()
        output_results_file_type_strings = {
            "output_text": "semantic_search",
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings
    except Exception as e:
        return await handle_swiss_army_llama_exception(e, client, inference_request, model_parameters, port, is_fallback, handle_swiss_army_llama_semantic_search)

async def handle_swiss_army_llama_advanced_semantic_search(client, inference_request, model_parameters, port, is_fallback):
    payload = {
        "query_text": model_parameters.get("query_text", ""),
        "llm_model_name": inference_request.requested_model_canonical_string.replace("swiss_army_llama-", ""),
        "embedding_pooling_method": model_parameters.get("embedding_pooling_method", "svd"),
        "corpus_identifier_string": model_parameters.get("corpus_identifier_string", "string"),
        "similarity_filter_percentage": model_parameters.get("similarity_filter_percentage", 0.01),
        "number_of_most_similar_strings_to_return": model_parameters.get("number_of_most_similar_strings_to_return", 0),
        "result_sorting_metric": model_parameters.get("result_sorting_metric", "hoeffding_d")
    }
    try:
        response = await client.post(
            f"http://localhost:{port}/advanced_search_stored_embeddings_with_query_string_for_semantic_similarity/",
            json=payload,
            params={"token": SWISS_ARMY_LLAMA_SECURITY_TOKEN}
        )
        response.raise_for_status()
        output_results = response.json()
        output_results_file_type_strings = {
            "output_text": "advanced_semantic_search",
            "output_files": ["NA"]
        }
        return output_results, output_results_file_type_strings
    except Exception as e:
        return await handle_swiss_army_llama_exception(e, client, inference_request, model_parameters, port, is_fallback, handle_swiss_army_llama_advanced_semantic_search)

async def submit_inference_request_to_swiss_army_llama(inference_request, is_fallback=False):
    logger.info("Now calling Swiss Army Llama with model {}".format(inference_request.requested_model_canonical_string))
    model_parameters = json.loads(base64.b64decode(inference_request.model_parameters_json_b64).decode("utf-8"))
    port = determine_swiss_army_llama_port()
    if not port:
        logger.error(f"Neither the local (port {SWISS_ARMY_LLAMA_PORT}) nor the remote (port {REMOTE_SWISS_ARMY_LLAMA_MAPPED_PORT}) Swiss Army Llama is responding!")
        return None, None
    async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS * 12)) as client:
        if inference_request.model_inference_type_string == "text_completion":
            return await handle_swiss_army_llama_text_completion(client, inference_request, model_parameters, port, is_fallback)
        elif inference_request.model_inference_type_string == "embedding_document":
            return await handle_swiss_army_llama_embedding_document(client, inference_request, model_parameters, port, is_fallback)
        elif inference_request.model_inference_type_string == "embedding_audio":
            return await handle_swiss_army_llama_embedding_audio(client, inference_request, model_parameters, port, is_fallback)
        elif inference_request.model_inference_type_string == "ask_question_about_an_image":
            return await handle_swiss_army_llama_image_question(client, inference_request, model_parameters, port, is_fallback)
        elif inference_request.model_inference_type_string == "semantic_search":
            return await handle_swiss_army_llama_semantic_search(client, inference_request, model_parameters, port, is_fallback)
        elif inference_request.model_inference_type_string == "advanced_semantic_search":
            return await handle_swiss_army_llama_advanced_semantic_search(client, inference_request, model_parameters, port, is_fallback)
        else:
            logger.warning("Unsupported inference type: {}".format(inference_request.model_inference_type_string))
            return None, None

async def execute_inference_request(inference_request_id: str) -> None:
    try:
        # Retrieve the inference API usage request from the database
        async with db_code.Session() as db:
            query = await db.exec(
                select(db_code.InferenceAPIUsageRequest).where(db_code.InferenceAPIUsageRequest.inference_request_id == inference_request_id)
            )
            inference_request = query.one_or_none()
        if inference_request is None:
            logger.warning(f"Invalid inference request ID: {inference_request_id}")
            return
        # Retrieve the inference API usage request response from the database
        async with db_code.Session() as db:
            query = await db.exec(
                select(db_code.InferenceAPIUsageResponse).where(db_code.InferenceAPIUsageResponse.inference_request_id == inference_request_id)
            )
            inference_response = query.one_or_none()
        if inference_request.requested_model_canonical_string.startswith("stability-"):
            output_results, output_results_file_type_strings = await submit_inference_request_to_stability_api(inference_request)
        elif inference_request.requested_model_canonical_string.startswith("openai-"):
            output_results, output_results_file_type_strings = await submit_inference_request_to_openai_api(inference_request)
        elif inference_request.requested_model_canonical_string.startswith("mistralapi-"):
            output_results, output_results_file_type_strings = await submit_inference_request_to_mistral_api(inference_request)
        elif inference_request.requested_model_canonical_string.startswith("groq-"):
            output_results, output_results_file_type_strings = await submit_inference_request_to_groq_api(inference_request)
        elif "claude" in inference_request.requested_model_canonical_string.lower():
            output_results, output_results_file_type_strings = await submit_inference_request_to_claude_api(inference_request)
        elif inference_request.requested_model_canonical_string.startswith("openrouter/"):
            output_results, output_results_file_type_strings = await submit_inference_request_to_openrouter(inference_request)
        elif inference_request.requested_model_canonical_string.startswith("swiss_army_llama-"):
            output_results, output_results_file_type_strings = await submit_inference_request_to_swiss_army_llama(inference_request)
        else:
            error_message = f"Unsupported provider or model selected for {inference_request.requested_model_canonical_string}: {inference_request.model_inference_type_string}"
            logger.error(error_message)
            raise ValueError(error_message)
        if output_results is not None and output_results_file_type_strings is not None:
            # Save the inference output results to the database
            await save_inference_output_results(inference_request_id, inference_response.inference_response_id, output_results, output_results_file_type_strings)
    except Exception as e:
        logger.error(f"Error executing inference request: {str(e)}")
        traceback.print_exc()
        raise

async def check_status_of_inference_request_results(inference_response_id: str) -> bool:
    try:
        async with db_code.Session() as db_session:
            # Retrieve the inference output result
            result = await db_session.exec(
                select(db_code.InferenceAPIOutputResult).where(db_code.InferenceAPIOutputResult.inference_response_id == inference_response_id)
            )
            inference_output_result = result.one_or_none()
            if inference_output_result is None:
                return False
            else:
                return True
    except Exception as e:
        logger.error(f"Error checking status of inference request results: {str(e)}")
        raise

async def get_inference_output_results_and_verify_authorization(inference_response_id: str, requesting_pastelid: str) -> db_code.InferenceAPIOutputResult:
    async with db_code.Session() as db_session:
        # Retrieve the inference output result
        query = await db_session.exec(
            select(db_code.InferenceAPIOutputResult).where(db_code.InferenceAPIOutputResult.inference_response_id == inference_response_id)
        )
        inference_output_result = query.one_or_none()
        if inference_output_result is None:
            raise ValueError("Inference output results not found")
        # Retrieve the inference request to verify requesting PastelID
        query = await db_session.exec(
            select(db_code.InferenceAPIUsageRequest).where(db_code.InferenceAPIUsageRequest.inference_request_id == inference_output_result.inference_request_id)
        )
        inference_request = query.one_or_none()
        if inference_request is None or inference_request.requesting_pastelid != requesting_pastelid:
            raise ValueError("Unauthorized access to inference output results")
        return inference_output_result

async def fetch_all_mnid_tickets_details():
    mnid_response = await tickets_list_id(rpc_connection, 'mn')
    if mnid_response is None or len(mnid_response) == 0:
        return []
    tickets_data = {ticket['txid']: ticket for ticket in mnid_response}
    async with db_code.Session() as session:
        async with session.begin():
            result = await session.execute(select(db_code.MNIDTicketDetails.txid).where(db_code.MNIDTicketDetails.txid.in_(tickets_data.keys())))
            existing_txids = result.scalars().all()  # Correct method to fetch scalar results directly
            existing_txids_set = set(existing_txids)
            new_tickets_to_insert = []
            for txid, ticket in tickets_data.items():
                if txid not in existing_txids_set:
                    new_ticket = db_code.MNIDTicketDetails(
                        txid=txid,
                        pastel_id=ticket['ticket']['pastelID'],
                        address=ticket['ticket']['address'],
                        pq_key=ticket['ticket']['pq_key'],
                        outpoint=ticket['ticket']['outpoint'],
                        block_height=ticket['height'],
                        timestamp=datetime.utcfromtimestamp(int(ticket['ticket']['timeStamp']))
                    )
                    new_tickets_to_insert.append(new_ticket)
            if new_tickets_to_insert:
                try:
                    session.add_all(new_tickets_to_insert)
                    await session.commit()
                except IntegrityError as e:
                    await session.rollback()
                    logger.error(f"Error inserting new tickets due to a unique constraint failure: {e}")
    return new_tickets_to_insert

async def fetch_active_supernodes_count_and_details(block_height: int):
    async with db_code.Session() as session:
        async with session.begin():  # Fetch all mnid tickets created up to the specified block height
            result = await session.execute(
                select(db_code.MNIDTicketDetails)
                .where(db_code.MNIDTicketDetails.block_height <= block_height)
            )
            mnid_tickets = result.scalars().all()
            active_supernodes = []
            for ticket in mnid_tickets:
                try:
                    # Check if outpoint is valid
                    if ticket.outpoint and isinstance(ticket.outpoint, str) and '-' in ticket.outpoint:
                        txid, vout_str = ticket.outpoint.split('-')
                        # Check if vout is a valid integer
                        if not vout_str.isdigit():
                            continue
                        vout = int(vout_str)
                        tx_info = await getrawtransaction(rpc_connection, txid, 1)
                        # Ensure tx_info is a dictionary and contains 'vout'
                        if tx_info and isinstance(tx_info, dict) and 'vout' in tx_info:
                            # Ensure vout is within the valid range of tx_info['vout']
                            if vout >= 0 and vout < len(tx_info['vout']):
                                vout_data = tx_info['vout'][vout]
                                # Ensure vout_data is a dictionary and contains necessary keys
                                if vout_data and isinstance(vout_data, dict) and 'n' in vout_data and 'value' in vout_data:
                                    # Check if the outpoint is still unspent and meets collateral requirements
                                    if vout_data['n'] == vout and vout_data['value'] >= masternode_collateral_amount:
                                        # Ensure timestamp is valid
                                        if isinstance(ticket.timestamp, (datetime, int)):
                                            timestamp = datetime.utcfromtimestamp(ticket.timestamp.timestamp() if isinstance(ticket.timestamp, datetime) else int(ticket.timestamp))
                                            supernode_details = {
                                                "txid": ticket.txid,
                                                "pastel_id": ticket.pastel_id,
                                                "address": ticket.address,
                                                "pq_key": ticket.pq_key,
                                                "outpoint": ticket.outpoint,
                                                "block_height": ticket.block_height,
                                                "timestamp": timestamp
                                            }
                                            active_supernodes.append(supernode_details)
                except (KeyError, ValueError, TypeError) as e:  # noqa: F841
                    pass
    active_supernodes_count = len(active_supernodes)
    return active_supernodes_count, active_supernodes

async def determine_current_credit_pack_balance_based_on_tracking_transactions(credit_pack_ticket_txid: str):
    logger.info(f"Retrieving credit pack ticket data for txid: {credit_pack_ticket_txid}")
    _, credit_pack_purchase_request_response, _ = await retrieve_credit_pack_ticket_using_txid(credit_pack_ticket_txid)
    credit_pack_purchase_request_fields_json = base64.b64decode(
        credit_pack_purchase_request_response.credit_pack_purchase_request_fields_json_b64
    ).decode('utf-8')
    credit_pack_purchase_request_dict = json.loads(credit_pack_purchase_request_fields_json)
    initial_credit_balance = credit_pack_purchase_request_dict['requested_initial_credits_in_credit_pack']
    credit_usage_tracking_psl_address = credit_pack_purchase_request_response.credit_usage_tracking_psl_address
    logger.info(f"Credit pack ticket data retrieved. Initial credit balance: {initial_credit_balance:,.1f}, Tracking address: {credit_usage_tracking_psl_address}")
    min_height_for_credit_pack_tickets = 700000
    try:
        logger.info(f"Now scanning blockchain for burn transactions sent from address {credit_usage_tracking_psl_address}...")
        params = {
            "addresses": [burn_address],
            "mempool": True,
            "minHeight": min_height_for_credit_pack_tickets,
            "sender": credit_usage_tracking_psl_address
        }
        burn_transactions = await getaddressutxosextra(rpc_connection, params)
        # Check if the result is valid
        if not burn_transactions:
            logger.info(f"No transactions found for address {credit_usage_tracking_psl_address}. Returning initial balance.")
            return initial_credit_balance, 0
        # Filter out transactions where 'patoshis' exceeds 100000
        burn_transactions = [tx for tx in burn_transactions if tx.get('patoshis', 0) <= 100000]
        # Calculate the total credits consumed
        total_credits_consumed = sum(tx.get('patoshis', 0) for tx in burn_transactions) / CREDIT_USAGE_TO_TRACKING_AMOUNT_MULTIPLIER
        current_credit_balance = initial_credit_balance - total_credits_consumed
        number_of_confirmation_transactions = len(burn_transactions)
        logger.info(f"Calculation completed. Initial credit balance: {initial_credit_balance:,.1f}; "
                    f"Total credits consumed: {total_credits_consumed:,.1f} across {number_of_confirmation_transactions:,} transactions; "
                    f"Current credit balance: {current_credit_balance:,.1f}")
        return current_credit_balance, number_of_confirmation_transactions
    except Exception as e:
        logger.error(f"Error while determining current credit pack balance: {str(e)}")
        return initial_credit_balance, 0
        
async def update_inference_sn_reputation_score(supernode_pastelid: str, reputation_score: float) -> bool:
    try:
        # TODO: Implement the logic to update the inference SN reputation score
        # This could involve storing the reputation score in a database or broadcasting it to other supernodes
        # For now, let's assume the reputation score is updated successfully
        return True
    except Exception as e:
        logger.error(f"Error updating inference SN reputation score: {str(e)}")
        raise

async def get_inference_api_usage_request_for_audit(inference_request_id: str) -> db_code.InferenceAPIUsageRequest:
    async with db_code.Session() as db_session:
        query = await db_session.exec(
            select(db_code.InferenceAPIUsageRequest).where(db_code.InferenceAPIUsageRequest.inference_request_id == inference_request_id)
        )
        result = query.one_or_none()
        return result
        
async def get_inference_api_usage_response_for_audit(inference_request_or_response_id: str) -> db_code.InferenceAPIUsageResponse:
    async with db_code.Session() as db_session:
        query1 = await db_session.exec(
            select(db_code.InferenceAPIUsageResponse).where(db_code.InferenceAPIUsageResponse.inference_request_id == inference_request_or_response_id)
        )
        result1 = query1.one_or_none()
        query2 = await db_session.exec(
            select(db_code.InferenceAPIUsageResponse).where(db_code.InferenceAPIUsageResponse.inference_response_id == inference_request_or_response_id)
        )
        result2 = query2.one_or_none()        
        if result1 is not None:
            result = result1
        elif result2 is not None:
            result = result2
        return result

async def get_inference_api_usage_result_for_audit(inference_request_or_response_id: str) -> db_code.InferenceAPIOutputResult:
    async with db_code.Session() as db_session:
        query1 = await db_session.exec(
            select(db_code.InferenceAPIOutputResult).where(db_code.InferenceAPIOutputResult.inference_request_id == inference_request_or_response_id)
        )
        result1 = query1.one_or_none()
        query2 = await db_session.exec(
            select(db_code.InferenceAPIOutputResult).where(db_code.InferenceAPIOutputResult.inference_response_id == inference_request_or_response_id)
        )
        result2 = query2.one_or_none()        
        if result1 is not None:
            result = result1
        elif result2 is not None:
            result = result2        
        return result
    
# ________________________________________________________________________________________________________________________________

# Blockchain ticket related functions:

def check_if_transparent_psl_address_is_valid_func(pastel_address_string):
    if len(pastel_address_string) == 35 and (pastel_address_string[0:2] == 'Pt'):
        pastel_address_is_valid = 1
    else:
        pastel_address_is_valid = 0
    return pastel_address_is_valid

def check_if_transparent_lsp_address_is_valid_func(pastel_address_string):
    if len(pastel_address_string) == 35 and (pastel_address_string[0:2] == 'tP'):
        pastel_address_is_valid = 1
    else:
        pastel_address_is_valid = 0
    return pastel_address_is_valid

def check_if_transparent_address_is_valid_func(pastel_address_string):
    pastel_address_is_valid = 0
    if rpc_port == '9932':
        if len(pastel_address_string) == 35 and (pastel_address_string[0:2] == 'Pt'):
            pastel_address_is_valid = 1
    elif rpc_port == '19932':
        if len(pastel_address_string) == 35 and (pastel_address_string[0:2] == 'tP'):
            pastel_address_is_valid = 1
    elif rpc_port == '29932':
        if len(pastel_address_string) == 36 and (pastel_address_string[0:2] == '44'):
            pastel_address_is_valid = 1
    return pastel_address_is_valid             

async def get_df_json_from_tickets_list_rpc_response_func(rpc_response):
    tickets_df = pd.DataFrame.from_records([rpc_response[idx]['ticket'] for idx, x in enumerate(rpc_response)])
    tickets_df['txid'] = [rpc_response[idx]['txid'] for idx, x in enumerate(rpc_response)]
    tickets_df['height'] = [rpc_response[idx]['height'] for idx, x in enumerate(rpc_response)]
    tickets_df_json = tickets_df.to_json(orient='index')
    return tickets_df_json

async def import_address_func(address: str, label: str = "", rescan: bool = False) -> None:
    try:
        await rpc_connection.importaddress(address, label, rescan)
        logger.info(f"Imported address: {address}")
    except Exception as e:
        logger.error(f"Error importing address: {address}. Error: {e}")

async def check_if_address_is_already_imported_in_local_wallet(address_to_check):
    address_amounts_dict = await listaddressamounts(rpc_connection)
    # Convert the dictionary into a list of dictionaries, each representing a row
    data = [{'address': address, 'amount': amount} for address, amount in address_amounts_dict.items()]
    # Create the DataFrame from the list of dictionaries
    address_amounts_df = pd.DataFrame(data)        
    # Filter the DataFrame for the specified address
    address_amounts_df_filtered = address_amounts_df[address_amounts_df['address'] == address_to_check]
    if address_amounts_df_filtered.empty:
        return False
    return True

async def get_and_decode_raw_transaction(txid: str) -> dict:
    try:
        # Retrieve the raw transaction data
        raw_tx_data = await getrawtransaction(rpc_connection, txid)
        if not raw_tx_data:
            logger.error(f"Failed to retrieve raw transaction data for {txid}")
            return {}
        return raw_tx_data
    except Exception as e:
        logger.error(f"Error in get_and_decode_transaction for {txid}: {e}")
        return {}

async def get_transaction_details(txid: str, include_watchonly: bool = False) -> dict:
    try:
        # Call the 'gettransaction' RPC method with the provided txid and includeWatchonly flag
        transaction_details = await gettransaction(rpc_connection, txid, include_watchonly)
        # Log the retrieved transaction details
        logger.info(f"Retrieved transaction details for {txid}: {transaction_details}")
        return transaction_details
    except Exception as e:
        logger.error(f"Error retrieving transaction details for {txid}: {e}")
        return {}

#Misc helper functions:
class MyTimer():
    def __init__(self):
        self.start = time.time()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        end = time.time()
        runtime = end - self.start
        msg = '({time} seconds to complete)'
        logger.info(msg.format(time=round(runtime, 2)))

def compute_elapsed_time_in_minutes_between_two_datetimes_func(start_datetime, end_datetime):
    time_delta = (end_datetime - start_datetime)
    total_seconds_elapsed = time_delta.total_seconds()
    total_minutes_elapsed = total_seconds_elapsed / 60
    return total_minutes_elapsed

def compute_elapsed_time_in_minutes_since_start_datetime_func(start_datetime):
    end_datetime = datetime.utcnow()
    total_minutes_elapsed = compute_elapsed_time_in_minutes_between_two_datetimes_func(start_datetime, end_datetime)
    return total_minutes_elapsed

def check_if_ip_address_is_valid_func(ip_address_string):
    try:
        _ = ipaddress.ip_address(ip_address_string)
        ip_address_is_valid = 1
    except Exception as e:
        logger.error('Validation Error: ' + str(e))
        ip_address_is_valid = 0
    return ip_address_is_valid

def compare_datetimes(datetime_input1, datetime_input2):
    # Check if the inputs are datetime objects, otherwise parse them
    if not isinstance(datetime_input1, datetime):
        datetime_input1 = pd.to_datetime(datetime_input1)
    if not isinstance(datetime_input2, datetime):
        datetime_input2 = pd.to_datetime(datetime_input2)
    # Ensure both datetime objects are timezone-aware
    if datetime_input1.tzinfo is None:
        datetime_input1 = datetime_input1.replace(tzinfo=pytz.UTC)
    if datetime_input2.tzinfo is None:
        datetime_input2 = datetime_input2.replace(tzinfo=pytz.UTC)
    # Calculate the difference in seconds
    difference_in_seconds = abs((datetime_input2 - datetime_input1).total_seconds())
    # Check if the difference is within the acceptable range
    datetimes_are_close_enough_to_consider_them_matching = (
        difference_in_seconds <= MAXIMUM_LOCAL_UTC_TIMESTAMP_DIFFERENCE_IN_SECONDS
    )
    if not datetimes_are_close_enough_to_consider_them_matching:
        logger.warning(f"Timestamps are too far apart: {difference_in_seconds} seconds")
    return difference_in_seconds, datetimes_are_close_enough_to_consider_them_matching

def get_sha256_hash_of_input_data_func(input_data_or_string):
    if isinstance(input_data_or_string, str):
        input_data_or_string = input_data_or_string.encode('utf-8')
    sha256_hash_of_input_data = hashlib.sha3_256(input_data_or_string).hexdigest()
    return sha256_hash_of_input_data

def sort_dict_by_keys(input_dict):
    # Sort the main dictionary by keys and then sort each nested dictionary by keys
    sorted_dict = {key: dict(sorted(value.items())) for key, value in sorted(input_dict.items())}
    return json.dumps(sorted_dict, indent=4)  # Convert the dictionary to a JSON string for output

async def extract_response_fields_from_credit_pack_ticket_message_data_as_json_func(model_instance: SQLModel) -> str:
    response_fields = {}
    last_hash_field_name = None
    last_signature_field_names = []
    for field_name in model_instance.__fields__.keys():
        if field_name.startswith("sha3_256_hash_of"):
            last_hash_field_name = field_name
        elif "_signature_on_" in field_name:
            last_signature_field_names.append(field_name)
    if isinstance(model_instance, db_code.CreditPackPurchasePriceAgreementRequestResponse):
        fields_to_exclude = [last_hash_field_name, 'id'] + last_signature_field_names
    else:
        fields_to_exclude = [last_hash_field_name, last_signature_field_names[-1], 'id']
    for field_name, field_value in model_instance.__dict__.items():
        if field_name in fields_to_exclude or '_sa_instance_state' in field_name:
            continue
        if field_value is not None:
            if isinstance(field_value, (datetime, date)):
                response_fields[field_name] = field_value.isoformat()
            elif isinstance(field_value, list):
                response_fields[field_name] = json.dumps(field_value, ensure_ascii=False, sort_keys=True)
            elif isinstance(field_value, dict):
                response_fields[field_name] = sort_dict_by_keys(field_value)
            elif isinstance(field_value, decimal.Decimal):
                response_fields[field_name] = str(field_value)
            elif isinstance(field_value, bool):
                response_fields[field_name] = int(field_value)
            else:
                response_fields[field_name] = field_value
    sorted_response_fields = dict(sorted(response_fields.items()))
    return json.dumps(sorted_response_fields, ensure_ascii=False, sort_keys=True)

async def compute_sha3_256_hash_of_sqlmodel_response_fields(model_instance: SQLModel) -> str:
    response_fields_json = await extract_response_fields_from_credit_pack_ticket_message_data_as_json_func(model_instance)
    sha256_hash_of_response_fields = get_sha256_hash_of_input_data_func(response_fields_json)
    return sha256_hash_of_response_fields

async def validate_credit_pack_blockchain_ticket_data_field_hashes(model_instance: SQLModel):
    validation_errors = []
    # model_instance.credit_purchase_request_response_message_version_string = str(model_instance.credit_purchase_request_response_message_version_string)
    response_fields_json = await extract_response_fields_from_credit_pack_ticket_message_data_as_json_func(model_instance)
    expected_hash = get_sha256_hash_of_input_data_func(response_fields_json)
    last_hash_field_name = None
    for field_name in model_instance.__fields__:
        if field_name.startswith("sha3_256_hash_of") and field_name.endswith("_fields"):
            last_hash_field_name = field_name
    if last_hash_field_name:
        actual_hash = getattr(model_instance, last_hash_field_name)
        if actual_hash != expected_hash:
            # print('Skipping hash validation check for now...') # TODO: Fix this!
            validation_errors.append(f"SHA3-256 hash in field {last_hash_field_name} does not match the computed hash of the response fields")    
    return validation_errors

async def validate_credit_pack_ticket_message_data_func(model_instance: SQLModel):
    validation_errors = []
    # Validate timestamp fields
    for field_name, field_value in model_instance.__dict__.items():
        if field_name.endswith("_timestamp_utc_iso_string"):
            try:
                pd.to_datetime(field_value)
            except ValueError:
                validation_errors.append(f"Invalid timestamp format for field {field_name}")
            # Check if the timestamp is within an acceptable range of the current time
            current_timestamp = pd.to_datetime(datetime.utcnow().replace(tzinfo=pytz.UTC))
            timestamp_diff, timestamps_match = compare_datetimes(field_value, current_timestamp)
            if not timestamps_match:
                validation_errors.append(f"Timestamp in field {field_name} is too far from the current time")
    # Validate pastel block height fields
    best_block_hash, best_block_merkle_root, best_block_height = await get_best_block_hash_and_merkle_root_func()
    for field_name, field_value in model_instance.__dict__.items():
        if field_name.endswith("_pastel_block_height"):
            if abs(field_value - best_block_height) > MAXIMUM_LOCAL_PASTEL_BLOCK_HEIGHT_DIFFERENCE_IN_BLOCKS:
                validation_errors.append(f"Pastel block height in field {field_name} does not match the current block height; difference is {abs(field_value - best_block_height)} blocks (local: {field_value}, remote: {best_block_height})")
    # Validate hash fields
    expected_hash = await compute_sha3_256_hash_of_sqlmodel_response_fields(model_instance)
    hash_field_name = None
    last_hash_field_name = None
    for field_name in model_instance.__fields__:
        if field_name.startswith("sha3_256_hash_of") and field_name.endswith("_fields"):
            last_hash_field_name = field_name
    if last_hash_field_name:
        actual_hash = getattr(model_instance, last_hash_field_name)
        if actual_hash != expected_hash:
            validation_errors.append(f"SHA3-256 hash in field {last_hash_field_name} does not match the computed hash of the response fields")
    # Validate pastelid signature fields
    if isinstance(model_instance, db_code.CreditPackPurchasePriceAgreementRequestResponse):
        signature_field_names = []
        hash_field_name = None
        for field_name in model_instance.__fields__:
            if "_pastelid" in field_name:
                first_pastelid = field_name
                break
        for field_name in model_instance.__fields__:
            if "_signature_on_" in field_name:
                signature_field_names.append(field_name)
            elif "sha3_256_hash_of_" in field_name and field_name.endswith("_fields"):
                hash_field_name = field_name
        if signature_field_names and hash_field_name:
            if hasattr(model_instance, first_pastelid):
                pastelid = getattr(model_instance, first_pastelid)
                for signature_field_name in signature_field_names:
                    if signature_field_name == "responding_supernode_signature_on_price_agreement_request_response_hash":
                        message_to_verify = getattr(model_instance, "sha3_256_hash_of_price_agreement_request_response_fields")
                    elif signature_field_name == "responding_supernode_signature_on_credit_pack_purchase_request_fields_json_b64":
                        message_to_verify = getattr(model_instance, "credit_pack_purchase_request_fields_json_b64")
                    else:
                        continue
                    signature = getattr(model_instance, signature_field_name)
                    verification_result = await verify_message_with_pastelid_func(pastelid, message_to_verify, signature)
                    if verification_result != 'OK':
                        validation_errors.append(f"Pastelid signature in field {signature_field_name} failed verification")
            else:
                validation_errors.append(f"Corresponding pastelid field {first_pastelid} not found for signature fields {signature_field_names}")
    else:
        # Validate pastelid signature fields
        last_signature_field_name = None
        last_hash_field_name = None
        for field_name in model_instance.__fields__:
            if "_pastelid" in field_name:
                first_pastelid = field_name
                break
        for field_name in model_instance.__fields__:
            if "_signature_on_" in field_name:
                last_signature_field_name = field_name
            elif "sha3_256_hash_of_" in field_name and field_name.endswith("_fields"):
                last_hash_field_name = field_name
        if last_signature_field_name and last_hash_field_name:
            signature_field_name = last_signature_field_name
            hash_field_name = last_hash_field_name
            if first_pastelid == last_signature_field_name:
                first_pastelid = "NA"
            if hasattr(model_instance, first_pastelid) or first_pastelid == "NA":
                if first_pastelid == "NA":
                    pastelid_and_signature_combined_field_name = last_signature_field_name
                    pastelid_and_signature_combined_field_json = getattr(model_instance, pastelid_and_signature_combined_field_name)
                    pastelid_and_signature_combined_field_dict = json.loads(pastelid_and_signature_combined_field_json)
                    pastelid_and_signature_combined_field_dict_keys = pastelid_and_signature_combined_field_dict.keys()
                    for current_key in pastelid_and_signature_combined_field_dict_keys:
                        if "pastelid" in current_key:
                            pastelid = pastelid_and_signature_combined_field_dict[current_key]
                        if "signature" in current_key:
                            signature = pastelid_and_signature_combined_field_dict[current_key]
                    message_to_verify = getattr(model_instance, hash_field_name)
                else:
                    pastelid = getattr(model_instance, first_pastelid)
                    message_to_verify = getattr(model_instance, hash_field_name)
                    signature = getattr(model_instance, signature_field_name)
                verification_result = await verify_message_with_pastelid_func(pastelid, message_to_verify, signature)
                if verification_result != 'OK':
                    validation_errors.append(f"Pastelid signature in field {signature_field_name} failed verification")
            else:
                validation_errors.append(f"Corresponding pastelid field {first_pastelid} not found for signature field {signature_field_name}")
        return validation_errors

async def validate_inference_request_message_data_func(model_instance: SQLModel):
    validation_errors = await validate_credit_pack_ticket_message_data_func(model_instance)
    return validation_errors

def get_external_ip_func() -> str:
    urls = [
        "https://ipinfo.io/ip",
        "https://api.ipify.org",
        "https://ifconfig.me"
    ]
    for url in urls:
        try:
            response = httpx.get(url)
            response.raise_for_status()
            return response.text.strip()
        except Exception as e:
            logger.error(f"Failed to get external IP from {url}: {e}")
    raise RuntimeError("Unable to get external IP address from all fallback options.")

def safe_highlight_func(text, pattern, replacement):
    try:
        return re.sub(pattern, replacement, text)
    except Exception as e:
        logger.warning(f"Failed to apply highlight rule: {e}")
        return text

def highlight_rules_func(text):
    rules = [
        (re.compile(r"\b(success\w*)\b", re.IGNORECASE), '#COLOR1_OPEN#', '#COLOR1_CLOSE#'),
        (re.compile(r"\b(error|fail\w*)\b", re.IGNORECASE), '#COLOR2_OPEN#', '#COLOR2_CLOSE#'),
        (re.compile(r"\b(pending)\b", re.IGNORECASE), '#COLOR3_OPEN#', '#COLOR3_CLOSE#'),
        (re.compile(r"\b(response)\b", re.IGNORECASE), '#COLOR4_OPEN#', '#COLOR4_CLOSE#'),
        (re.compile(r'\"(.*?)\"', re.IGNORECASE), '#COLOR5_OPEN#', '#COLOR5_CLOSE#'),
        (re.compile(r"\'(.*?)\'", re.IGNORECASE), "#COLOR6_OPEN#", '#COLOR6_CLOSE#'),
        (re.compile(r"\`(.*?)\`", re.IGNORECASE), '#COLOR7_OPEN#', '#COLOR7_CLOSE#'),
        (re.compile(r"\b(https?://\S+)\b", re.IGNORECASE), '#COLOR8_OPEN#', '#COLOR8_CLOSE#'),
        (re.compile(r"\b(\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2},\d{3})\b", re.IGNORECASE), '#COLOR9_OPEN#', '#COLOR9_CLOSE#'),
        (re.compile(r"\b(_{100,})\b", re.IGNORECASE), '#COLOR10_OPEN#', '#COLOR10_CLOSE#'),
        (re.compile(r"\b(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d+)\b", re.IGNORECASE), '#COLOR11_OPEN#', '#COLOR11_CLOSE#'),
        (re.compile(r"\b([a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12})\b", re.IGNORECASE), '#COLOR12_OPEN#', '#COLOR12_CLOSE#'),
        (re.compile(r"\b([a-f0-9]{64})\b", re.IGNORECASE), '#COLOR13_OPEN#', '#COLOR13_CLOSE#')                                
    ]
    for pattern, replacement_open, replacement_close in rules:
        text = pattern.sub(f"{replacement_open}\\1{replacement_close}", text)
    text = html.escape(text)
    text = text.replace('#COLOR1_OPEN#', '<span style="color: #baffc9;">').replace('#COLOR1_CLOSE#', '</span>')
    text = text.replace('#COLOR2_OPEN#', '<span style="color: #ffb3ba;">').replace('#COLOR2_CLOSE#', '</span>')
    text = text.replace('#COLOR3_OPEN#', '<span style="color: #ffdfba;">').replace('#COLOR3_CLOSE#', '</span>')
    text = text.replace('#COLOR4_OPEN#', '<span style="color: #ffffba;">').replace('#COLOR4_CLOSE#', '</span>')
    text = text.replace('#COLOR5_OPEN#', '<span style="color: #bdc7e7;">').replace('#COLOR5_CLOSE#', '</span>')
    text = text.replace('#COLOR6_OPEN#', "<span style='color: #d5db9c;'>").replace('#COLOR6_CLOSE#', '</span>')
    text = text.replace('#COLOR7_OPEN#', '<span style="color: #a8d8ea;">').replace('#COLOR7_CLOSE#', '</span>')
    text = text.replace('#COLOR8_OPEN#', '<span style="color: #e2a8a8;">').replace('#COLOR8_CLOSE#', '</span>')
    text = text.replace('#COLOR9_OPEN#', '<span style="color: #ece2d0;">').replace('#COLOR9_CLOSE#', '</span>')
    text = text.replace('#COLOR10_OPEN#', '<span style="color: #d6e0f0;">').replace('#COLOR10_CLOSE#', '</span>')
    text = text.replace('#COLOR11_OPEN#', '<span style="color: #f2d2e2;">').replace('#COLOR11_CLOSE#', '</span>')
    text = text.replace('#COLOR12_OPEN#', '<span style="color: #d5f2ea;">').replace('#COLOR12_CLOSE#', '</span>')
    text = text.replace('#COLOR13_OPEN#', '<span style="color: #f2ebd3;">').replace('#COLOR13_CLOSE#', '</span>')
    return text

async def ensure_burn_address_imported_as_watch_address_in_local_wallet():
    burn_address_already_imported = await check_if_address_is_already_imported_in_local_wallet(burn_address)
    if not burn_address_already_imported:
        logger.info(f"Burn address is NOT yet imported! Now attempting to import burn address {burn_address} as a watch address in the local wallet...")
        await import_address_func(burn_address, "burn_address", True)
    else:
        logger.info(f"Burn address {burn_address} is already imported as a watch address in the local wallet!")
    

#_______________________________________________________________


rpc_host, rpc_port, rpc_user, rpc_password, other_flags = get_local_rpc_settings_func()
network, burn_address = get_network_info(rpc_port)
masternode_collateral_amount = required_collateral(network)
rpc_connection = AsyncAuthServiceProxy(f"http://{rpc_user}:{rpc_password}@{rpc_host}:{rpc_port}")

if rpc_port == '9932':
    burn_address = 'PtpasteLBurnAddressXXXXXXXXXXbJ5ndd'
elif rpc_port == '19932':
    burn_address = 'tPpasteLBurnAddressXXXXXXXXXXX3wy7u'
elif rpc_port == '29932':
    burn_address = '44oUgmZSL997veFEQDq569wv5tsT6KXf9QY7' # https://blockchain-devel.slack.com/archives/C03Q2MCQG9K/p1705896449986459

encryption_key = generate_or_load_encryption_key_sync()  # Generate or load the encryption key synchronously    
decrypt_sensitive_fields()
MY_PASTELID = asyncio.run(get_my_local_pastelid_func())
logger.info(f"Using local PastelID: {MY_PASTELID}")
if 'genpassphrase' in other_flags.keys():
    LOCAL_PASTEL_ID_PASSPHRASE = other_flags['genpassphrase']

use_encrypt_new_secrets = 0
if use_encrypt_new_secrets:
    encrypted_openai_key = encrypt_sensitive_data("abc123", encryption_key)
    print(f"Encrypted OpenAI key: {encrypted_openai_key}")
    
    encrypted_groq_key = encrypt_sensitive_data("abc123", encryption_key)
    print(f"Encrypted groq key: {encrypted_groq_key}")

    encrypted_mistral_key = encrypt_sensitive_data("abc123", encryption_key)
    print(f"Encrypted mistral key: {encrypted_mistral_key}")
    
    encrypted_stability_key = encrypt_sensitive_data("abc123", encryption_key)
    print(f"Encrypted stability key: {encrypted_stability_key}")    
    
    encrypted_openrouter_key = encrypt_sensitive_data("abc123", encryption_key)
    print(f"Encrypted openrouter key: {encrypted_openrouter_key}")
    
use_test_market_price_data = 0
if use_test_market_price_data:
    current_psl_price = asyncio.run(fetch_current_psl_market_price())
    
use_get_inference_model_menu_on_start = 0
if use_get_inference_model_menu_on_start:
    random_async_wait_duration_in_seconds = round(random.random()*10.0, 3)
    logger.info(f"Checking API keys and getting inference model menu (but first waiting for a random period of {random_async_wait_duration_in_seconds} seconds to not overwhelm the APIs)...")
    asyncio.run(asyncio.sleep(random_async_wait_duration_in_seconds))
    use_verbose=1
    asyncio.run(get_inference_model_menu(use_verbose))


---
./model_menu.json
---
{
  "models": [
    {
      "model_name": "swiss_army_llama-Llama-3.1-8B-Lexi-Uncensored_V2_Q5",
      "model_url": "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/blob/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf",
      "description": "The Llama-3.1-8B-Lexi-Uncensored V2, based on the powerful Meta Llama-3.1-8B model, is designed for uncensored text generation with a focus on creative and innovative content.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Grammar File String: The grammar file used to restrict text generation [Optional] (Default is to not use any grammar file. Examples: `json`, `list`)"
        },
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "text_completion": {
          "input_tokens": 1.6,
          "output_tokens": 1.2,
          "compute_cost": 0.9,
          "memory_cost": 0.8
        },
        "embedding": {
          "input_tokens": 0.6,
          "compute_cost": 0.7,
          "memory_cost": 0.5
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.15,
          "total_sentences": 0.08,
          "query_string_included": 0.35,
          "compute_cost": 1.0,
          "memory_cost": 0.7
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.03,
          "query_string_included": 0.35,
          "compute_cost": 1.1,
          "memory_cost": 0.9
        }
      }
    },
    {
      "model_name": "swiss_army_llama-Hermes-3-Llama-3.1-8B.Q4_K_M",
      "model_url": "https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/blob/main/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf",
      "description": "Hermes 2 is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house. This version excels at Function Calling, JSON Structured Outputs, and has improved on several other metrics, scoring 90% on function calling evaluation and 84% on structured JSON Output evaluation.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Grammar File String: The grammar file used to restrict text generation [Optional] (Default is to not use any grammar file. Examples: `json`, `list`)"
        },
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "text_completion": {
          "input_tokens": 1.8,
          "output_tokens": 1.4,
          "compute_cost": 1.0,
          "memory_cost": 0.9
        },
        "embedding": {
          "input_tokens": 0.7,
          "compute_cost": 0.8,
          "memory_cost": 0.5
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.2,
          "total_sentences": 0.1,
          "query_string_included": 0.4,
          "compute_cost": 1.2,
          "memory_cost": 0.8
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.5,
          "query_string_included": 1.5,
          "compute_cost": 1.2,
          "memory_cost": 1.0
        }
      }
    },
    {
      "model_name": "swiss_army_llama-Meta-Llama-3.1-8B-Instruct-Q5_K_M",
      "model_url": "https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
      "description": "The Meta-Llama-3.1-8B-Instruct model is designed for instruction-following tasks with optimized quantization for efficient computation.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Grammar File String: The grammar file used to restrict text generation [Optional] (Default is to not use any grammar file. Examples: `json`, `list`)"
        },
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "text_completion": {
          "input_tokens": 1.5,
          "output_tokens": 1.1,
          "compute_cost": 0.8,
          "memory_cost": 0.7
        },
        "embedding": {
          "input_tokens": 0.5,
          "compute_cost": 0.6,
          "memory_cost": 0.4
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.1,
          "total_sentences": 0.05,
          "query_string_included": 0.3,
          "compute_cost": 0.9,
          "memory_cost": 0.6
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.02,
          "query_string_included": 0.3,
          "compute_cost": 1.0,
          "memory_cost": 0.8
        }
      }
    },
    {
      "model_name": "swiss_army_llama-llava-llama-3-8b-v1_1-int4",
      "model_url": "https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf/resolve/main/llava-llama-3-8b-v1_1-int4.gguf",
      "description": "The LLaVA-LLaMA-3-8B-v1.1 model is a powerful language model developed by XTuner; it allows you to ask questions about images and get answers in a conversational manner.",
      "supported_inference_type_strings": ["ask_question_about_an_image"],
      "input_fields": [
        {
          "name": "image_data",
          "file_type": "base64",
          "optional": false
        },
        {
          "name": "question",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "ask_question_about_an_image": {
          "input_tokens": 1.5,
          "output_tokens": 1.1,
          "compute_cost": 1.2,
          "memory_cost": 1.0
        }
      }
    },
    {
      "model_name": "swiss_army_llama-bge-m3-q8_0",
      "model_url": "https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf",
      "description": "BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It performs dense retrieval, multi-vector retrieval, and sparse retrieval, providing a unified model foundation for real-world IR applications.",
      "supported_inference_type_strings": [
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "embeddings",
          "file_type": "json",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "embedding": {
          "input_tokens": 0.4,
          "compute_cost": 0.5,
          "memory_cost": 0.3
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.05,
          "total_sentences": 0.03,
          "query_string_included": 0.2,
          "compute_cost": 0.6,
          "memory_cost": 0.4
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.01,
          "query_string_included": 0.2,
          "compute_cost": 0.6,
          "memory_cost": 0.5
        }
      }
    },
    {
      "model_name": "swiss_army_llama-nomic-embed-text-v1.5.Q6_K",
      "model_url": "https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF/resolve/main/nomic-embed-text-v1.5.Q6_K.gguf",
      "description": "Nomic Embed Text v1.5 is a powerful language model developed by Nomic AI. It provides high-quality embeddings for semantic search and clustering tasks.",
      "supported_inference_type_strings": [
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "embeddings",
          "file_type": "json",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "embedding": {
          "input_tokens": 0.2,
          "compute_cost": 0.2,
          "memory_cost": 0.1
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.05,
          "total_sentences": 0.02,
          "query_string_included": 0.2,
          "compute_cost": 0.4,
          "memory_cost": 0.3
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.01,
          "query_string_included": 0.2,
          "compute_cost": 0.5,
          "memory_cost": 0.4
        }
      }
    },
    {
      "model_name": "openai-gpt-4o",
      "model_url": "",
      "description": "GPT-4o is OpenAI's most advanced multimodal model that is faster and cheaper than GPT-4 Turbo with stronger vision capabilities. The model has 128K context and an October 2023 knowledge cutoff.",
      "supported_inference_type_strings": ["text_completion"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": true
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openai-gpt-4o-vision",
      "model_url": "",
      "description": "GPT-4o is OpenAI's most advanced multimodal model that\u2019s faster and cheaper than GPT-4 Turbo with stronger vision capabilities. The model has 128K context and an October 2023 knowledge cutoff.",
      "supported_inference_type_strings": ["ask_question_about_an_image"],
      "input_fields": [
        {
          "name": "image_data",
          "file_type": "base64",
          "optional": false
        },
        {
          "name": "question",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openai-gpt-4-turbo",
      "model_url": "",
      "description": "GPT-4 Turbo is offered at 128K context with an April 2023 knowledge cutoff and basic support for vision.",
      "supported_inference_type_strings": ["text_completion"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": true
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openai-text-embedding-ada-002",
      "model_url": "",
      "description": "OpenAI's text embedding model. Provides high-quality embeddings for semantic search and clustering tasks.",
      "supported_inference_type_strings": ["embedding"],
      "input_fields": [
        {
          "name": "input_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "embedding",
          "file_type": "embedding",
          "optional": false
        }
      ],
      "model_parameters": [],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "claude3-haiku",
      "model_url": "",
      "description": "The Claude3 Haiku model is a powerful language model developed by Anthropic. It offers advanced natural language processing capabilities and can be accessed through the Claude API.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "inference_types_parameter_applies_to": "['text_completion']",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "claude3-opus",
      "model_url": "",
      "description": "The Claude3 Opus model is a powerful language model developed by Anthropic. It offers advanced natural language processing capabilities and can be accessed through the Claude API.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "claude3.5-sonnet",
      "model_url": "",
      "description": "The Claude3.5 Sonnet model is a powerful language model developed by Anthropic. It offers advanced natural language processing capabilities and can be accessed through the Claude API.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama-3.1-70b-versatile",
      "model_url": "",
      "description": "LLaMA3.1 70b versatile is a powerful language model developed by Meta with a context window of 131,072 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama-3.1-8b-instant",
      "model_url": "",
      "description": "LLaMA3.1 8b instant is a powerful language model developed by Meta with a context window of 131,072 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama3-70b-8192",
      "model_url": "",
      "description": "LLaMA3 70b is a powerful language model developed by Meta with a context window of 8,192 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama3-8b-8192",
      "model_url": "",
      "description": "LLaMA3 8b is a powerful but compact language model developed by Meta with a context window of 8,192 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-mixtral-8x7b-32768",
      "model_url": "",
      "description": "Mixtral 8x7b is a powerful language model developed by Mistral with a context window of 32,768 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-gemma2-9b-it",
      "model_url": "",
      "description": "Gemma 2 9b is an LLM developed by Google with a context window of 8,192 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-mistral-small-latest",
      "model_url": "",
      "description": "Mistral small is a powerful language model developed by Mistral AI.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-mistral-medium-latest",
      "model_url": "",
      "description": "Mistral medium is a powerful language model developed by Mistral AI.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-mistral-large-latest",
      "model_url": "",
      "description": "Mistral large is a powerful language model developed by Mistral AI.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-core",
      "model_url": "",
      "description": "Stable Image Core is the best quality achievable at high speed for text-to-image generation.",
      "supported_inference_type_strings": [
        "text_to_image"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "aspect_ratio",
          "type": "string",
          "default": "1:1",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The aspect ratio of the generated image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Random seed to use for generation [Optional]"
        },
        {
          "name": "style_preset",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Style Preset: Guides the image model towards a particular style (e.g., '3d-model', 'analog-film', 'anime', etc.) [Optional]"
        },
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The format of the generated image (e.g., 'png', 'jpeg', 'webp') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-sd3-large",
      "model_url": "",
      "description": "Stable Diffusion 3 Large model for high-quality image generation.",
      "supported_inference_type_strings": [
        "text_to_image"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "aspect_ratio",
          "type": "string",
          "default": "1:1",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The aspect ratio of the generated image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Random seed to use for generation [Optional]"
        },
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The format of the generated image (e.g., 'png', 'jpeg') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-conservative-upscale",
      "model_url": "",
      "description": "Conservative upscaling model that preserves image details.",
      "supported_inference_type_strings": [
        "conservative_upscale"
      ],
      "input_fields": [
        {
          "name": "image",
          "file_type": "image",
          "optional": false
        },
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "upscaled_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["conservative_upscale"],
          "description": "The format of the upscaled image (e.g., 'png', 'jpeg', 'webp') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["conservative_upscale"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["conservative_upscale"],
          "description": "Random seed to use for upscaling [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-creative-upscale",
      "model_url": "",
      "description": "Creative upscaling model that can enhance and add details to images.",
      "supported_inference_type_strings": [
        "creative_upscale"
      ],
      "input_fields": [
        {
          "name": "image",
          "file_type": "image",
          "optional": false
        },
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "upscaled_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["creative_upscale"],
          "description": "The format of the upscaled image (e.g., 'png', 'jpeg', 'webp') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["creative_upscale"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["creative_upscale"],
          "description": "Random seed to use for upscaling [Optional]"
        },
        {
          "name": "creativity",
          "type": "float",
          "default": 0.3,
          "min": 0,
          "max": 0.35,
          "inference_types_parameter_applies_to": ["creative_upscale"],
          "description": "Controls the creativity level of the upscaling process [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-sd3-medium",
      "model_url": "",
      "description": "Stable Diffusion 3 Medium model for balanced performance and quality in image generation.",
      "supported_inference_type_strings": [
        "text_to_image"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "aspect_ratio",
          "type": "string",
          "default": "1:1",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The aspect ratio of the generated image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Random seed to use for generation [Optional]"
        },
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The format of the generated image (e.g., 'png', 'jpeg') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-sd3-large-turbo",
      "model_url": "",
      "description": "Stable Diffusion 3 Large Turbo model for faster inference with high-quality results.",
      "supported_inference_type_strings": [
        "text_to_image"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "aspect_ratio",
          "type": "string",
          "default": "1:1",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The aspect ratio of the generated image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Random seed to use for generation [Optional]"
        },
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The format of the generated image (e.g., 'png', 'jpeg') [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/auto",
      "model_url": "",
      "description": "OpenRouter's auto-routing model that automatically selects the best available model for the given task.",
      "supported_inference_type_strings": ["text_completion"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    }
  ]
}


---
./pyproject.toml
---
[tool.ruff]
ignore = ["E501"]

---


---
./end_user_standalone_client.py
---
import asyncio
import json
import httpx
import os
import logging
import shutil
import queue
import zstandard as zstd
import base64
import pytz
import hashlib
import urllib.parse as urlparse
import re
import random
import time
import traceback
import uuid
from decimal import Decimal
import decimal
import pandas as pd
from datetime import datetime, date, timezone
from typing import List, Dict, Union, Any, Optional, Tuple
from logging.handlers import RotatingFileHandler, QueueHandler, QueueListener
from httpx import AsyncClient, Limits, Timeout
from decouple import Config as DecoupleConfig, RepositoryEnv
from sqlmodel import SQLModel, Field, Column, JSON

# Note: you must have `minrelaytxfee=0.00001` in your pastel.conf to allow "dust" transactions for the inference request confirmation transactions to work!

logger = logging.getLogger("pastel_supernode_inference_client")

config = DecoupleConfig(RepositoryEnv('.env'))
MESSAGING_TIMEOUT_IN_SECONDS = config.get("MESSAGING_TIMEOUT_IN_SECONDS", default=60, cast=int)
MY_LOCAL_PASTELID = config.get("MY_LOCAL_PASTELID", cast=str)
# MY_PASTELID_PASSPHRASE = config.get("MY_PASTELID_PASSPHRASE", cast=str)
MY_PASTELID_PASSPHRASE = "5QcX9nX67buxyeC"
MAXIMUM_LOCAL_CREDIT_PRICE_DIFFERENCE_TO_ACCEPT_CREDIT_PRICING = config.get("MAXIMUM_LOCAL_CREDIT_PRICE_DIFFERENCE_TO_ACCEPT_CREDIT_PRICING", default=0.1, cast=float)
MAXIMUM_LOCAL_PASTEL_BLOCK_HEIGHT_DIFFERENCE_IN_BLOCKS = config.get("MAXIMUM_LOCAL_PASTEL_BLOCK_HEIGHT_DIFFERENCE_IN_BLOCKS", default=1, cast=int)
TARGET_VALUE_PER_CREDIT_IN_USD = config.get("TARGET_VALUE_PER_CREDIT_IN_USD", default=0.1, cast=float)
TARGET_PROFIT_MARGIN = config.get("TARGET_PROFIT_MARGIN", default=0.1, cast=float)
MAXIMUM_LOCAL_CREDIT_PRICE_DIFFERENCE_TO_ACCEPT_CREDIT_PRICING = config.get("MAXIMUM_LOCAL_CREDIT_PRICE_DIFFERENCE_TO_ACCEPT_CREDIT_PRICING", default=0.1, cast=float)
MAXIMUM_PER_CREDIT_PRICE_IN_PSL_FOR_CLIENT = config.get("MAXIMUM_PER_CREDIT_PRICE_IN_PSL_FOR_CLIENT", default=100.0, cast=float)

def setup_logger():
    if logger.handlers:
        return logger
    old_logs_dir = 'old_logs'
    if not os.path.exists(old_logs_dir):
        os.makedirs(old_logs_dir)
    logger.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    log_file_path = 'pastel_supernode_inference_client.log'
    log_queue = queue.Queue(-1)  # Create a queue for the handlers
    fh = RotatingFileHandler(log_file_path, maxBytes=10*1024*1024, backupCount=5)
    fh.setFormatter(formatter)
    def namer(default_log_name):  # Function to move rotated logs to the old_logs directory
        return os.path.join(old_logs_dir, os.path.basename(default_log_name))
    def rotator(source, dest):
        shutil.move(source, dest)
    fh.namer = namer
    fh.rotator = rotator
    sh = logging.StreamHandler()  # Stream handler
    sh.setFormatter(formatter)
    queue_handler = QueueHandler(log_queue)  # Create QueueHandler
    queue_handler.setFormatter(formatter)
    logger.addHandler(queue_handler)
    listener = QueueListener(log_queue, fh, sh)  # Create QueueListener with real handlers
    listener.start()
    logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)  # Configure SQLalchemy logging
    return logger

logger = setup_logger()

def get_local_rpc_settings_func(directory_with_pastel_conf=os.path.expanduser("~/.pastel/")):
    with open(os.path.join(directory_with_pastel_conf, "pastel.conf"), 'r') as f:
        lines = f.readlines()
    other_flags = {}
    rpchost = '127.0.0.1'
    rpcport = '19932'
    for line in lines:
        if line.startswith('rpcport'):
            value = line.split('=')[1]
            rpcport = value.strip()
        elif line.startswith('rpcuser'):
            value = line.split('=')[1]
            rpcuser = value.strip()
        elif line.startswith('rpcpassword'):
            value = line.split('=')[1]
            rpcpassword = value.strip()
        elif line.startswith('rpchost'):
            pass
        elif line == '\n':
            pass
        else:
            current_flag = line.strip().split('=')[0].strip()
            current_value = line.strip().split('=')[1].strip()
            other_flags[current_flag] = current_value
    return rpchost, rpcport, rpcuser, rpcpassword, other_flags

def write_rpc_settings_to_env_file_func(rpc_host, rpc_port, rpc_user, rpc_password, other_flags):
    with open('.env', 'w') as f:
        f.write(f"RPC_HOST={rpc_host}\n")
        f.write(f"RPC_PORT={rpc_port}\n")
        f.write(f"RPC_USER={rpc_user}\n")
        f.write(f"RPC_PASSWORD={rpc_password}\n")
        for current_flag in other_flags:
            current_value = other_flags[current_flag]
            try:
                f.write(f"{current_flag}={current_value}\n")
            except Exception as e:
                logger.error(f"Error writing to .env file: {e}")
                pass
    return

class JSONRPCException(Exception):
    def __init__(self, rpc_error):
        parent_args = []
        try:
            parent_args.append(rpc_error['message'])
        except Exception as e:
            logger.error(f"Error occurred in JSONRPCException: {e}")
            pass
        Exception.__init__(self, *parent_args)
        self.error = rpc_error
        self.code = rpc_error['code'] if 'code' in rpc_error else None
        self.message = rpc_error['message'] if 'message' in rpc_error else None

    def __str__(self):
        return '%d: %s' % (self.code, self.message)

    def __repr__(self):
        return '<%s \'%s\'>' % (self.__class__.__name__, self)

def EncodeDecimal(o):
    if isinstance(o, Decimal):
        return float(round(o, 8))
    raise TypeError(repr(o) + " is not JSON serializable")
    
class AsyncAuthServiceProxy:
    max_concurrent_requests = 5000
    _semaphore = asyncio.BoundedSemaphore(max_concurrent_requests)
    def __init__(self, service_url, service_name=None, reconnect_timeout=15, reconnect_amount=2, request_timeout=20):
        self.service_url = service_url
        self.service_name = service_name
        self.url = urlparse.urlparse(service_url)        
        self.client = AsyncClient(timeout=Timeout(request_timeout), limits=Limits(max_connections=200, max_keepalive_connections=10))
        self.id_count = 0
        user = self.url.username
        password = self.url.password
        authpair = f"{user}:{password}".encode('utf-8')
        self.auth_header = b'Basic ' + base64.b64encode(authpair)
        self.reconnect_timeout = reconnect_timeout
        self.reconnect_amount = reconnect_amount
        self.request_timeout = request_timeout

    def __getattr__(self, name):
        if name.startswith('__') and name.endswith('__'):
            raise AttributeError
        if self.service_name is not None:
            name = f"{self.service_name}.{name}"
        return AsyncAuthServiceProxy(self.service_url, name)

    async def __call__(self, *args):
        async with self._semaphore: # Acquire a semaphore
            self.id_count += 1
            postdata = json.dumps({
                'version': '1.1',
                'method': self.service_name,
                'params': args,
                'id': self.id_count
            }, default=EncodeDecimal)
            headers = {
                'Host': self.url.hostname,
                'User-Agent': "AuthServiceProxy/0.1",
                'Authorization': self.auth_header,
                'Content-type': 'application/json'
            }
            for i in range(self.reconnect_amount):
                try:
                    if i > 0:
                        logger.warning(f"Reconnect try #{i+1}")
                        sleep_time = self.reconnect_timeout * (2 ** i)
                        await asyncio.sleep(sleep_time)
                    response = await self.client.post(
                        self.service_url, headers=headers, data=postdata)
                    break
                except Exception as e:
                    logger.error(f"Error occurred in __call__: {e}")
                    err_msg = f"Failed to connect to {self.url.hostname}:{self.url.port}"
                    rtm = self.reconnect_timeout
                    if rtm:
                        err_msg += f". Waiting {rtm} seconds."
                    logger.exception(err_msg)
            else:
                logger.error("Reconnect tries exceeded.")
                return
            response_json = response.json()
            if response_json['error'] is not None:
                raise JSONRPCException(response_json['error'])
            elif 'result' not in response_json:
                raise JSONRPCException({
                    'code': -343, 'message': 'missing JSON-RPC result'})
            else:
                return response_json['result']
        
async def check_masternode_top_func():
    global rpc_connection
    masternode_top_command_output = await rpc_connection.masternode('top')
    return masternode_top_command_output

async def check_supernode_list_func():
    global rpc_connection
    masternode_list_full_command_output = await rpc_connection.masternodelist('full')
    masternode_list_rank_command_output = await rpc_connection.masternodelist('rank')
    masternode_list_pubkey_command_output = await rpc_connection.masternodelist('pubkey')
    masternode_list_extra_command_output = await rpc_connection.masternodelist('extra')
    masternode_list_full_df = pd.DataFrame([masternode_list_full_command_output[x].split() for x in masternode_list_full_command_output])
    masternode_list_full_df['txid_vout'] = [x for x in masternode_list_full_command_output]
    masternode_list_full_df.columns = ['supernode_status', 'protocol_version', 'supernode_psl_address', 'lastseentime', 'activeseconds', 'lastpaidtime', 'lastpaidblock', 'ipaddress:port', 'txid_vout']
    masternode_list_full_df.index = masternode_list_full_df['txid_vout']
    masternode_list_full_df.drop(columns=['txid_vout'], inplace=True)
    for txid_vout in masternode_list_full_df.index:
        rank = masternode_list_rank_command_output.get(txid_vout)
        pubkey = masternode_list_pubkey_command_output.get(txid_vout)
        extra = masternode_list_extra_command_output.get(txid_vout, {})
        masternode_list_full_df.at[txid_vout, 'rank'] = rank if rank is not None else 'Unknown'
        masternode_list_full_df.at[txid_vout, 'pubkey'] = pubkey if pubkey is not None else 'Unknown'
        masternode_list_full_df.at[txid_vout, 'extAddress'] = extra.get('extAddress', 'Unknown')
        masternode_list_full_df.at[txid_vout, 'extP2P'] = extra.get('extP2P', 'Unknown')
        masternode_list_full_df.at[txid_vout, 'extKey'] = extra.get('extKey', 'Unknown')
    masternode_list_full_df['lastseentime'] = pd.to_numeric(masternode_list_full_df['lastseentime'], downcast='integer')
    masternode_list_full_df['lastpaidtime'] = pd.to_numeric(masternode_list_full_df['lastpaidtime'], downcast='integer')
    masternode_list_full_df['lastseentime'] = pd.to_datetime(masternode_list_full_df['lastseentime'], unit='s')
    masternode_list_full_df['lastpaidtime'] = pd.to_datetime(masternode_list_full_df['lastpaidtime'], unit='s')
    masternode_list_full_df['activeseconds'] = masternode_list_full_df['activeseconds'].astype(int)
    masternode_list_full_df['lastpaidblock'] = masternode_list_full_df['lastpaidblock'].astype(int)
    masternode_list_full_df['activedays'] = masternode_list_full_df['activeseconds'].apply(lambda x: float(x)/86400.0)
    masternode_list_full_df['rank'] = masternode_list_full_df['rank'].astype(int, errors='ignore')
    masternode_list_full_df = masternode_list_full_df[masternode_list_full_df['supernode_status'].isin(['ENABLED', 'PRE_ENABLED'])]
    masternode_list_full_df = masternode_list_full_df[masternode_list_full_df['ipaddress:port'] != '154.38.164.75:29933']  # TODO: Remove this
    masternode_list_full_df__json = masternode_list_full_df.to_json(orient='index')
    return masternode_list_full_df, masternode_list_full_df__json

def get_top_supernode_url(supernode_list_df):
    if not supernode_list_df.empty:
        supernode_list_df = supernode_list_df[supernode_list_df['supernode_status']=='ENABLED'] 
        top_supernode = supernode_list_df.loc[supernode_list_df['rank'] == supernode_list_df['rank'].min()]
        if not top_supernode.empty:
            ipaddress_port = top_supernode['ipaddress:port'].values[0]
            ipaddress = ipaddress_port.split(':')[0]
            supernode_url = f"http://{ipaddress}:7123"
            return supernode_url
    return None

async def get_current_pastel_block_height_func():
    global rpc_connection
    best_block_hash = await rpc_connection.getbestblockhash()
    best_block_details = await rpc_connection.getblock(best_block_hash)
    curent_block_height = best_block_details['height']
    return curent_block_height

async def get_best_block_hash_and_merkle_root_func():
    global rpc_connection
    best_block_height = await get_current_pastel_block_height_func()
    best_block_hash = await rpc_connection.getblockhash(best_block_height)
    best_block_details = await rpc_connection.getblock(best_block_hash)
    best_block_merkle_root = best_block_details['merkleroot']
    return best_block_hash, best_block_merkle_root, best_block_height

def compute_sha3_256_hexdigest(input_str):
    """Compute the SHA3-256 hash of the input string and return the hexadecimal digest."""
    return hashlib.sha3_256(input_str.encode()).hexdigest()

def get_sha256_hash_of_input_data_func(input_data_or_string):
    if isinstance(input_data_or_string, str):
        input_data_or_string = input_data_or_string.encode('utf-8')
    sha256_hash_of_input_data = hashlib.sha3_256(input_data_or_string).hexdigest()
    return sha256_hash_of_input_data

def base64_encode_json(json_input):
    return base64.b64encode(json.dumps(json_input, sort_keys=True).encode('utf-8')).decode('utf-8')

async def extract_response_fields_from_credit_pack_ticket_message_data_as_json_func(model_instance: SQLModel) -> str:
    response_fields = {}
    last_hash_field_name = None
    last_signature_field_names = []
    for field_name in model_instance.__fields__.keys():
        if field_name.startswith("sha3_256_hash_of"):
            last_hash_field_name = field_name
        elif "_signature_on_" in field_name:
            last_signature_field_names.append(field_name)
    fields_to_exclude = [last_hash_field_name, last_signature_field_names[-1], 'id']
    for field_name, field_value in model_instance.__dict__.items():
        if field_name in fields_to_exclude or '_sa_instance_state' in field_name:
            continue
        if field_value is not None:
            if isinstance(field_value, (datetime, date)):
                response_fields[field_name] = field_value.isoformat()
            elif isinstance(field_value, (list, dict)):
                response_fields[field_name] = json.dumps(field_value, ensure_ascii=False, sort_keys=True)
            elif isinstance(field_value, decimal.Decimal):
                response_fields[field_name] = str(field_value)
            elif isinstance(field_value, bool):
                response_fields[field_name] = int(field_value)
            else:
                response_fields[field_name] = field_value
    sorted_response_fields = dict(sorted(response_fields.items()))
    return json.dumps(sorted_response_fields, ensure_ascii=False, sort_keys=True)

async def compute_sha3_256_hash_of_sqlmodel_response_fields(model_instance: SQLModel) -> str:
    response_fields_json = await extract_response_fields_from_credit_pack_ticket_message_data_as_json_func(model_instance)
    sha256_hash_of_response_fields = get_sha256_hash_of_input_data_func(response_fields_json)
    return sha256_hash_of_response_fields

def compare_datetimes(datetime_input1, datetime_input2):
    # Check if the inputs are datetime objects, otherwise parse them
    if not isinstance(datetime_input1, datetime):
        datetime_input1 = pd.to_datetime(datetime_input1)
    if not isinstance(datetime_input2, datetime):
        datetime_input2 = pd.to_datetime(datetime_input2)
    # Ensure both datetime objects are timezone-aware
    if datetime_input1.tzinfo is None:
        datetime_input1 = datetime_input1.replace(tzinfo=pytz.UTC)
    if datetime_input2.tzinfo is None:
        datetime_input2 = datetime_input2.replace(tzinfo=pytz.UTC)
    # Calculate the difference in seconds
    difference_in_seconds = abs((datetime_input2 - datetime_input1).total_seconds())
    # Check if the difference is within the acceptable range
    datetimes_are_close_enough_to_consider_them_matching = (
        difference_in_seconds <= MAXIMUM_LOCAL_CREDIT_PRICE_DIFFERENCE_TO_ACCEPT_CREDIT_PRICING
    )
    return difference_in_seconds, datetimes_are_close_enough_to_consider_them_matching

async def verify_message_with_pastelid_func(pastelid, message_to_verify, pastelid_signature_on_message) -> str:
    global rpc_connection
    verification_result = await rpc_connection.pastelid('verify', message_to_verify, pastelid_signature_on_message, pastelid, 'ed448')
    return verification_result['verification']

async def validate_credit_pack_ticket_message_data_func(model_instance: SQLModel):
    validation_errors = []
    # Validate timestamp fields
    for field_name, field_value in model_instance.__dict__.items():
        if field_name.endswith("_timestamp_utc_iso_string"):
            try:
                pd.to_datetime(field_value)
            except ValueError:
                validation_errors.append(f"Invalid timestamp format for field {field_name}")
            # Check if the timestamp is within an acceptable range of the current time
            current_timestamp = pd.to_datetime(datetime.utcnow().replace(tzinfo=pytz.UTC))
            timestamp_diff, timestamps_match = compare_datetimes(field_value, current_timestamp)
            if not timestamps_match:
                validation_errors.append(f"Timestamp in field {field_name} is too far from the current time")
    # Validate pastel block height fields
    best_block_hash, best_block_merkle_root, best_block_height = await get_best_block_hash_and_merkle_root_func()
    for field_name, field_value in model_instance.__dict__.items():
        if field_name.endswith("_pastel_block_height"):
            if abs(field_value - best_block_height) > MAXIMUM_LOCAL_PASTEL_BLOCK_HEIGHT_DIFFERENCE_IN_BLOCKS:
                validation_errors.append(f"Pastel block height in field {field_name} does not match the current block height; difference is {abs(field_value - best_block_height)} blocks (local: {field_value}, remote: {best_block_height})")
    # Validate hash fields
    expected_hash = await compute_sha3_256_hash_of_sqlmodel_response_fields(model_instance)
    hash_field_name = None
    for field_name in model_instance.__fields__:
        if "sha3_256_hash_of_" in field_name and field_name.endswith("_fields"):
            hash_field_name = field_name
            break
    if hash_field_name:
        actual_hash = getattr(model_instance, hash_field_name)
        if actual_hash != expected_hash:
            validation_errors.append(f"SHA3-256 hash in field {hash_field_name} does not match the computed hash of the response fields")
    # Validate pastelid signature fields
    last_signature_field_name = None
    last_hash_field_name = None
    for field_name in model_instance.__fields__:
        if "_pastelid" in field_name:
            first_pastelid = field_name
            break
    for field_name in model_instance.__fields__:
        if "_signature_on_" in field_name:
            last_signature_field_name = field_name
        elif "sha3_256_hash_of_" in field_name and field_name.endswith("_fields"):
            last_hash_field_name = field_name
    if last_signature_field_name and last_hash_field_name:
        signature_field_name = last_signature_field_name
        hash_field_name = last_hash_field_name
        if first_pastelid == last_signature_field_name:
            first_pastelid = "NA"
        if hasattr(model_instance, first_pastelid) or first_pastelid == "NA":
            if first_pastelid == "NA":
                pastelid_and_signature_combined_field_name = last_signature_field_name
                pastelid_and_signature_combined_field_json = getattr(model_instance, pastelid_and_signature_combined_field_name)
                pastelid_and_signature_combined_field_dict = json.loads(pastelid_and_signature_combined_field_json)
                pastelid_and_signature_combined_field_dict_keys = pastelid_and_signature_combined_field_dict.keys()
                for current_key in pastelid_and_signature_combined_field_dict_keys:
                    if "pastelid" in current_key:
                        pastelid = pastelid_and_signature_combined_field_dict[current_key]
                    if "signature" in current_key:
                        signature = pastelid_and_signature_combined_field_dict[current_key]
                message_to_verify = getattr(model_instance, hash_field_name)
            else:
                pastelid = getattr(model_instance, first_pastelid)
                message_to_verify = getattr(model_instance, hash_field_name)
                signature = getattr(model_instance, signature_field_name)
            verification_result = await verify_message_with_pastelid_func(pastelid, message_to_verify, signature)
            if verification_result != 'OK':
                validation_errors.append(f"Pastelid signature in field {signature_field_name} failed verification")
        else:
            validation_errors.append(f"Corresponding pastelid field {first_pastelid} not found for signature field {signature_field_name}")
    return validation_errors

async def calculate_xor_distance(pastelid1: str, pastelid2: str) -> int:
    hash1 = hashlib.sha3_256(pastelid1.encode()).hexdigest()
    hash2 = hashlib.sha3_256(pastelid2.encode()).hexdigest()
    xor_result = int(hash1, 16) ^ int(hash2, 16)
    return xor_result

def check_if_pastelid_is_valid_func(input_string: str) -> bool:
    # Define the regex pattern to match the conditions:
    # Starts with 'jX'; Followed by characters that are only alphanumeric and are shown in the example;
    pattern = r'^jX[A-Za-z0-9]{84}$'
    if re.match(pattern, input_string):
        return True
    else:
        return False

async def get_supernode_url_from_pastelid_func(pastelid: str, supernode_list_df: pd.DataFrame) -> str:
    is_valid_pastelid = check_if_pastelid_is_valid_func(pastelid)
    if not is_valid_pastelid:
        raise ValueError(f"Invalid PastelID: {pastelid}")
    supernode_row = supernode_list_df[supernode_list_df['extKey'] == pastelid]
    if not supernode_row.empty:
        supernode_ipaddress_port = supernode_row['ipaddress:port'].values[0]
        ipaddress = supernode_ipaddress_port.split(':')[0]
        supernode_url = f"http://{ipaddress}:7123"
        return supernode_url
    else:
        raise ValueError(f"Supernode with PastelID {pastelid} not found in the supernode list")

async def get_closest_supernode_to_pastelid_url(input_pastelid: str, supernode_list_df: pd.DataFrame) -> Tuple[Optional[str], Optional[str]]:
    if not supernode_list_df.empty:
        list_of_supernode_pastelids = supernode_list_df['extKey'].tolist()
        closest_supernode_pastelid = await get_closest_supernode_pastelid_from_list(input_pastelid, list_of_supernode_pastelids)
        supernode_url = await get_supernode_url_from_pastelid_func(closest_supernode_pastelid, supernode_list_df)
        return supernode_url, closest_supernode_pastelid
    return None, None

async def get_n_closest_supernodes_to_pastelid_urls(n: int, input_pastelid: str, supernode_list_df: pd.DataFrame) -> List[Tuple[str, str]]:
    if not supernode_list_df.empty:
        list_of_supernode_pastelids = supernode_list_df['extKey'].tolist()
        xor_distances = [(supernode_pastelid, await calculate_xor_distance(input_pastelid, supernode_pastelid)) for supernode_pastelid in list_of_supernode_pastelids]
        sorted_xor_distances = sorted(xor_distances, key=lambda x: x[1])
        closest_supernodes = sorted_xor_distances[:n]
        supernode_urls_and_pastelids = [(await get_supernode_url_from_pastelid_func(pastelid, supernode_list_df), pastelid) for pastelid, _ in closest_supernodes]
        return supernode_urls_and_pastelids
    return []

async def get_closest_supernode_pastelid_from_list(local_pastelid: str, supernode_pastelids: List[str]) -> str:
    xor_distances = [(supernode_pastelid, await calculate_xor_distance(local_pastelid, supernode_pastelid)) for supernode_pastelid in supernode_pastelids]
    closest_supernode = min(xor_distances, key=lambda x: x[1])
    return closest_supernode[0]

async def fetch_current_psl_market_price():
    async def check_prices():
        async with httpx.AsyncClient() as client:
            try:
                # Fetch data from CoinMarketCap
                response_cmc = await client.get("https://coinmarketcap.com/currencies/pastel/")
                price_cmc = float(re.search(r'price today is \$([0-9\.]+) USD', response_cmc.text).group(1))
            except (httpx.RequestError, AttributeError, ValueError):
                price_cmc = None
            try:
                # Fetch data from CoinGecko
                response_cg = await client.get("https://api.coingecko.com/api/v3/simple/price?ids=pastel&vs_currencies=usd")
                if response_cg.status_code == 200:
                    data = response_cg.json()
                    price_cg = data.get("pastel", {}).get("usd")
                else:
                    price_cg = None
            except (httpx.RequestError, AttributeError, ValueError):
                price_cg = None
        return price_cmc, price_cg
    price_cmc, price_cg = await check_prices()
    if price_cmc is None and price_cg is None:
        #Sleep for a couple seconds and try again:
        await asyncio.sleep(2)
        price_cmc, price_cg = await check_prices()
    # Calculate the average price
    prices = [price for price in [price_cmc, price_cg] if price is not None]
    if not prices:
        raise ValueError("Could not retrieve PSL price from any source.")
    average_price = sum(prices) / len(prices)
    # Validate the price
    if not 0.0000001 < average_price < 0.02:
        raise ValueError(f"Invalid PSL price: {average_price}")
    return average_price

async def estimated_market_price_of_inference_credits_in_psl_terms() -> float:
    try:
        psl_price_usd = await fetch_current_psl_market_price()
        target_value_per_credit_usd = TARGET_VALUE_PER_CREDIT_IN_USD
        target_profit_margin = TARGET_PROFIT_MARGIN
        # Calculate the cost per credit in USD, considering the profit margin
        cost_per_credit_usd = target_value_per_credit_usd / (1 - target_profit_margin)
        # Convert the cost per credit from USD to PSL
        cost_per_credit_psl = cost_per_credit_usd / psl_price_usd
        return cost_per_credit_psl
    except (ValueError, ZeroDivisionError) as e:
        logger.error(f"Error calculating estimated market price of inference credits: {str(e)}")
        raise

def parse_and_format(value):
    try:
        # Check if the JSON string is already formatted
        if isinstance(value, str) and "\n" in value:
            return value
        # Unescape the JSON string if it's a string
        if isinstance(value, str):
            unescaped_value = json.loads(json.dumps(value))
            parsed_value = json.loads(unescaped_value)
        else:
            parsed_value = value
        return json.dumps(parsed_value, indent=4)
    except (json.JSONDecodeError, TypeError):
        return value

def format_list(input_list):
    def json_serialize(item):
        if isinstance(item, uuid.UUID):
            return json.dumps(str(item), indent=4)
        if isinstance(item, SQLModel):
            item = item.dict()  # Convert SQLModel instance to dictionary
        elif isinstance(item, dict):
            return json.dumps(pretty_json_func(item), indent=4)
        elif isinstance(item, list):
            return format_list(item)
        else:
            return json.dumps(item, indent=4)
    formatted_list = "[\n" + ",\n".join("    " + json_serialize(item).replace("\n", "\n    ") for item in input_list) + "\n]"
    return formatted_list

def pretty_json_func(data):
    if isinstance(data, SQLModel):
        data = data.dict()  # Convert SQLModel instance to dictionary
    if isinstance(data, dict):
        formatted_data = {}
        for key, value in data.items():
            if isinstance(value, uuid.UUID):  # Convert UUIDs to string
                formatted_data[key] = str(value)
            elif isinstance(value, dict):  # Recursively handle dictionary values
                formatted_data[key] = pretty_json_func(value)
            elif isinstance(value, list):  # Special handling for lists
                formatted_data[key] = format_list(value)
            elif key.endswith("_json"):  # Handle keys that end with '_json'
                formatted_data[key] = parse_and_format(value)
            else:  # Handle other types of values
                formatted_data[key] = value
        return json.dumps(formatted_data, indent=4)
    elif isinstance(data, list):  # Top-level list handling
        formatted_list = []
        for item in data:
            if isinstance(item, str):
                try:
                    parsed_item = json.loads(item)
                    formatted_item = json.dumps(parsed_item, indent=4)
                    formatted_list.append(formatted_item)
                except json.JSONDecodeError:
                    formatted_list.append(json.dumps(item))  # Wrap the string in quotes
            elif isinstance(item, dict):
                formatted_list.append(json.dumps(pretty_json_func(item), indent=4))
            elif isinstance(item, uuid.UUID):
                formatted_list.append(json.dumps(str(item)))  # Convert UUID to string and wrap in quotes
            else:
                formatted_list.append(json.dumps(item))
        return "[\n" + ",\n".join("    " + item for item in formatted_list) + "\n]"
    elif isinstance(data, str):  # Handle string type data separately
        return parse_and_format(data)
    else:
        return json.dumps(data)  # Convert other types to JSON string
    
def log_action_with_payload(action_string, payload_name, json_payload):
    logger.info(f"Now {action_string} {payload_name} with payload:\n{pretty_json_func(json_payload)}")
    
def transform_credit_pack_purchase_request_response(result: dict) -> dict:
    transformed_result = result.copy()
    fields_to_convert = [
        "list_of_potentially_agreeing_supernodes",
        "list_of_blacklisted_supernode_pastelids",
        "list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms",
        "list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion",
        "selected_agreeing_supernodes_signatures_dict",
    ]
    for field in fields_to_convert:
        if field in transformed_result:
            transformed_result[field] = json.dumps(transformed_result[field])
    return transformed_result            

async def send_to_address_func(address, amount, comment="", comment_to="", subtract_fee_from_amount=False):
    """
    Send an amount to a given Pastel address.

    Args:
        address (str): The Pastel address to send to.
        amount (float): The amount in PSL to send.
        comment (str, optional): A comment used to store what the transaction is for.
                                    This is not part of the transaction, just kept in your wallet.
                                    Defaults to an empty string.
        comment_to (str, optional): A comment to store the name of the person or organization
                                    to which you're sending the transaction. This is not part of
                                    the transaction, just kept in your wallet. Defaults to an empty string.
        subtract_fee_from_amount (bool, optional): Whether to deduct the fee from the amount being sent.
                                                    If True, the recipient will receive less Pastel than you enter
                                                    in the amount field. Defaults to False.

    Returns:
        str: The transaction ID if successful, None otherwise.

    Example:
        send_to_address_func("PtczsZ91Bt3oDPDQotzUsrx1wjmsFVgf28n", 0.1, "donation", "seans outpost", True)
    """
    global rpc_connection
    try:
        result = await rpc_connection.sendtoaddress(address, amount, comment, comment_to, subtract_fee_from_amount)
        return result
    except Exception as e:
        logger.error(f"Error in send_to_address_func: {e}")
        return None

async def send_many_func(amounts, min_conf=1, comment="", change_address=""):
    """
    Send multiple amounts to multiple recipients.

    Args:
        amounts (dict): A dictionary representing the amounts to send.
                        Each key is the Pastel address, and the corresponding value is the amount in PSL to send.
        min_conf (int, optional): The minimum number of confirmations required for the funds to be used. Defaults to 1.
        comment (str, optional): A comment to include with the transaction. Defaults to an empty string.
        change_address (str, optional): The Pastel address to receive the change from the transaction. Defaults to an empty string.

    Returns:
        str: The transaction ID if successful, None otherwise.

    Example:
        amounts = {
            "PtczsZ91Bt3oDPDQotzUsrx1wjmsFVgf28n": 0.01,
            "PtczsZ91Bt3oDPDQotzUsrx1wjmsFVgf28n": 0.02
        }
        send_many_func(amounts, min_conf=6, comment="testing", change_address="PtczsZ91Bt3oDPDQotzUsrx1wjmsFVgf28n")
    """
    global rpc_connection
    try:
        # Set the 'fromaccount' parameter to an empty string
        from_account = ""
        # Call the 'sendmany' RPC method
        result = await rpc_connection.sendmany(from_account, amounts, min_conf, comment, [""], change_address)
        return result
    except Exception as e:
        logger.error(f"Error in send_many_func: {e}")
        return None

async def z_send_many_with_change_to_sender_func(from_address, amounts, min_conf=1, fee=0.1):
    """
    Send multiple amounts from a given address to multiple recipients.

    Args:
        from_address (str): The taddr or zaddr to send the funds from.
        amounts (list): A list of dictionaries representing the amounts to send.
                        Each dictionary should have the following keys:
                        - "address" (str): The taddr or zaddr to send funds to.
                        - "amount" (float): The amount in PSL to send.
                        - "memo" (str, optional): If the address is a zaddr, raw data represented in hexadecimal string format.
        min_conf (int, optional): The minimum number of confirmations required for the funds to be used. Defaults to 1.
        fee (float, optional): The fee amount to attach to the transaction. Defaults to 0.1.

    Returns:
        str: The operation ID if successful, None otherwise.

    Example:
        amounts = [
            {"address": "PzSSk8QJFqjo133DoFZvn9wwcCxt5RYeeLFJZRgws6xgJ3LroqRgXKNkhkG3ENmC8oe82UTr3PHcQB9mw7DSLXhyP6atQQ5", "amount": 5.0},
            {"address": "PzXFZjHx6KzqNpAaMewvrUj8x1fvj7UZLFZYEuN8jJJuQhSfPYaVoAF1qrFSh3q2zUmCg7QkfQr4nAVrdovwKA4KDwPp5g", "amount": 10.0, "memo": "0xabcd"}
        ]
        z_send_many_with_change_to_sender_func("PtczsZ91Bt3oDPDQotzUsrx1wjmsFVgf28n", amounts, min_conf=2, fee=0.05)
    """
    global rpc_connection
    try:
        result = await rpc_connection.z_sendmanywithchangetosender(from_address, amounts, min_conf, fee)
        return result
    except Exception as e:
        logger.error(f"Error in z_send_many_with_change_to_sender_func: {e}")
        return None

async def z_get_operation_status_func(operation_ids=None):
    """
    Get the status of one or more operations.

    Args:
        operation_ids (list, optional): A list of operation IDs to query the status for.
                                        If not provided, all known operations will be examined.

    Returns:
        list: A list of JSON objects containing the operation status and any associated result or error data.

    Example:
        operation_ids = ["opid-1234", "opid-5678"]
        z_get_operation_status_func(operation_ids)
    """
    global rpc_connection
    try:
        if operation_ids is None:
            operation_ids = []
        result = await rpc_connection.z_getoperationstatus(operation_ids)
        return result
    except Exception as e:
        logger.error(f"Error in z_get_operation_status_func: {e}")
        return None

async def check_psl_address_balance_alternative_func(address_to_check):
    global rpc_connection
    address_amounts_dict = await rpc_connection.listaddressamounts()
    # Convert the dictionary into a list of dictionaries, each representing a row
    data = [{'address': address, 'amount': amount} for address, amount in address_amounts_dict.items()]
    # Create the DataFrame from the list of dictionaries
    address_amounts_df = pd.DataFrame(data)
    # Filter the DataFrame for the specified address
    address_amounts_df_filtered = address_amounts_df[address_amounts_df['address'] == address_to_check]
    # Calculate the sum of the 'amount' column for the filtered DataFrame
    balance_at_address = address_amounts_df_filtered['amount'].sum()
    return balance_at_address
        
async def create_and_fund_new_psl_credit_tracking_address(amount_of_psl_to_fund_address_with: float):
    global rpc_connection
    new_credit_tracking_address = await rpc_connection.getnewaddress()
    txid = await send_to_address_func(new_credit_tracking_address, amount_of_psl_to_fund_address_with, comment="Funding new credit tracking address ", comment_to="", subtract_fee_from_amount=False)
    return new_credit_tracking_address, txid

async def check_psl_address_balance_func(address_to_check):
    global rpc_connection
    balance_at_address = await rpc_connection.z_getbalance(address_to_check)
    return balance_at_address

async def check_if_address_is_already_imported_in_local_wallet(address_to_check):
    global rpc_connection
    address_amounts_dict = await rpc_connection.listaddressamounts()
    # Convert the dictionary into a list of dictionaries, each representing a row
    data = [{'address': address, 'amount': amount} for address, amount in address_amounts_dict.items()]
    # Create the DataFrame from the list of dictionaries
    address_amounts_df = pd.DataFrame(data)        
    # Filter the DataFrame for the specified address
    address_amounts_df_filtered = address_amounts_df[address_amounts_df['address'] == address_to_check]
    if address_amounts_df_filtered.empty:
        return False
    return True

async def get_and_decode_raw_transaction(txid: str, blockhash: str = None) -> dict:
    """
    Retrieves and decodes detailed information about a specified transaction
    from the Pastel network using the RPC calls.

    Args:
        txid (str): The transaction id to fetch and decode.
        blockhash (str, optional): The block hash to specify which block to search for the transaction.

    Returns:
        dict: A dictionary containing detailed decoded information about the transaction.
    """
    global rpc_connection
    try:
        # Retrieve the raw transaction data
        raw_tx_data = await rpc_connection.getrawtransaction(txid, 0, blockhash)
        if not raw_tx_data:
            logger.error(f"Failed to retrieve raw transaction data for {txid}")
            return {}

        # Decode the raw transaction data
        decoded_tx_data = await rpc_connection.decoderawtransaction(raw_tx_data)
        if not decoded_tx_data:
            logger.error(f"Failed to decode raw transaction data for {txid}")
            return {}

        # Log the decoded transaction details

        return decoded_tx_data
    except Exception as e:
        logger.error(f"Error in get_and_decode_transaction for {txid}: {e}")
        return {}

async def get_transaction_details(txid: str, include_watchonly: bool = False) -> dict:
    """
    Fetches detailed information about a specified transaction from the Pastel network using the RPC call.

    Args:
        txid (str): The transaction id to fetch details for.
        include_watchonly (bool, optional): Whether to include watchonly addresses in the details. Defaults to False.

    Returns:
        dict: A dictionary containing detailed information about the transaction.
    """
    global rpc_connection
    try:
        # Call the 'gettransaction' RPC method with the provided txid and includeWatchonly flag
        transaction_details = await rpc_connection.gettransaction(txid, include_watchonly)
        
        # Log the retrieved transaction details

        return transaction_details
    except Exception as e:
        logger.error(f"Error retrieving transaction details for {txid}: {e}")
        return {}
    
async def send_tracking_amount_from_control_address_to_burn_address_to_confirm_inference_request(
    inference_request_id: str,
    credit_usage_tracking_psl_address: str,
    credit_usage_tracking_amount_in_psl: float,
    burn_address: str,
):
    """
    Send the tracking amount from the control address to the burn address to confirm an inference request.

    Args:
        inference_request_id (str): The ID of the inference request.
        credit_usage_tracking_psl_address (str): The control address to send the tracking amount from.
        credit_usage_tracking_amount_in_psl (float): The tracking amount in PSL to send.
        burn_address (str): The burn address to send the tracking amount to.

    Returns:
        str: The transaction ID (txid) if the transaction is successfully confirmed, None otherwise.

    Example:
        send_tracking_amount_from_control_address_to_burn_address_to_confirm_inference_request(
            inference_request_id="abc123",
            credit_usage_tracking_psl_address="PtczsZ91Bt3oDPDQotzUsrx1wjmsFVgf28n",
            credit_usage_tracking_amount_in_psl=0.5,
            burn_address="PtpasteLBurnAddressXXXXXXXXXXbJ5ndd"
        )
    """
    try:
        amounts = {
            burn_address: credit_usage_tracking_amount_in_psl
        }
        txid = await send_many_func(
            amounts=amounts,
            min_conf=0,
            comment="Confirmation tracking transaction for inference request with request_id " + inference_request_id, 
            change_address=credit_usage_tracking_psl_address
        )
        if txid is not None:
            transaction_info = await rpc_connection.gettransaction(txid)
            if transaction_info:
                return txid
            else:
                logger.error(f"No transaction info found for TXID: {txid} to confirm inference request {inference_request_id}")
            return None
        else:
            logger.error(f"Failed to send {credit_usage_tracking_amount_in_psl} PSL from {credit_usage_tracking_psl_address} to {burn_address} to confirm inference request {inference_request_id}")
            return None
    except Exception as e:
        logger.error(f"Error in send_tracking_amount_from_control_address_to_burn_address_to_confirm_inference_request: {e}")
        raise    

async def import_address_func(address: str, label: str = "", rescan: bool = False) -> None:
    """
    Import an address or script (in hex) that can be watched as if it were in your wallet but cannot be used to spend.

    Args:
        address (str): The address to import.
        label (str, optional): An optional label for the address. Defaults to an empty string.
        rescan (bool, optional): Rescan the wallet for transactions. Defaults to False.

    Returns:
        None

    Raises:
        RPCError: If an error occurs during the RPC call.

    Example:
        import_address_func("myaddress", "testing", False)
    """
    global rpc_connection
    try:
        await rpc_connection.importaddress(address, label, rescan)
    except Exception as e:
        logger.error(f"Error importing address: {address}. Error: {e}")
    
async def compress_data_with_zstd_func(input_data):
    zstd_compression_level = 20
    zstandard_compressor = zstd.ZstdCompressor(level=zstd_compression_level, write_content_size=True, write_checksum=True)
    zstd_compressed_data = zstandard_compressor.compress(input_data)
    zstd_compressed_data__base64_encoded = base64.b64encode(zstd_compressed_data).decode('utf-8')
    return zstd_compressed_data, zstd_compressed_data__base64_encoded

async def decompress_data_with_zstd_func(compressed_input_data):
    zstd_decompressor = zstd.ZstdDecompressor()
    zstd_decompressed_data = zstd_decompressor.decompress(compressed_input_data)
    return zstd_decompressed_data

async def sign_message_with_pastelid_func(pastelid, message_to_sign, passphrase) -> str:
    global rpc_connection
    results_dict = await rpc_connection.pastelid('sign', message_to_sign, pastelid, passphrase, 'ed448')
    return results_dict['signature']

#____________________________________________________________________________________________________________________________
# SQLModel model classes based on the server's database_code.py

# Messaging related models:

class Message(SQLModel, table=True):
    id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
    sending_sn_pastelid: str = Field(index=True)
    receiving_sn_pastelid: str = Field(index=True)
    sending_sn_txid_vout: str = Field(index=True)
    receiving_sn_txid_vout: str = Field(index=True)
    message_type: str = Field(index=True)
    message_body: str = Field(sa_column=Column(JSON))
    signature: str
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
    def __repr__(self):
        return f"<Message(id={self.id}, sending_sn_pastelid='{self.sending_sn_pastelid}', receiving_sn_pastelid='{self.receiving_sn_pastelid}', message_type='{self.message_type}', timestamp='{self.timestamp}')>"
    class Config:
        arbitrary_types_allowed = True  # Allow arbitrary types
        json_schema_extra = {
            "example": {
                "sending_sn_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "receiving_sn_pastelid": "jXa1s9mKDr4m6P8s7bKK1rYFgL7hkfGMLX1NozVSX4yTnfh9EjuP",
                "sending_sn_txid_vout": "0x1234...:0",
                "receiving_sn_txid_vout": "0x5678...:0",
                "message_type": "text",
                "message_body": "Hello, how are you?",
                "signature": "0xabcd...",
                "timestamp": "2023-06-01T12:00:00Z"
            }
        }

class UserMessage(SQLModel, table=True):
    id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
    from_pastelid: str = Field(index=True)
    to_pastelid: str = Field(index=True)
    message_body: str = Field(sa_column=Column(JSON))
    message_signature: str
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
    class Config:
        arbitrary_types_allowed = True  # Allow arbitrary types
        json_schema_extra = {
            "example": {
                "from_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "to_pastelid": "jXa1s9mKDr4m6P8s7bKK1rYFgL7hkfGMLX1NozVSX4yTnfh9EjuP",
                "message_body": "Hey, let's meet up!",
                "message_signature": "0xdef0...",
                "timestamp": "2023-06-01T12:30:00Z"
            }
        }
        
# Credit pack purchasing/provisioning related models:        

class CreditPackPurchaseRequest(SQLModel, table=True):
    id: uuid.UUID = Field(default_factory=uuid.uuid4, index=True, nullable=True)
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
    requesting_end_user_pastelid: str = Field(index=True)
    requested_initial_credits_in_credit_pack: int
    list_of_authorized_pastelids_allowed_to_use_credit_pack: str = Field(sa_column=Column(JSON))
    credit_usage_tracking_psl_address: str = Field(index=True)
    request_timestamp_utc_iso_string: str
    request_pastel_block_height: int
    credit_purchase_request_message_version_string: str
    requesting_end_user_pastelid_signature_on_request_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "id": "79df343b-4ad3-435c-800e-e59e616ff84d",
                "requesting_end_user_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "requested_initial_credits_in_credit_pack": 1000,
                "list_of_authorized_pastelids_allowed_to_use_credit_pack": ["jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk"],
                "credit_usage_tracking_psl_address": "tPj2wX5mjQErTju6nueVRkxGMCPuMkLn8CWdViJ38m9Wf6PBK5jV",
                "request_timestamp_utc_iso_string": "2023-06-01T12:00:00Z",
                "request_pastel_block_height": 123456,
                "credit_purchase_request_message_version_string": "1.0",
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x5678...",
                "requesting_end_user_pastelid_signature_on_request_hash": "0xabcd..."
            }
        }

class CreditPackPurchaseRequestRejection(SQLModel, table=True):
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
    credit_pack_purchase_request_fields_json_b64: str
    rejection_reason_string: str
    rejection_timestamp_utc_iso_string: str
    rejection_pastel_block_height: int
    credit_purchase_request_rejection_message_version_string: str
    responding_supernode_pastelid: str = Field(index=True)
    sha3_256_hash_of_credit_pack_purchase_request_rejection_fields: str = Field(unique=True, index=True)
    responding_supernode_signature_on_credit_pack_purchase_request_rejection_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "credit_pack_purchase_request_fields_json_b64": 'eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9',
                "rejection_reason_string": "Invalid credit usage tracking PSL address",
                "rejection_timestamp_utc_iso_string": "2023-06-01T12:10:00Z",
                "rejection_pastel_block_height": 123457,
                "credit_purchase_request_rejection_message_version_string": "1.0",
                "responding_supernode_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "sha3_256_hash_of_credit_pack_purchase_request_rejection_fields": "0xabcd...",
                "responding_supernode_signature_on_credit_pack_purchase_request_rejection_hash": "0xdef0..."
            }
        }
        
class CreditPackPurchaseRequestPreliminaryPriceQuote(SQLModel, table=True):
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
    credit_usage_tracking_psl_address: str = Field(index=True)
    credit_pack_purchase_request_fields_json_b64: str
    preliminary_quoted_price_per_credit_in_psl: float
    preliminary_total_cost_of_credit_pack_in_psl: float
    preliminary_price_quote_timestamp_utc_iso_string: str
    preliminary_price_quote_pastel_block_height: int
    preliminary_price_quote_message_version_string: str
    responding_supernode_pastelid: str = Field(index=True)
    sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields: str = Field(unique=True, index=True)
    responding_supernode_signature_on_credit_pack_purchase_request_preliminary_price_quote_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "credit_usage_tracking_psl_address": "tPj2wX5mjQErTju6nueVRkxGMCPuMkLn8CWdViJ38m9Wf6PBK5jV",
                "credit_pack_purchase_request_fields_json_b64": 'eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9',
                "preliminary_quoted_price_per_credit_in_psl": 0.1,
                "preliminary_total_cost_of_credit_pack_in_psl": 100,
                "preliminary_price_quote_timestamp_utc_iso_string": "2023-06-01T12:05:00Z",
                "preliminary_price_quote_pastel_block_height": 123456,
                "preliminary_price_quote_message_version_string": "1.0",
                "responding_supernode_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields": "0xabcd...",
                "responding_supernode_signature_on_credit_pack_purchase_request_preliminary_price_quote_hash": "0xdef0..."
            }
        }

class CreditPackPurchaseRequestPreliminaryPriceQuoteResponse(SQLModel, table=True):
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
    sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields: str = Field(index=True)
    credit_pack_purchase_request_fields_json_b64: str
    agree_with_preliminary_price_quote: bool
    credit_usage_tracking_psl_address: str = Field(index=True)
    preliminary_quoted_price_per_credit_in_psl: float
    preliminary_price_quote_response_timestamp_utc_iso_string: str
    preliminary_price_quote_response_pastel_block_height: int
    preliminary_price_quote_response_message_version_string: str
    requesting_end_user_pastelid: str = Field(index=True)
    sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_response_fields: str = Field(unique=True, index=True)
    requesting_end_user_pastelid_signature_on_preliminary_price_quote_response_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields": "0x5678...",
                "credit_pack_purchase_request_fields_json_b64": 'eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9',
                "agree_with_preliminary_price_quote": True,
                "credit_usage_tracking_psl_address": "tPj2wX5mjQErTju6nueVRkxGMCPuMkLn8CWdViJ38m9Wf6PBK5jV",
                "preliminary_quoted_price_per_credit_in_psl": 0.1,
                "preliminary_price_quote_response_timestamp_utc_iso_string": "2023-06-01T12:10:00Z",
                "preliminary_price_quote_response_pastel_block_height": 123457,
                "preliminary_price_quote_response_message_version_string": "1.0",
                "requesting_end_user_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_response_fields": "0xdef0...",
                "requesting_end_user_pastelid_signature_on_preliminary_price_quote_response_hash": "0x1234..."
            }
        }

class CreditPackPurchaseRequestResponseTermination(SQLModel, table=True):
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
    credit_pack_purchase_request_fields_json_b64: str
    termination_reason_string: str
    termination_timestamp_utc_iso_string: str
    termination_pastel_block_height: int
    credit_purchase_request_termination_message_version_string: str
    responding_supernode_pastelid: str = Field(index=True)
    sha3_256_hash_of_credit_pack_purchase_request_termination_fields: str = Field(unique=True, index=True)
    responding_supernode_signature_on_credit_pack_purchase_request_termination_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "credit_pack_purchase_request_fields_json_b64": 'eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9',
                "termination_reason_string": "Insufficient agreeing supernodes",
                "termination_timestamp_utc_iso_string": "2023-06-01T12:30:00Z",
                "termination_pastel_block_height": 123459,
                "credit_purchase_request_termination_message_version_string": "1.0",
                "responding_supernode_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "sha3_256_hash_of_credit_pack_purchase_request_termination_fields": "0xabcd...",
                "responding_supernode_signature_on_credit_pack_purchase_request_termination_hash": "0xdef0..."
            }
        }        
        
class CreditPackPurchaseRequestResponse(SQLModel, table=True):
    id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(foreign_key="creditpackpurchaserequest.sha3_256_hash_of_credit_pack_purchase_request_fields", index=True)
    credit_pack_purchase_request_fields_json_b64: str
    psl_cost_per_credit: float
    proposed_total_cost_of_credit_pack_in_psl: float
    credit_usage_tracking_psl_address: str = Field(index=True)
    request_response_timestamp_utc_iso_string: str
    request_response_pastel_block_height: int
    best_block_merkle_root: str
    best_block_height: int
    credit_purchase_request_response_message_version_string: str
    responding_supernode_pastelid: str = Field(index=True)
    list_of_blacklisted_supernode_pastelids: str = Field(sa_column=Column(JSON))
    list_of_potentially_agreeing_supernodes: str = Field(sa_column=Column(JSON))
    list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms: str = Field(sa_column=Column(JSON))
    list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion: str = Field(sa_column=Column(JSON))
    selected_agreeing_supernodes_signatures_dict: str = Field(sa_column=Column(JSON))
    sha3_256_hash_of_credit_pack_purchase_request_response_fields: str = Field(unique=True, index=True)
    responding_supernode_signature_on_credit_pack_purchase_request_response_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "id": "79df343b-4ad3-435c-800e-e59e616ff84d",
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "credit_pack_purchase_request_fields_json_b64": 'eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9',
                "psl_cost_per_credit": 0.1,
                "proposed_total_cost_of_credit_pack_in_psl": 100,
                "credit_usage_tracking_psl_address": "tPj2wX5mjQErTju6nueVRkxGMCPuMkLn8CWdViJ38m9Wf6PBK5jV",
                "request_response_timestamp_utc_iso_string": "2023-06-01T12:15:00Z",
                "request_response_pastel_block_height": 123457,
                "best_block_merkle_root": "0x5678...",
                "best_block_height": 123456,
                "credit_purchase_request_response_message_version_string": "1.0",
                "responding_supernode_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "list_of_blacklisted_supernode_pastelids": ["jXa1s9mKDr4m6P8s7bKK1rYFgL7hkfGMLX1NozVSX4yTnfh9EjuP"],
                "list_of_potentially_agreeing_supernodes": ["jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk", "jXa1s9mKDr4m6P8s7bKK1rYFgL7hkfGMLX1NozVSX4yTnfh9EjuP"],
                "list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms": ["jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk", "jXa1s9mKDr4m6P8s7bKK1rYFgL7hkfGMLX1NozVSX4yTnfh9EjuP"],
                "list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion": ["jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk"],
                "selected_agreeing_supernodes_signatures_dict": "['jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk': {'price_agreement_request_response_hash_signature': '0x1234...', 'credit_pack_purchase_request_fields_json_b64_signature': '0x5678...'}]",
                "sha3_256_hash_of_credit_pack_purchase_request_response_fields": "0x9abc...",
                "responding_supernode_signature_on_credit_pack_purchase_request_response_hash": "0xdef0..."
            }
        }

class CreditPackPurchaseRequestConfirmation(SQLModel, table=True):
    id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(foreign_key="creditpackpurchaserequest.sha3_256_hash_of_credit_pack_purchase_request_fields", index=True)
    sha3_256_hash_of_credit_pack_purchase_request_response_fields: str = Field(foreign_key="creditpackpurchaserequestresponse.sha3_256_hash_of_credit_pack_purchase_request_response_fields", index=True)
    credit_pack_purchase_request_fields_json_b64: str
    requesting_end_user_pastelid: str = Field(index=True)
    txid_of_credit_purchase_burn_transaction: str = Field(index=True)
    credit_purchase_request_confirmation_utc_iso_string: str
    credit_purchase_request_confirmation_pastel_block_height: int
    credit_purchase_request_confirmation_message_version_string: str
    sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str = Field(unique=True, index=True)
    requesting_end_user_pastelid_signature_on_sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str
    class Config:
        json_schema_extra = {
            "example": {
                "id": "79df343b-4ad3-435c-800e-e59e616ff84d",
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "sha3_256_hash_of_credit_pack_purchase_request_response_fields": "0x5678...",
                "credit_pack_purchase_request_fields_json_b64": 'eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9',
                "requesting_end_user_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "txid_of_credit_purchase_burn_transaction": "0xabcd...",
                "credit_purchase_request_confirmation_utc_iso_string": "2023-06-01T12:30:00Z",
                "credit_purchase_request_confirmation_pastel_block_height": 123458,
                "credit_purchase_request_confirmation_message_version_string": "1.0",
                "sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields": "0xdef0...",
                "requesting_end_user_pastelid_signature_on_sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields": "0x1234..."
            }
        }

class CreditPackPurchaseRequestConfirmationResponse(SQLModel, table=True):
    id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(foreign_key="creditpackpurchaserequest.sha3_256_hash_of_credit_pack_purchase_request_fields", index=True)
    sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str = Field(foreign_key="creditpackpurchaserequestconfirmation.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields", index=True)
    credit_pack_confirmation_outcome_string: str
    pastel_api_credit_pack_ticket_registration_txid: str = Field(index=True)
    credit_pack_confirmation_failure_reason_if_applicable: str
    credit_purchase_request_confirmation_response_utc_iso_string: str
    credit_purchase_request_confirmation_response_pastel_block_height: int
    credit_purchase_request_confirmation_response_message_version_string: str
    responding_supernode_pastelid: str = Field(index=True)
    sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields: str = Field(unique=True, index=True)
    responding_supernode_signature_on_credit_pack_purchase_request_confirmation_response_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "id": "79df343b-4ad3-435c-800e-e59e616ff84d",
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields": "0x5678...",
                "credit_pack_confirmation_outcome_string": "success",
                "pastel_api_credit_pack_ticket_registration_txid": "0xabcd...",
                "credit_pack_confirmation_failure_reason_if_applicable": "",
                "credit_purchase_request_confirmation_response_utc_iso_string": "2023-06-01T12:45:00Z",
                "credit_purchase_request_confirmation_response_pastel_block_height": 123459,
                "credit_purchase_request_confirmation_response_message_version_string": "1.0",
                "responding_supernode_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields": "0xdef0...",
                "responding_supernode_signature_on_credit_pack_purchase_request_confirmation_response_hash": "0x1234..."
            }
        }

class CreditPackRequestStatusCheck(SQLModel):
    sha3_256_hash_of_credit_pack_purchase_request_fields: str
    requesting_end_user_pastelid: str
    requesting_end_user_pastelid_signature_on_sha3_256_hash_of_credit_pack_purchase_request_fields: str
    class Config:
        json_schema_extra = {
            "example": {
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "requesting_end_user_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "requesting_end_user_pastelid_signature_on_sha3_256_hash_of_credit_pack_purchase_request_fields": "0x5678..."
            }
        }

class CreditPackPurchaseRequestStatus(SQLModel, table=True):
    id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(foreign_key="creditpackpurchaserequest.sha3_256_hash_of_credit_pack_purchase_request_fields", index=True)
    sha3_256_hash_of_credit_pack_purchase_request_response_fields: str = Field(foreign_key="creditpackpurchaserequestresponse.sha3_256_hash_of_credit_pack_purchase_request_response_fields", index=True)
    status: str = Field(index=True)
    status_details: str
    status_update_timestamp_utc_iso_string: str
    status_update_pastel_block_height: int
    credit_purchase_request_status_message_version_string: str
    responding_supernode_pastelid: str = Field(index=True)
    sha3_256_hash_of_credit_pack_purchase_request_status_fields: str = Field(unique=True, index=True)
    responding_supernode_signature_on_credit_pack_purchase_request_status_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "id": "79df343b-4ad3-435c-800e-e59e616ff84d",
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "status": "in_progress",
                "status_details": "Waiting for price agreement responses from supernodes",
                "status_update_timestamp_utc_iso_string": "2023-06-01T12:30:00Z",
                "status_update_pastel_block_height": 123456,
                "credit_purchase_request_status_message_version_string": "1.0",
                "responding_supernode_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "sha3_256_hash_of_credit_pack_purchase_request_status_fields": "0x5678...",
                "responding_supernode_signature_on_credit_pack_purchase_request_status_hash": "0xef01..."
            }
        }

class CreditPackStorageRetryRequest(SQLModel, table=True):
    sha3_256_hash_of_credit_pack_purchase_request_response_fields: str = Field(primary_key=True, index=True)
    credit_pack_purchase_request_fields_json_b64: str
    requesting_end_user_pastelid: str = Field(index=True)
    closest_agreeing_supernode_to_retry_storage_pastelid: str = Field(index=True)
    credit_pack_storage_retry_request_timestamp_utc_iso_string: str
    credit_pack_storage_retry_request_pastel_block_height: int
    credit_pack_storage_retry_request_message_version_string: str
    sha3_256_hash_of_credit_pack_storage_retry_request_fields: str
    requesting_end_user_pastelid_signature_on_credit_pack_storage_retry_request_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "sha3_256_hash_of_credit_pack_purchase_request_response_fields": "0x1234...",
                "credit_pack_purchase_request_fields_json_b64": 'eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9',
                "requesting_end_user_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "closest_agreeing_supernode_to_retry_storage_pastelid": "jXa1s9mKDr4m6P8s7bKK1rYFgL7hkfGMLX1NozVSX4yTnfh9EjuP",
                "credit_pack_storage_retry_request_timestamp_utc_iso_string": "2023-06-01T12:50:00Z",
                "credit_pack_storage_retry_request_pastel_block_height": 123460,
                "credit_pack_storage_retry_request_message_version_string": "1.0",
                "sha3_256_hash_of_credit_pack_storage_retry_request_fields": "0xabcd...",
                "requesting_end_user_pastelid_signature_on_credit_pack_storage_retry_request_hash": "0xdef0..."
            }
        }

class CreditPackStorageRetryRequestResponse(SQLModel, table=True):
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
    sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str
    credit_pack_storage_retry_confirmation_outcome_string: str
    pastel_api_credit_pack_ticket_registration_txid: str
    credit_pack_storage_retry_confirmation_failure_reason_if_applicable: str
    credit_pack_storage_retry_confirmation_response_utc_iso_string: str
    credit_pack_storage_retry_confirmation_response_pastel_block_height: int
    credit_pack_storage_retry_confirmation_response_message_version_string: str
    closest_agreeing_supernode_to_retry_storage_pastelid: str = Field(index=True)
    sha3_256_hash_of_credit_pack_storage_retry_confirmation_response_fields: str
    closest_agreeing_supernode_to_retry_storage_pastelid_signature_on_credit_pack_storage_retry_confirmation_response_hash: str    
    class Config:
        json_schema_extra = {
            "example": {
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields": "0x5678...",
                "credit_pack_storage_retry_confirmation_outcome_string": "success",
                "pastel_api_credit_pack_ticket_registration_txid": "0xabcd...",
                "credit_pack_storage_retry_confirmation_failure_reason_if_applicable": "",
                "credit_pack_storage_retry_confirmation_response_utc_iso_string": "2023-06-01T12:55:00Z",
                "credit_pack_storage_retry_confirmation_response_pastel_block_height": 123461,
                "credit_pack_storage_retry_confirmation_response_message_version_string": "1.0",
                "closest_agreeing_supernode_to_retry_storage_pastelid": "jXa1s9mKDr4m6P8s7bKK1rYFgL7hkfGMLX1NozVSX4yTnfh9EjuP",
                "sha3_256_hash_of_credit_pack_storage_retry_confirmation_response_fields": "0xdef0...",
                "closest_agreeing_supernode_to_retry_storage_pastelid_signature_on_credit_pack_storage_retry_confirmation_response_hash": "0x1234..."
            }
        }

# Inference request related models:

class InferenceAPIUsageRequest(SQLModel, table=True):
    id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
    inference_request_id: str = Field(unique=True, index=True)
    requesting_pastelid: str = Field(index=True)
    credit_pack_ticket_pastel_txid: str = Field(index=True)
    requested_model_canonical_string: str
    model_inference_type_string: str
    model_parameters_json_b64: str
    model_input_data_json_b64: str
    inference_request_utc_iso_string: str
    inference_request_pastel_block_height: int
    status: str = Field(index=True)
    inference_request_message_version_string: str
    sha3_256_hash_of_inference_request_fields: str
    requesting_pastelid_signature_on_request_hash: str
    class Config:
        protected_namespaces = ()
        json_schema_extra = {
            "example": {
                id: "79df343b-4ad3-435c-800e-e59e616ff84d",
                "inference_request_id": "0x1234...",
                "requesting_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "credit_pack_ticket_pastel_txid": "0x5678...",
                "requested_model_canonical_string": "gpt-3.5-turbo",
                "model_inference_type_string": "text-completion",
                "model_parameters_json_b64": "eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9",
                "model_input_data_json_b64": "eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9",
                "inference_request_utc_iso_string": "2023-06-01T12:00:00Z",
                "inference_request_pastel_block_height": 123456,
                "status": "in_progress",
                "inference_request_message_version_string": "1.0",
                "sha3_256_hash_of_inference_request_fields": "0x5678...",
                "requesting_pastelid_signature_on_request_hash": "0xabcd..."
            }
        }

class InferenceAPIUsageResponse(SQLModel, table=True):
    id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
    inference_response_id: str = Field(unique=True, index=True)
    inference_request_id: str = Field(foreign_key="inferenceapiusagerequest.inference_request_id", index=True)
    proposed_cost_of_request_in_inference_credits: float
    remaining_credits_in_pack_after_request_processed: float
    credit_usage_tracking_psl_address: str = Field(index=True)
    request_confirmation_message_amount_in_patoshis: int
    max_block_height_to_include_confirmation_transaction: int
    inference_request_response_utc_iso_string: str
    inference_request_response_pastel_block_height: int
    inference_request_response_message_version_string: str    
    sha3_256_hash_of_inference_request_response_fields: str
    supernode_pastelid_and_signature_on_inference_request_response_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "id": "79df343b-4ad3-435c-800e-e59e616ff84d",
                "inference_response_id": "0x1234...",
                "inference_request_id": "0x5678...",
                "proposed_cost_of_request_in_inference_credits": 10,
                "remaining_credits_in_pack_after_request_processed": 990,
                "credit_usage_tracking_psl_address": "tPj2wX5mjQErTju6nueVRkxGMCPuMkLn8CWdViJ38m9Wf6PBK5jV",
                "request_confirmation_message_amount_in_patoshis": 1000,
                "max_block_height_to_include_confirmation_transaction": 123456,
                "inference_request_response_utc_iso_string": "2023-06-01T12:00:00Z",
                "inference_request_response_pastel_block_height": 123456,
                "inference_request_response_message_version_string": "1.0",
                "sha3_256_hash_of_inference_request_response_fields": "0x5678...",
                "supernode_pastelid_and_signature_on_inference_request_response_hash": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk:0xabcd..."
            }
        }
        
class InferenceAPIOutputResult(SQLModel, table=True):
    id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
    inference_result_id: str = Field(unique=True, index=True)
    inference_request_id: str = Field(foreign_key="inferenceapiusagerequest.inference_request_id", index=True)
    inference_response_id: str = Field(foreign_key="inferenceapiusageresponse.inference_response_id", index=True)
    responding_supernode_pastelid: str = Field(index=True)
    inference_result_json_base64: str
    inference_result_file_type_strings: str
    inference_result_utc_iso_string: str
    inference_result_pastel_block_height: int
    inference_result_message_version_string: str    
    sha3_256_hash_of_inference_result_fields: str    
    responding_supernode_signature_on_inference_result_id: str
    class Config:
        json_schema_extra = {
            "example": {
                "id": "79df343b-4ad3-435c-800e-e59e616ff84d",
                "inference_result_id": "0x1234...",
                "inference_request_id": "0x5678...",
                "inference_response_id": "0x9abc...",
                "responding_supernode_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "inference_result_json_base64": "eyJvdXRwdXQiOiAiSGVsbG8sIEknbSBkb2luZyBncmVhdCEgSG93IGFib3V0IHlvdT8ifQ==",
                "inference_result_file_type_strings": "json",
                "inference_result_utc_iso_string": "2023-06-01T12:00:00Z",
                "inference_result_pastel_block_height": 123456,
                "inference_result_message_version_string": "1.0",
                "sha3_256_hash_of_inference_result_fields": "0x5678...",
                "responding_supernode_signature_on_inference_result_id": "0xdef0..."
            }
        }
        
class InferenceConfirmation(SQLModel):
    inference_request_id: str
    requesting_pastelid: str
    confirmation_transaction: dict
    class Config:
        json_schema_extra = {
            "example": {
                "inference_request_id": "0x1234...",
                "requesting_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "confirmation_transaction": {
                    "txid": "0x5678...",
                    "amount": 1000,
                    "block_height": 123456
                }
            }
        }

#____________________________________________________________________________________________

# Class for handling interactions with the supernode servers:

class PastelInferenceClient:
    def __init__(self, pastelid: str, passphrase: str):
        self.pastelid = pastelid
        self.passphrase = passphrase

    async def request_and_sign_challenge(self, supernode_url: str) -> Dict[str, str]:
        async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
            response = await client.get(f"{supernode_url}/request_challenge/{self.pastelid}")
            response.raise_for_status()
            result = response.json()
            challenge = result["challenge"]
            challenge_id = result["challenge_id"]
            # Sign the challenge string using the local RPC client
            signature = await sign_message_with_pastelid_func(self.pastelid, challenge, self.passphrase)
            return {
                "challenge": challenge,
                "challenge_id": challenge_id,
                "signature": signature
            }
                    
    async def send_user_message(self, supernode_url: str, user_message: UserMessage) -> Dict[str, Any]:
        # Request and sign a challenge
        challenge_result = await self.request_and_sign_challenge(supernode_url)
        challenge = challenge_result["challenge"]
        challenge_id = challenge_result["challenge_id"]
        challenge_signature = challenge_result["signature"]
        # Prepare the user message payload
        payload = user_message.model_dump()
        async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
            response = await client.post(
                f"{supernode_url}/send_user_message",
                json={
                    "user_message": payload,
                    "challenge": challenge,
                    "challenge_id": challenge_id,
                    "challenge_signature": challenge_signature
                }
            )
            response.raise_for_status()
            result = response.json()
            return result

    async def get_user_messages(self, supernode_url: str) -> List[UserMessage]:
        # Request and sign a challenge
        challenge_result = await self.request_and_sign_challenge(supernode_url)
        challenge = challenge_result["challenge"]
        challenge_id = challenge_result["challenge_id"]
        signature = challenge_result["signature"]
        # Get the user messages
        params = {
            "pastelid": self.pastelid,
            "challenge": challenge,
            "challenge_id": challenge_id,
            "challenge_signature": signature
        }
        async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
            response = await client.get(f"{supernode_url}/get_user_messages", params=params)
            response.raise_for_status()
            result = response.json()
            return [UserMessage.model_validate(message) for message in result]
        
    # Credit pack related client methods:        
    async def get_credit_pack_ticket_from_txid(self, supernode_url: str, txid: str) -> CreditPackPurchaseRequestResponse:
        challenge_result = await self.request_and_sign_challenge(supernode_url)
        challenge = challenge_result["challenge"]
        challenge_id = challenge_result["challenge_id"]
        challenge_signature = challenge_result["signature"]
        params = {
            "txid": txid,
            "pastelid": self.pastelid,
            "challenge": challenge,
            "challenge_id": challenge_id,
            "challenge_signature": challenge_signature
        }
        log_action_with_payload("retrieving", "credit pack ticket from txid", params)
        async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
            response = await client.get(f"{supernode_url}/get_credit_pack_ticket_from_txid", params=params)
            response.raise_for_status()
            credit_pack_ticket = response.json()
            credit_pack_purchase_request_response = CreditPackPurchaseRequestResponse.model_validate(transform_credit_pack_purchase_request_response(credit_pack_ticket['credit_pack_purchase_request_response']))
            credit_pack_purchase_request_confirmation = CreditPackPurchaseRequestConfirmation.model_validate(credit_pack_ticket['credit_pack_purchase_request_confirmation'])
            log_action_with_payload("receiving", "credit pack ticket from Supernode", [credit_pack_purchase_request_response.model_dump(), credit_pack_purchase_request_confirmation.model_dump()])
            return credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation

    async def credit_pack_ticket_initial_purchase_request(self, supernode_url: str, credit_pack_request: CreditPackPurchaseRequest) -> Union[CreditPackPurchaseRequestPreliminaryPriceQuote, CreditPackPurchaseRequestRejection]:
        challenge_result = await self.request_and_sign_challenge(supernode_url)
        challenge = challenge_result["challenge"]
        challenge_id = challenge_result["challenge_id"]
        challenge_signature = challenge_result["signature"]
        payload = credit_pack_request.model_dump()
        payload = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in payload.items()}
        log_action_with_payload("requesting", "a new Pastel credit pack ticket", payload)
        async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
            response = await client.post(
                f"{supernode_url}/credit_purchase_initial_request",
                json={
                    "credit_pack_request": payload,
                    "challenge": challenge,
                    "challenge_id": challenge_id,
                    "challenge_signature": challenge_signature
                }
            )
            response.raise_for_status()
            result = response.json()
            if "rejection_reason_string" in result:
                logger.error(f"Credit pack purchase request rejected: {result['rejection_reason_string']}")
                return CreditPackPurchaseRequestRejection.model_validate(result)
            else:
                log_action_with_payload("receiving", "response to credit pack purchase request", result)
                return CreditPackPurchaseRequestPreliminaryPriceQuote.model_validate(result)
            
    async def calculate_price_difference_percentage(self, quoted_price: float, estimated_price: float) -> float:
        if estimated_price == 0:
            raise ValueError("Estimated price cannot be zero.")
        difference_percentage = abs(quoted_price - estimated_price) / estimated_price
        return difference_percentage

    async def confirm_preliminary_price_quote(
        self,
        preliminary_price_quote: CreditPackPurchaseRequestPreliminaryPriceQuote,
        maximum_total_credit_pack_price_in_psl: Optional[float] = None,
        maximum_per_credit_price_in_psl: Optional[float] = None
    ) -> bool:
        if maximum_total_credit_pack_price_in_psl is None and maximum_per_credit_price_in_psl is None:
            maximum_per_credit_price_in_psl = MAXIMUM_PER_CREDIT_PRICE_IN_PSL_FOR_CLIENT
        # Extract the relevant fields from the preliminary price quote
        quoted_price_per_credit = preliminary_price_quote.preliminary_quoted_price_per_credit_in_psl
        quoted_total_price = preliminary_price_quote.preliminary_total_cost_of_credit_pack_in_psl
        # Parse the credit pack purchase request fields JSON
        credit_pack_purchase_request_fields_json = base64.b64decode(preliminary_price_quote.credit_pack_purchase_request_fields_json_b64).decode('utf-8')
        request_fields = json.loads(credit_pack_purchase_request_fields_json)
        requested_credits = request_fields["requested_initial_credits_in_credit_pack"]
        # Calculate the missing maximum price parameter if not provided
        if maximum_total_credit_pack_price_in_psl is None:
            maximum_total_credit_pack_price_in_psl = maximum_per_credit_price_in_psl * requested_credits
        elif maximum_per_credit_price_in_psl is None:
            maximum_per_credit_price_in_psl = maximum_total_credit_pack_price_in_psl / requested_credits
        # Estimate the fair market price for the credits
        estimated_price_per_credit = await estimated_market_price_of_inference_credits_in_psl_terms()
        # Calculate the price difference percentage
        price_difference_percentage = await self.calculate_price_difference_percentage(quoted_price_per_credit, estimated_price_per_credit)
        # Compare the quoted prices with the maximum prices and the estimated fair price
        if (
            quoted_price_per_credit <= maximum_per_credit_price_in_psl and
            quoted_total_price <= maximum_total_credit_pack_price_in_psl and
            price_difference_percentage <= MAXIMUM_LOCAL_CREDIT_PRICE_DIFFERENCE_TO_ACCEPT_CREDIT_PRICING
        ):
            return True
        else:
            logger.warning(f"Preliminary price quote exceeds the maximum acceptable price or the price difference from the estimated fair price is too high! Quoted price: {quoted_price_per_credit} PSL per credit, {quoted_total_price:,.1f} PSL total, maximum price: {maximum_per_credit_price_in_psl:,.1f} PSL per credit, {maximum_total_credit_pack_price_in_psl:,.1f} PSL total. The price difference from the estimated fair market price is {100*price_difference_percentage:.2f}%, which exceeds the allowed maximum of {100*MAXIMUM_LOCAL_CREDIT_PRICE_DIFFERENCE_TO_ACCEPT_CREDIT_PRICING:.2f}%.")
            return False
        
    async def internal_estimate_of_credit_pack_ticket_cost_in_psl(self, desired_number_of_credits: float, price_cushion_pct: float):
        estimated_price_per_credit = await estimated_market_price_of_inference_credits_in_psl_terms()
        estimated_total_cost_of_ticket = round(desired_number_of_credits * estimated_price_per_credit * (1 + price_cushion_pct), 2)
        return estimated_total_cost_of_ticket
        
    async def credit_pack_ticket_preliminary_price_quote_response(
        self,
        supernode_url: str,
        credit_pack_request: CreditPackPurchaseRequest,
        preliminary_price_quote: Union[CreditPackPurchaseRequestPreliminaryPriceQuote, CreditPackPurchaseRequestRejection],
        maximum_total_credit_pack_price_in_psl: Optional[float] = None,
        maximum_per_credit_price_in_psl: Optional[float] = None
    ) -> Union[CreditPackPurchaseRequestResponse, CreditPackPurchaseRequestResponseTermination]:
        if isinstance(preliminary_price_quote, CreditPackPurchaseRequestRejection):
            logger.error(f"Credit pack purchase request rejected: {preliminary_price_quote.rejection_reason_string}")
            return None
        # Check if the end user agrees with the preliminary price quote
        agree_with_price_quote = await self.confirm_preliminary_price_quote(preliminary_price_quote, maximum_total_credit_pack_price_in_psl, maximum_per_credit_price_in_psl)
        if not agree_with_price_quote:
            agree_with_preliminary_price_quote = False
        else:
            agree_with_preliminary_price_quote = True        
        # Prepare the CreditPackPurchaseRequestPreliminaryPriceQuoteResponse
        price_quote_response = CreditPackPurchaseRequestPreliminaryPriceQuoteResponse(
            sha3_256_hash_of_credit_pack_purchase_request_fields=credit_pack_request.sha3_256_hash_of_credit_pack_purchase_request_fields,
            sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields=preliminary_price_quote.sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields,
            credit_pack_purchase_request_fields_json_b64=preliminary_price_quote.credit_pack_purchase_request_fields_json_b64,
            agree_with_preliminary_price_quote=agree_with_preliminary_price_quote,
            credit_usage_tracking_psl_address=preliminary_price_quote.credit_usage_tracking_psl_address,
            preliminary_quoted_price_per_credit_in_psl=preliminary_price_quote.preliminary_quoted_price_per_credit_in_psl,
            preliminary_price_quote_response_timestamp_utc_iso_string=datetime.now(timezone.utc).isoformat(),
            preliminary_price_quote_response_pastel_block_height=await get_current_pastel_block_height_func(),
            preliminary_price_quote_response_message_version_string="1.0",
            requesting_end_user_pastelid=credit_pack_request.requesting_end_user_pastelid,
            sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_response_fields="",
            requesting_end_user_pastelid_signature_on_preliminary_price_quote_response_hash=""
        )
        # Generate the hash and signature fields
        price_quote_response.sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_response_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(price_quote_response)
        price_quote_response.requesting_end_user_pastelid_signature_on_preliminary_price_quote_response_hash = await sign_message_with_pastelid_func(
            credit_pack_request.requesting_end_user_pastelid,
            price_quote_response.sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_response_fields,
            self.passphrase
        )
        # Send the CreditPackPurchaseRequestPreliminaryPriceQuoteResponse to the supernode
        challenge_result = await self.request_and_sign_challenge(supernode_url)
        challenge = challenge_result["challenge"]
        challenge_id = challenge_result["challenge_id"]
        challenge_signature = challenge_result["signature"]
        payload = price_quote_response.model_dump()
        payload = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in payload.items()}
        log_action_with_payload("sending", "price quote response to supernode", payload)
        async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS*4)) as client:
            response = await client.post(
                f"{supernode_url}/credit_purchase_preliminary_price_quote_response",
                json={
                    "challenge": challenge,
                    "challenge_id": challenge_id,
                    "challenge_signature": challenge_signature,
                    "preliminary_price_quote_response": payload,
                }
            )
            response.raise_for_status()
            result = response.json()
            if "termination_reason_string" in result:
                logger.error(f"Credit pack purchase request response terminated: {result['termination_reason_string']}")
                return CreditPackPurchaseRequestResponseTermination.model_validate(result)
            else:
                transformed_result = transform_credit_pack_purchase_request_response(result)
                log_action_with_payload("receiving", "response to credit pack purchase request", transformed_result)
                return CreditPackPurchaseRequestResponse.model_validate(transformed_result)

    async def check_status_of_credit_purchase_request(self, supernode_url: str, credit_pack_purchase_request_hash: str) -> CreditPackPurchaseRequestStatus:
        challenge_result = await self.request_and_sign_challenge(supernode_url)
        challenge = challenge_result["challenge"]
        challenge_id = challenge_result["challenge_id"]
        challenge_signature = challenge_result["signature"]
        # Create the CreditPackRequestStatusCheck model instance
        status_check = CreditPackRequestStatusCheck(
            sha3_256_hash_of_credit_pack_purchase_request_fields=credit_pack_purchase_request_hash,
            requesting_end_user_pastelid=self.pastelid,
            requesting_end_user_pastelid_signature_on_sha3_256_hash_of_credit_pack_purchase_request_fields=await sign_message_with_pastelid_func(self.pastelid, credit_pack_purchase_request_hash, self.passphrase)
        )
        # Convert the model instance to JSON payload
        payload = status_check.model_dump()
        payload = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in payload.items()}
        log_action_with_payload("checking", "status of credit pack purchase request", payload)
        async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
            response = await client.post(
                f"{supernode_url}/check_status_of_credit_purchase_request",
                json={
                    "credit_pack_request_status_check": payload,
                    "challenge": challenge,
                    "challenge_id": challenge_id,
                    "challenge_signature": challenge_signature
                }
            )
            response.raise_for_status()
            result = response.json()
            log_action_with_payload("receiving", "credit pack purchase request response from Supernode", result)
            return CreditPackPurchaseRequestStatus.model_validate(result)

    async def confirm_credit_purchase_request(self, supernode_url: str, credit_pack_purchase_request_confirmation: CreditPackPurchaseRequestConfirmation) -> CreditPackPurchaseRequestConfirmationResponse:
        challenge_result = await self.request_and_sign_challenge(supernode_url)
        challenge = challenge_result["challenge"]
        challenge_id = challenge_result["challenge_id"]
        challenge_signature = challenge_result["signature"]
        payload = credit_pack_purchase_request_confirmation.model_dump()
        payload = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in payload.items()}
        log_action_with_payload("confirming", "credit pack purchase request", payload)        
        async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS*30)) as client: # Need to be patient with the timeout here since it requires the transaction to be mined/confirmed
            response = await client.post(
                f"{supernode_url}/confirm_credit_purchase_request",
                json={
                    "confirmation": payload,
                    "challenge": challenge,
                    "challenge_id": challenge_id,
                    "challenge_signature": challenge_signature
                }
            )
            response.raise_for_status()
            result = response.json()
            log_action_with_payload("receiving", "response to credit pack purchase confirmation", result)
            return CreditPackPurchaseRequestConfirmationResponse.model_validate(result)

    async def credit_pack_purchase_completion_announcement(self, supernode_url: str, credit_pack_purchase_request_confirmation: CreditPackPurchaseRequestConfirmation) -> None:
        challenge_result = await self.request_and_sign_challenge(supernode_url)
        challenge = challenge_result["challenge"]
        challenge_id = challenge_result["challenge_id"]
        challenge_signature = challenge_result["signature"]
        payload = credit_pack_purchase_request_confirmation.model_dump()
        payload = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in payload.items()}        
        # log_action_with_payload("sending", "purchase completion announcement message", payload)
        async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
            response = await client.post(
                f"{supernode_url}/credit_pack_purchase_completion_announcement",
                json={
                    "confirmation": payload,
                    "challenge": challenge,
                    "challenge_id": challenge_id,
                    "challenge_signature": challenge_signature
                }
            )
            response.raise_for_status()

    async def credit_pack_storage_retry_request(self, supernode_url: str, credit_pack_storage_retry_request: CreditPackStorageRetryRequest) -> CreditPackStorageRetryRequestResponse:
        challenge_result = await self.request_and_sign_challenge(supernode_url)
        challenge = challenge_result["challenge"]
        challenge_id = challenge_result["challenge_id"]
        challenge_signature = challenge_result["signature"]
        payload = credit_pack_storage_retry_request.model_dump()
        payload = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in payload.items()}        
        log_action_with_payload("sending", "credit pack storage retry request", payload)
        async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
            response = await client.post(
                f"{supernode_url}/credit_pack_storage_retry_request",
                json={
                    "request": payload,
                    "challenge": challenge,
                    "challenge_id": challenge_id,
                    "challenge_signature": challenge_signature
                }
            )
            response.raise_for_status()
            result = response.json()
            log_action_with_payload("receiving", "response to credit pack storage retry request", result)
            return CreditPackStorageRetryRequestResponse.model_validate(result)

    async def credit_pack_storage_retry_completion_announcement(self, supernode_url: str, credit_pack_storage_retry_request_response: CreditPackStorageRetryRequestResponse) -> None:
        challenge_result = await self.request_and_sign_challenge(supernode_url)
        challenge = challenge_result["challenge"]
        challenge_id = challenge_result["challenge_id"]
        challenge_signature = challenge_result["signature"]
        payload = credit_pack_storage_retry_request_response.model_dump()
        payload = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in payload.items()}        
        log_action_with_payload("sending", "storage retry completion announcement message", payload)
        async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
            response = await client.post(
                f"{supernode_url}/credit_pack_storage_retry_completion_announcement",
                json={
                    "response": payload,
                    "challenge": challenge,
                    "challenge_id": challenge_id,
                    "challenge_signature": challenge_signature
                }
            )
            response.raise_for_status()

    async def make_inference_api_usage_request(self, supernode_url: str, request_data: InferenceAPIUsageRequest) -> InferenceAPIUsageResponse:
        challenge_result = await self.request_and_sign_challenge(supernode_url)
        challenge = challenge_result["challenge"]
        challenge_id = challenge_result["challenge_id"]
        challenge_signature = challenge_result["signature"]
        payload = request_data.model_dump()
        payload = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in payload.items()}        
        log_action_with_payload("making", "inference usage request", payload)
        async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS*6)) as client:
            response = await client.post(
                f"{supernode_url}/make_inference_api_usage_request",
                json={
                    "inference_api_usage_request": payload,
                    "challenge": challenge,
                    "challenge_id": challenge_id,
                    "challenge_signature": challenge_signature
                }
            )
            response.raise_for_status()
            result = response.json()
            log_action_with_payload("received", "response to inference usage request", result)
            return InferenceAPIUsageResponse.model_validate(result)

    async def send_inference_confirmation(self, supernode_url: str, confirmation_data: InferenceConfirmation) -> Dict[str, Any]:
        challenge_result = await self.request_and_sign_challenge(supernode_url)
        challenge = challenge_result["challenge"]
        challenge_id = challenge_result["challenge_id"]
        challenge_signature = challenge_result["signature"]
        payload = confirmation_data.model_dump()
        payload = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in payload.items()}        
        log_action_with_payload("sending", "inference confirmation", payload)
        async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS*4)) as client:
            response = await client.post(
                f"{supernode_url}/confirm_inference_request",
                json={
                    "inference_confirmation": payload,
                    "challenge": challenge,
                    "challenge_id": challenge_id,
                    "challenge_signature": challenge_signature
                }
            )
            response.raise_for_status()
            result = response.json()
            log_action_with_payload("receiving", "response to inference confirmation", result)
            return result

    async def check_status_of_inference_request_results(self, supernode_url: str, inference_response_id: str) -> bool:
        async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
            try:
                response = await client.get(f"{supernode_url}/check_status_of_inference_request_results/{inference_response_id}")
                response.raise_for_status()
                result = response.json()
                log_action_with_payload("receiving", f"status of inference request results for ID {inference_response_id}", result)
                return result if isinstance(result, bool) else False
            except httpx.HTTPStatusError as e:
                logger.error(f"HTTP error in check_status_of_inference_request_results from Supernode URL: {supernode_url}: {e}")
                return False
            except Exception as e:
                logger.error(f"Error in check_status_of_inference_request_results from Supernode URL: {supernode_url}: {e}")
                return False

    async def retrieve_inference_output_results(self, supernode_url: str, inference_request_id: str, inference_response_id: str) -> InferenceAPIOutputResult:
        challenge_result = await self.request_and_sign_challenge(supernode_url)
        challenge = challenge_result["challenge"]
        challenge_id = challenge_result["challenge_id"]
        challenge_signature = challenge_result["signature"]
        params = {
            "inference_response_id": inference_response_id,
            "pastelid": self.pastelid,
            "challenge": challenge,
            "challenge_id": challenge_id,
            "challenge_signature": challenge_signature
        }
        log_action_with_payload("attempting", f"to retrieve inference output results for response ID {inference_response_id}", params)
        async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
            response = await client.post(f"{supernode_url}/retrieve_inference_output_results", params=params)
            response.raise_for_status()
            result = response.json()
            log_action_with_payload("receiving", "inference output results", result)
            return InferenceAPIOutputResult.model_validate(result)

    async def call_audit_inference_request_response(self, supernode_url: str, inference_response_id: str) -> InferenceAPIUsageResponse:
        try:
            signature = await sign_message_with_pastelid_func(self.pastelid, inference_response_id, self.passphrase)
            payload = {
                "inference_response_id": inference_response_id,
                "pastel_id": self.pastelid,
                "signature": signature
            }
            log_action_with_payload("calling", "audit inference request response", payload)
            async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS*2)) as client:
                response = await client.post(f"{supernode_url}/audit_inference_request_response", json=payload)
                response.raise_for_status()
                result = response.json()
                log_action_with_payload("receiving", "response to audit inference request response", result)
                return InferenceAPIUsageResponse.model_validate(result)
        except Exception as e:
            logger.error(f"Error in audit_inference_request_response from Supernode URL: {supernode_url}: {e}")
            traceback.print_exc()
            raise

    async def call_audit_inference_request_result(self, supernode_url: str, inference_response_id: str) -> InferenceAPIOutputResult:
        try:
            signature = await sign_message_with_pastelid_func(self.pastelid, inference_response_id, self.passphrase)
            payload = {
                "inference_response_id": inference_response_id,
                "pastel_id": self.pastelid,
                "signature": signature
            }
            log_action_with_payload("calling", "audit inference request result", payload)
            async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS*2)) as client:
                response = await client.post(f"{supernode_url}/audit_inference_request_result", json=payload)
                response.raise_for_status()
                result = response.json()
                log_action_with_payload("receiving", "response to audit inference request result", result)
                return InferenceAPIOutputResult.model_validate(result)
        except Exception as e:
            logger.error(f"Error in audit_inference_request_result from Supernode URL: {supernode_url}: {e}")
            raise
            
    async def audit_inference_request_response_id(self, inference_response_id: str, pastelid_of_supernode_to_audit: str):
        supernode_list_df, _ = await check_supernode_list_func()
        n = 4
        supernode_urls_and_pastelids = await get_n_closest_supernodes_to_pastelid_urls(n, self.pastelid, supernode_list_df)
        list_of_supernode_pastelids = [x[1] for x in supernode_urls_and_pastelids if x[1] != pastelid_of_supernode_to_audit]
        list_of_supernode_urls = [x[0] for x in supernode_urls_and_pastelids if x[1] != pastelid_of_supernode_to_audit]
        list_of_supernode_ips = [x.split('//')[1].split(':')[0] for x in list_of_supernode_urls]
        # Audit the inference request response
        response_audit_tasks = [self.call_audit_inference_request_response(url, inference_response_id) for url in list_of_supernode_urls]
        response_audit_results = await asyncio.gather(*response_audit_tasks)
        # Wait for 20 seconds before auditing the inference request result
        await asyncio.sleep(20)
        # Audit the inference request result
        result_audit_tasks = [self.call_audit_inference_request_result(url, inference_response_id) for url in list_of_supernode_urls]
        result_audit_results = await asyncio.gather(*result_audit_tasks)
        # Combine the audit results
        audit_results = response_audit_results + result_audit_results
        return audit_results
    
    async def get_valid_credit_pack_tickets_for_pastelid(self, supernode_url: str, pastelid: str) -> List[Dict[str, Any]]:
        challenge_result = await self.request_and_sign_challenge(supernode_url)
        challenge = challenge_result["challenge"]
        challenge_id = challenge_result["challenge_id"]
        challenge_signature = challenge_result["signature"]
        payload = {
            "pastelid": pastelid,
            "challenge": challenge,
            "challenge_id": challenge_id,
            "challenge_signature": challenge_signature
        }
        log_action_with_payload("requesting", "valid credit pack tickets for PastelID", payload)
        async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
            response = await client.post(
                f"{supernode_url}/get_valid_credit_pack_tickets_for_pastelid",
                json=payload
            )
            response.raise_for_status()
            result = response.json()
            log_action_with_payload("receiving", "valid credit pack tickets for PastelID", result)
            return result
    
    async def check_if_supernode_supports_desired_model(self, supernode_url: str, model_canonical_string: str, model_inference_type_string: str, model_parameters_json: str) -> bool:
        try:
            async with httpx.AsyncClient(timeout=Timeout(MESSAGING_TIMEOUT_IN_SECONDS)) as client:
                response = await client.get(f"{supernode_url}/get_inference_model_menu")
                response.raise_for_status()
                model_menu = response.json()
                desired_parameters = json.loads(model_parameters_json)  # Convert JSON string to dictionary
                for model in model_menu["models"]:
                    if model["model_name"] == model_canonical_string and \
                    model_inference_type_string in model["supported_inference_type_strings"]:
                        # Track unsupported parameters
                        unsupported_parameters = []
                        for desired_param, desired_value in desired_parameters.items():
                            param_found = False
                            for param in model["model_parameters"]:
                                if param["name"] == desired_param:
                                    # Check if the desired parameter value is within the valid range or options
                                    if "type" in param:
                                        if param["type"] == "int" and isinstance(desired_value, int):
                                            param_found = True
                                        elif param["type"] == "float" and isinstance(desired_value, float):
                                            param_found = True
                                        elif param["type"] == "string" and isinstance(desired_value, str):
                                            if "options" in param and desired_value in param["options"]:
                                                param_found = True
                                            elif "options" not in param:
                                                param_found = True
                                    else:
                                        param_found = True
                                    break
                            if not param_found:
                                unsupported_parameters.append(desired_param)
                        if not unsupported_parameters:
                            return True  # All desired parameters are supported
                        else:
                            # Log unsupported parameters and return False
                            unsupported_param_str = ", ".join(unsupported_parameters)
                            logger.error(f"Unsupported model parameters for {model_canonical_string}: {unsupported_param_str}")
                            return False
                return False  # Model not found or does not support the desired inference type
        except Exception as e:
            logger.error(f"Error in check_if_supernode_supports_desired_model from Supernode URL: {supernode_url}: {e}")
            return False
        
    async def get_closest_supernode_url_that_supports_desired_model(self, desired_model_canonical_string: str, desired_model_inference_type_string: str, desired_model_parameters_json: str):
        supernode_list_df, _ = await check_supernode_list_func()
        n = len(supernode_list_df)
        supernode_urls_and_pastelids = await get_n_closest_supernodes_to_pastelid_urls(n, self.pastelid, supernode_list_df)
        list_of_supernode_pastelids = [x[1] for x in supernode_urls_and_pastelids]
        list_of_supernode_urls = [x[0] for x in supernode_urls_and_pastelids]
        list_of_supernode_ips = [x.split('//')[1].split(':')[0] for x in list_of_supernode_urls]
        # Check which supernodes support the desired model
        model_support_tasks = [self.check_if_supernode_supports_desired_model(url, desired_model_canonical_string, desired_model_inference_type_string, desired_model_parameters_json) for url in list_of_supernode_urls]
        model_support_results = await asyncio.gather(*model_support_tasks)
        supernode_support_dict = {pastelid: supports for pastelid, supports in zip(list_of_supernode_pastelids, model_support_results)}
        closest_supporting_supernode_pastelid = list_of_supernode_pastelids[model_support_results.index(True)] if True in model_support_results else None
        closest_supporting_supernode_url = list_of_supernode_urls[model_support_results.index(True)] if True in model_support_results else None
        return supernode_support_dict, closest_supporting_supernode_pastelid, closest_supporting_supernode_url


def validate_inference_response_fields(response_audit_results: List[InferenceAPIUsageResponse], usage_request_response: InferenceAPIUsageResponse) -> Dict[str, bool]:
    # Count the occurrences of each value for the relevant fields in response_audit_results
    inference_response_id_counts = {}
    inference_request_id_counts = {}
    proposed_cost_in_credits_counts = {}
    remaining_credits_after_request_counts = {}
    credit_usage_tracking_psl_address_counts = {}
    request_confirmation_message_amount_in_patoshis_counts = {}
    max_block_height_to_include_confirmation_transaction_counts = {}
    supernode_pastelid_and_signature_on_inference_response_id_counts = {}
    for result in response_audit_results:
        inference_response_id_counts[result.inference_response_id] = inference_response_id_counts.get(result.inference_response_id, 0) + 1
        inference_request_id_counts[result.inference_request_id] = inference_request_id_counts.get(result.inference_request_id, 0) + 1
        proposed_cost_in_credits_counts[result.proposed_cost_of_request_in_inference_credits] = proposed_cost_in_credits_counts.get(result.proposed_cost_of_request_in_inference_credits, 0) + 1
        remaining_credits_after_request_counts[result.remaining_credits_in_pack_after_request_processed] = remaining_credits_after_request_counts.get(result.remaining_credits_in_pack_after_request_processed, 0) + 1
        credit_usage_tracking_psl_address_counts[result.credit_usage_tracking_psl_address] = credit_usage_tracking_psl_address_counts.get(result.credit_usage_tracking_psl_address, 0) + 1
        request_confirmation_message_amount_in_patoshis_counts[result.request_confirmation_message_amount_in_patoshis] = request_confirmation_message_amount_in_patoshis_counts.get(result.request_confirmation_message_amount_in_patoshis, 0) + 1
        max_block_height_to_include_confirmation_transaction_counts[result.max_block_height_to_include_confirmation_transaction] = max_block_height_to_include_confirmation_transaction_counts.get(result.max_block_height_to_include_confirmation_transaction, 0) + 1
        supernode_pastelid_and_signature_on_inference_response_id_counts[result.supernode_pastelid_and_signature_on_inference_request_response_hash] = supernode_pastelid_and_signature_on_inference_response_id_counts.get(result.supernode_pastelid_and_signature_on_inference_request_response_hash, 0) + 1
    # Determine the majority value for each field
    majority_inference_response_id = max(inference_response_id_counts, key=inference_response_id_counts.get) if inference_response_id_counts else None
    majority_inference_request_id = max(inference_request_id_counts, key=inference_request_id_counts.get) if inference_request_id_counts else None
    majority_proposed_cost_in_credits = max(proposed_cost_in_credits_counts, key=proposed_cost_in_credits_counts.get) if proposed_cost_in_credits_counts else None
    majority_remaining_credits_after_request = max(remaining_credits_after_request_counts, key=remaining_credits_after_request_counts.get) if remaining_credits_after_request_counts else None
    majority_credit_usage_tracking_psl_address = max(credit_usage_tracking_psl_address_counts, key=credit_usage_tracking_psl_address_counts.get) if credit_usage_tracking_psl_address_counts else None
    majority_request_confirmation_message_amount_in_patoshis = max(request_confirmation_message_amount_in_patoshis_counts, key=request_confirmation_message_amount_in_patoshis_counts.get) if request_confirmation_message_amount_in_patoshis_counts else None
    majority_max_block_height_to_include_confirmation_transaction = max(max_block_height_to_include_confirmation_transaction_counts, key=max_block_height_to_include_confirmation_transaction_counts.get) if max_block_height_to_include_confirmation_transaction_counts else None
    majority_supernode_pastelid_and_signature_on_inference_response_id = max(supernode_pastelid_and_signature_on_inference_response_id_counts, key=supernode_pastelid_and_signature_on_inference_response_id_counts.get) if supernode_pastelid_and_signature_on_inference_response_id_counts else None
    # Compare the majority values with the values from the usage_request_response
    validation_results = {
        "inference_response_id": majority_inference_response_id == usage_request_response.inference_response_id,
        "inference_request_id": majority_inference_request_id == usage_request_response.inference_request_id,
        "proposed_cost_in_credits": majority_proposed_cost_in_credits == usage_request_response.proposed_cost_of_request_in_inference_credits,
        "remaining_credits_after_request": majority_remaining_credits_after_request == usage_request_response.remaining_credits_in_pack_after_request_processed,
        "credit_usage_tracking_psl_address": majority_credit_usage_tracking_psl_address == usage_request_response.credit_usage_tracking_psl_address,
        "request_confirmation_message_amount_in_patoshis": majority_request_confirmation_message_amount_in_patoshis == usage_request_response.request_confirmation_message_amount_in_patoshis,
        "max_block_height_to_include_confirmation_transaction": majority_max_block_height_to_include_confirmation_transaction == usage_request_response.max_block_height_to_include_confirmation_transaction,
        "supernode_pastelid_and_signature_on_inference_response_id": majority_supernode_pastelid_and_signature_on_inference_response_id == usage_request_response.supernode_pastelid_and_signature_on_inference_request_response_hash
    }
    return validation_results

def validate_inference_result_fields(result_audit_results: List[InferenceAPIOutputResult], usage_result: InferenceAPIOutputResult) -> Dict[str, bool]:
    # Count the occurrences of each value for the relevant fields in result_audit_results
    inference_result_id_counts = {}
    inference_request_id_counts = {}
    inference_response_id_counts = {}
    responding_supernode_pastelid_counts = {}
    inference_result_json_base64_counts = {}
    inference_result_file_type_strings_counts = {}
    responding_supernode_signature_on_inference_result_id_counts = {}
    for result in result_audit_results:
        inference_result_id_counts[result.inference_result_id] = inference_result_id_counts.get(result.inference_result_id, 0) + 1
        inference_request_id_counts[result.inference_request_id] = inference_request_id_counts.get(result.inference_request_id, 0) + 1
        inference_response_id_counts[result.inference_response_id] = inference_response_id_counts.get(result.inference_response_id, 0) + 1
        responding_supernode_pastelid_counts[result.responding_supernode_pastelid] = responding_supernode_pastelid_counts.get(result.responding_supernode_pastelid, 0) + 1
        inference_result_json_base64_counts[result.inference_result_json_base64[:32]] = inference_result_json_base64_counts.get(result.inference_result_json_base64[:32], 0) + 1
        inference_result_file_type_strings_counts[result.inference_result_file_type_strings] = inference_result_file_type_strings_counts.get(result.inference_result_file_type_strings, 0) + 1
        responding_supernode_signature_on_inference_result_id_counts[result.responding_supernode_signature_on_inference_result_id] = responding_supernode_signature_on_inference_result_id_counts.get(result.responding_supernode_signature_on_inference_result_id, 0) + 1
    # Determine the majority value for each field
    majority_inference_result_id = max(inference_result_id_counts, key=inference_result_id_counts.get) if inference_result_id_counts else None
    majority_inference_request_id = max(inference_request_id_counts, key=inference_request_id_counts.get) if inference_request_id_counts else None
    majority_inference_response_id = max(inference_response_id_counts, key=inference_response_id_counts.get) if inference_response_id_counts else None
    majority_responding_supernode_pastelid = max(responding_supernode_pastelid_counts, key=responding_supernode_pastelid_counts.get) if responding_supernode_pastelid_counts else None
    majority_inference_result_json_base64 = max(inference_result_json_base64_counts, key=inference_result_json_base64_counts.get) if inference_result_json_base64_counts else None
    majority_inference_result_file_type_strings = max(inference_result_file_type_strings_counts, key=inference_result_file_type_strings_counts.get) if inference_result_file_type_strings_counts else None
    majority_responding_supernode_signature_on_inference_result_id = max(responding_supernode_signature_on_inference_result_id_counts, key=responding_supernode_signature_on_inference_result_id_counts.get) if responding_supernode_signature_on_inference_result_id_counts else None
    # Compare the majority values with the values from the usage_result
    validation_results = {
        "inference_result_id": majority_inference_result_id == usage_result.inference_result_id,
        "inference_request_id": majority_inference_request_id == usage_result.inference_request_id,
        "inference_response_id": majority_inference_response_id == usage_result.inference_response_id,
        "responding_supernode_pastelid": majority_responding_supernode_pastelid == usage_result.responding_supernode_pastelid,
        "inference_result_json_base64": majority_inference_result_json_base64 == usage_result.inference_result_json_base64[:32],
        "inference_result_file_type_strings": majority_inference_result_file_type_strings == usage_result.inference_result_file_type_strings,
        "responding_supernode_signature_on_inference_result_id": majority_responding_supernode_signature_on_inference_result_id == usage_result.responding_supernode_signature_on_inference_result_id
    }
    return validation_results
            
def validate_inference_data(inference_result_dict: Dict[str, Any], audit_results: List[Union[InferenceAPIUsageResponse, InferenceAPIOutputResult]]) -> Dict[str, Dict[str, bool]]:
    # Extract relevant fields from inference_result_dict
    usage_request_response = InferenceAPIUsageResponse.model_validate(inference_result_dict["usage_request_response"])
    usage_result = InferenceAPIOutputResult.model_validate(inference_result_dict["output_results"])
    # Validate InferenceAPIUsageResponse fields
    response_validation_results = validate_inference_response_fields(
        [result for result in audit_results if isinstance(result, InferenceAPIUsageResponse)],
        usage_request_response
    )
    # Validate InferenceAPIOutputResult fields
    result_validation_results = validate_inference_result_fields(
        [result for result in audit_results if isinstance(result, InferenceAPIOutputResult)],
        usage_result
    )
    # Combine validation results
    validation_results = {
        "response_validation": response_validation_results,
        "result_validation": result_validation_results
    }
    return validation_results
        
async def send_message_and_check_for_new_incoming_messages(
    to_pastelid: str,
    message_body: str
):
    global MY_LOCAL_PASTELID, MY_PASTELID_PASSPHRASE
    # Create inference client to use:
    inference_client = PastelInferenceClient(MY_LOCAL_PASTELID, MY_PASTELID_PASSPHRASE)    
    # Get the list of Supernodes
    supernode_list_df, supernode_list_json = await check_supernode_list_func()
    # Send a user message
    # Lookup the 3 closest supernodes to the recipient pastelid
    closest_supernodes_to_recipient = await get_n_closest_supernodes_to_pastelid_urls(3, to_pastelid, supernode_list_df)
    # Create a UserMessage object
    user_message = UserMessage(
        from_pastelid=MY_LOCAL_PASTELID,
        to_pastelid=to_pastelid,
        message_body=json.dumps(message_body),  # Convert message_body to JSON
        message_signature=await sign_message_with_pastelid_func(MY_LOCAL_PASTELID, message_body, MY_PASTELID_PASSPHRASE)
    )
    # Send the message to the 3 closest Supernodes concurrently
    send_tasks = []
    for supernode_url, _ in closest_supernodes_to_recipient:
        send_task = asyncio.create_task(inference_client.send_user_message(supernode_url, user_message))
        send_tasks.append(send_task)
    send_results = await asyncio.gather(*send_tasks)
    # Get user messages from the 3 closest Supernodes
    # Lookup the 3 closest supernodes to the local pastelid
    closest_supernodes_to_local = await get_n_closest_supernodes_to_pastelid_urls(3, inference_client.pastelid, supernode_list_df)
    # Retrieve messages from the 3 closest Supernodes concurrently
    message_retrieval_tasks = []
    for supernode_url, _ in closest_supernodes_to_local:
        message_retrieval_task = asyncio.create_task(inference_client.get_user_messages(supernode_url))
        message_retrieval_tasks.append(message_retrieval_task)
    message_lists = await asyncio.gather(*message_retrieval_tasks)
    # Combine the message lists and remove duplicates
    unique_messages = []
    message_ids = set()
    for message_list in message_lists:
        for message in message_list:
            if message.id not in message_ids:
                unique_messages.append(message)
                message_ids.add(message.id)
    message_dict = {
        "sent_messages": send_results,
        "received_messages": unique_messages
    }        
    return message_dict

async def handle_credit_pack_ticket_end_to_end(
    number_of_credits: float,
    credit_usage_tracking_psl_address: str,
    burn_address: str,
    maximum_total_credit_pack_price_in_psl: Optional[float] = None,
    maximum_per_credit_price_in_psl: Optional[float] = None
    ):
    global MY_LOCAL_PASTELID, MY_PASTELID_PASSPHRASE
    # Create inference client to use:
    inference_client = PastelInferenceClient(MY_LOCAL_PASTELID, MY_PASTELID_PASSPHRASE)    
    # Get the list of Supernodes
    supernode_list_df, supernode_list_json = await check_supernode_list_func()
    # Prepare the credit pack request
    credit_pack_request = CreditPackPurchaseRequest(
        requesting_end_user_pastelid=MY_LOCAL_PASTELID,
        requested_initial_credits_in_credit_pack=number_of_credits,
        list_of_authorized_pastelids_allowed_to_use_credit_pack=json.dumps([MY_LOCAL_PASTELID]),
        credit_usage_tracking_psl_address=credit_usage_tracking_psl_address,
        request_timestamp_utc_iso_string=datetime.now(timezone.utc).isoformat(),
        request_pastel_block_height=await get_current_pastel_block_height_func(),
        credit_purchase_request_message_version_string="1.0",
        sha3_256_hash_of_credit_pack_purchase_request_fields="",
        requesting_end_user_pastelid_signature_on_request_hash=""
    )
    credit_pack_request.sha3_256_hash_of_credit_pack_purchase_request_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(credit_pack_request)
    credit_pack_request.requesting_end_user_pastelid_signature_on_request_hash = await sign_message_with_pastelid_func(MY_LOCAL_PASTELID, credit_pack_request.sha3_256_hash_of_credit_pack_purchase_request_fields, MY_PASTELID_PASSPHRASE)
    # Send the credit pack request to the highest-ranked supernode
    closest_supernodes = await get_n_closest_supernodes_to_pastelid_urls(1, MY_LOCAL_PASTELID, supernode_list_df)
    highest_ranked_supernode_url = closest_supernodes[0][0]
    preliminary_price_quote = await inference_client.credit_pack_ticket_initial_purchase_request(highest_ranked_supernode_url, credit_pack_request)
    # Check if the end user agrees with the preliminary price quote
    signed_credit_pack_ticket_or_rejection = await inference_client.credit_pack_ticket_preliminary_price_quote_response(highest_ranked_supernode_url, credit_pack_request, preliminary_price_quote, maximum_total_credit_pack_price_in_psl, maximum_per_credit_price_in_psl)
    if isinstance(signed_credit_pack_ticket_or_rejection, CreditPackPurchaseRequestResponseTermination):
        logger.error(f"Credit pack purchase request terminated: {signed_credit_pack_ticket_or_rejection.termination_reason_string}")
        return None
    signed_credit_pack_ticket = signed_credit_pack_ticket_or_rejection
    # Send the required PSL from the credit usage tracking address to the burn address
    burn_transaction_txid = await send_to_address_func(burn_address, round(signed_credit_pack_ticket.proposed_total_cost_of_credit_pack_in_psl, 5), "Burn transaction for credit pack ticket")
    if burn_transaction_txid is None:
        logger.error("Error sending PSL to burn address for credit pack ticket")
        return None
    # Prepare the credit pack purchase request confirmation
    credit_pack_purchase_request_confirmation = CreditPackPurchaseRequestConfirmation(
        sha3_256_hash_of_credit_pack_purchase_request_fields=credit_pack_request.sha3_256_hash_of_credit_pack_purchase_request_fields,
        sha3_256_hash_of_credit_pack_purchase_request_response_fields=signed_credit_pack_ticket.sha3_256_hash_of_credit_pack_purchase_request_response_fields,
        credit_pack_purchase_request_fields_json_b64=signed_credit_pack_ticket.credit_pack_purchase_request_fields_json_b64,
        requesting_end_user_pastelid=MY_LOCAL_PASTELID,
        txid_of_credit_purchase_burn_transaction=burn_transaction_txid,
        credit_purchase_request_confirmation_utc_iso_string=datetime.now(timezone.utc).isoformat(),
        credit_purchase_request_confirmation_pastel_block_height=await get_current_pastel_block_height_func(),
        credit_purchase_request_confirmation_message_version_string="1.0",
        sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields="",
        requesting_end_user_pastelid_signature_on_sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields=""
    )
    credit_pack_purchase_request_confirmation.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(credit_pack_purchase_request_confirmation)
    credit_pack_purchase_request_confirmation.requesting_end_user_pastelid_signature_on_sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields = await sign_message_with_pastelid_func(MY_LOCAL_PASTELID, credit_pack_purchase_request_confirmation.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields, MY_PASTELID_PASSPHRASE)
    credit_pack_purchase_request_confirmation_response = await inference_client.confirm_credit_purchase_request(highest_ranked_supernode_url, credit_pack_purchase_request_confirmation)
    # Send the credit pack purchase completion announcement to the responding supernode:
    list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms = json.loads(signed_credit_pack_ticket.list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms)
    for supernode_pastelid in list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms:
        try:
            is_valid_pastelid = check_if_pastelid_is_valid_func(supernode_pastelid)
            if is_valid_pastelid:
                supernode_url = await get_supernode_url_from_pastelid_func(supernode_pastelid, supernode_list_df)
                await inference_client.credit_pack_purchase_completion_announcement(supernode_url, credit_pack_purchase_request_confirmation)
        except Exception as e:
            logger.error(f"Error sending credit_pack_purchase_completion_announcement to Supernode URL: {supernode_url}: {e}")
    # Check the status of the credit purchase request
    for i, (supernode_url, _) in enumerate(closest_supernodes):
        try:
            credit_pack_purchase_request_status = await inference_client.check_status_of_credit_purchase_request(supernode_url, credit_pack_request.sha3_256_hash_of_credit_pack_purchase_request_fields)
            break
        except Exception as e:
            logger.error(f"Error checking status of credit purchase request with Supernode {i+1}: {e}")
            if i == len(closest_supernodes) - 1:
                logger.error("Failed to check status of credit purchase request with all Supernodes")
                return None
    if credit_pack_purchase_request_status.status != "completed":
        logger.error(f"Credit pack purchase request failed: {credit_pack_purchase_request_status.status}")
        # Retry the storage with the closest agreeing supernode
        closest_agreeing_supernode_pastelid = await get_closest_supernode_pastelid_from_list(MY_LOCAL_PASTELID, list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms)
        credit_pack_storage_retry_request = CreditPackStorageRetryRequest(
            sha3_256_hash_of_credit_pack_purchase_request_response_fields=signed_credit_pack_ticket.sha3_256_hash_of_credit_pack_purchase_request_response_fields,
            credit_pack_purchase_request_fields_json_b64=signed_credit_pack_ticket.credit_pack_purchase_request_fields_json_b64,
            requesting_end_user_pastelid=MY_LOCAL_PASTELID,
            closest_agreeing_supernode_to_retry_storage_pastelid=closest_agreeing_supernode_pastelid,
            credit_pack_storage_retry_request_timestamp_utc_iso_string=datetime.now(timezone.utc).isoformat(),
            credit_pack_storage_retry_request_pastel_block_height=await get_current_pastel_block_height_func(),
            credit_pack_storage_retry_request_message_version_string="1.0",
            sha3_256_hash_of_credit_pack_storage_retry_request_fields="",
            requesting_end_user_pastelid_signature_on_credit_pack_storage_retry_request_hash=""
        )
        credit_pack_storage_retry_request.sha3_256_hash_of_credit_pack_storage_retry_request_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(credit_pack_storage_retry_request)
        credit_pack_storage_retry_request.requesting_end_user_pastelid_signature_on_credit_pack_storage_retry_request_hash = await sign_message_with_pastelid_func(MY_LOCAL_PASTELID, credit_pack_storage_retry_request.sha3_256_hash_of_credit_pack_storage_retry_request_fields, MY_PASTELID_PASSPHRASE)
        closest_agreeing_supernode_url = await get_supernode_url_from_pastelid_func(closest_agreeing_supernode_pastelid, supernode_list_df)
        credit_pack_storage_retry_request_response = await inference_client.credit_pack_storage_retry_request(closest_agreeing_supernode_url, credit_pack_storage_retry_request)
        # Send the credit pack purchase completion announcement to the responding supernode:
        for supernode_pastelid in list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms:
            try:
                is_valid_pastelid = check_if_pastelid_is_valid_func(supernode_pastelid)
                if is_valid_pastelid:
                    supernode_url = await get_supernode_url_from_pastelid_func(supernode_pastelid, supernode_list_df)
                    await inference_client.credit_pack_purchase_completion_announcement(supernode_url, credit_pack_storage_retry_request_response)        
            except Exception as e:
                logger.error(f"Error sending credit_pack_purchase_completion_announcement to Supernode URL: {supernode_url}: {e}")
        return credit_pack_storage_retry_request_response
    else:
        return credit_pack_purchase_request_confirmation_response

async def get_credit_pack_ticket_info_end_to_end(credit_pack_ticket_pastel_txid: str):
    # Create messaging client to use:
    inference_client = PastelInferenceClient(MY_LOCAL_PASTELID, MY_PASTELID_PASSPHRASE)       
    # Get the closest Supernode URL
    supernode_list_df, supernode_list_json = await check_supernode_list_func()
    supernode_url, _ = await get_closest_supernode_to_pastelid_url(MY_LOCAL_PASTELID, supernode_list_df)
    credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation = await inference_client.get_credit_pack_ticket_from_txid(supernode_url, credit_pack_ticket_pastel_txid)
    return credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation

async def get_valid_credit_pack_tickets_for_pastelid_end_to_end(pastelid: str) -> List[Dict[str, Any]]:
    # Create messaging client to use:
    inference_client = PastelInferenceClient(MY_LOCAL_PASTELID, MY_PASTELID_PASSPHRASE)
    # Get the closest Supernode URL
    supernode_list_df, supernode_list_json = await check_supernode_list_func()
    supernode_url, _ = await get_closest_supernode_to_pastelid_url(MY_LOCAL_PASTELID, supernode_list_df)
    valid_tickets = await inference_client.get_valid_credit_pack_tickets_for_pastelid(supernode_url, pastelid)
    return valid_tickets

async def handle_inference_request_end_to_end(
    credit_pack_ticket_pastel_txid: str,
    input_prompt_to_llm: str,
    requested_model_canonical_string: str,
    model_inference_type_string: str,
    model_parameters: dict,
    maximum_inference_cost_in_credits: float,
    burn_address: str
):
    # Create inference client to use:
    inference_client = PastelInferenceClient(MY_LOCAL_PASTELID, MY_PASTELID_PASSPHRASE)       
    # Get the closest Supernode URL
    model_parameters_json = json.dumps(model_parameters)
    supernode_support_dict, closest_supporting_supernode_pastelid, closest_supporting_supernode_url = await inference_client.get_closest_supernode_url_that_supports_desired_model(requested_model_canonical_string, model_inference_type_string, model_parameters_json) 
    supernode_url = closest_supporting_supernode_url
    supernode_pastelid = closest_supporting_supernode_pastelid
    try:
        assert(supernode_url is not None)
    except AssertionError:
        logger.error(f"Error! No supporting Supernode found for the desired model: {requested_model_canonical_string} with inference type: {model_inference_type_string}")
    input_prompt_to_llm__base64_encoded = base64.b64encode(input_prompt_to_llm.encode()).decode('utf-8')
    # Prepare the inference API usage request
    inference_request_data = InferenceAPIUsageRequest(
        inference_request_id=str(uuid.uuid4()),
        requesting_pastelid=MY_LOCAL_PASTELID,
        credit_pack_ticket_pastel_txid=credit_pack_ticket_pastel_txid,
        requested_model_canonical_string=requested_model_canonical_string,
        model_inference_type_string=model_inference_type_string,
        model_parameters_json_b64=base64.b64encode(model_parameters_json.encode('utf-8')).decode('utf-8'),
        model_input_data_json_b64=input_prompt_to_llm__base64_encoded,
        inference_request_utc_iso_string=datetime.now(timezone.utc).isoformat(),
        inference_request_pastel_block_height=await get_current_pastel_block_height_func(),
        status="initiating",
        inference_request_message_version_string="1.0",
        sha3_256_hash_of_inference_request_fields="",
        requesting_pastelid_signature_on_request_hash=""
    )
    sha3_256_hash_of_inference_request_fields = await compute_sha3_256_hash_of_sqlmodel_response_fields(inference_request_data)
    inference_request_data.sha3_256_hash_of_inference_request_fields = sha3_256_hash_of_inference_request_fields
    requesting_pastelid_signature_on_request_hash = await sign_message_with_pastelid_func(MY_LOCAL_PASTELID, sha3_256_hash_of_inference_request_fields, MY_PASTELID_PASSPHRASE)
    inference_request_data.requesting_pastelid_signature_on_request_hash = requesting_pastelid_signature_on_request_hash    
    # Send the inference API usage request
    usage_request_response = await inference_client.make_inference_api_usage_request(supernode_url, inference_request_data)
    logger.info(f"Received inference API usage request response from SN:\n {usage_request_response}")
    # Check the validity of the response
    validation_errors = await validate_credit_pack_ticket_message_data_func(usage_request_response)
    if validation_errors:
        raise ValueError(f"Invalid inference request response from Supernode URL {supernode_url}: {', '.join(validation_errors)}")    
    # Extract the relevant information from the response
    usage_request_response_dict = usage_request_response.model_dump()
    usage_request_response_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in usage_request_response_dict.items()}
    inference_request_id = usage_request_response_dict["inference_request_id"]
    inference_response_id = usage_request_response_dict["inference_response_id"]
    proposed_cost_in_credits = float(usage_request_response_dict["proposed_cost_of_request_in_inference_credits"])
    credit_usage_tracking_psl_address = usage_request_response_dict["credit_usage_tracking_psl_address"]
    credit_usage_tracking_amount_in_psl = float(usage_request_response_dict["request_confirmation_message_amount_in_patoshis"])/(10**5) # Divide by number of Patoshis per PSL
    # Check if tracking address contains enough PSL to send tracking amount:
    tracking_address_balance = await check_psl_address_balance_alternative_func(credit_usage_tracking_psl_address)
    if tracking_address_balance < credit_usage_tracking_amount_in_psl:
        logger.error(f"Insufficient balance in tracking address: {credit_usage_tracking_psl_address}; amount needed: {credit_usage_tracking_amount_in_psl}; current balance: {tracking_address_balance}; shortfall: {credit_usage_tracking_amount_in_psl - tracking_address_balance}")
        return None, None, None
    if proposed_cost_in_credits <= maximum_inference_cost_in_credits: # Check if the quoted price is less than or equal to the maximum allowed cost
        # TODO: Check if the credit pack has sufficient credits based on the actual credit pack data
        # Send the required PSL coins to authorize the request:
        tracking_transaction_txid = await send_tracking_amount_from_control_address_to_burn_address_to_confirm_inference_request(inference_request_id, credit_usage_tracking_psl_address, credit_usage_tracking_amount_in_psl, burn_address)
        txid_looks_valid = bool(re.match("^[0-9a-fA-F]{64}$", tracking_transaction_txid))
        if txid_looks_valid: # Prepare the inference confirmation
            confirmation_data = InferenceConfirmation(
                inference_request_id=inference_request_id,
                requesting_pastelid=MY_LOCAL_PASTELID,
                confirmation_transaction={"txid": tracking_transaction_txid}
            )
            confirmation_result = await inference_client.send_inference_confirmation(supernode_url, confirmation_data) # Send the inference confirmation
            logger.info(f"Sent inference confirmation: {confirmation_result}")
            max_tries_to_get_confirmation = 10
            initial_wait_time_in_seconds = 10
            wait_time_in_seconds = initial_wait_time_in_seconds
            for cnt in range(max_tries_to_get_confirmation):
                wait_time_in_seconds = wait_time_in_seconds*(1.2**cnt)
                logger.info(f"Waiting for the inference results for {round(wait_time_in_seconds, 1)} seconds... (Attempt {cnt+1}/{max_tries_to_get_confirmation}); Checking with Supernode URL: {supernode_url}")
                await asyncio.sleep(wait_time_in_seconds)
                assert(len(inference_request_id)>0)
                assert(len(inference_response_id)>0)
                results_available = await inference_client.check_status_of_inference_request_results(supernode_url, inference_response_id) # Get the inference output results
                if results_available:
                    output_results = await inference_client.retrieve_inference_output_results(supernode_url, inference_request_id, inference_response_id)
                    output_results_dict = output_results.model_dump()
                    output_results_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in output_results_dict.items()}
                    output_results_size = len(output_results.inference_result_json_base64)
                    max_response_size_to_log = 20000
                    if output_results_size < max_response_size_to_log:
                        logger.info(f"Retrieved inference output results: {output_results}")
                    # Create the inference_result_dict with all relevant information
                    inference_request_data_dict = {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in inference_request_data.model_dump().items()}
                    inference_result_dict = {
                        "supernode_url": supernode_url,
                        "request_data": inference_request_data_dict,
                        "usage_request_response": usage_request_response_dict,
                        "input_prompt_to_llm": input_prompt_to_llm,
                        "output_results": output_results_dict,
                    }
                    if model_inference_type_string == "text_to_image":
                        inference_result_dict["generated_image_base64"] = output_results.inference_result_json_base64
                        inference_result_dict["generated_image_decoded"] = base64.b64decode(output_results.inference_result_json_base64)
                    else:
                        inference_result_decoded = base64.b64decode(output_results.inference_result_json_base64).decode()
                        logger.info(f"Decoded response:\n {inference_result_decoded}")
                        inference_result_dict["inference_result_decoded"] = inference_result_decoded
                    use_audit_feature = 1
                    if use_audit_feature:
                        logger.info("Waiting 3 seconds for audit results to be available...")
                        await asyncio.sleep(3) # Wait for the audit results to be available
                        audit_results = await inference_client.audit_inference_request_response_id(inference_response_id, supernode_pastelid)
                        validation_results = validate_inference_data(inference_result_dict, audit_results)
                        logger.info(f"Validation results: {validation_results}")      
                    else:
                        audit_results = ""
                        validation_results = ""
                    return inference_result_dict, audit_results, validation_results
                else:
                    logger.info("Inference results not available yet; retrying...")
        else:
            logger.error(f"Invalid tracking transaction TXID: {tracking_transaction_txid}")
    else:
        logger.info(f"Quoted price of {proposed_cost_in_credits:,.1f} credits exceeds the maximum allowed cost of {maximum_inference_cost_in_credits:,.1f} credits. Inference request not confirmed.")
    return None, None, None
    
async def main():
    global rpc_connection
    rpc_host, rpc_port, rpc_user, rpc_password, other_flags = get_local_rpc_settings_func()
    rpc_connection = AsyncAuthServiceProxy(f"http://{rpc_user}:{rpc_password}@{rpc_host}:{rpc_port}")
    if rpc_port == '9932':
        burn_address = 'PtpasteLBurnAddressXXXXXXXXXXbJ5ndd'
    elif rpc_port == '19932':
        burn_address = 'tPpasteLBurnAddressXXXXXXXXXXX3wy7u'
    elif rpc_port == '29932':
        burn_address = '44oUgmZSL997veFEQDq569wv5tsT6KXf9QY7' # https://blockchain-devel.slack.com/archives/C03Q2MCQG9K/p1705896449986459
        
    use_test_messaging_functionality = 0
    use_test_credit_pack_ticket_functionality = 0
    use_test_credit_pack_ticket_usage = 1
    use_test_inference_request_functionality = 0
    use_test_llm_text_completion = 1
    use_test_image_generation = 0

    if use_test_messaging_functionality:
        # Sample message data:
        message_body = "Hello, this is a brand 🍉 NEW test message from a regular user!"
        to_pastelid = "jXXiVgtFzLto4eYziePHjjb1hj3c6eXdABej5ndnQ62B8ouv1GYveJaD5QUMfainQM3b4MTieQuzFEmJexw8Cr"        
        message_dict = await send_message_and_check_for_new_incoming_messages(to_pastelid, message_body)

    #________________________________________________________

    if use_test_credit_pack_ticket_functionality:
        # Test credit pack ticket functionality
        desired_number_of_credits = 1500
        amount_of_psl_for_tracking_transactions = 10.0
        credit_price_cushion_percentage = 0.15
        maximum_total_amount_of_psl_to_fund_in_new_tracking_address = 130000.0
        inference_client = PastelInferenceClient(MY_LOCAL_PASTELID, MY_PASTELID_PASSPHRASE)       
        estimated_total_cost_in_psl_for_credit_pack = await inference_client.internal_estimate_of_credit_pack_ticket_cost_in_psl(desired_number_of_credits, credit_price_cushion_percentage)
        if estimated_total_cost_in_psl_for_credit_pack > maximum_total_amount_of_psl_to_fund_in_new_tracking_address:
            logger.error(f"Estimated total cost of credit pack exceeds the maximum allowed amount of {maximum_total_amount_of_psl_to_fund_in_new_tracking_address:,.1f} PSL")
            raise ValueError(f"Estimated total cost of credit pack exceeds the maximum allowed amount of {maximum_total_amount_of_psl_to_fund_in_new_tracking_address:,.1f} PSL")
        amount_to_fund_credit_tracking_address = round(amount_of_psl_for_tracking_transactions + estimated_total_cost_in_psl_for_credit_pack, 2)
        credit_usage_tracking_psl_address, _ = await create_and_fund_new_psl_credit_tracking_address(amount_to_fund_credit_tracking_address)
        credit_pack_purchase_request_confirmation_response = await handle_credit_pack_ticket_end_to_end(
            desired_number_of_credits,
            credit_usage_tracking_psl_address,
            burn_address,
        )
        if credit_pack_purchase_request_confirmation_response:
            logger.info(f"Credit pack ticket stored on the blockchain with transaction ID: {credit_pack_purchase_request_confirmation_response.pastel_api_credit_pack_ticket_registration_txid}")
            logger.info(f"Credit pack details: {credit_pack_purchase_request_confirmation_response}")
        else:
            logger.error("Credit pack ticket storage failed!")

    if 'credit_pack_purchase_request_confirmation_response' in locals():
        credit_pack_ticket_pastel_txid = credit_pack_purchase_request_confirmation_response.pastel_api_credit_pack_ticket_registration_txid
    else:
        credit_pack_ticket_pastel_txid = "94b8c97e87079c2b6e34c1924dd3411809d848a9cf22d8d1e4f0493f61e7d6ae"
    # TODO: Add all credit pack tickets we create to local client database and make function that can automatically select the credit pack ticket with the largest remaining balance of credits and its corresponding psl tracking address.
    
    if use_test_credit_pack_ticket_usage:
        start_time = time.time()
        credit_pack_purchase_request_response, credit_pack_purchase_request_confirmation = await get_credit_pack_ticket_info_end_to_end(credit_pack_ticket_pastel_txid)
        credit_pack_purchase_request_fields_json = base64.b64decode(credit_pack_purchase_request_response.credit_pack_purchase_request_fields_json_b64).decode('utf-8')
        credit_pack_purchase_request_dict = json.loads(credit_pack_purchase_request_fields_json)
        credit_usage_tracking_psl_address = credit_pack_purchase_request_dict['credit_usage_tracking_psl_address']
        credit_usage_tracking_psl_address_current_balance = await check_psl_address_balance_alternative_func(credit_usage_tracking_psl_address)
        min_psl_balance_in_tracking_address = 10.0 
        if credit_usage_tracking_psl_address_current_balance < min_psl_balance_in_tracking_address:
            await send_to_address_func(credit_usage_tracking_psl_address, 10.0, "Sending more coin to the tracking address")
        initial_credit_pack_balance = credit_pack_purchase_request_dict['requested_initial_credits_in_credit_pack']
        valid_tickets = await get_valid_credit_pack_tickets_for_pastelid_end_to_end(MY_LOCAL_PASTELID)
        logger.info(f"Valid credit pack tickets for {MY_LOCAL_PASTELID}: {valid_tickets}")
        end_time = time.time()
        duration_in_seconds = (end_time - start_time)
                
    if use_test_inference_request_functionality:
        if use_test_llm_text_completion:
            start_time = time.time()
            # input_prompt_text_to_llm = "Explain to me with detailed examples what a Galois group is and how it helps understand the roots of a polynomial equation: "
            # input_prompt_text_to_llm = "What made the Battle of Salamus so important? What clever ideas were used in the battle? What mistakes were made?"
            input_prompt_text_to_llm = "Write a Shakespearean sonnet about a cryptocurrency called pastel network that now allows users to do AI inference tasks in a totally decentralized way. Make sure it's exactly 12 lines long and rhymes and follows the rhyme scheme ABAB CDCD EFEF GG. It should sound like a real sonnet by the Bard."
            # requested_model_canonical_string = "mistralapi-mistral-large-latest" # "groq-mixtral-8x7b-32768" # "claude3-opus" "claude3-sonnet" "mistral-7b-instruct-v0.2" # "claude3-haiku" # "phi-2" , "mistral-7b-instruct-v0.2", "groq-mixtral-8x7b-32768", "groq-llama2-70b-4096", "groq-gemma-7b-it", "mistralapi-mistral-small-latest", "mistralapi-mistral-large-latest"
            requested_model_canonical_string = "claude3-opus" # "groq-mixtral-8x7b-32768" # "groq-mixtral-8x7b-32768" # "claude3-opus" "claude3-sonnet" "mistral-7b-instruct-v0.2" # "claude3-haiku" # "phi-2" , "mistral-7b-instruct-v0.2", "groq-mixtral-8x7b-32768", "groq-llama2-70b-4096", "groq-gemma-7b-it", "mistralapi-mistral-small-latest", "mistralapi-mistral-large-latest"
            model_inference_type_string = "text_completion" # "embedding"        
            # model_parameters = {"number_of_tokens_to_generate": 200, "temperature": 0.7, "grammar_file_string": "", "number_of_completions_to_generate": 1}
            model_parameters = {"number_of_tokens_to_generate": 2000, "number_of_completions_to_generate": 1}
            max_credit_cost_to_approve_inference_request = 200.0
            inference_dict, audit_results, validation_results = await handle_inference_request_end_to_end(
                credit_pack_ticket_pastel_txid,
                input_prompt_text_to_llm,
                requested_model_canonical_string,
                model_inference_type_string,
                model_parameters,
                max_credit_cost_to_approve_inference_request,
                burn_address
            )
            end_time = time.time()
            duration_in_minutes = (end_time - start_time)/60

        if use_test_image_generation:
            # Test image generation
            start_time = time.time()
            # input_prompt_text_to_llm = "A stunning house with a beautiful garden and a pool, in a photorealistic style."
            input_prompt_text_to_llm = "A picture of a clown holding a sign that says PASTEL"
            requested_model_canonical_string = "stability-core"
            model_inference_type_string = "text_to_image"
            style_strings_list = ["3d-model", "analog-film", "anime", "cinematic", "comic-book", "digital-art", "enhance", "fantasy-art", "isometric", "line-art", "low-poly", "modeling-compound", "neon-punk", "origami", "photographic", "pixel-art", "tile-texture"] 
            style_preset_string = style_strings_list[-3] # "photographic"
            output_format_list = ["png", "jpeg", "webp"]
            output_format_string = output_format_list[0] # "png"
            aspect_ratio_list = ["16:9", "1:1", "21:9", "2:3", "3:2", "4:5", "5:4", "9:16", "9:21"]
            aspect_ratio_string = aspect_ratio_list[0] # "16:9"
            random_seed = random.randint(0, 1000)
            if "core" in requested_model_canonical_string:
                model_parameters = {
                    "aspect_ratio": aspect_ratio_string,
                    "seed": random_seed,
                    "style_preset": style_preset_string,
                    "output_format": output_format_string,
                    "negative_prompt": "low quality, blurry, pixelated"
                }
            else:
                model_parameters = {
                    "height": 512,
                    "width": 512,
                    "steps": 50,
                    "seed": 0,
                    "num_samples": 1,
                    "negative_prompt": "low quality, blurry, pixelated",
                    "style_preset": style_preset_string
                }
            max_credit_cost_to_approve_inference_request = 200.0
            inference_dict, audit_results, validation_results = await handle_inference_request_end_to_end(
                credit_pack_ticket_pastel_txid,
                input_prompt_text_to_llm,
                requested_model_canonical_string,
                model_inference_type_string,
                model_parameters,
                max_credit_cost_to_approve_inference_request,
                burn_address
            )
            
            # Save the generated image to a file
            current_datetime_string = datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
            image_generation_prompt_without_whitespace_or_newlines_abbreviated_to_100_characters = re.sub(r'\s+', '_', input_prompt_text_to_llm)[:100]
            generated_image_filename = f"generated_image__prompt__{image_generation_prompt_without_whitespace_or_newlines_abbreviated_to_100_characters}__generated_on_{current_datetime_string}.{output_format_string}"
            generated_image_folder_name = 'generated_images'
            if not os.path.exists(generated_image_folder_name):
                os.makedirs(generated_image_folder_name)
            generated_image_file_path = os.path.join(generated_image_folder_name, generated_image_filename)    
            image_data = inference_dict['generated_image_decoded']
            with open(generated_image_file_path, "wb") as f:
                f.write(image_data)
            end_time = time.time()
            duration_in_minutes = (end_time - start_time)/60
            
if __name__ == "__main__":
    asyncio.run(main())

---
./blockchain_ticket_storage.py
---
import hashlib
import struct
import os
import io
import asyncio
import base64
import json
import random
import time
import traceback
from binascii import unhexlify, hexlify
import urllib.parse as urlparse
from decimal import Decimal
import plyvel
import shutil
import tempfile
import zstandard as zstd
from httpx import AsyncClient, Limits, Timeout
from logger_config import logger
import database_code as db_code

base_transaction_amount = Decimal(0.1)
max_concurrent_requests = 1000
FEEPERKB = Decimal(0.00001)
COIN = 100000 # patoshis in 1 PSL
OP_CHECKSIG = b'\xac'
OP_CHECKMULTISIG = b'\xae'
OP_PUSHDATA1 = b'\x4c'
OP_DUP = b'\x76'
OP_HASH160 = b'\xa9'
OP_EQUALVERIFY = b'\x88'
    
def get_network_info(rpc_port):
    if rpc_port == '9932':
        network = 'mainnet'
        burn_address = 'PtpasteLBurnAddressXXXXXXXXXXbJ5ndd'
    elif rpc_port == '19932':
        network = 'testnet'
        burn_address = 'tPpasteLBurnAddressXXXXXXXXXXX3wy7u'
    elif rpc_port == '29932':
        network = 'devnet'
        burn_address = '44oUgmZSL997veFEQDq569wv5tsT6KXf9QY7'
    else:
        raise ValueError(f"Unknown RPC port: {rpc_port}")
    return network, burn_address

def required_collateral(network):
    if network == 'mainnet':
        return 5000000  # 5 million PSL for mainnet
    else:
        return 1000000  # 1 million PSL for testnet/devnet

class JSONRPCException(Exception):
    def __init__(self, rpc_error):
        parent_args = []
        try:
            parent_args.append(rpc_error['message'])
        except Exception as e:
            logger.info(f"Error occurred in JSONRPCException: {e}")
            pass
        Exception.__init__(self, *parent_args)
        self.error = rpc_error
        self.code = rpc_error['code'] if 'code' in rpc_error else None
        self.message = rpc_error['message'] if 'message' in rpc_error else None

    def __str__(self):
        return '%d: %s' % (self.code, self.message)

    def __repr__(self):
        return '<%s \'%s\'>' % (self.__class__.__name__, self)

def EncodeDecimal(o):
    if isinstance(o, Decimal):
        return float(round(o, 8))
    raise TypeError(repr(o) + " is not JSON serializable")

class AsyncAuthServiceProxy:
    _semaphore = asyncio.BoundedSemaphore(max_concurrent_requests)
    def __init__(self, service_url, service_name=None, reconnect_timeout=25, max_retries=3, request_timeout=120, fallback_url=None):
        self.service_url = service_url
        self.service_name = service_name
        self.url = urlparse.urlparse(service_url)        
        self.client = AsyncClient(timeout=Timeout(request_timeout), limits=Limits(max_connections=max_concurrent_requests, max_keepalive_connections=100))
        self.id_count = 0
        user = self.url.username
        password = self.url.password
        authpair = f"{user}:{password}".encode('utf-8')
        self.auth_header = b'Basic ' + base64.b64encode(authpair)
        self.reconnect_timeout = reconnect_timeout
        self.max_retries = max_retries
        self.request_timeout = request_timeout
        self.circuit_breaker_open = False
        self.circuit_breaker_timeout = 60
        self.circuit_breaker_failure_threshold = 5
        self.circuit_breaker_failure_count = 0
        self.fallback_url = fallback_url
        self.max_backoff_time = 120
        self.health_check_endpoint = "/health"
        self.health_check_interval = 60
        self.use_health_check = 0

    def __getattr__(self, name):
        if name.startswith('__') and name.endswith('__'):
            raise AttributeError
        if self.service_name is not None:
            name = f"{self.service_name}.{name}"
        return AsyncAuthServiceProxy(self.service_url, name)

    async def __call__(self, *args):
        async with self._semaphore:  # Acquire a semaphore
            if self.circuit_breaker_open:
                if self.circuit_breaker_timeout > 0:
                    logger.warning("Circuit breaker is open. Waiting for timeout...")
                    await asyncio.sleep(self.circuit_breaker_timeout)
                    self.circuit_breaker_timeout = 0
                else:
                    logger.info("Testing circuit breaker with a request...")
                    self.circuit_breaker_failure_count = 0
            self.id_count += 1
            postdata = json.dumps({
                'version': '1.1',
                'method': self.service_name,
                'params': args,
                'id': self.id_count
            }, default=EncodeDecimal)
            headers = {
                'Host': self.url.hostname,
                'User-Agent': "AuthServiceProxy/0.1",
                'Authorization': self.auth_header,
                'Content-type': 'application/json'
            }
            start_time = time.time()
            for i in range(self.max_retries):
                try:
                    if i > 0:
                        logger.warning(f"Retry attempt #{i+1}")
                        sleep_time = min(self.reconnect_timeout * (2 ** i) + random.uniform(0, self.reconnect_timeout), self.max_backoff_time)
                        logger.info(f"Waiting for {sleep_time:.2f} seconds before retrying.")
                        await asyncio.sleep(sleep_time)
                    if self.use_health_check:
                        await self.health_check()
                    response = await self.client.post(
                        self.service_url, headers=headers, data=postdata)
                    self.circuit_breaker_failure_count = 0
                    self.circuit_breaker_open = False
                    elapsed_time = time.time() - start_time
                    self.adapt_circuit_breaker_timeout(elapsed_time)
                    break
                except Exception as e:
                    logger.error(f"Error occurred in __call__: {e}")
                    logger.exception("Full stack trace:")
                    self.circuit_breaker_failure_count += 1
                    if self.circuit_breaker_failure_count >= self.circuit_breaker_failure_threshold:
                        logger.warning("Circuit breaker threshold reached. Opening circuit.")
                        self.circuit_breaker_open = True
                        self.circuit_breaker_timeout = 60
                        if self.fallback_url:
                            logger.info("Switching to fallback URL.")
                            self.service_url = self.fallback_url
                            self.url = urlparse.urlparse(self.service_url)
            else:
                logger.error("Max retries exceeded.")
                return
            response_json = response.json()
            if response_json['error'] is not None:
                raise JSONRPCException(response_json['error'])
            elif 'result' not in response_json:
                raise JSONRPCException({
                    'code': -343, 'message': 'missing JSON-RPC result'})
            else:
                return response_json['result']

    async def health_check(self):
        try:
            health_check_url = self.service_url + self.health_check_endpoint
            response = await self.client.get(health_check_url)
            if response.status_code != 200:
                raise Exception("Health check failed.")
        except Exception as e:
            logger.warning(f"Health check failed: {e}")
            raise

    def adapt_circuit_breaker_timeout(self, elapsed_time):
        if elapsed_time > self.circuit_breaker_timeout:
            self.circuit_breaker_timeout = min(self.circuit_breaker_timeout * 1.5, 300)
        elif elapsed_time < self.circuit_breaker_timeout / 2:
            self.circuit_breaker_timeout = max(self.circuit_breaker_timeout * 0.8, 60)

def get_sha3_256_hash(input_data):
    if isinstance(input_data, str):
        input_data = input_data.encode('utf-8')
    return hashlib.sha3_256(input_data).hexdigest()
    
def get_raw_sha3_256_hash(input_data):
    if isinstance(input_data, str):
        input_data = input_data.encode('utf-8')
    return hashlib.sha3_256(input_data).digest()
        
def compress_data(input_data):
    if isinstance(input_data, str):
        input_data = input_data.encode('utf-8')
    zstd_compression_level = 22
    zstandard_compressor = zstd.ZstdCompressor(level=zstd_compression_level, write_content_size=True, write_checksum=True)
    zstd_compressed_data = zstandard_compressor.compress(input_data)
    return zstd_compressed_data

def decompress_data(compressed_data):
    return zstd.decompress(compressed_data)

async def get_unspent_transactions():
    global rpc_connection
    unspent_transactions = await rpc_connection.listunspent()
    return unspent_transactions

async def select_txins(value, number_of_utxos_to_review=10):
    global rpc_connection
    unspent = await get_unspent_transactions()
    random.shuffle(unspent)
    valid_unspent = []
    reviewed_utxos = 0
    for tx in unspent:
        if not tx['spendable'] or tx['address'] == burn_address or tx['generated'] or tx['amount'] < value:
            continue  # Skip UTXOs that are not spendable or belong to the burn address or which are coinbase transactions
        valid_unspent.append(tx)
        reviewed_utxos += 1
        if reviewed_utxos >= number_of_utxos_to_review:
            break  # Stop reviewing UTXOs if the limit is reached
    # Sort the valid UTXOs by the amount to prioritize larger amounts
    valid_unspent.sort(key=lambda x: x['amount'], reverse=True)
    selected_txins = []
    total_amount = 0
    for tx in valid_unspent:
        # Check if the wallet has the private key for the UTXO's address
        address_info = await rpc_connection.validateaddress(tx['address'])
        if not address_info['ismine'] and not address_info['iswatchonly']:
            continue  # Skip this UTXO if the wallet doesn't have the private key
        # Check if the UTXO is still valid and unspent
        txout = await rpc_connection.gettxout(tx['txid'], tx['vout'])
        if txout is None:
            continue  # Skip UTXOs that are not found or already spent        
        selected_txins.append(tx)
        total_amount += tx['amount']
        if total_amount >= value:
            break
    if total_amount < value:
        raise Exception("Insufficient funds")
    else:
        return selected_txins, Decimal(total_amount)
    

def pushint(n):
    assert 0 < n <= 16
    return bytes([0x51 + n-1])
    
def checkmultisig_scriptpubkey_dump(fd):
    data = fd.read(65*3)
    if not data:
        return None
    r = pushint(1)
    n = 0
    while data:
        chunk = data[0:65]
        data = data[65:]
        if len(chunk) < 33:
            chunk += b'\x00'*(33-len(chunk))
        elif len(chunk) < 65:
            chunk += b'\x00'*(65-len(chunk))
        r += pushdata(chunk)
        n += 1
    r += pushint(n) + OP_CHECKMULTISIG
    return r
    
def addr2bytes(s):
    digits58 = '123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz'
    n = 0
    for c in s:
        n *= 58
        if c not in digits58:
            raise ValueError
        n += digits58.index(c)
    h = '%x' % n
    if len(h) % 2:
        h = '0' + h
    for c in s:
        if c == digits58[0]:
            h = '00' + h
        else:
            break
    decoded_address = unhexstr(h)
    prefix_length = 2
    return decoded_address[prefix_length:-4]

def unhexstr(str):
    return unhexlify(str.encode('utf8'))

def pushdata(data):
    length = len(data)
    if length < 0x4c:
        return bytes([length]) + data
    elif length <= 0xff:
        return b'\x4c' + bytes([length]) + data
    elif length <= 0xffff:
        return b'\x4d' + struct.pack('<H', length) + data
    else:
        return b'\x4e' + struct.pack('<I', length) + data

def varint(n):
    if n < 0xfd:
        return bytes([n])
    elif n <= 0xffff:
        return b'\xfd' + struct.pack('<H', n)
    elif n <= 0xffffffff:
        return b'\xfe' + struct.pack('<I', n)
    else:
        return b'\xff' + struct.pack('<Q', n)
            
class CTxIn:
    def __init__(self, prevout_hash, prevout_n, script_sig=b'', sequence=0xffffffff):
        self.prevout_hash = prevout_hash
        self.prevout_n = prevout_n
        self.script_sig = script_sig
        self.sequence = sequence
    
class CMutableTransaction:
    def __init__(self):
        version = 4  #  SAPLING_TX_VERSION
        overwinter_flag = 1 << 31  # Set the "overwintered" flag
        version |= overwinter_flag        
        self.version = version 
        self.version_group_id = 0x892f2085  # SAPLING_VERSION_GROUP_ID
        self.vin = []
        self.vout = []
        self.lock_time = 0
        self.expiry_height = 0
        self.value_balance = 0
        self.vShieldedSpend = []
        self.vShieldedOutput = []
        self.binding_sig = b'\x00' * 64  # Placeholder binding signature

def packtx(tx):
    # Serialize transaction fields
    tx_data = struct.pack('<I', tx.version)  # Transaction version (4 bytes)
    tx_data += struct.pack('<I', tx.version_group_id)  # Version group ID (4 bytes)
    # Serialize transaction inputs
    tx_data += varint(len(tx.vin))  # Number of inputs (varint)
    for txin in tx.vin:
        tx_data += txin.prevout_hash[::-1]  # Transaction ID (32 bytes) in little-endian
        tx_data += struct.pack('<I', txin.prevout_n)  # Output index (4 bytes)
        tx_data += varint(len(txin.script_sig))  # scriptSig length (varint)
        tx_data += txin.script_sig  # scriptSig (variable length)        
        tx_data += struct.pack('<I', txin.sequence)  # Sequence number (4 bytes)
    # Serialize transaction outputs
    tx_data += varint(len(tx.vout))  # Number of outputs (varint)
    for txout in tx.vout:
        output_value = int(txout[0]*COIN)
        if 0 <= output_value <= 0xffffffffffffffff:
            tx_data += struct.pack('<Q', output_value)  # Transaction amount in patoshis (8 bytes)
        else:
            logger.error(f"Invalid output value: {output_value}. Skipping this output.")        
        tx_data += varint(len(txout[1]))  # scriptPubKey length (varint)
        tx_data += txout[1]  # scriptPubKey (variable length)
    tx_data += struct.pack('<I', tx.lock_time)  # Locktime (4 bytes)
    tx_data += struct.pack('<I', tx.expiry_height)  # Expiry height (4 bytes)
    if 0 <= tx.value_balance <= 0xffffffffffffffff:
        tx_data += struct.pack('<Q', tx.value_balance)  # Value balance (8 bytes)
    else:
        logger.error(f"Invalid value balance: {tx.value_balance}. Setting it to 0.")
        tx_data += struct.pack('<Q', 0)  # Set value balance to 0 if it's outside the valid range    
    # Serialize Sapling-specific fields
    tx_data += varint(len(tx.vShieldedSpend))  # Number of shielded spends (varint)
    tx_data += varint(len(tx.vShieldedOutput))  # Number of shielded outputs (varint)
    if tx.vShieldedOutput:
        tx_data += varint(len(tx.vShieldedOutput))  # Number of shielded outputs (varint)
        # Serialize shielded outputs
    else:
        tx_data += varint(0)  # No shielded outputs    
    if tx.vShieldedSpend or tx.vShieldedOutput:
        consensus_branch_id = 0x5efaaeef  # Vermeer consensus branch ID
        tx_data += struct.pack('<I', consensus_branch_id)  # Consensus branch ID (4 bytes)
        tx_data += tx.binding_sig  # Binding signature (64 bytes)
    return tx_data
    
async def store_data_in_blockchain(input_data):
    global rpc_connection
    try:    
        compressed_data = compress_data(input_data)
        uncompressed_data_hash = get_raw_sha3_256_hash(input_data)
        compressed_data_hash = get_raw_sha3_256_hash(compressed_data)
        compressed_data_length = len(compressed_data)
        identifier = "CREDIT_PACK_STORAGE_TICKET"
        identifier_padded = identifier.encode('utf-8').ljust(32, b'\x00')  # Pad the identifier to 32 bytes
        header = identifier_padded + compressed_data_length.to_bytes(2, 'big') + uncompressed_data_hash + compressed_data_hash
        data_with_header = header + compressed_data
        combined_data_hex = hexlify(data_with_header)
        txins, change = await select_txins(0.00001)
        raw_transaction = CMutableTransaction()
        raw_transaction.vin = [CTxIn(unhexlify(txin['txid']), txin['vout']) for txin in txins]
        txouts = []
        fd = io.BytesIO(combined_data_hex)
        while True:
            script_pubkey = checkmultisig_scriptpubkey_dump(fd)
            if script_pubkey is None:
                break
            value = round(Decimal(100/COIN), 5)            
            txouts.append((value, script_pubkey))
            change -= value   
        out_value = round(base_transaction_amount, 5) 
        change -= out_value
        receiving_address = await rpc_connection.getnewaddress()
        txouts.append((float(out_value), OP_DUP + OP_HASH160 + pushdata(addr2bytes(receiving_address)) + OP_EQUALVERIFY + OP_CHECKSIG))
        change_address = await rpc_connection.getnewaddress() # change output
        txouts.append([round(float(change), 5), OP_DUP + OP_HASH160 + pushdata(addr2bytes(change_address)) + OP_EQUALVERIFY + OP_CHECKSIG])
        logger.info(f"Original Data length: {len(input_data):,} bytes; Compressed data length: {len(compressed_data):,} bytes; Number of multisig outputs: {len(txouts):,}; Total size of multisig outputs in bytes: {sum(len(txout[1]) for txout in txouts):,}") 
        raw_transaction.vout = txouts        
        unsigned_tx = packtx(raw_transaction)
        signed_tx_before_fees = await rpc_connection.signrawtransaction(hexlify(unsigned_tx).decode('utf-8'))
        fee = round(Decimal(len(signed_tx_before_fees)/1000) * FEEPERKB, 5)
        if fee > change:
            logger.error(f"Transaction fee exceeds change amount. Fee: {fee:.5f} PSL; Change: {change:.5f} PSL")
            change = 0
        else:
            change -= fee
        txouts[-1][0] = round(float(max(change, 0)), 5)
        final_tx = packtx(raw_transaction)
        signed_tx_after_fees = await rpc_connection.signrawtransaction(hexlify(final_tx).decode('utf-8'))
        assert(signed_tx_after_fees['complete'])
        hex_signed_transaction = signed_tx_after_fees['hex']
        final_signed_transaction_size_in_bytes = len(hex_signed_transaction)/2
        logger.info(f"Final signed transaction size: {final_signed_transaction_size_in_bytes:,} bytes; Overall expansion factor versus compressed data size: {final_signed_transaction_size_in_bytes/len(compressed_data):.2f}; Total transaction fee: {fee:.5f} PSL")
        logger.info(f"Sending data transaction to address: {receiving_address}")
        txid = await rpc_connection.sendrawtransaction(hex_signed_transaction)
        logger.info(f"TXID of Data Transaction: {txid}")
        return txid, final_signed_transaction_size_in_bytes
    except Exception as e:
        logger.error(f"Error occurred while storing data in the blockchain: {e}")
        traceback.print_exc()
        return None

async def retrieve_data_from_blockchain(txid):
    global rpc_connection
    try:
        # Get the raw transaction from the blockchain using the transaction ID
        raw_transaction = await rpc_connection.getrawtransaction(txid, 1)  # Verbose output includes decoded data
        outputs = raw_transaction['vout']  # Extract outputs from the transaction
        # Concatenate all scriptPubKey hex strings excluding the last two (change and receiving address outputs)
        encoded_hex_data = ''.join(output['scriptPubKey']['hex'][4:-4] for output in outputs[:-2])
        # Decode the hex data to bytes and clean up extraneous characters and padding
        reconstructed_combined_data = unhexlify(encoded_hex_data)
        reconstructed_combined_data_cleaned = reconstructed_combined_data.decode('utf-8').replace("A", "").rstrip("\x00")
        if len(reconstructed_combined_data_cleaned) % 2 != 0:
            reconstructed_combined_data_cleaned = reconstructed_combined_data_cleaned.replace("!","")
        if len(reconstructed_combined_data_cleaned) % 2 != 0:
            reconstructed_combined_data_cleaned = reconstructed_combined_data_cleaned[:-1]  # Remove any trailing characters
        data_buffer = unhexlify(reconstructed_combined_data_cleaned)
        # Extract the identifier
        identifier_padded = data_buffer[:32]
        identifier = identifier_padded.rstrip(b'\x00').decode('utf-8')
        data_buffer = data_buffer[32:]  # Remove the identifier from the buffer
        # Extract compressed data length
        compressed_data_length = int.from_bytes(data_buffer[:2], 'big')
        data_buffer = data_buffer[2:]  # Remove the compressed data length from the buffer
        # Extract hashes
        uncompressed_data_hash = data_buffer[:32]
        compressed_data_hash = data_buffer[32:64]
        data_buffer = data_buffer[64:]  # Remove the hashes from the buffer
        # Extract compressed data
        compressed_data = data_buffer[:compressed_data_length]
        data_buffer = data_buffer[compressed_data_length:]  # Remove the compressed data from the buffer
        # Validate the compressed data hash
        if get_raw_sha3_256_hash(compressed_data) != compressed_data_hash:
            logger.error("Compressed data hash verification failed")
            return None
        # Decompress the data and validate the uncompressed data hash and length
        decompressed_data = decompress_data(compressed_data)
        if get_raw_sha3_256_hash(decompressed_data) != uncompressed_data_hash:
            logger.error("Uncompressed data hash verification failed")
            return None
        # Log successful retrieval and return the decompressed data
        logger.info(f"Data retrieved successfully from the blockchain. Identifier: {identifier}, Length: {len(decompressed_data):,} bytes")
        return decompressed_data
    except Exception as e:
        logger.error(f"Error occurred while retrieving data from the blockchain: {e}")
        traceback.print_exc()
        return None
    
def get_local_rpc_settings_func(directory_with_pastel_conf=os.path.expanduser("~/.pastel/")):
    with open(os.path.join(directory_with_pastel_conf, "pastel.conf"), 'r') as f:
        lines = f.readlines()
    other_flags = {}
    rpchost = '127.0.0.1'
    rpcport = '19932'
    for line in lines:
        if line.startswith('rpcport'):
            value = line.split('=')[1]
            rpcport = value.strip()
        elif line.startswith('rpcuser'):
            value = line.split('=')[1]
            rpcuser = value.strip()
        elif line.startswith('rpcpassword'):
            value = line.split('=')[1]
            rpcpassword = value.strip()
        elif line.startswith('rpchost'):
            pass
        elif line == '\n':
            pass
        else:
            current_flag = line.strip().split('=')[0].strip()
            current_value = line.strip().split('=')[1].strip()
            other_flags[current_flag] = current_value
    return rpchost, rpcport, rpcuser, rpcpassword, other_flags

rpc_host, rpc_port, rpc_user, rpc_password, other_flags = get_local_rpc_settings_func()
network, burn_address = get_network_info(rpc_port)
masternode_collateral_amount = required_collateral(network)
rpc_connection = AsyncAuthServiceProxy(f"http://{rpc_user}:{rpc_password}@{rpc_host}:{rpc_port}")
use_direct_ticket_scanning = 0

#_________________________________________________________________________________________________________

#Code for getting ticket data directly from the blockchain files without using the slow RPC methods

if use_direct_ticket_scanning:
    def deserialize_block(raw_block_bytes):
        f = io.BytesIO(raw_block_bytes)
        def read_bytes(n):
            data = f.read(n)
            if len(data) < n:
                raise ValueError(f"Not enough data to read {n} bytes")
            return data
        def read_varint():
            b = f.read(1)
            if not b:
                raise ValueError("Not enough data to read varint")
            n = int.from_bytes(b, 'little')
            if n < 0xfd:
                return n
            elif n == 0xfd:
                return int.from_bytes(read_bytes(2), 'little')
            elif n == 0xfe:
                return int.from_bytes(read_bytes(4), 'little')
            else:
                return int.from_bytes(read_bytes(8), 'little')
        try:
            version = struct.unpack('<I', read_bytes(4))[0]
            prev_block_hash = read_bytes(32)[::-1].hex()
            merkle_root = read_bytes(32)[::-1].hex()
            final_sapling_root = read_bytes(32)[::-1].hex()
            timestamp = struct.unpack('<I', read_bytes(4))[0]
            bits = struct.unpack('<I', read_bytes(4))[0]
            pastel_id_length = read_varint()
            pastel_id = read_bytes(pastel_id_length).decode('utf-8')
            signature_length = read_varint()
            signature = read_bytes(signature_length)
            nonce = read_bytes(32).hex()
            solution_length = len(raw_block_bytes) - f.tell()
            solution = read_bytes(solution_length).hex()
            num_transactions = read_varint()
            transactions = []
            for _ in range(num_transactions):
                transaction = deserialize_transaction(f)
                transactions.append(transaction)
            return {
                'version': version,
                'prev_block_hash': prev_block_hash,
                'merkle_root': merkle_root,
                'final_sapling_root': final_sapling_root,
                'timestamp': timestamp,
                'bits': bits,
                'pastel_id': pastel_id,
                'signature': signature,
                'nonce': nonce,
                'solution': solution,
                'num_transactions': num_transactions,
                'transactions': transactions
            }
        except (struct.error, ValueError) as e:
            print(f"Error deserializing block: {e}")
            return None
        
    def deserialize_transaction(f):
        def read_bytes(n):
            data = f.read(n)
            if len(data) < n:
                raise ValueError(f"Not enough data to read {n} bytes")
            return data
        def read_varint():
            b = f.read(1)
            if not b:
                raise ValueError("Not enough data to read varint")
            n = int.from_bytes(b, 'little')
            if n < 0xfd:
                return n
            elif n == 0xfd:
                return int.from_bytes(read_bytes(2), 'little')
            elif n == 0xfe:
                return int.from_bytes(read_bytes(4), 'little')
            else:
                return int.from_bytes(read_bytes(8), 'little')
        try:
            version = struct.unpack('<I', read_bytes(4))[0]
            version_group_id = struct.unpack('<I', read_bytes(4))[0]
            num_inputs = read_varint()
            vin = []
            for _ in range(num_inputs):
                prevout_hash = read_bytes(32)[::-1]
                prevout_n = struct.unpack('<I', read_bytes(4))[0]
                script_sig_length = read_varint()
                script_sig = read_bytes(script_sig_length)
                sequence = struct.unpack('<I', read_bytes(4))[0]
                vin.append((prevout_hash, prevout_n, script_sig, sequence))
            num_outputs = read_varint()
            vout = []
            for _ in range(num_outputs):
                value = struct.unpack('<Q', read_bytes(8))[0]
                script_pubkey_length = read_varint()
                script_pubkey = read_bytes(script_pubkey_length)
                vout.append((value, script_pubkey))
            lock_time = struct.unpack('<I', read_bytes(4))[0]
            expiry_height = struct.unpack('<I', read_bytes(4))[0]
            value_balance = struct.unpack('<Q', read_bytes(8))[0]
            num_shielded_spends = read_varint()
            num_shielded_outputs = read_varint()
            if num_shielded_outputs > 0:
                num_shielded_outputs = read_varint()
            return {
                'version': version,
                'version_group_id': version_group_id,
                'vin': vin,
                'vout': vout,
                'lock_time': lock_time,
                'expiry_height': expiry_height,
                'value_balance': value_balance,
                'num_shielded_spends': num_shielded_spends,
                'num_shielded_outputs': num_shielded_outputs,
            }
        except (struct.error, ValueError) as e:
            print(f"Error deserializing transaction: {e}")
            return None
        
    def get_block_files(network):
        pastel_dir = os.path.expanduser("~/.pastel")
        if network == "mainnet":
            blocks_dir = os.path.join(pastel_dir, "blocks")
        else:
            blocks_dir = os.path.join(pastel_dir, network, "blocks")
        return [os.path.join(blocks_dir, f) for f in os.listdir(blocks_dir) if f.startswith("blk") and f.endswith(".dat")], blocks_dir

    async def process_block_files(block_files):
        try:
            for block_file in block_files:
                with open(block_file, "rb") as f:
                    magic = f.read(4)
                    while magic == b'\xf9\xbe\xb4\xd9':
                        block_size = struct.unpack("<I", f.read(4))[0]
                        raw_block_bytes = f.read(block_size)
                        block = deserialize_block(raw_block_bytes)
                        if block is not None:
                            for transaction in block['transactions']:
                                txid = transaction['txid']
                                await process_transaction(transaction, txid)
                        magic = f.read(4)
        except Exception as e:
            logger.error(f"Error occurred while processing block files: {e}")
            traceback.print_exc()

    class CDiskTxPos:
        def __init__(self, nFile=0, nPos=0, nTxSize=0):
            self.nFile = nFile
            self.nPos = nPos
            self.nTxSize = nTxSize

        def serialize(self):
            return struct.pack("<iii", self.nFile, self.nPos, self.nTxSize)

        @classmethod
        def deserialize(cls, raw_data):
            nFile, nPos, nTxSize = struct.unpack("<iii", raw_data)
            return cls(nFile, nPos, nTxSize)

        def __repr__(self):
            return f"CDiskTxPos(nFile={self.nFile}, nPos={self.nPos}, nTxSize={self.nTxSize})"

    def decode_compactsize(raw_hex):
        """Decodes the compactsize encoding used in the UTXO data"""
        if len(raw_hex) == 0:
            return 0, 0
        first_byte = raw_hex[0]
        if first_byte < 253:
            return first_byte, 1
        elif first_byte == 253:
            return struct.unpack("<H", raw_hex[1:3])[0], 3
        elif first_byte == 254:
            return struct.unpack("<I", raw_hex[1:5])[0], 5
        else:
            return struct.unpack("<Q", raw_hex[1:9])[0], 9

    def extract_tx_pos(raw_utxo_data):
        try:
            # Extract the transaction position from the UTXO data
            # Assuming the UTXO data has the following structure:
            # [value (8 bytes)][script length (varint)][script (variable length)]
            value_hex = raw_utxo_data[:8]
            script_length, varint_size = decode_compactsize(raw_utxo_data[8:])
            script_start = 8 + varint_size
            script_hex = raw_utxo_data[script_start:script_start+script_length]
            return value_hex, script_hex
        except Exception as e:
            logger.error(f"Error occurred while extracting UTXO data: {e}")
            traceback.print_exc()
            return None, None
        
    def get_block_file_path(block_file_index, blocks_dir):
        # Generate the block file path based on the block file index
        # Assuming the block files are named in the format "blk00000.dat", "blk00001.dat", etc.
        block_file_name = f"blk{block_file_index:05d}.dat"
        block_file_path = os.path.join(blocks_dir, block_file_name)
        return block_file_path

    def is_data_storage_script(script_hex):
        # Check if the script matches the pattern used for storing arbitrary data
        # Assuming the data storage script follows the format:
        # OP_RETURN <data> (where <data> contains the identifier "CREDIT_PACK_STORAGE_TICKET")
        if script_hex.startswith('6a'):  # Check if the script starts with OP_RETURN (0x6a)
            data = script_hex[2:]  # Extract the data part of the script
            if 'CREDIT_PACK_STORAGE_TICKET' in bytes.fromhex(data).decode('utf-8', errors='ignore'):
                return True
        return False
            
    def retrieve_raw_transaction_bytes(txid, raw_utxo_data):
        try:
            # Extract the transaction position from the UTXO data
            tx_pos = extract_tx_pos(raw_utxo_data)
            if tx_pos is None:
                logger.error(f"Failed to extract transaction position for UTXO {txid}")
                return None
            # Open the block file and seek to the transaction position
            block_file_path = get_block_file_path(tx_pos.nFile)
            with open(block_file_path, "rb") as block_file:
                block_file.seek(tx_pos.nPos)
                # Read the block header
                block_header_bytes = block_file.read(80)  # Assuming a fixed block header size of 80 bytes
                # Extract the transaction offset from the block header
                tx_offset = struct.unpack("<I", block_header_bytes[-4:])[0]
                # Seek to the transaction position within the block
                block_file.seek(tx_pos.nPos + tx_offset)
                # Read the transaction bytes
                raw_transaction_bytes = block_file.read(tx_pos.nTxSize)
            return raw_transaction_bytes
        except Exception as e:
            logger.error(f"Error occurred while retrieving raw transaction bytes for UTXO {txid}: {e}")
            traceback.print_exc()
            return None

    async def process_utxo(txid, raw_utxo_data):
        try:
            raw_transaction_bytes = await retrieve_raw_transaction_bytes(txid, raw_utxo_data)
            if raw_transaction_bytes is not None:
                transaction = deserialize_transaction(raw_transaction_bytes)
                if transaction is not None:
                    await attempt_to_reconstruct_data_from_raw_transaction_data(transaction)
        except Exception as e:
            logger.error(f"Error occurred while processing UTXO {txid}: {e}")
            traceback.print_exc()
            
    async def process_transaction(transaction, txid):
        try:
            for output in transaction['vout']:
                script_pubkey = output['scriptPubKey']
                if is_data_storage_script(script_pubkey):
                    await process_utxo(txid, script_pubkey)
        except Exception as e:
            logger.error(f"Error occurred while processing transaction {txid}: {e}")
            traceback.print_exc()

    async def process_leveldb_files(leveldb_dir):
        try:
            db = plyvel.DB(leveldb_dir)
            for txid_raw, raw_utxo_data in db.iterator():
                txid_hex = txid_raw.hex()[::-1]
                await process_utxo(txid_hex, raw_utxo_data)
        except Exception as e:
            logger.error(f"Error occurred while processing LevelDB files: {e}")
            traceback.print_exc()
        finally:
            db.close()
        
    def get_leveldb_files(network):
        pastel_dir = os.path.expanduser("~/.pastel")
        if network == "mainnet":
            chainstate_dir = os.path.join(pastel_dir, "chainstate")
        else:
            chainstate_dir = os.path.join(pastel_dir, network, "chainstate")
        # Create a temporary directory to store the copied LevelDB files
        temp_dir = tempfile.mkdtemp()
        # Copy the entire chainstate directory to the temporary directory
        dst_chainstate_dir = os.path.join(temp_dir, "chainstate")
        logger.info(f"Making a copy of the leveldb files so we can read them; about to copy {sum(os.path.getsize(os.path.join(chainstate_dir, f)) for f in os.listdir(chainstate_dir))/1024/1024} mb of data")
        shutil.copytree(chainstate_dir, dst_chainstate_dir)
        # Remove the LOCK file from the copied directory
        logger.info("Done copying leveldb files to temp directory; now removing copied LOCK file...")
        lock_file_path = os.path.join(dst_chainstate_dir, "LOCK")
        if os.path.exists(lock_file_path):
            os.remove(lock_file_path)
        return dst_chainstate_dir

    async def attempt_to_reconstruct_data_from_raw_transaction_data(raw_transaction_bytes):
        try:
            transaction = deserialize_transaction(raw_transaction_bytes)
            outputs = transaction['vout']  # Extract outputs from the transaction
            # Concatenate all scriptPubKey hex strings excluding the last two (change and receiving address outputs)
            encoded_hex_data = ''.join(output['scriptPubKey']['hex'][4:-4] for output in outputs[:-2])
            # Decode the hex data to bytes and clean up extraneous characters and padding
            reconstructed_combined_data = bytes.fromhex(encoded_hex_data)
            reconstructed_combined_data_cleaned = reconstructed_combined_data.decode('utf-8').replace("A", "").rstrip("\x00")
            data_buffer = bytes.fromhex(reconstructed_combined_data_cleaned)
            # Extract the identifier
            identifier_padded = data_buffer[:32]
            identifier = identifier_padded.rstrip(b'\x00').decode('utf-8')
            data_buffer = data_buffer[32:]  # Remove the identifier from the buffer
            # Extract compressed data length
            compressed_data_length = int.from_bytes(data_buffer[:2], 'big')
            data_buffer = data_buffer[2:]  # Remove the compressed data length from the buffer
            # Extract hashes
            uncompressed_data_hash = data_buffer[:32]
            compressed_data_hash = data_buffer[32:64]
            data_buffer = data_buffer[64:]  # Remove the hashes from the buffer
            # Extract compressed data
            compressed_data = data_buffer[:compressed_data_length]
            data_buffer = data_buffer[compressed_data_length:]  # Remove the compressed data from the buffer
            # Validate the compressed data hash
            if get_raw_sha3_256_hash(compressed_data) != compressed_data_hash:
                logger.error("Compressed data hash verification failed")
                return None
            # Decompress the data and validate the uncompressed data hash and length
            decompressed_data = decompress_data(compressed_data)
            if get_raw_sha3_256_hash(decompressed_data) != uncompressed_data_hash:
                logger.error("Uncompressed data hash verification failed")
                return None
            # Log successful retrieval and return the decompressed data
            logger.info(f"Data retrieved successfully from the blockchain. Identifier: {identifier}, Length: {len(decompressed_data):,} bytes")
            return decompressed_data
        except Exception as e:
            logger.error(f"Error occurred while retrieving data from the blockchain: {e}")
            traceback.print_exc()
            return None

    async def save_credit_pack_purchase_request_response(credit_pack_purchase_request_response: db_code.CreditPackPurchaseRequestResponse) -> None:
        try:
            async with db_code.Session() as db_session:
                db_session.add(credit_pack_purchase_request_response)
                await db_session.commit()
        except Exception as e:
            logger.error(f"Error saving credit pack purchase request response: {str(e)}")
            raise
                
    async def background_monitor(block_files, leveldb_dir):
        tasks = [
            asyncio.create_task(process_block_files(block_files)),
            asyncio.create_task(process_leveldb_files(leveldb_dir))
        ]
        await asyncio.gather(*tasks)

    def main():
        block_files, blocks_dir = get_block_files(network)
        leveldb_dir = get_leveldb_files(network)
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(background_monitor(block_files, leveldb_dir))
        finally:
            if os.path.exists(leveldb_dir):
                shutil.rmtree(leveldb_dir)

    if __name__ == "__main__":
        main()

---
./requirements.txt
---
aiosqlite
anthropic
asyncio
cachetools 
cryptography
diskcache
fastapi
fuzzywuzzy 
groq
httpx[http2]
libpastelid
magika
mistralai
mutagen
pandas
picologging
pillow
plotly
plyvel
psutil
python-decouple
python-Levenshtein
pytz
ruff
sqlalchemy
sqlmodel
sshtunnel
starlette
tiktoken
torch
torchaudio
torchvision
transformers
uvicorn[standard]
uvloop
zstandard

---
./logger_config.py
---
import picologging as logging
import os
import shutil
import queue
from picologging.handlers import RotatingFileHandler, QueueHandler, QueueListener

logger = logging.getLogger("pastel_supernode_inference_layer")

def setup_logger():
    if logger.handlers:
        return logger
    old_logs_dir = 'old_logs'
    if not os.path.exists(old_logs_dir):
        os.makedirs(old_logs_dir)
    logger.setLevel(logging.INFO)
    formatter = logging.Formatter(fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    log_file_path = 'pastel_supernode_inference_layer.log'
    log_queue = queue.Queue(-1)
    fh = RotatingFileHandler(log_file_path, maxBytes=10*1024*1024, backupCount=5)
    fh.setFormatter(formatter)
    def namer(default_log_name):
        return os.path.join(old_logs_dir, os.path.basename(default_log_name))
    def rotator(source, dest):
        shutil.move(source, dest)
    fh.namer = namer
    fh.rotator = rotator
    queue_handler = QueueHandler(log_queue)
    logger.addHandler(queue_handler)
    listener = QueueListener(log_queue, fh)
    listener.start()
    logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)
    return logger


---
./database_code.py
---
import warnings
import uuid
from logger_config import logger
from datetime import datetime, timezone
from typing import Optional
from contextlib import asynccontextmanager
from sqlmodel import Field, SQLModel, Relationship, Column, JSON
from sqlmodel.ext.asyncio.session import AsyncSession as SQLModelSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy import text as sql_text
from sqlalchemy.ext.asyncio import create_async_engine
from sqlalchemy.pool import NullPool
from decouple import Config as DecoupleConfig, RepositoryEnv
        
config = DecoupleConfig(RepositoryEnv('.env'))
DATABASE_URL = config.get("DATABASE_URL", cast=str, default="sqlite+aiosqlite:///super_node_inference_layer.sqlite")

# Ignore specific warnings related to shadowing of fields in SQLModel or Pydantic
warnings.filterwarnings("ignore", message="Field name .* shadows an attribute in parent .*")

#SQLModel Models (combined SQLalchemy ORM models with Pydantic response models)

#_________________________________________________________________________________________
# Messaging related models:

class Message(SQLModel, table=True):
    id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
    sending_sn_pastelid: str = Field(index=True)
    receiving_sn_pastelid: str = Field(index=True)
    sending_sn_txid_vout: str = Field(index=True)
    receiving_sn_txid_vout: str = Field(index=True)
    message_type: str = Field(index=True)
    message_body: str = Field(sa_column=Column(JSON))
    signature: str
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
    def __repr__(self):
        return f"<Message(id={self.id}, sending_sn_pastelid='{self.sending_sn_pastelid}', receiving_sn_pastelid='{self.receiving_sn_pastelid}', message_type='{self.message_type}', timestamp='{self.timestamp}')>"
    class Config:
        arbitrary_types_allowed = True  # Allow arbitrary types
        json_schema_extra = {
            "example": {
                "sending_sn_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "receiving_sn_pastelid": "jXa1s9mKDr4m6P8s7bKK1rYFgL7hkfGMLX1NozVSX4yTnfh9EjuP",
                "sending_sn_txid_vout": "0x1234...:0",
                "receiving_sn_txid_vout": "0x5678...:0",
                "message_type": "text",
                "message_body": "Hello, how are you?",
                "signature": "0xabcd...",
                "timestamp": "2023-06-01T12:00:00Z"
            }
        }

class UserMessage(SQLModel, table=True):
    id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
    from_pastelid: str = Field(index=True)
    to_pastelid: str = Field(index=True)
    message_body: str = Field(sa_column=Column(JSON))
    message_signature: str
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
    class Config:
        arbitrary_types_allowed = True  # Allow arbitrary types
        json_schema_extra = {
            "example": {
                "from_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "to_pastelid": "jXa1s9mKDr4m6P8s7bKK1rYFgL7hkfGMLX1NozVSX4yTnfh9EjuP",
                "message_body": "Hey, let's meet up!",
                "message_signature": "0xdef0...",
                "timestamp": "2023-06-01T12:30:00Z"
            }
        }
        
class SupernodeUserMessage(Message, table=True):
    user_message_id: Optional[int] = Field(default=None, foreign_key="usermessage.id", index=True)
    user_message: Optional[UserMessage] = Relationship(back_populates="supernode_user_messages")
    class Config:
        arbitrary_types_allowed = True  # Allow arbitrary types
        json_schema_extra = {
            "example": {
                "sending_sn_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "receiving_sn_pastelid": "jXa1s9mKDr4m6P8s7bKK1rYFgL7hkfGMLX1NozVSX4yTnfh9EjuP",
                "sending_sn_txid_vout": "0x1234...:0",
                "receiving_sn_txid_vout": "0x5678...:0",
                "message_type": "user_message",
                "message_body": '{"from_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk", "to_pastelid": "jXa1s9mKDr4m6P8s7bKK1rYFgL7hkfGMLX1NozVSX4yTnfh9EjuP", "message_body": "Hey, let\'s meet up!", "message_signature": "0xdef0..."}',
                "signature": "0xabcd...",
                "timestamp": "2023-06-01T12:30:00Z",
                "user_message_id": 1
            }
        }

class MessageMetadata(SQLModel, table=True):
    id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
    total_messages: int
    total_senders: int
    total_receivers: int
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
    class Config:
        json_schema_extra = {
            "example": {
                "total_messages": 1000,
                "total_senders": 100,
                "total_receivers": 200,
                "timestamp": "2023-06-01T12:00:00Z"
            }
        }

class MessageSenderMetadata(SQLModel, table=True):
    id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
    sending_sn_pastelid: str = Field(index=True)
    sending_sn_txid_vout: str = Field(index=True)
    sending_sn_pubkey: str = Field(index=True)
    total_messages_sent: int
    total_data_sent_bytes: float
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
    class Config:
        json_schema_extra = {
            "example": {
                "sending_sn_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "sending_sn_txid_vout": "0x1234...:0",
                "sending_sn_pubkey": "0xabcd...",
                "total_messages_sent": 500,
                "total_data_sent_bytes": 1000000,
                "timestamp": "2023-06-01T12:00:00Z"
            }
        }
        
class MessageReceiverMetadata(SQLModel, table=True):
    id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
    receiving_sn_pastelid: str = Field(index=True)
    receiving_sn_txid_vout: str = Field(index=True)
    total_messages_received: int
    total_data_received_bytes: float
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
    class Config:
        json_schema_extra = {
            "example": {
                "receiving_sn_pastelid": "jXa1s9mKDr4m6P8s7bKK1rYFgL7hkfGMLX1NozVSX4yTnfh9EjuP",
                "receiving_sn_txid_vout": "0x5678...:0",
                "total_messages_received": 300,
                "total_data_received_bytes": 500000,
                "timestamp": "2023-06-01T12:00:00Z"
            }
        }

class MessageSenderReceiverMetadata(SQLModel, table=True):
    id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
    sending_sn_pastelid: str = Field(index=True)
    receiving_sn_pastelid: str = Field(index=True)
    total_messages: int
    total_data_bytes: float
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
    class Config:
        json_schema_extra = {
            "example": {
                "sending_sn_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "receiving_sn_pastelid": "jXa1s9mKDr4m6P8s7bKK1rYFgL7hkfGMLX1NozVSX4yTnfh9EjuP",
                "total_messages": 200,
                "total_data_bytes": 400000,
                "timestamp": "2023-06-01T12:00:00Z"
            }
        }

class SendMessageResponse(SQLModel):
    status: str
    message: str    
    class Config:
        json_schema_extra = {
            "example": {
                "status": "success",
                "message": "Message sent successfully"
            }
        }
        
#_________________________________________________________________________________________
# Misc. data models:

class SupernodeData(SQLModel):
    supernode_status: str
    protocol_version: str
    supernode_psl_address: str
    lastseentime: datetime
    activeseconds: int
    lastpaidtime: datetime
    lastpaidblock: int
    ipaddress_port: str
    rank: int
    pubkey: str
    extAddress: Optional[str]
    extP2P: Optional[str]
    extKey: Optional[str]
    activedays: float
    class Config:
        json_schema_extra = {
            "example": {
                "supernode_status": "active",
                "protocol_version": "1.0",
                "supernode_psl_address": "tPj2wX5mjQErTju6nueVRkxGMCPuMkLn8CWdViJ38m9Wf6PBK5jV",
                "lastseentime": "2023-06-01T12:00:00Z",
                "activeseconds": 3600,
                "lastpaidtime": "2023-06-01T00:00:00Z",
                "lastpaidblock": 123456,
                "ipaddress_port": "127.0.0.1:9932",
                "rank": 1,
                "pubkey": "0xabcd...",
                "extAddress": "external_address",
                "extP2P": "external_p2p",
                "extKey": "external_key",
                "activedays": 30.5
            }
        }
        
class LocalMachineSupernodeInfo(SQLModel):
    local_machine_supernode_data: SupernodeData
    local_sn_rank: int
    local_sn_pastelid: str
    local_machine_ip_with_proper_port: str
    class Config:
        json_schema_extra = {
            "example": {
                "local_machine_supernode_data": {
                    "supernode_status": "active",
                    "protocol_version": "1.0",
                    "supernode_psl_address": "tPj2wX5mjQErTju6nueVRkxGMCPuMkLn8CWdViJ38m9Wf6PBK5jV",
                    "lastseentime": "2023-06-01T12:00:00Z",
                    "activeseconds": 3600,
                    "lastpaidtime": "2023-06-01T00:00:00Z",
                    "lastpaidblock": 123456,
                    "ipaddress_port": "127.0.0.1:9999",
                    "rank": 1,
                    "pubkey": "0xabcd...",
                    "extAddress": "external_address",
                    "extP2P": "external_p2p",
                    "extKey": "external_key",
                    "activedays": 30.5
                },
                "local_sn_rank": 1,
                "local_sn_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "local_machine_ip_with_proper_port": "127.0.0.1:9999"
            }
        }
        
class ReputationScoreUpdate(SQLModel):
    supernode_pastelid: str
    reputation_score: float
    class Config:
        json_schema_extra = {
            "example": {
                "supernode_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "reputation_score": 4.5
            }
        }
            
class MNIDTicketDetails(SQLModel, table=True):
    id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True, index=True)
    txid: str = Field(index=True, unique=True)
    pastel_id: str
    address: str
    pq_key: str
    outpoint: str
    block_height: int
    timestamp: datetime
    class Config:
        json_schema_extra = {
            "example": {
                "txid": "a55a83125475d78a76d9c9f406b111b35609a4f25855bacfe62fb9db8d1bb340",
                "pastel_id": "jXanJYsUZLLm54tgKvDMjrZrHMdrKB8X3itB1hvDoaD7fUp23D6LDPxbFcUrAMwRUdTrBPzS6oKpfpTSvBbot4",
                "address": "PtZt2Lqe8aKypGFhUckwFf22EPyZSpbPBep",
                "pq_key": "ExxdG2...KoxrruxMS4vxBWnWc",
                "outpoint": "b12c660af935bed3bea15a29135512675ad64e8a2e820901baf7cea38430ba0e-0",
                "block_height": 319474,
                "timestamp": "2023-06-01T12:00:00Z"
            }
        }
#_________________________________________________________________________________________
# Credit pack related models:

class CreditPackPurchaseRequest(SQLModel, table=True):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()), index=True, nullable=True)
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
    requesting_end_user_pastelid: str = Field(index=True)
    requested_initial_credits_in_credit_pack: int
    list_of_authorized_pastelids_allowed_to_use_credit_pack: str = Field(sa_column=Column(JSON))
    credit_usage_tracking_psl_address: str = Field(index=True)
    request_timestamp_utc_iso_string: str
    request_pastel_block_height: int
    credit_purchase_request_message_version_string: str
    requesting_end_user_pastelid_signature_on_request_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "id": "79df343b-4ad3-435c-800e-e59e616ff84d",
                "requesting_end_user_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "requested_initial_credits_in_credit_pack": 1000,
                "list_of_authorized_pastelids_allowed_to_use_credit_pack": ["jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk"],
                "credit_usage_tracking_psl_address": "tPj2wX5mjQErTju6nueVRkxGMCPuMkLn8CWdViJ38m9Wf6PBK5jV",
                "request_timestamp_utc_iso_string": "2023-06-01T12:00:00Z",
                "request_pastel_block_height": 123456,
                "credit_purchase_request_message_version_string": "1.0",
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x5678...",
                "requesting_end_user_pastelid_signature_on_request_hash": "0xabcd..."
            }
        }

class CreditPackPurchaseRequestRejection(SQLModel, table=True):
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
    credit_pack_purchase_request_fields_json_b64: str
    rejection_reason_string: str
    rejection_timestamp_utc_iso_string: str
    rejection_pastel_block_height: int
    credit_purchase_request_rejection_message_version_string: str
    responding_supernode_pastelid: str = Field(index=True)
    sha3_256_hash_of_credit_pack_purchase_request_rejection_fields: str = Field(unique=True, index=True)
    responding_supernode_signature_on_credit_pack_purchase_request_rejection_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "credit_pack_purchase_request_fields_json_b64": 'eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9',
                "rejection_reason_string": "Invalid credit usage tracking PSL address",
                "rejection_timestamp_utc_iso_string": "2023-06-01T12:10:00Z",
                "rejection_pastel_block_height": 123457,
                "credit_purchase_request_rejection_message_version_string": "1.0",
                "responding_supernode_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "sha3_256_hash_of_credit_pack_purchase_request_rejection_fields": "0xabcd...",
                "responding_supernode_signature_on_credit_pack_purchase_request_rejection_hash": "0xdef0..."
            }
        }
        
class CreditPackPurchaseRequestPreliminaryPriceQuote(SQLModel, table=True):
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
    credit_usage_tracking_psl_address: str = Field(index=True)
    credit_pack_purchase_request_fields_json_b64: str
    preliminary_quoted_price_per_credit_in_psl: float
    preliminary_total_cost_of_credit_pack_in_psl: float
    preliminary_price_quote_timestamp_utc_iso_string: str
    preliminary_price_quote_pastel_block_height: int
    preliminary_price_quote_message_version_string: str
    responding_supernode_pastelid: str = Field(index=True)
    sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields: str = Field(unique=True, index=True)
    responding_supernode_signature_on_credit_pack_purchase_request_preliminary_price_quote_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "credit_usage_tracking_psl_address": "tPj2wX5mjQErTju6nueVRkxGMCPuMkLn8CWdViJ38m9Wf6PBK5jV",
                "credit_pack_purchase_request_fields_json_b64": 'eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9',
                "preliminary_quoted_price_per_credit_in_psl": 0.1,
                "preliminary_total_cost_of_credit_pack_in_psl": 100,
                "preliminary_price_quote_timestamp_utc_iso_string": "2023-06-01T12:05:00Z",
                "preliminary_price_quote_pastel_block_height": 123456,
                "preliminary_price_quote_message_version_string": "1.0",
                "responding_supernode_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields": "0xabcd...",
                "responding_supernode_signature_on_credit_pack_purchase_request_preliminary_price_quote_hash": "0xdef0..."
            }
        }

class CreditPackPurchaseRequestPreliminaryPriceQuoteResponse(SQLModel, table=True):
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
    sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields: str = Field(index=True)
    credit_pack_purchase_request_fields_json_b64: str
    agree_with_preliminary_price_quote: bool
    credit_usage_tracking_psl_address: str = Field(index=True)
    preliminary_quoted_price_per_credit_in_psl: float
    preliminary_price_quote_response_timestamp_utc_iso_string: str
    preliminary_price_quote_response_pastel_block_height: int
    preliminary_price_quote_response_message_version_string: str
    requesting_end_user_pastelid: str = Field(index=True)
    sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_response_fields: str = Field(unique=True, index=True)
    requesting_end_user_pastelid_signature_on_preliminary_price_quote_response_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_fields": "0x5678...",
                "credit_pack_purchase_request_fields_json_b64": 'eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9',
                "agree_with_preliminary_price_quote": True,
                "credit_usage_tracking_psl_address": "tPj2wX5mjQErTju6nueVRkxGMCPuMkLn8CWdViJ38m9Wf6PBK5jV",
                "preliminary_quoted_price_per_credit_in_psl": 0.1,
                "preliminary_price_quote_response_timestamp_utc_iso_string": "2023-06-01T12:10:00Z",
                "preliminary_price_quote_response_pastel_block_height": 123457,
                "preliminary_price_quote_response_message_version_string": "1.0",
                "requesting_end_user_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "sha3_256_hash_of_credit_pack_purchase_request_preliminary_price_quote_response_fields": "0xdef0...",
                "requesting_end_user_pastelid_signature_on_preliminary_price_quote_response_hash": "0x1234..."
            }
        }
        
class CreditPackPurchasePriceAgreementRequest(SQLModel, table=True):
    id: Optional[str] = Field(default_factory=lambda: str(uuid.uuid4()), primary_key=True, index=True)
    sha3_256_hash_of_credit_pack_purchase_request_response_fields: str = Field(index=True)
    supernode_requesting_price_agreement_pastelid: str = Field(index=True)
    credit_pack_purchase_request_fields_json_b64: str
    credit_usage_tracking_psl_address: str = Field(index=True)
    proposed_psl_price_per_credit: float
    price_agreement_request_timestamp_utc_iso_string: str
    price_agreement_request_pastel_block_height: int
    price_agreement_request_message_version_string: str
    sha3_256_hash_of_price_agreement_request_fields: str = Field(index=True)
    supernode_requesting_price_agreement_pastelid_signature_on_request_hash: str

    class Config:
        json_schema_extra = {
            "example": {
                "id": "79df343b-4ad3-435c-800e-e59e616ff84d",
                "sha3_256_hash_of_credit_pack_purchase_request_response_fields": "0x1234...",
                "supernode_requesting_price_agreement_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "credit_pack_purchase_request_fields_json_b64": 'eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9',
                "credit_usage_tracking_psl_address": "tPj2wX5mjQErTju6nueVRkxGMCPuMkLn8CWdViJ38m9Wf6PBK5jV",
                "proposed_psl_price_per_credit": 0.1,
                "price_agreement_request_timestamp_utc_iso_string": "2023-06-01T12:20:00Z",
                "price_agreement_request_pastel_block_height": 123457,
                "price_agreement_request_message_version_string": "1.0",
                "sha3_256_hash_of_price_agreement_request_fields": "0xabcd...",
                "supernode_requesting_price_agreement_pastelid_signature_on_request_hash": "0xdef0..."
            }
        }
        
class CreditPackPurchasePriceAgreementRequestResponse(SQLModel, table=True):
    sha3_256_hash_of_price_agreement_request_fields: str = Field(primary_key=True, index=True)
    credit_pack_purchase_request_fields_json_b64: str
    agree_with_proposed_price: bool
    credit_usage_tracking_psl_address: str = Field(unique=True,index=True)
    proposed_psl_price_per_credit: float
    proposed_price_agreement_response_timestamp_utc_iso_string: str
    proposed_price_agreement_response_pastel_block_height: int
    proposed_price_agreement_response_message_version_string: str
    responding_supernode_signature_on_credit_pack_purchase_request_fields_json_b64: str
    responding_supernode_pastelid: str = Field(index=True)
    sha3_256_hash_of_price_agreement_request_response_fields: str = Field(unique=True, index=True)
    responding_supernode_signature_on_price_agreement_request_response_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "sha3_256_hash_of_price_agreement_request_fields": "0x1234...",
                "credit_pack_purchase_request_fields_json_b64": 'eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9',
                "agree_with_proposed_price": True,
                "credit_usage_tracking_psl_address": "tPj2wX5mjQErTju6nueVRkxGMCPuMkLn8CWdViJ38m9Wf6PBK5jV",
                "proposed_psl_price_per_credit": 0.1,
                "proposed_price_agreement_response_timestamp_utc_iso_string": "2023-06-01T12:25:00Z",
                "proposed_price_agreement_response_pastel_block_height": 123458,
                "proposed_price_agreement_response_message_version_string": "1.0",
                "responding_supernode_signature_on_credit_pack_purchase_request_fields_json_b64": "0x1234...",
                "responding_supernode_pastelid": "jXa1s9mKDr4m6P8s7bKK1rYFgL7hkfGMLX1NozVSX4yTnfh9EjuP",
                "sha3_256_hash_of_price_agreement_request_response_fields": "0xabcd...",
                "responding_supernode_signature_on_price_agreement_request_response_hash": "0xdef0...",
            }
        }
        
class CreditPackPurchaseRequestResponseTermination(SQLModel, table=True):
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
    credit_pack_purchase_request_fields_json_b64: str
    termination_reason_string: str
    termination_timestamp_utc_iso_string: str
    termination_pastel_block_height: int
    credit_purchase_request_termination_message_version_string: str
    responding_supernode_pastelid: str = Field(index=True)
    sha3_256_hash_of_credit_pack_purchase_request_termination_fields: str = Field(unique=True, index=True)
    responding_supernode_signature_on_credit_pack_purchase_request_termination_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "credit_pack_purchase_request_fields_json_b64": 'eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9',
                "termination_reason_string": "Insufficient agreeing supernodes",
                "termination_timestamp_utc_iso_string": "2023-06-01T12:30:00Z",
                "termination_pastel_block_height": 123459,
                "credit_purchase_request_termination_message_version_string": "1.0",
                "responding_supernode_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "sha3_256_hash_of_credit_pack_purchase_request_termination_fields": "0xabcd...",
                "responding_supernode_signature_on_credit_pack_purchase_request_termination_hash": "0xdef0..."
            }
        }        
        
class CreditPackPurchaseRequestResponse(SQLModel, table=True):
    id: Optional[str] = Field(default_factory=lambda: str(uuid.uuid4()), primary_key=True, index=True)
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(foreign_key="creditpackpurchaserequest.sha3_256_hash_of_credit_pack_purchase_request_fields", index=True)
    credit_pack_purchase_request_fields_json_b64: str
    psl_cost_per_credit: float
    proposed_total_cost_of_credit_pack_in_psl: float
    credit_usage_tracking_psl_address: str = Field(index=True)
    request_response_timestamp_utc_iso_string: str
    request_response_pastel_block_height: int
    best_block_merkle_root: str
    best_block_height: int
    credit_purchase_request_response_message_version_string: str
    responding_supernode_pastelid: str = Field(index=True)
    list_of_blacklisted_supernode_pastelids: str = Field(sa_column=Column(JSON))
    list_of_potentially_agreeing_supernodes: str = Field(sa_column=Column(JSON))
    list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms: str = Field(sa_column=Column(JSON))
    list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion: str = Field(sa_column=Column(JSON))
    selected_agreeing_supernodes_signatures_dict: str = Field(sa_column=Column(JSON))
    sha3_256_hash_of_credit_pack_purchase_request_response_fields: str = Field(unique=True, index=True)
    responding_supernode_signature_on_credit_pack_purchase_request_response_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "id": "79df343b-4ad3-435c-800e-e59e616ff84d",
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "credit_pack_purchase_request_fields_json_b64": 'eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9',
                "psl_cost_per_credit": 0.1,
                "proposed_total_cost_of_credit_pack_in_psl": 100,
                "credit_usage_tracking_psl_address": "tPj2wX5mjQErTju6nueVRkxGMCPuMkLn8CWdViJ38m9Wf6PBK5jV",
                "request_response_timestamp_utc_iso_string": "2023-06-01T12:15:00Z",
                "request_response_pastel_block_height": 123457,
                "best_block_merkle_root": "0x5678...",
                "best_block_height": 123456,
                "credit_purchase_request_response_message_version_string": "1.0",
                "responding_supernode_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "list_of_blacklisted_supernode_pastelids": ["jXa1s9mKDr4m6P8s7bKK1rYFgL7hkfGMLX1NozVSX4yTnfh9EjuP"],
                "list_of_potentially_agreeing_supernodes": ["jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk", "jXa1s9mKDr4m6P8s7bKK1rYFgL7hkfGMLX1NozVSX4yTnfh9EjuP"],
                "list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms": ["jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk", "jXa1s9mKDr4m6P8s7bKK1rYFgL7hkfGMLX1NozVSX4yTnfh9EjuP"],
                "list_of_supernode_pastelids_agreeing_to_credit_pack_purchase_terms_selected_for_signature_inclusion": ["jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk"],
                "selected_agreeing_supernodes_signatures_dict": "['jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk': {'price_agreement_request_response_hash_signature': '0x1234...', 'credit_pack_purchase_request_fields_json_b64_signature': '0x5678...'}]",
                "sha3_256_hash_of_credit_pack_purchase_request_response_fields": "0x9abc...",
                "responding_supernode_signature_on_credit_pack_purchase_request_response_hash": "0xdef0..."
            }
        }        
        
class CreditPackPurchaseRequestResponseTxidMapping(SQLModel, table=True):
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(foreign_key="creditpackpurchaserequestresponse.sha3_256_hash_of_credit_pack_purchase_request_fields", primary_key=True, index=True)
    pastel_api_credit_pack_ticket_registration_txid: str = Field(unique=True, index=True)        
    class Config:
        json_schema_extra = {
            "example": {
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "pastel_api_credit_pack_ticket_registration_txid": "0123456789abcdef..."
            }
        }
        
class CreditPackPurchaseRequestConfirmation(SQLModel, table=True):
    id: Optional[str] = Field(default_factory=lambda: str(uuid.uuid4()), primary_key=True, index=True)
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(index=True, foreign_key="creditpackpurchaserequest.sha3_256_hash_of_credit_pack_purchase_request_fields")
    sha3_256_hash_of_credit_pack_purchase_request_response_fields: str = Field(index=True, foreign_key="creditpackpurchaserequestresponse.sha3_256_hash_of_credit_pack_purchase_request_response_fields")
    credit_pack_purchase_request_fields_json_b64: str
    requesting_end_user_pastelid: str = Field(index=True)
    txid_of_credit_purchase_burn_transaction: str = Field(index=True)
    credit_purchase_request_confirmation_utc_iso_string: str
    credit_purchase_request_confirmation_pastel_block_height: int
    credit_purchase_request_confirmation_message_version_string: str
    sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str = Field(unique=True, index=True)
    requesting_end_user_pastelid_signature_on_sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str
    class Config:
        json_schema_extra = {
            "example": {
                "id": "79df343b-4ad3-435c-800e-e59e616ff84d",
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "sha3_256_hash_of_credit_pack_purchase_request_response_fields": "0x5678...",
                "credit_pack_purchase_request_fields_json_b64": 'eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9',
                "requesting_end_user_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "txid_of_credit_purchase_burn_transaction": "0xabcd...",
                "credit_purchase_request_confirmation_utc_iso_string": "2023-06-01T12:30:00Z",
                "credit_purchase_request_confirmation_pastel_block_height": 123458,
                "credit_purchase_request_confirmation_message_version_string": "1.0",
                "sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields": "0xdef0...",
                "requesting_end_user_pastelid_signature_on_sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields": "0x1234..."
            }
        }

class CreditPackPurchaseRequestConfirmationResponse(SQLModel, table=True):
    id: Optional[str] = Field(default_factory=lambda: str(uuid.uuid4()), primary_key=True, index=True)
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(index=True, foreign_key="creditpackpurchaserequest.sha3_256_hash_of_credit_pack_purchase_request_fields")
    sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str = Field(index=True, foreign_key="creditpackpurchaserequestconfirmation.sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields")
    credit_pack_confirmation_outcome_string: str
    pastel_api_credit_pack_ticket_registration_txid: str = Field(index=True)
    credit_pack_confirmation_failure_reason_if_applicable: str
    credit_purchase_request_confirmation_response_utc_iso_string: str
    credit_purchase_request_confirmation_response_pastel_block_height: int
    credit_purchase_request_confirmation_response_message_version_string: str
    responding_supernode_pastelid: str = Field(index=True)
    sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields: str = Field(unique=True, index=True)
    responding_supernode_signature_on_credit_pack_purchase_request_confirmation_response_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "id": "79df343b-4ad3-435c-800e-e59e616ff84d",
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields": "0x5678...",
                "credit_pack_confirmation_outcome_string": "success",
                "pastel_api_credit_pack_ticket_registration_txid": "0xabcd...",
                "credit_pack_confirmation_failure_reason_if_applicable": "",
                "credit_purchase_request_confirmation_response_utc_iso_string": "2023-06-01T12:45:00Z",
                "credit_purchase_request_confirmation_response_pastel_block_height": 123459,
                "credit_purchase_request_confirmation_response_message_version_string": "1.0",
                "responding_supernode_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "sha3_256_hash_of_credit_pack_purchase_request_confirmation_response_fields": "0xdef0...",
                "responding_supernode_signature_on_credit_pack_purchase_request_confirmation_response_hash": "0x1234..."
            }
        }
        
class CreditPackRequestStatusCheck(SQLModel, table=True):
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
    requesting_end_user_pastelid: str = Field(index=True)
    requesting_end_user_pastelid_signature_on_sha3_256_hash_of_credit_pack_purchase_request_fields: str
    class Config:
        json_schema_extra = {
            "example": {
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "requesting_end_user_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "requesting_end_user_pastelid_signature_on_sha3_256_hash_of_credit_pack_purchase_request_fields": "0x5678..."
            }
        }

class CreditPackPurchaseRequestStatus(SQLModel, table=True):
    id: Optional[str] = Field(default_factory=lambda: str(uuid.uuid4()), primary_key=True, index=True)
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(foreign_key="creditpackpurchaserequest.sha3_256_hash_of_credit_pack_purchase_request_fields", index=True)
    sha3_256_hash_of_credit_pack_purchase_request_response_fields: str = Field(foreign_key="creditpackpurchaserequestresponse.sha3_256_hash_of_credit_pack_purchase_request_response_fields", index=True)
    status: str = Field(index=True)
    status_details: str
    status_update_timestamp_utc_iso_string: str
    status_update_pastel_block_height: int
    credit_purchase_request_status_message_version_string: str
    responding_supernode_pastelid: str = Field(index=True)
    sha3_256_hash_of_credit_pack_purchase_request_status_fields: str = Field(unique=True, index=True)
    responding_supernode_signature_on_credit_pack_purchase_request_status_hash: str

    class Config:
        json_schema_extra = {
            "example": {
                "id": "79df343b-4ad3-435c-800e-e59e616ff84d",
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "sha3_256_hash_of_credit_pack_purchase_request_response_fields": "0x5678...",
                "status": "in_progress",
                "status_details": "Waiting for price agreement responses from supernodes",
                "status_update_timestamp_utc_iso_string": "2023-06-01T12:30:00Z",
                "status_update_pastel_block_height": 123456,
                "credit_purchase_request_status_message_version_string": "1.0",
                "responding_supernode_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "sha3_256_hash_of_credit_pack_purchase_request_status_fields": "0x5678...",
                "responding_supernode_signature_on_credit_pack_purchase_request_status_hash": "0xef01..."
            }
        }
        
class CreditPackStorageRetryRequest(SQLModel, table=True):
    sha3_256_hash_of_credit_pack_purchase_request_response_fields: str = Field(primary_key=True, index=True)
    credit_pack_purchase_request_fields_json_b64: str
    requesting_end_user_pastelid: str = Field(index=True)
    closest_agreeing_supernode_to_retry_storage_pastelid: str = Field(index=True)
    credit_pack_storage_retry_request_timestamp_utc_iso_string: str
    credit_pack_storage_retry_request_pastel_block_height: int
    credit_pack_storage_retry_request_message_version_string: str
    sha3_256_hash_of_credit_pack_storage_retry_request_fields: str
    requesting_end_user_pastelid_signature_on_credit_pack_storage_retry_request_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "sha3_256_hash_of_credit_pack_purchase_request_response_fields": "0x1234...",
                "credit_pack_purchase_request_fields_json_b64": 'eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9',
                "requesting_end_user_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "closest_agreeing_supernode_to_retry_storage_pastelid": "jXa1s9mKDr4m6P8s7bKK1rYFgL7hkfGMLX1NozVSX4yTnfh9EjuP",
                "credit_pack_storage_retry_request_timestamp_utc_iso_string": "2023-06-01T12:50:00Z",
                "credit_pack_storage_retry_request_pastel_block_height": 123460,
                "credit_pack_storage_retry_request_message_version_string": "1.0",
                "sha3_256_hash_of_credit_pack_storage_retry_request_fields": "0xabcd...",
                "requesting_end_user_pastelid_signature_on_credit_pack_storage_retry_request_hash": "0xdef0..."
            }
        }

class CreditPackStorageRetryRequestResponse(SQLModel, table=True):
    sha3_256_hash_of_credit_pack_purchase_request_fields: str = Field(primary_key=True, index=True)
    sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields: str
    credit_pack_storage_retry_confirmation_outcome_string: str
    pastel_api_credit_pack_ticket_registration_txid: str
    credit_pack_storage_retry_confirmation_failure_reason_if_applicable: str
    credit_pack_storage_retry_confirmation_response_utc_iso_string: str
    credit_pack_storage_retry_confirmation_response_pastel_block_height: int
    credit_pack_storage_retry_confirmation_response_message_version_string: str
    closest_agreeing_supernode_to_retry_storage_pastelid: str = Field(index=True)
    sha3_256_hash_of_credit_pack_storage_retry_confirmation_response_fields: str
    closest_agreeing_supernode_to_retry_storage_pastelid_signature_on_credit_pack_storage_retry_confirmation_response_hash: str    
    class Config:
        json_schema_extra = {
            "example": {
                "sha3_256_hash_of_credit_pack_purchase_request_fields": "0x1234...",
                "sha3_256_hash_of_credit_pack_purchase_request_confirmation_fields": "0x5678...",
                "credit_pack_storage_retry_confirmation_outcome_string": "success",
                "pastel_api_credit_pack_ticket_registration_txid": "0xabcd...",
                "credit_pack_storage_retry_confirmation_failure_reason_if_applicable": "",
                "credit_pack_storage_retry_confirmation_response_utc_iso_string": "2023-06-01T12:55:00Z",
                "credit_pack_storage_retry_confirmation_response_pastel_block_height": 123461,
                "credit_pack_storage_retry_confirmation_response_message_version_string": "1.0",
                "closest_agreeing_supernode_to_retry_storage_pastelid": "jXa1s9mKDr4m6P8s7bKK1rYFgL7hkfGMLX1NozVSX4yTnfh9EjuP",
                "sha3_256_hash_of_credit_pack_storage_retry_confirmation_response_fields": "0xdef0...",
                "closest_agreeing_supernode_to_retry_storage_pastelid_signature_on_credit_pack_storage_retry_confirmation_response_hash": "0x1234..."
            }
        }
        
class CreditPackKnownBadTXID(SQLModel, table=True):
    id: Optional[str] = Field(default_factory=lambda: str(uuid.uuid4()), primary_key=True, index=True)
    credit_pack_ticket_txid: str = Field(index=True)
    list_of_reasons_it_is_known_bad: str = Field(sa_column=Column(JSON))
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
    class Config:
        json_schema_extra = {
            "example": {
                "id": "79df343b-4ad3-435c-800e-e59e616ff84d",
                "credit_pack_ticket_txid": "0x1234...",
                "list_of_reasons_it_is_known_bad": [
                    "Hash of credit pack request response object stored in blockchain does not match the hash included in the object.",
                    "Hash of credit pack request confirmation object stored in blockchain does not match the hash included in the object."
                ],
                "timestamp": "2023-06-01T12:00:00Z"
            }
        }
        
class CreditPackCompleteTicketWithBalance(SQLModel, table=True):
    id: Optional[str] = Field(default_factory=lambda: str(uuid.uuid4()), primary_key=True, index=True)
    credit_pack_ticket_registration_txid: str = Field(index=True)
    complete_credit_pack_data_json: str = Field(sa_column=Column(JSON))
    datetime_last_updated: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), index=True)
    class Config:
        json_schema_extra = {
            "example": {
                "id": "79df343b-4ad3-435c-800e-e59e616ff84d",
                "credit_pack_ticket_registration_txid": "0x1234...",
                "complete_credit_pack_data_json": "",
                "datetime_last_updated": "2023-06-01T12:00:00Z"
            }
        }   
                
##______________________________________________________________________________________________________________________
# Inference request related models (i.e., using the credit packs to do inferences):
    

class InferenceAPIUsageRequest(SQLModel, table=True):
    id: Optional[str] = Field(default_factory=lambda: str(uuid.uuid4()), primary_key=True, index=True)
    inference_request_id: str = Field(unique=True, index=True)
    requesting_pastelid: str = Field(index=True)
    credit_pack_ticket_pastel_txid: str = Field(index=True)
    requested_model_canonical_string: str
    model_inference_type_string: str
    model_parameters_json_b64: str
    model_input_data_json_b64: str
    inference_request_utc_iso_string: str
    inference_request_pastel_block_height: int
    status: str = Field(index=True)
    inference_request_message_version_string: str
    sha3_256_hash_of_inference_request_fields: str
    requesting_pastelid_signature_on_request_hash: str
    class Config:
        protected_namespaces = ()
        json_schema_extra = {
            "example": {
                "id": "79df343b-4ad3-435c-800e-e59e616ff84d",
                "inference_request_id": "0x1234...",
                "requesting_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "credit_pack_ticket_pastel_txid": "0x5678...",
                "requested_model_canonical_string": "gpt-3.5-turbo",
                "model_inference_type_string": "text-completion",
                "model_parameters_json_b64": "eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9",
                "model_input_data_json_b64": "eyJwcm9tcHQiOiAiSGVsbG8sIGhvdyBhcmUgeW91PyJ9",
                "inference_request_utc_iso_string": "2023-06-01T12:00:00Z",
                "inference_request_pastel_block_height": 123456,
                "status": "in_progress",
                "inference_request_message_version_string": "1.0",
                "sha3_256_hash_of_inference_request_fields": "0x5678...",
                "requesting_pastelid_signature_on_request_hash": "0xabcd..."
            }
        }

class InferenceAPIUsageResponse(SQLModel, table=True):
    id: Optional[str] = Field(default_factory=lambda: str(uuid.uuid4()), primary_key=True, index=True)
    inference_response_id: str = Field(unique=True, index=True)
    inference_request_id: str = Field(index=True, foreign_key="inferenceapiusagerequest.inference_request_id")
    proposed_cost_of_request_in_inference_credits: float
    remaining_credits_in_pack_after_request_processed: float
    credit_usage_tracking_psl_address: str = Field(index=True)
    request_confirmation_message_amount_in_patoshis: int
    max_block_height_to_include_confirmation_transaction: int
    inference_request_response_utc_iso_string: str
    inference_request_response_pastel_block_height: int
    inference_request_response_message_version_string: str
    sha3_256_hash_of_inference_request_response_fields: str
    supernode_pastelid_and_signature_on_inference_request_response_hash: str
    class Config:
        json_schema_extra = {
            "example": {
                "id": "79df343b-4ad3-435c-800e-e59e616ff84d",
                "inference_response_id": "0x1234...",
                "inference_request_id": "0x5678...",
                "proposed_cost_of_request_in_inference_credits": 10,
                "remaining_credits_in_pack_after_request_processed": 990,
                "credit_usage_tracking_psl_address": "tPj2wX5mjQErTju6nueVRkxGMCPuMkLn8CWdViJ38m9Wf6PBK5jV",
                "request_confirmation_message_amount_in_patoshis": 1000,
                "max_block_height_to_include_confirmation_transaction": 123456,
                "inference_request_response_utc_iso_string": "2023-06-01T12:00:00Z",
                "inference_request_response_pastel_block_height": 123456,
                "inference_request_response_message_version_string": "1.0",
                "sha3_256_hash_of_inference_request_response_fields": "0x5678...",
                "supernode_pastelid_and_signature_on_inference_request_response_hash": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk:0xabcd..."
            }
        }
        
class InferenceAPIOutputResult(SQLModel, table=True):
    id: Optional[str] = Field(default_factory=lambda: str(uuid.uuid4()), primary_key=True, index=True)
    inference_result_id: str = Field(unique=True, index=True)
    inference_request_id: str = Field(index=True, foreign_key="inferenceapiusagerequest.inference_request_id")
    inference_response_id: str = Field(index=True, foreign_key="inferenceapiusageresponse.inference_response_id")
    responding_supernode_pastelid: str = Field(index=True)
    inference_result_json_base64: str
    inference_result_file_type_strings: str
    inference_result_utc_iso_string: str
    inference_result_pastel_block_height: int
    inference_result_message_version_string: str
    sha3_256_hash_of_inference_result_fields: str
    responding_supernode_signature_on_inference_result_id: str
    class Config:
        json_schema_extra = {
            "example": {
                "id": "79df343b-4ad3-435c-800e-e59e616ff84d",
                "inference_result_id": "0x1234...",
                "inference_request_id": "0x5678...",
                "inference_response_id": "0x9abc...",
                "responding_supernode_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "inference_result_json_base64": "eyJvdXRwdXQiOiAiSGVsbG8sIEknbSBkb2luZyBncmVhdCEgSG93IGFib3V0IHlvdT8ifQ==",
                "inference_result_file_type_strings": "json",
                "inference_result_utc_iso_string": "2023-06-01T12:00:00Z",
                "inference_result_pastel_block_height": 123456,
                "inference_result_message_version_string": "1.0",
                "sha3_256_hash_of_inference_result_fields": "0x5678...",
                "responding_supernode_signature_on_inference_result_id": "0xdef0..."
            }
        }
        
class InferenceConfirmation(SQLModel):
    inference_request_id: str
    requesting_pastelid: str
    confirmation_transaction: dict
    class Config:
        json_schema_extra = {
            "example": {
                "inference_request_id": "0x1234...",
                "requesting_pastelid": "jXYJud3rmrR1Sk2scvR47N4E4J5Vv48uCC6se2nUHyfSJ17wacN7rVZLe6Sk",
                "confirmation_transaction": {
                    "txid": "0x5678...",
                    "amount": 1000,
                    "block_height": 123456
                }
            }
        }


#_____________________________________________________________________________

# Increase the pool size and set a reasonable timeout
engine = create_async_engine(
    DATABASE_URL,
    echo=False,
    future=True,
    poolclass=NullPool,  # Use NullPool for SQLite
    connect_args={"check_same_thread": False},
)

async_session_factory = sessionmaker(
    engine, class_=SQLModelSession, expire_on_commit=False
)

@asynccontextmanager
async def Session() -> SQLModelSession:
    async with async_session_factory() as session:
        try:
            yield session
        finally:
            await session.close()
            
async def initialize_db():
    list_of_sqlite_pragma_strings = [
        "PRAGMA journal_mode=WAL;",
        "PRAGMA synchronous = NORMAL;",
        "PRAGMA cache_size = -262144;",
        "PRAGMA busy_timeout = 3000;",
        "PRAGMA wal_autocheckpoint = 100;",
        "PRAGMA mmap_size = 30000000000;",
        "PRAGMA threads = 6;",
        "PRAGMA optimize;",
        "PRAGMA secure_delete = OFF;",
        "PRAGMA temp_store = MEMORY;",
        "PRAGMA page_size = 4096;",
        "PRAGMA auto_vacuum = INCREMENTAL;"
    ]
    list_of_sqlite_pragma_justification_strings = [
        "Set SQLite to use Write-Ahead Logging (WAL) mode (from default DELETE mode) so that reads and writes can occur simultaneously",
        "Set synchronous mode to NORMAL (from FULL) so that writes are not blocked by reads",
        "Set cache size to 1GB (from default 2MB) so that more data can be cached in memory and not read from disk; to make this 256MB, set it to -262144 instead",
        "Increase the busy timeout to 3 seconds so that the database waits",
        "Set the WAL autocheckpoint to 100 (from default 1000) so that the WAL file is checkpointed more frequently",
        "Set the maximum size of the memory-mapped I/O cache to 30GB to improve performance by accessing the database file directly from memory",
        "Enable multi-threaded mode in SQLite and set the number of worker threads to 6 to allow concurrent access to the database",
        "Optimize the database by running a set of optimization steps to improve query performance",
        "Disable the secure delete feature to improve deletion performance at the cost of potentially leaving deleted data recoverable",
        "Set the temporary store to MEMORY to improve performance by avoiding disk I/O",
        "Set the page size to 4096 to improve performance by reducing the number of disk I/O operations",
        "Enable auto-vacuum to improve performance by automatically reclaiming space from deleted data"
    ]
    assert(len(list_of_sqlite_pragma_strings) == len(list_of_sqlite_pragma_justification_strings))
    try:
        async with engine.begin() as conn:
            try:
                for pragma_string in list_of_sqlite_pragma_strings:
                    await conn.execute(sql_text(pragma_string))
                await conn.run_sync(SQLModel.metadata.create_all)  # Create tables if they don't exist
            finally:
                await conn.close()  # Close the connection explicitly
        await engine.dispose()
        return True
    except Exception as e:
        logger.error(f"Database Initialization Error: {e}")
        return False
    
async def consolidate_wal_data():
    """
    Forces SQLite to consolidate all data in the WAL/shm files into the main database file.
    """
    consolidate_command = "PRAGMA wal_checkpoint(FULL);"
    try:
        async with engine.begin() as conn:  # Use the existing engine for connection
            result = await conn.execute(sql_text(consolidate_command))
            result_fetch = result.fetchone()
            return result_fetch  # This typically returns (0, 0, 0) on success
    except Exception as e:
        logger.error(f"Error during WAL consolidation: {e}")
        return None


---
./benchmark_inference_ram_and_flops_usage.py
---
import time
import psutil
import os
from typing import Callable

def benchmark_inference(inference_func: Callable, *args, **kwargs) -> dict:
    """
    Benchmark an inference task and measure FLOPS and RAM usage.

    Args:
        inference_func (Callable): The inference function to benchmark.
        *args: Positional arguments to pass to the inference function.
        **kwargs: Keyword arguments to pass to the inference function.

    Returns:
        dict: A dictionary containing the benchmark results.
    """
    # Get the process ID of the current Python process
    pid = os.getpid()
    process = psutil.Process(pid)
    # Measure the start time and initial memory usage
    start_time = time.time()
    initial_memory = process.memory_info().rss
    # Run the inference function
    result = inference_func(*args, **kwargs)
    # Measure the end time and final memory usage
    end_time = time.time()
    final_memory = process.memory_info().rss
    # Calculate the execution time and memory usage
    execution_time = end_time - start_time
    memory_used = final_memory - initial_memory
    # Calculate the average memory usage
    avg_memory = (initial_memory + final_memory) / 2
    # Estimate the number of FLOPS (floating-point operations)
    # This is a rough estimate based on the execution time and assumes a certain number of FLOPS per second
    # Adjust the FLOPS_PER_SECOND value based on your system's specifications and the specific inference task
    FLOPS_PER_SECOND = 1e9  # Assuming 1 GFLOPS (adjust this value based on your system)
    estimated_flops = execution_time * FLOPS_PER_SECOND
    # Create a dictionary with the benchmark results
    benchmark_results = {
        "execution_time": execution_time,
        "memory_used": memory_used,
        "avg_memory": avg_memory,
        "estimated_flops": estimated_flops,
        "result": result
    }
    return benchmark_results

# Example usage with llama_cpp
def llama_cpp_inference(prompt: str, model_path: str, n_threads: int = 4, n_predict: int = 128, temp: float = 0.8, top_k: int = 40, top_p: float = 0.9) -> str:
    import llama_cpp
    # Load the model
    model = llama_cpp.Llama(model_path, n_threads=n_threads)
    # Set the inference parameters
    model.set_n_predict(n_predict)
    model.set_temperature(temp)
    model.set_top_k(top_k)
    model.set_top_p(top_p)
    # Run the inference
    output = model.generate(prompt, max_tokens=n_predict)
    # Clean up the generated output
    result = output.strip()
    return result

# Example usage with Huggingface Transformers
def huggingface_inference(prompt: str, model_name: str = "facebook/opt-1.3b", max_length: int = 128, num_beams: int = 4, no_repeat_ngram_size: int = 2, early_stopping: bool = True) -> str:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    # Load the tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    # Tokenize the input prompt
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    # Run the inference
    output = model.generate(
        input_ids,
        max_length=max_length,
        num_beams=num_beams,
        no_repeat_ngram_size=no_repeat_ngram_size,
        early_stopping=early_stopping
    )
    # Decode the generated output
    result = tokenizer.decode(output[0], skip_special_tokens=True)
    return result


# Example usage of the benchmark_inference function
prompt = "The meaning of life is"

llama_model_path = "/path/to/llama/model"
llama_cpp_benchmark = benchmark_inference(llama_cpp_inference, prompt, model_path=llama_model_path, n_threads=4, n_predict=128, temp=0.8, top_k=40, top_p=0.9)
print("llama_cpp benchmark results:")
print(llama_cpp_benchmark)

huggingface_model_name = "facebook/opt-1.3b"
huggingface_benchmark = benchmark_inference(huggingface_inference, prompt, model_name=huggingface_model_name, max_length=128, num_beams=4, no_repeat_ngram_size=2, early_stopping=True)
print("Huggingface Transformers benchmark results:")
print(huggingface_benchmark)

---
./setup_scripts_and_playbooks/copy_various_inference_env_files_ansible_playbook.yml
---
---
- name: Copy a specific files to all machines
  hosts: all
  tasks:
    - name: Copy key to remote directory with mode 600
      copy:
        src: "vastai_privkey"
        dest: "/home/ubuntu/vastai_privkey"
        mode: '0600'

    - name: Copy key to remote directory with mode 600
      copy:
        src: "env_encryption_key_for_supernode_inference_app"
        dest: "/home/ubuntu/env_encryption_key_for_supernode_inference_app"
        mode: '0755'

---
./setup_scripts_and_playbooks/initial_inference_server_setup_script.sh
---
#!/bin/bash

# Determine the user shell
user_shell=$(echo $SHELL)
echo "Detected user shell: $user_shell"

# Set shell path and profile file based on user shell
if [[ $user_shell == *"/zsh"* ]]; then
  profile_file="zshrc"
  shell_cmd="zsh"
else
  profile_file="bashrc"
  shell_cmd="bash"
fi
echo "Using profile file: ~/.$profile_file with shell command: $shell_cmd"

# Update code block with appropriate shell execution
if [ -d ~/python_inference_layer_server ]; then
  echo "Directory exists. Stashing and pulling latest code..."
  cd ~/python_inference_layer_server
  git stash
  git pull
else
  echo "Directory does not exist. Cloning repository..."
  cd ~
  git clone https://github.com/pastelnetwork/python_inference_layer_server.git
  cd python_inference_layer_server
fi

# Get the name of the existing tmux session, create one if it doesn't exist
tmux_session_name=$(tmux list-sessions -F '#{session_name}' | head -1)
if [[ -z "$tmux_session_name" ]]; then
  echo "No tmux session found. Creating a new session..."
  tmux new-session -d -s default_session
  tmux_session_name="default_session"
else
  echo "Found existing tmux session: $tmux_session_name"
fi

# Check if 'supernode_script' window exists in tmux
window_exists=$(tmux list-windows -t $tmux_session_name -F '#{window_name}' | grep -q '^supernode_script$'; echo $?)
if [[ $window_exists -eq 0 ]]; then
  echo "Supernode_script window exists. Killing window..."
  tmux kill-window -t $tmux_session_name:supernode_script
else
  echo "No existing 'supernode_script' window found. Proceeding to create one..."
fi

# Create temporary script
echo "Creating temporary script..."
cat << EOF > ~/run_script.sh
#!/bin/$shell_cmd
source ~/.$profile_file
cd ~/python_inference_layer_server
pyenv local 3.12
source venv/bin/activate
python -m pip install --upgrade pip
python -m pip install --upgrade setuptools
pip install wheel
pip install -r requirements.txt
# pip list --outdated --format=columns | tail -n +3 | awk '{print $1}' | xargs -n1 pip install -U
python main.py
EOF
chmod 0755 ~/run_script.sh

# Launch script in new tmux window using the appropriate shell
echo "Launching script in new tmux window..."
tmux new-window -t $tmux_session_name: -n supernode_script -d "$shell_cmd ~/run_script.sh"


---
./setup_scripts_and_playbooks/pastel_inference_layer_deployment_playbook.yml
---
---
- name: Pastel Inference Layer Deployment
  hosts: all
  become: yes
  vars:
    ubuntu_user: ubuntu
    oh_my_zsh_install_script: "https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh"
    rust_install_script: "https://sh.rustup.rs"
    atuin_install_script: "https://setup.atuin.sh"
    zshrc_path: "/home/{{ ubuntu_user }}/.zshrc"
    bashrc_path: "/home/{{ ubuntu_user }}/.bashrc"
    oh_my_zsh_install_flag: "/home/{{ ubuntu_user }}/.oh-my-zsh"
    home_dir: "/home/{{ ubuntu_user }}"

  tasks:
    - name: Update and upgrade apt packages
      ansible.builtin.apt:
        update_cache: yes
        upgrade: dist
        autoremove: yes

    - name: Check if zsh is installed
      command: which zsh
      register: zsh_installed
      ignore_errors: yes

    - name: Ensure zsh_installed is defined
      set_fact:
        zsh_installed: { "rc": 1 }
      when: zsh_installed is not defined

    - name: Check if Oh My Zsh is installed
      stat:
        path: "{{ oh_my_zsh_install_flag }}"
      register: oh_my_zsh_installed

    - name: Install Oh My Zsh
      become_user: "{{ ubuntu_user }}"
      shell: >
        sh -c "$(curl -fsSL {{ oh_my_zsh_install_script }})" && touch {{ oh_my_zsh_install_flag }}
      when: not oh_my_zsh_installed.stat.exists

    - name: Install Rust
      become_user: "{{ ubuntu_user }}"
      shell: >
        curl -fsSL {{ rust_install_script }} | sh -s -- -y

    - name: Ensure Rust environment is loaded
      lineinfile:
        path: "{{ zshrc_path }}"
        regexp: 'source $HOME/.cargo/env'
        line: 'source $HOME/.cargo/env'
        state: present
      become_user: "{{ ubuntu_user }}"

    - name: Install Atuin
      become_user: "{{ ubuntu_user }}"
      shell: >
        /bin/bash -c "$(curl --proto '=https' --tlsv1.2 -sSf {{ atuin_install_script }})"

    - name: Ensure Atuin environment is loaded
      lineinfile:
        path: "{{ zshrc_path }}"
        regexp: 'eval "$(atuin init zsh)"'
        line: 'eval "$(atuin init zsh)"'
        state: present
      become_user: "{{ ubuntu_user }}"

    - name: Install dependencies for pyenv
      apt:
        name:
          - build-essential
          - libssl-dev
          - zlib1g-dev
          - libbz2-dev
          - libreadline-dev
          - libsqlite3-dev
          - wget
          - curl
          - llvm
          - libncurses5-dev
          - libncursesw5-dev
          - xz-utils
          - tk-dev
          - libffi-dev
          - liblzma-dev
          - python3-openssl
          - git
          - redis
          - redis-server
          - redis-tools
          - multitail
        state: present
        update_cache: yes

    - name: Clone pyenv repository
      git:
        repo: 'https://github.com/pyenv/pyenv.git'
        dest: '{{ home_dir }}/.pyenv'
        update: yes
        force: yes

    - name: Set pyenv environment variables in .zshrc if zsh is found
      blockinfile:
        path: '{{ zshrc_path }}'
        block: |
          export PYENV_ROOT="$HOME/.pyenv"
          export PATH="$PYENV_ROOT/bin:$PATH"
          eval "$(pyenv init --path)"
      when: zsh_installed.rc == 0

    - name: Set pyenv environment variables in .bashrc if zsh is not found
      blockinfile:
        path: '{{ bashrc_path }}'
        block: |
          export PYENV_ROOT="$HOME/.pyenv"
          export PATH="$PYENV_ROOT/bin:$PATH"
          eval "$(pyenv init --path)"
      when: zsh_installed.rc != 0

    - name: Ensure pyenv directories have correct permissions
      file:
        path: /home/ubuntu/.pyenv
        state: directory
        owner: ubuntu
        group: ubuntu
        mode: '0755'
        recurse: yes

    - name: Ensure pyenv is available and install Python 3.12
      become: no
      shell: |
        export PATH="/home/ubuntu/.pyenv/bin:$PATH"
        eval "$(pyenv init --path)"
        eval "$(pyenv init -)"
        pyenv install 3.12
      args:
        executable: /bin/bash
      environment:
        PYENV_ROOT: "/home/ubuntu/.pyenv"
      become_user: ubuntu

    - name: Check again if zsh is installed
      command: which zsh
      register: zsh_installed
      ignore_errors: yes
      failed_when: false

    - name: Replace /root/ with /home/ubuntu/ in .zshrc
      replace:
        path: "/home/{{ ubuntu_user }}/.zshrc"
        regexp: "/root/"
        replace: "/home/{{ ubuntu_user }}/"

    - name: Determine the user shell
      shell: echo $SHELL
      register: user_shell

    - name: Set shell path and profile file
      set_fact:
        shell_path: "{{ user_shell.stdout }}"
        profile_file: "{{ 'zshrc' if '/zsh' in user_shell.stdout else 'bashrc' }}"

    - name: Check if the application directory exists
      stat:
        path: /home/{{ ubuntu_user }}/python_inference_layer_server
      register: app_dir

    - name: Clone the repository if the directory doesn't exist
      git:
        repo: https://github.com/pastelnetwork/python_inference_layer_server
        dest: /home/{{ ubuntu_user }}/python_inference_layer_server
      when: not app_dir.stat.exists

    - name: Run initial setup script if the directory was just created
      shell: |
        chmod +x setup_scripts_and_playbooks/initial_inference_server_setup_script.sh
        ./setup_scripts_and_playbooks/initial_inference_server_setup_script.sh
      args:
        chdir: /home/{{ ubuntu_user }}/python_inference_layer_server
      when: not app_dir.stat.exists

    - name: Update code
      shell: |
        source /home/{{ ubuntu_user }}/.{{ profile_file }}
        git stash
        git pull
      args:
        chdir: /home/{{ ubuntu_user }}/python_inference_layer_server
        executable: "{{ shell_path }}"

    - name: Get the name of the existing tmux session
      shell: tmux list-sessions -F '#{session_name}' | head -1
      register: tmux_session_name
      ignore_errors: true

    - name: Create tmux session if it doesn't exist
      shell: tmux new-session -d -s default_session
      when: tmux_session_name.stdout == ""
      args:
        executable: "{{ shell_path }}"

    - name: Set the tmux session name
      set_fact:
        session_name: "{{ tmux_session_name.stdout if tmux_session_name.stdout else 'default_session' }}"

    - name: Check if supernode_script window exists
      shell: tmux list-windows -t {{ session_name }} -F '#{window_name}' | grep -q '^supernode_script$'
      register: window_exists
      ignore_errors: true

    - name: Kill supernode_script window if it exists
      shell: tmux kill-window -t {{ session_name }}:supernode_script
      when: window_exists.rc == 0

    - name: Create temporary script
      copy:
        content: |
          #!/bin/{{ 'zsh' if '/zsh' in shell_path else 'bash' }}
          source /home/{{ ubuntu_user }}/.{{ profile_file }}
          cd /home/{{ ubuntu_user }}/python_inference_layer_server
          pyenv local 3.12
          source venv/bin/activate
          python -m pip install --upgrade pip
          python -m pip install --upgrade setuptools
          python -m pip install wheel
          pip install -r requirements.txt
          python main.py
        dest: /home/{{ ubuntu_user }}/run_script.sh
        mode: '0755'

    - name: Launch script in new tmux window
      shell: |
        tmux new-window -t {{ session_name }}: -n supernode_script -d "{{ shell_path }} -c '/home/{{ ubuntu_user }}/run_script.sh'"
      args:
        executable: "{{ shell_path }}"

    - name: Remove temporary script
      file:
        path: /home/{{ ubuntu_user }}/run_script.sh
        state: absent


---
./setup_scripts_and_playbooks/ansible_playbook_to_update_and_start_pastel_inference_layer__async_version.yml
---
---
- name: Update code and run Python script in tmux
  hosts: all
  gather_facts: false
  become: true
  become_user: ubuntu
  tasks:
    - name: Determine the user shell
      shell: echo $SHELL
      register: user_shell
      async: 600
      poll: 10
      ignore_errors: yes

    - name: Set shell path and profile file
      set_fact:
        shell_path: "{{ user_shell.stdout }}"
        profile_file: "{{ 'zshrc' if '/zsh' in user_shell.stdout else 'bashrc' }}"
      ignore_errors: yes

    - name: Update code
      shell: |
        source ~/.{{ profile_file }}
        git stash
        git pull
      args:
        chdir: /home/ubuntu/python_inference_layer_server
        executable: "{{ shell_path }}"
      async: 600
      poll: 10
      ignore_errors: yes

    - name: Get the name of the existing tmux session
      shell: tmux list-sessions -F '#{session_name}' | head -1
      register: tmux_session_name
      ignore_errors: yes
      async: 600
      poll: 10

    - name: Create tmux session if it doesn't exist
      shell: tmux new-session -d -s default_session
      when: tmux_session_name.stdout == ""
      args:
        executable: "{{ shell_path }}"
      async: 600
      poll: 10
      ignore_errors: yes

    - name: Set the tmux session name
      set_fact:
        session_name: "{{ tmux_session_name.stdout if tmux_session_name.stdout else 'default_session' }}"
      ignore_errors: yes

    - name: Check if supernode_script window exists
      shell: tmux list-windows -t {{ session_name }} -F '#{window_name}' | grep -q '^supernode_script$'
      register: window_exists
      ignore_errors: yes
      async: 600
      poll: 10

    - name: Kill supernode_script window if it exists
      shell: tmux kill-window -t {{ session_name }}:supernode_script
      when: window_exists.rc == 0
      async: 600
      poll: 10
      ignore_errors: yes

    - name: Check if inference_log window exists
      shell: tmux list-windows -t {{ session_name }} -F '#{window_name}' | grep -q '^inference_log$'
      register: inference_log_exists
      ignore_errors: yes
      async: 600
      poll: 10

    - name: Kill inference_log window if it exists
      shell: tmux kill-window -t {{ session_name }}:inference_log
      when: inference_log_exists.rc == 0
      async: 600
      poll: 10
      ignore_errors: yes

    - name: Create temporary script
      copy:
        content: |
          #!/bin/{{ 'zsh' if '/zsh' in shell_path else 'bash' }}
          source ~/.{{ profile_file }}
          cd /home/ubuntu/python_inference_layer_server
          pyenv local 3.12
          source venv/bin/activate
          python -m pip install --upgrade pip
          python -m pip install --upgrade setuptools
          python -m pip install wheel
          pip install -r requirements.txt
          # pip list --outdated --format=columns | tail -n +3 | awk '{print $1}' | xargs -n1 pip install -U
          python main.py
        dest: /home/ubuntu/run_script.sh
        mode: '0755'
      retries: 3
      delay: 60
      ignore_errors: yes

    - name: Launch script in new tmux window
      shell: |
        tmux new-window -t {{ session_name }}: -n supernode_script -d "{{ shell_path }} -c '/home/ubuntu/run_script.sh'"
      args:
        executable: "{{ shell_path }}"
      async: 600
      poll: 10
      ignore_errors: yes

    - name: Launch inference log in new tmux window
      shell: |
        tmux new-window -t {{ session_name }}: -n inference_log -d "{{ shell_path }} -c 'multitail /home/ubuntu/python_inference_layer_server/pastel_supernode_inference_layer.log'"
      args:
        executable: "{{ shell_path }}"
      async: 600
      poll: 10
      ignore_errors: yes

    - name: Remove temporary script
      file:
        path: /home/ubuntu/run_script.sh
        state: absent
      retries: 3
      delay: 60
      ignore_errors: yes

---
./setup_scripts_and_playbooks/run_setup_script_for_raw_new_ubuntu_instance.yml
---
- name: Run setup script on remote server
  hosts: all
  become: yes
  tasks:
    - name: Run setup script via curl and bash
      ansible.builtin.shell: |
        sudo curl -sSL https://raw.githubusercontent.com/pastelnetwork/ansible_playbook_for_initial_setup_of_fresh_machine_for_sn/master/automatic_sn_setup.sh | sudo bash 
      args:
        executable: /bin/bash


---
./setup_scripts_and_playbooks/ansible_playbook_to_update_and_start_pastel_inference_layer.yml
---
---
- name: Update code and run Python script in tmux
  hosts: all
  gather_facts: false
  tasks:
    - name: Determine the user shell
      shell: echo $SHELL
      register: user_shell

    - name: Set shell path and profile file
      set_fact:
        shell_path: "{{ user_shell.stdout }}"
        profile_file: "{{ 'zshrc' if '/zsh' in user_shell.stdout else 'bashrc' }}"

    - name: Update code
      shell: |
        source ~/.{{ profile_file }}
        git stash
        git pull
      args:
        chdir: ~/python_inference_layer_server
        executable: "{{ shell_path }}"

    - name: Get the name of the existing tmux session
      shell: tmux list-sessions -F '#{session_name}' | head -1
      register: tmux_session_name
      ignore_errors: true

    - name: Create tmux session if it doesn't exist
      shell: tmux new-session -d -s default_session
      when: tmux_session_name.stdout == ""
      args:
        executable: "{{ shell_path }}"

    - name: Set the tmux session name
      set_fact:
        session_name: "{{ tmux_session_name.stdout if tmux_session_name.stdout else 'default_session' }}"

    - name: Check if supernode_script window exists
      shell: tmux list-windows -t {{ session_name }} -F '#{window_name}' | grep -q '^supernode_script$'
      register: window_exists
      ignore_errors: true

    - name: Kill supernode_script window if it exists
      shell: tmux kill-window -t {{ session_name }}:supernode_script
      when: window_exists.rc == 0

    - name: Create temporary script
      copy:
        content: |
          #!/bin/{{ 'zsh' if '/zsh' in shell_path else 'bash' }}
          source ~/.{{ profile_file }}
          cd ~/python_inference_layer_server
          pyenv local 3.12
          source venv/bin/activate
          python -m pip install --upgrade pip
          python -m pip install --upgrade setuptools
          python -m pip install wheel
          pip install -r requirements.txt
          # pip list --outdated --format=columns | tail -n +3 | awk '{print $1}' | xargs -n1 pip install -U
          python main.py
        dest: ~/run_script.sh
        mode: '0755'

    - name: Launch script in new tmux window
      shell: |
        tmux new-window -t {{ session_name }}: -n supernode_script -d "{{ shell_path }} -c '~/run_script.sh'"
      args:
        executable: "{{ shell_path }}"

    - name: Remove temporary script
      file:
        path: ~/run_script.sh
        state: absent

---
./setup_scripts_and_playbooks/ansible_playbook_to_deploy_exo_pastel_code.yml
---
---
- name: Update code and run Exo Python script in tmux
  hosts: all
  gather_facts: false
  become: true
  become_user: ubuntu
  tasks:
    - name: Determine the user shell
      shell: echo $SHELL
      register: user_shell
      async: 600
      poll: 10
      ignore_errors: yes

    - name: Set shell path and profile file
      set_fact:
        shell_path: "{{ user_shell.stdout }}"
        profile_file: "{{ 'zshrc' if '/zsh' in user_shell.stdout else 'bashrc' }}"
      ignore_errors: yes

    - name: Check if exo_pastel directory exists
      stat:
        path: /home/ubuntu/exo_pastel
      register: exo_dir

    - name: Clone Exo repository if it doesn't exist
      shell: |
        git clone https://github.com/pastelnetwork/exo_pastel
      args:
        chdir: /home/ubuntu
        executable: "{{ shell_path }}"
      when: not exo_dir.stat.exists
      async: 600
      poll: 10
      ignore_errors: yes

    - name: Change to exo_pastel directory and set Python version with pyenv
      shell: |
        export PYENV_ROOT="$HOME/.pyenv"
        export PATH="$PYENV_ROOT/bin:$PATH"
        if command -v pyenv >/dev/null; then
          eval "$(pyenv init --path)"
          eval "$(pyenv init -)"
        fi
        cd /home/ubuntu/exo_pastel
        pyenv local 3.12
      args:
        executable: "{{ shell_path }}"
      async: 600
      poll: 10
      ignore_errors: yes

    - name: Pull latest code if exo_pastel directory exists
      shell: |
        git stash
        git pull
      args:
        chdir: /home/ubuntu/exo_pastel
        executable: "{{ shell_path }}"
      when: exo_dir.stat.exists
      async: 600
      poll: 10
      ignore_errors: yes

    - name: Create temporary script for setting up and running Exo
      copy:
        content: |
          #!/bin/{{ 'zsh' if '/zsh' in shell_path else 'bash' }}
          source ~/.{{ profile_file }}
          cd /home/ubuntu/exo_pastel
          pyenv local 3.12
          python -m venv venv
          source venv/bin/activate
          python -m pip install --upgrade pip
          python -m pip install wheel
          python -m pip install --upgrade setuptools wheel
          pip install .
          python main.py
        dest: /home/ubuntu/run_exo.sh
        mode: '0755'
      retries: 3
      delay: 60
      ignore_errors: yes

    - name: Get the name of the existing tmux session
      shell: tmux list-sessions -F '#{session_name}' | head -1
      register: tmux_session_name
      ignore_errors: yes
      async: 600
      poll: 10

    - name: Create tmux session if it doesn't exist
      shell: tmux new-session -d -s default_session
      when: tmux_session_name.stdout == ""
      args:
        executable: "{{ shell_path }}"
      async: 600
      poll: 10
      ignore_errors: yes

    - name: Set the tmux session name
      set_fact:
        session_name: "{{ tmux_session_name.stdout if tmux_session_name.stdout else 'default_session' }}"
      ignore_errors: yes

    - name: Check if exo window exists
      shell: tmux list-windows -t {{ session_name }} -F '#{window_name}' | grep -q '^exo$'
      register: exo_window_exists
      ignore_errors: yes
      async: 600
      poll: 10

    - name: Kill exo window if it exists
      shell: tmux kill-window -t {{ session_name }}:exo
      when: exo_window_exists.rc == 0
      async: 600
      poll: 10
      ignore_errors: yes

    - name: Launch Exo script in new tmux window
      shell: |
        tmux new-window -t {{ session_name }}: -n exo -d "{{ shell_path }} -c '/home/ubuntu/run_exo.sh'"
      args:
        executable: "{{ shell_path }}"
      async: 600
      poll: 10
      ignore_errors: yes

    - name: Remove temporary script
      file:
        path: /home/ubuntu/run_exo.sh
        state: absent
      retries: 3
      delay: 60
      ignore_errors: yes


---
./setup_scripts_and_playbooks/playbook_to_check_and_delete_incomplete_swiss_army_llama_model_files.yml
---
---
- name: Verify and clean up model files
  hosts: all
  gather_facts: no

  vars:
    model_files:
      - { name: "bge-m3-q8_0.gguf", size: 617542656 }
      - { name: "download.lock", size: 0 }
      - { name: "Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf", size: 4920733728 }
      - { name: "Lexi-Llama-3-8B-Uncensored_Q5_K_M.gguf", size: 5732987136 }
      - { name: "llava-llama-3-8b-v1_1-int4.gguf", size: 4921246944 }
      - { name: "llava-llama-3-8b-v1_1-mmproj-f16.gguf", size: 624434368 }
      - { name: "Meta-Llama-3-8B-Instruct.Q3_K_S.gguf", size: 3665011936 }
      - { name: "nomic-embed-text-v1.5.Q6_K.gguf", size: 113042528 }

  tasks:
    - name: Check and delete invalid files
      shell: |
        if [ -f /home/ubuntu/swiss_army_llama/models/{{ item.name }} ]; then
          actual_size=$(stat -c%s /home/ubuntu/swiss_army_llama/models/{{ item.name }})
          if [ "$actual_size" -ne {{ item.size }} ]; then
            rm /home/ubuntu/swiss_army_llama/models/{{ item.name }}
          fi
        fi
      loop: "{{ model_files }}"
      args:
        warn: false


---
