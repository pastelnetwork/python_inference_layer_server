{
  "models": [
    {
      "model_name": "swiss_army_llama-Llama-3.1-8B-Lexi-Uncensored_V2_Q5",
      "model_url": "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/blob/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf",
      "description": "The Llama-3.1-8B-Lexi-Uncensored V2, based on the powerful Meta Llama-3.1-8B model, is designed for uncensored text generation with a focus on creative and innovative content.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Grammar File String: The grammar file used to restrict text generation [Optional] (Default is to not use any grammar file. Examples: `json`, `list`)"
        },
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "text_completion": {
          "input_tokens": 1.6,
          "output_tokens": 1.2,
          "compute_cost": 0.9,
          "memory_cost": 0.8
        },
        "embedding": {
          "input_tokens": 0.6,
          "compute_cost": 0.7,
          "memory_cost": 0.5
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.15,
          "total_sentences": 0.08,
          "query_string_included": 0.35,
          "compute_cost": 1.0,
          "memory_cost": 0.7
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.03,
          "query_string_included": 0.35,
          "compute_cost": 1.1,
          "memory_cost": 0.9
        }
      }
    },
    {
      "model_name": "swiss_army_llama-Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M",
      "model_url": "https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf",
      "description": "Hermes 2 Pro is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house. This version excels at Function Calling, JSON Structured Outputs, and has improved on several other metrics, scoring 90% on function calling evaluation and 84% on structured JSON Output evaluation.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Grammar File String: The grammar file used to restrict text generation [Optional] (Default is to not use any grammar file. Examples: `json`, `list`)"
        },
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "text_completion": {
          "input_tokens": 1.8,
          "output_tokens": 1.4,
          "compute_cost": 1.0,
          "memory_cost": 0.9
        },
        "embedding": {
          "input_tokens": 0.7,
          "compute_cost": 0.8,
          "memory_cost": 0.5
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.2,
          "total_sentences": 0.1,
          "query_string_included": 0.4,
          "compute_cost": 1.2,
          "memory_cost": 0.8
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.5,
          "query_string_included": 1.5,
          "compute_cost": 1.2,
          "memory_cost": 1.0
        }
      }
    },
    {
      "model_name": "swiss_army_llama-Meta-Llama-3.1-8B-Instruct-Q5_K_M",
      "model_url": "https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
      "description": "The Meta-Llama-3.1-8B-Instruct model is designed for instruction-following tasks with optimized quantization for efficient computation.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Grammar File String: The grammar file used to restrict text generation [Optional] (Default is to not use any grammar file. Examples: `json`, `list`)"
        },
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "text_completion": {
          "input_tokens": 1.5,
          "output_tokens": 1.1,
          "compute_cost": 0.8,
          "memory_cost": 0.7
        },
        "embedding": {
          "input_tokens": 0.5,
          "compute_cost": 0.6,
          "memory_cost": 0.4
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.1,
          "total_sentences": 0.05,
          "query_string_included": 0.3,
          "compute_cost": 0.9,
          "memory_cost": 0.6
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.02,
          "query_string_included": 0.3,
          "compute_cost": 1.0,
          "memory_cost": 0.8
        }
      }
    },
    {
      "model_name": "swiss_army_llama-llava-llama-3-8b-v1_1-int4",
      "model_url": "https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf/resolve/main/llava-llama-3-8b-v1_1-int4.gguf",
      "description": "The LLaVA-LLaMA-3-8B-v1.1 model is a powerful language model developed by XTuner; it allows you to ask questions about images and get answers in a conversational manner.",
      "supported_inference_type_strings": ["ask_question_about_an_image"],
      "input_fields": [
        {
          "name": "image_data",
          "file_type": "base64",
          "optional": false
        },
        {
          "name": "question",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "ask_question_about_an_image": {
          "input_tokens": 1.5,
          "output_tokens": 1.1,
          "compute_cost": 1.2,
          "memory_cost": 1.0
        }
      }
    },
    {
      "model_name": "swiss_army_llama-bge-m3-q8_0",
      "model_url": "https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf",
      "description": "BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It performs dense retrieval, multi-vector retrieval, and sparse retrieval, providing a unified model foundation for real-world IR applications.",
      "supported_inference_type_strings": [
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "embeddings",
          "file_type": "json",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "embedding": {
          "input_tokens": 0.4,
          "compute_cost": 0.5,
          "memory_cost": 0.3
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.05,
          "total_sentences": 0.03,
          "query_string_included": 0.2,
          "compute_cost": 0.6,
          "memory_cost": 0.4
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.01,
          "query_string_included": 0.2,
          "compute_cost": 0.6,
          "memory_cost": 0.5
        }
      }
    },
    {
      "model_name": "swiss_army_llama-nomic-embed-text-v1.5.Q6_K",
      "model_url": "https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF/resolve/main/nomic-embed-text-v1.5.Q6_K.gguf",
      "description": "Nomic Embed Text v1.5 is a powerful language model developed by Nomic AI. It provides high-quality embeddings for semantic search and clustering tasks.",
      "supported_inference_type_strings": [
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "embeddings",
          "file_type": "json",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "embedding": {
          "input_tokens": 0.2,
          "compute_cost": 0.2,
          "memory_cost": 0.1
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.05,
          "total_sentences": 0.02,
          "query_string_included": 0.2,
          "compute_cost": 0.4,
          "memory_cost": 0.3
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.01,
          "query_string_included": 0.2,
          "compute_cost": 0.5,
          "memory_cost": 0.4
        }
      }
    },
    {
      "model_name": "openai-gpt-4o",
      "model_url": "",
      "description": "GPT-4o is OpenAI's most advanced multimodal model that is faster and cheaper than GPT-4 Turbo with stronger vision capabilities. The model has 128K context and an October 2023 knowledge cutoff.",
      "supported_inference_type_strings": ["text_completion"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": true
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openai-gpt-4o-vision",
      "model_url": "",
      "description": "GPT-4o is OpenAI's most advanced multimodal model that\u2019s faster and cheaper than GPT-4 Turbo with stronger vision capabilities. The model has 128K context and an October 2023 knowledge cutoff.",
      "supported_inference_type_strings": ["ask_question_about_an_image"],
      "input_fields": [
        {
          "name": "image_data",
          "file_type": "base64",
          "optional": false
        },
        {
          "name": "question",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openai-gpt-4-turbo",
      "model_url": "",
      "description": "GPT-4 Turbo is offered at 128K context with an April 2023 knowledge cutoff and basic support for vision.",
      "supported_inference_type_strings": ["text_completion"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": true
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openai-text-embedding-ada-002",
      "model_url": "",
      "description": "OpenAI's text embedding model. Provides high-quality embeddings for semantic search and clustering tasks.",
      "supported_inference_type_strings": ["embedding"],
      "input_fields": [
        {
          "name": "input_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "embedding",
          "file_type": "embedding",
          "optional": false
        }
      ],
      "model_parameters": [],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "claude3-haiku",
      "model_url": "",
      "description": "The Claude3 Haiku model is a powerful language model developed by Anthropic. It offers advanced natural language processing capabilities and can be accessed through the Claude API.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "inference_types_parameter_applies_to": "['text_completion']",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "claude3-opus",
      "model_url": "",
      "description": "The Claude3 Opus model is a powerful language model developed by Anthropic. It offers advanced natural language processing capabilities and can be accessed through the Claude API.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "claude3.5-sonnet",
      "model_url": "",
      "description": "The Claude3.5 Sonnet model is a powerful language model developed by Anthropic. It offers advanced natural language processing capabilities and can be accessed through the Claude API.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama-3.1-70b-versatile",
      "model_url": "",
      "description": "LLaMA3.1 70b versatile is a powerful language model developed by Meta with a context window of 131,072 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama-3.1-8b-instant",
      "model_url": "",
      "description": "LLaMA3.1 8b instant is a powerful language model developed by Meta with a context window of 131,072 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama3-70b-8192",
      "model_url": "",
      "description": "LLaMA3 70b is a powerful language model developed by Meta with a context window of 8,192 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama3-8b-8192",
      "model_url": "",
      "description": "LLaMA3 8b is a powerful but compact language model developed by Meta with a context window of 8,192 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-mixtral-8x7b-32768",
      "model_url": "",
      "description": "Mixtral 8x7b is a powerful language model developed by Mistral with a context window of 32,768 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-gemma2-9b-it",
      "model_url": "",
      "description": "Gemma 2 9b is an LLM developed by Google with a context window of 8,192 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-mistral-small-latest",
      "model_url": "",
      "description": "Mistral small is a powerful language model developed by Mistral AI.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-mistral-medium-latest",
      "model_url": "",
      "description": "Mistral medium is a powerful language model developed by Mistral AI.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-mistral-large-latest",
      "model_url": "",
      "description": "Mistral large is a powerful language model developed by Mistral AI.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-core",
      "model_url": "",
      "description": "Stable Image Core is the best quality achievable at high speed for text-to-image generation.",
      "supported_inference_type_strings": [
        "text_to_image"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "aspect_ratio",
          "type": "string",
          "default": "1:1",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The aspect ratio of the generated image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Random seed to use for generation [Optional]"
        },
        {
          "name": "style_preset",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Style Preset: Guides the image model towards a particular style (e.g., '3d-model', 'analog-film', 'anime', etc.) [Optional]"
        },
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The format of the generated image (e.g., 'png', 'jpeg', 'webp') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-sd3-large",
      "model_url": "",
      "description": "Stable Diffusion 3 Large model for high-quality image generation.",
      "supported_inference_type_strings": [
        "text_to_image"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "aspect_ratio",
          "type": "string",
          "default": "1:1",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The aspect ratio of the generated image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Random seed to use for generation [Optional]"
        },
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The format of the generated image (e.g., 'png', 'jpeg') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-conservative-upscale",
      "model_url": "",
      "description": "Conservative upscaling model that preserves image details.",
      "supported_inference_type_strings": [
        "conservative_upscale"
      ],
      "input_fields": [
        {
          "name": "image",
          "file_type": "image",
          "optional": false
        },
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "upscaled_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["conservative_upscale"],
          "description": "The format of the upscaled image (e.g., 'png', 'jpeg', 'webp') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["conservative_upscale"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["conservative_upscale"],
          "description": "Random seed to use for upscaling [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-creative-upscale",
      "model_url": "",
      "description": "Creative upscaling model that can enhance and add details to images.",
      "supported_inference_type_strings": [
        "creative_upscale"
      ],
      "input_fields": [
        {
          "name": "image",
          "file_type": "image",
          "optional": false
        },
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "upscaled_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["creative_upscale"],
          "description": "The format of the upscaled image (e.g., 'png', 'jpeg', 'webp') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["creative_upscale"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["creative_upscale"],
          "description": "Random seed to use for upscaling [Optional]"
        },
        {
          "name": "creativity",
          "type": "float",
          "default": 0.3,
          "min": 0,
          "max": 0.35,
          "inference_types_parameter_applies_to": ["creative_upscale"],
          "description": "Controls the creativity level of the upscaling process [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-sd3-medium",
      "model_url": "",
      "description": "Stable Diffusion 3 Medium model for balanced performance and quality in image generation.",
      "supported_inference_type_strings": [
        "text_to_image"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "aspect_ratio",
          "type": "string",
          "default": "1:1",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The aspect ratio of the generated image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Random seed to use for generation [Optional]"
        },
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The format of the generated image (e.g., 'png', 'jpeg') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-sd3-large-turbo",
      "model_url": "",
      "description": "Stable Diffusion 3 Large Turbo model for faster inference with high-quality results.",
      "supported_inference_type_strings": [
        "text_to_image"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "aspect_ratio",
          "type": "string",
          "default": "1:1",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The aspect ratio of the generated image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Random seed to use for generation [Optional]"
        },
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The format of the generated image (e.g., 'png', 'jpeg') [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/auto",
      "model_url": "",
      "description": "OpenRouter's auto-routing model that automatically selects the best available model for the given task.",
      "supported_inference_type_strings": ["text_completion"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    }
  ]
}
