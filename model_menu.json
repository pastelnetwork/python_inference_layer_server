{
  "models": [
    {
      "model_name": "mistral-7b-instruct-v0.2",
      "model_url": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf",
      "description": "The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.2. Mistral-7B-v0.2 has the following changes compared to Mistral-7B-v0.1: * 32k context window (vs 8k context in v0.1); *  Rope-theta = 1e6; *No Sliding-Window Attention",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "token_level_embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "description": "The grammar file used to restrict text generation (optional; default is to not use any grammar file). Examples: `json`, `list`"
        }
      ],
      "credit_costs": {
        "input_tokens": 1.5,
        "output_tokens": 1.1,
        "compute_cost": 0.5,
        "memory_cost": 0.5
      }
    },
    {
      "model_name": "phi-2",
      "model_url": "https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q5_K_M.gguf",
      "description": "Phi-2 is a Transformer with 2.7 billion parameters. It was trained using the same data sources as Phi-1.5, augmented with a new data source that consists of various NLP synthetic texts and filtered websites (for safety and educational value). When assessed against benchmarks testing common sense, language understanding, and logical reasoning, Phi-2 showcased a nearly state-of-the-art performance among models with less than 13 billion parameters.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "token_level_embedding"
      ],
      "input_fields": [
        {
          "name": "text",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "description": "The grammar file used to restrict text generation (optional; default is to not use any grammar file). Examples: `json`, `list`"
        }
      ],
      "credit_costs": {
        "input_tokens": 0.5,
        "output_tokens": 0.75,
        "compute_cost": 0.25,
        "memory_cost": 0.1
      }
    },
    {
      "model_name": "claude3-haiku",
      "model_url": "",
      "description": "The Claude3 Haiku model is a powerful language model developed by Anthropic. It offers advanced natural language processing capabilities and can be accessed through the Claude API.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "input_tokens": 2.0,
        "output_tokens": 1.5,
        "compute_cost": 1.0,
        "memory_cost": 0.8
      }
    },
    {
      "model_name": "claude3-opus",
      "model_url": "",
      "description": "The Claude3 Opus model is a powerful language model developed by Anthropic. It offers advanced natural language processing capabilities and can be accessed through the Claude API.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "input_tokens": 2.0,
        "output_tokens": 1.5,
        "compute_cost": 1.0,
        "memory_cost": 0.8
      }
    },
    {
      "model_name": "claude3-sonnet",
      "model_url": "",
      "description": "The Claude3 Sonnet model is a powerful language model developed by Anthropic. It offers advanced natural language processing capabilities and can be accessed through the Claude API.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "input_tokens": 2.0,
        "output_tokens": 1.5,
        "compute_cost": 1.0,
        "memory_cost": 0.8
      }
    },
    {
      "model_name": "groq-llama2-70b-4096",
      "model_url": "",
      "description": "LLaMA2 70b is a powerful language model developed by Meta with a context window of 4,096 tokens.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "input_tokens": 2.0,
        "output_tokens": 1.5,
        "compute_cost": 1.0,
        "memory_cost": 0.8
      }
    },
    {
      "model_name": "groq-mixtral-8x7b-32768",
      "model_url": "",
      "description": "Mixtral 8x7b is a powerful language model developed by Mistral with a context window of 32,768 tokens.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "input_tokens": 2.0,
        "output_tokens": 1.5,
        "compute_cost": 1.0,
        "memory_cost": 0.8
      }
    },
    {
      "model_name": "groq-gemma-7b-it",
      "model_url": "",
      "description": "Gemma 7b is a powerful language model developed by Google with a context window of 8,192 tokens.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "input_tokens": 2.0,
        "output_tokens": 1.5,
        "compute_cost": 1.0,
        "memory_cost": 0.8
      }
    },
    {
      "model_name": "mistralapi-mistral-small-latest",
      "model_url": "",
      "description": "Mistral small is a powerful language model developed by Mistral AI.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "input_tokens": 2.0,
        "output_tokens": 1.5,
        "compute_cost": 1.0,
        "memory_cost": 0.8
      }
    },
    {
      "model_name": "mistralapi-mistral-medium-latest",
      "model_url": "",
      "description": "Mistral medium is a powerful language model developed by Mistral AI.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "input_tokens": 2.0,
        "output_tokens": 1.5,
        "compute_cost": 1.0,
        "memory_cost": 0.8
      }
    },
    {
      "model_name": "mistralapi-mistral-large-latest",
      "model_url": "",
      "description": "Mistral large is a powerful language model developed by Mistral AI.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "input_tokens": 2.0,
        "output_tokens": 1.5,
        "compute_cost": 1.0,
        "memory_cost": 0.8
      }
    },
    {
      "model_name": "stability-core",
      "model_url": "",
      "description": "Stable Image Core is the best quality achievable at high speed for text-to-image generation.",
      "supported_inference_type_strings": [
        "text_to_image",
        "creative_upscale"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "aspect_ratio",
          "type": "string",
          "default": "1:1",
          "description": "The aspect ratio of the generated image"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "description": "Random seed to use for generation"
        },
        {
          "name": "style_preset",
          "type": "string",
          "default": null,
          "description": "Guides the image model towards a particular style (e.g., 3d-model, analog-film, anime, etc.)"
        },
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "description": "The format of the generated image"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "description": "A blurb of text describing what you do not wish to see in the output image"
        }                   
      ],
      "credit_costs": {
        "input_tokens": 0,
        "output_tokens": 0,
        "compute_cost": 1500.0,
        "memory_cost": 0
      }
    },
    {
      "model_name": "stability-sdxl-1.0",
      "model_url": "",
      "description": "The standard base model for image generation from Stability AI.",
      "supported_inference_type_strings": [
        "text_to_image"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "height",
          "type": "int",
          "default": 512,
          "description": "The height of the generated image"
        },
        {
          "name": "width",
          "type": "int",
          "default": 512,
          "description": "The width of the generated image"
        },
        {
          "name": "cfg_scale",
          "type": "float",
          "default": 7,
          "description": "CFG scale factor"
        },
        {
          "name": "sampler",
          "type": "string",
          "default": null,
          "description": "Sampler to use for generation (e.g., ddim, plms, k_euler, etc.)"
        },
        {
          "name": "steps",
          "type": "int",
          "default": 50,
          "description": "Number of steps for generation"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "description": "Random seed to use for generation"
        },
        {
          "name": "num_samples",
          "type": "int",
          "default": 1,
          "description": "Number of samples to generate"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "description": "A blurb of text describing what you do not wish to see in the output image"
        },
        {
          "name": "style_preset",
          "type": "string",
          "default": null,
          "description": "Guides the image model towards a particular style (e.g., 3d-model, analog-film, anime, etc.)"
        }
      ],
      "credit_costs": {
        "input_tokens": 0,
        "output_tokens": 0,
        "compute_cost": 750.0,
        "memory_cost": 0
      }
    },
    {
      "model_name": "stability-sd-1.6",
      "model_url": "",
      "description": "Flexible-resolution base model for image generation from Stability AI.",
      "supported_inference_type_strings": [
        "text_to_image"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "height",
          "type": "int",
          "default": 512,
          "description": "The height of the generated image"
        },
        {
          "name": "width",
          "type": "int",
          "default": 512,
          "description": "The width of the generated image"
        },
        {
          "name": "cfg_scale",
          "type": "float",
          "default": 7,
          "description": "CFG scale factor"
        },
        {
          "name": "sampler",
          "type": "string",
          "default": null,
          "description": "Sampler to use for generation (e.g., ddim, plms, k_euler, etc.)"
        },
        {
          "name": "steps",
          "type": "int",
          "default": 50,
          "description": "Number of steps for generation"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "description": "Random seed to use for generation"
        },
        {
          "name": "num_samples",
          "type": "int",
          "default": 1,
          "description": "Number of samples to generate"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "description": "A blurb of text describing what you do not wish to see in the output image"
        },
        {
          "name": "style_preset",
          "type": "string",
          "default": null,
          "description": "Guides the image model towards a particular style (e.g., 3d-model, analog-film, anime, etc.)"
        }
      ],
      "credit_costs": {
        "input_tokens": 0,
        "output_tokens": 0,
        "compute_cost": 400.0,
        "memory_cost": 0
      }
    },
    {
      "model_name": "whisper-large-v2",
      "model_url": "https://huggingface.co/openai/whisper-large-v2",
      "description": "Whisper large-v2 model for audio transcription",
      "input_fields": [
        {
          "name": "audio",
          "file_type": "audio",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "language",
          "type": "string",
          "default": "en",
          "description": "The target language for transcription"
        },
        {
          "name": "task",
          "type": "string",
          "default": "transcribe",
          "description": "The task to perform (transcribe or translate)"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.0,
          "description": "The temperature for sampling"
        }
      ],
      "credit_costs": {
        "input_tokens": 0,
        "output_tokens": 1.5,
        "compute_cost": 1.0,
        "memory_cost": 0.5
      }
    },
    {
      "model_name": "clip-interrogator",
      "model_url": "https://huggingface.co/pharma/ci-preprocess",
      "description": "CLIP Interrogator model for image-to-text generation",
      "input_fields": [
        {
          "name": "image",
          "file_type": "image",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "min_length",
          "type": "int",
          "default": 10,
          "description": "The minimum length of the generated text"
        },
        {
          "name": "max_length",
          "type": "int",
          "default": 100,
          "description": "The maximum length of the generated text"
        },
        {
          "name": "num_beams",
          "type": "int",
          "default": 4,
          "description": "The number of beams for beam search"
        }
      ],
      "credit_costs": {
        "input_tokens": 0,
        "output_tokens": 1.2,
        "compute_cost": 0.8,
        "memory_cost": 0.3
      }
    },
    {
      "model_name": "videocap-transformer",
      "model_url": "https://huggingface.co/ArhanK005/videocap-transformer",
      "description": "Videocap Transformer model for video captioning",
      "input_fields": [
        {
          "name": "video",
          "file_type": "video",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "max_length",
          "type": "int",
          "default": 100,
          "description": "The maximum length of the generated caption"
        },
        {
          "name": "num_beams",
          "type": "int",
          "default": 4,
          "description": "The number of beams for beam search"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 1.0,
          "description": "The temperature for sampling"
        }
      ],
      "credit_costs": {
        "input_tokens": 0,
        "output_tokens": 1.8,
        "compute_cost": 1.2,
        "memory_cost": 0.6
      }
    }
  ]
}