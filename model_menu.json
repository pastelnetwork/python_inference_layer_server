{
  "models": [
    {
      "model_name": "swiss_army_llama-Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M",
      "model_url": "https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf",
      "description": "Hermes 2 Pro is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house. This version excels at Function Calling, JSON Structured Outputs, and has improved on several other metrics, scoring 90% on function calling evaluation and 84% on structured JSON Output evaluation.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "embedding_document",
        "embedding_audio",
        "token_level_embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The grammar file used to restrict text generation (optional; default is to not use any grammar file). Examples: `json`, `list`"
        },
        {
          "name": "query_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "The query string used to perform semantic search on the embeddings (optional; default is to not perform any search)."
        },
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Identifies the subset of the stored embeddings to use in the semantic search (optional; default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "text_completion": {
          "input_tokens": 1.8,
          "output_tokens": 1.4,
          "compute_cost": 1.0,
          "memory_cost": 0.9
        },
        "embedding": {
          "input_tokens": 0.7,
          "compute_cost": 0.8,
          "memory_cost": 0.5
        },
        "token_level_embedding": {
          "input_tokens": 0.8,
          "compute_cost": 0.9,
          "memory_cost": 0.6
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.2,
          "total_sentences": 0.1,
          "query_string_included": 0.4,
          "compute_cost": 1.2,
          "memory_cost": 0.8
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.04,
          "query_string_included": 0.4,
          "compute_cost": 1.2,
          "memory_cost": 1.0
        }
      }
    },
    {
      "model_name": "swiss_army_llama-Meta-Llama-3-8B-Instruct.Q3_K_S",
      "model_url": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_S.gguf",
      "description": "The Meta-Llama-3-8B-Instruct model is designed for instruction-following tasks with optimized quantization for efficient computation.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "embedding_document",
        "embedding_audio",
        "token_level_embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The grammar file used to restrict text generation (optional; default is to not use any grammar file). Examples: `json`, `list`"
        },
        {
          "name": "query_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "The query string used to perform semantic search on the embeddings (optional; default is to not perform any search)."
        },
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Identifies the subset of the stored embeddings to use in the semantic search (optional; default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "text_completion": {
          "input_tokens": 1.5,
          "output_tokens": 1.1,
          "compute_cost": 0.8,
          "memory_cost": 0.7
        },
        "embedding": {
          "input_tokens": 0.5,
          "compute_cost": 0.6,
          "memory_cost": 0.4
        },
        "token_level_embedding": {
          "input_tokens": 0.6,
          "compute_cost": 0.7,
          "memory_cost": 0.5
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.1,
          "total_sentences": 0.05,
          "query_string_included": 0.3,
          "compute_cost": 0.9,
          "memory_cost": 0.6
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.02,
          "query_string_included": 0.3,
          "compute_cost": 1.0,
          "memory_cost": 0.8
        }
      }
    },
    {
      "model_name": "swiss_army_llama-Lexi-Llama-3-8B-Uncensored_Q5_K_M",
      "model_url": "https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored_Q5_K_M.gguf",
      "description": "The Lexi-Llama-3-8B-Uncensored, based on the powerful Meta Llama-3-8B model, is designed for uncensored text generation with a focus on creative and innovative content.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "embedding_document",
        "embedding_audio",
        "token_level_embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The grammar file used to restrict text generation (optional; default is to not use any grammar file). Examples: `json`, `list`"
        },
        {
          "name": "query_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "The query string used to perform semantic search on the embeddings (optional; default is to not perform any search)."
        },
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Identifies the subset of the stored embeddings to use in the semantic search (optional; default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "text_completion": {
          "input_tokens": 1.6,
          "output_tokens": 1.2,
          "compute_cost": 0.9,
          "memory_cost": 0.8
        },
        "embedding": {
          "input_tokens": 0.6,
          "compute_cost": 0.7,
          "memory_cost": 0.5
        },
        "token_level_embedding": {
          "input_tokens": 0.7,
          "compute_cost": 0.8,
          "memory_cost": 0.6
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.15,
          "total_sentences": 0.08,
          "query_string_included": 0.35,
          "compute_cost": 1.0,
          "memory_cost": 0.7
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.03,
          "query_string_included": 0.35,
          "compute_cost": 1.1,
          "memory_cost": 0.9
        }
      }
    },
    {
      "model_name": "swiss_army_llama-bge-m3-q8_0",
      "model_url": "https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf",
      "description": "BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It performs dense retrieval, multi-vector retrieval, and sparse retrieval, providing a unified model foundation for real-world IR applications.",
      "supported_inference_type_strings": [
        "embedding",
        "embedding_document",
        "embedding_audio",
        "token_level_embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "embeddings",
          "file_type": "json",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "query_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "The query string used to perform semantic search on the embeddings (optional; default is to not perform any search)."
        },
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Identifies the subset of the stored embeddings to use in the semantic search (optional; default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "embedding": {
          "input_tokens": 0.4,
          "compute_cost": 0.5,
          "memory_cost": 0.3
        },
        "token_level_embedding": {
          "input_tokens": 0.5,
          "compute_cost": 0.6,
          "memory_cost": 0.4
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.05,
          "total_sentences": 0.03,
          "query_string_included": 0.2,
          "compute_cost": 0.6,
          "memory_cost": 0.4
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.01,
          "query_string_included": 0.2,
          "compute_cost": 0.6,
          "memory_cost": 0.5
        }
      }
    },
    {
      "model_name": "openai-gpt-4o",
      "model_url": "",
      "description": "GPT-4o is OpenAI's most advanced multimodal model that’s faster and cheaper than GPT-4 Turbo with stronger vision capabilities. The model has 128K context and an October 2023 knowledge cutoff.",
      "supported_inference_type_strings": [
        "text_completion",
        "image_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": true
        },
        {
          "name": "input_image",
          "file_type": "image",
          "optional": true
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        },
        {
          "name": "generated_image",
          "file_type": "image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": {
          "text": 5.00,
          "image": 15.00
        }
      }
    },
    {
      "model_name": "openai-gpt-4-turbo",
      "model_url": "",
      "description": "GPT-4 Turbo is offered at 128K context with an April 2023 knowledge cutoff and basic support for vision.",
      "supported_inference_type_strings": [
        "text_completion",
        "image_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": true
        },
        {
          "name": "input_image",
          "file_type": "image",
          "optional": true
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        },
        {
          "name": "generated_image",
          "file_type": "image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate",
          "inference_types_parameter_applies_to": [
            "text_completion",
            "image_completion"
          ]
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling",
          "inference_types_parameter_applies_to": [
            "text_completion",
            "image_completion"
          ]
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate",
          "inference_types_parameter_applies_to": [
            "text_completion",
            "image_completion"
          ]
        }
      ],
      "credit_costs": {
        "api_based_pricing": {
          "text": 10.00,
          "image": 30.00
        }
      }
    },
    {
      "model_name": "openai-gpt-3.5-turbo-0125",
      "model_url": "",
      "description": "GPT-3.5 Turbo is our fast and inexpensive model for simpler tasks. gpt-3.5-turbo-0125 is the flagship model of this family, supports a 16K context window and is optimized for dialog.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 0.50
      }
    },
    {
      "model_name": "openai-gpt-3.5-turbo-instruct",
      "model_url": "",
      "description": "GPT-3.5 Turbo is OpenAI's fast and inexpensive model for simpler tasks. gpt-3.5-turbo-instruct is an Instruct model and only supports a 4K context window.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 0.50
      }
    },
    {
      "model_name": "openai-text-embedding-ada-002",
      "model_url": "",
      "description": "OpenAI's text embedding model. Provides high-quality embeddings for semantic search and clustering tasks.",
      "supported_inference_type_strings": [
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "embedding",
          "file_type": "embedding",
          "optional": false
        }
      ],
      "model_parameters": [],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "claude3-haiku",
      "model_url": "",
      "description": "The Claude3 Haiku model is a powerful language model developed by Anthropic. It offers advanced natural language processing capabilities and can be accessed through the Claude API.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "claude3-opus",
      "model_url": "",
      "description": "The Claude3 Opus model is a powerful language model developed by Anthropic. It offers advanced natural language processing capabilities and can be accessed through the Claude API.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "claude3-sonnet",
      "model_url": "",
      "description": "The Claude3 Sonnet model is a powerful language model developed by Anthropic. It offers advanced natural language processing capabilities and can be accessed through the Claude API.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama3-70b-8192",
      "model_url": "",
      "description": "LLaMA3 70b is a powerful language model developed by Meta with a context window of 8,192 tokens.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama3-8b-8192",
      "model_url": "",
      "description": "LLaMA3 8b is a powerful but compact language model developed by Meta with a context window of 8,192 tokens.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-mixtral-8x7b-32768",
      "model_url": "",
      "description": "Mixtral 8x7b is a powerful language model developed by Mistral with a context window of 32,768 tokens.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-gemma-7b-it",
      "model_url": "",
      "description": "Gemma 7b is a powerful language model developed by Google with a context window of 8,192 tokens.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-mistral-small-latest",
      "model_url": "",
      "description": "Mistral small is a powerful language model developed by Mistral AI.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-mistral-medium-latest",
      "model_url": "",
      "description": "Mistral medium is a powerful language model developed by Mistral AI.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-mistral-large-latest",
      "model_url": "",
      "description": "Mistral large is a powerful language model developed by Mistral AI.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-core",
      "model_url": "",
      "description": "Stable Image Core is the best quality achievable at high speed for text-to-image generation.",
      "supported_inference_type_strings": [
        "text_to_image",
        "creative_upscale"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "aspect_ratio",
          "type": "string",
          "default": "1:1",
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "The aspect ratio of the generated image"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "Random seed to use for generation"
        },
        {
          "name": "style_preset",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "Guides the image model towards a particular style (e.g., 3d-model, analog-film, anime, etc.)"
        },
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "The format of the generated image"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "A blurb of text describing what you do not wish to see in the output image"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-sdxl-1.0",
      "model_url": "",
      "description": "The standard base model for image generation from Stability AI.",
      "supported_inference_type_strings": [
        "text_to_image"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "height",
          "type": "int",
          "default": 512,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "The height of the generated image"
        },
        {
          "name": "width",
          "type": "int",
          "default": 512,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "The width of the generated image"
        },
        {
          "name": "cfg_scale",
          "type": "float",
          "default": 7,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "CFG scale factor"
        },
        {
          "name": "sampler",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "Sampler to use for generation (e.g., ddim, plms, k_euler, etc.)"
        },
        {
          "name": "steps",
          "type": "int",
          "default": 50,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "Number of steps for generation"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "Random seed to use for generation"
        },
        {
          "name": "num_samples",
          "type": "int",
          "default": 1,
          "description": "Number of samples to generate"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "A blurb of text describing what you do not wish to see in the output image"
        },
        {
          "name": "style_preset",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "Guides the image model towards a particular style (e.g., 3d-model, analog-film, anime, etc.)"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-sd-1.6",
      "model_url": "",
      "description": "Flexible-resolution base model for image generation from Stability AI.",
      "supported_inference_type_strings": [
        "text_to_image"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "height",
          "type": "int",
          "default": 512,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "The height of the generated image"
        },
        {
          "name": "width",
          "type": "int",
          "default": 512,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "The width of the generated image"
        },
        {
          "name": "cfg_scale",
          "type": "float",
          "default": 7,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "CFG scale factor"
        },
        {
          "name": "sampler",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "Sampler to use for generation (e.g., ddim, plms, k_euler, etc.)"
        },
        {
          "name": "steps",
          "type": "int",
          "default": 50,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "Number of steps for generation"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "Random seed to use for generation"
        },
        {
          "name": "num_samples",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "Number of samples to generate"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "A blurb of text describing what you do not wish to see in the output image"
        },
        {
          "name": "style_preset",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": "['text_to_image']",
          "description": "Guides the image model towards a particular style (e.g., 3d-model, analog-film, anime, etc.)"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/auto",
      "model_url": "",
      "description": "OpenRouter's auto-routing model that automatically selects the best available model for the given task.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/google/palm-2-chat-bison",
      "model_url": "",
      "description": "Google's PaLM 2 Chat Bison model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/mistralai/mistral-7b-instruct",
      "model_url": "",
      "description": "Mistral's Mistral 7B Instruct model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "whisper-large-v2",
      "model_url": "https://huggingface.co/openai/whisper-large-v2",
      "description": "Whisper large-v2 model for audio transcription",
      "input_fields": [
        {
          "name": "audio",
          "file_type": "audio",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "language",
          "type": "string",
          "default": "en",
          "description": "The target language for transcription"
        },
        {
          "name": "task",
          "type": "string",
          "default": "transcribe",
          "description": "The task to perform (transcribe or translate)"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.0,
          "description": "The temperature for sampling"
        }
      ],
      "credit_costs": {
        "input_tokens": 0,
        "output_tokens": 1.5,
        "compute_cost": 1.0,
        "memory_cost": 0.5
      }
    },
    {
      "model_name": "clip-interrogator",
      "model_url": "https://huggingface.co/pharma/ci-preprocess",
      "description": "CLIP Interrogator model for image-to-text generation",
      "input_fields": [
        {
          "name": "image",
          "file_type": "image",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "min_length",
          "type": "int",
          "default": 10,
          "description": "The minimum length of the generated text"
        },
        {
          "name": "max_length",
          "type": "int",
          "default": 100,
          "description": "The maximum length of the generated text"
        },
        {
          "name": "num_beams",
          "type": "int",
          "default": 4,
          "description": "The number of beams for beam search"
        }
      ],
      "credit_costs": {
        "input_tokens": 0,
        "output_tokens": 1.2,
        "compute_cost": 0.8,
        "memory_cost": 0.3
      }
    },
    {
      "model_name": "videocap-transformer",
      "model_url": "https://huggingface.co/ArhanK005/videocap-transformer",
      "description": "Videocap Transformer model for video captioning",
      "input_fields": [
        {
          "name": "video",
          "file_type": "video",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "max_length",
          "type": "int",
          "default": 100,
          "description": "The maximum length of the generated caption"
        },
        {
          "name": "num_beams",
          "type": "int",
          "default": 4,
          "description": "The number of beams for beam search"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 1.0,
          "description": "The temperature for sampling"
        }
      ],
      "credit_costs": {
        "input_tokens": 0,
        "output_tokens": 1.8,
        "compute_cost": 1.2,
        "memory_cost": 0.6
      }
    }
  ]
}