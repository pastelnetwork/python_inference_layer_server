{
  "models": [
    {
      "model_name": "swiss_army_llama-Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M",
      "model_url": "https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf",
      "description": "Hermes 2 Pro is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house. This version excels at Function Calling, JSON Structured Outputs, and has improved on several other metrics, scoring 90% on function calling evaluation and 84% on structured JSON Output evaluation.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "token_level_embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "description": "The grammar file used to restrict text generation (optional; default is to not use any grammar file). Examples: `json`, `list`"
        }
      ],
      "credit_costs": {
        "input_tokens": 1.5,
        "output_tokens": 1.1,
        "compute_cost": 0.8,
        "memory_cost": 0.7
      }
    },
    {
      "model_name": "swiss_army_llama-Meta-Llama-3-8B-Instruct.Q3_K_S",
      "model_url": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_S.gguf",
      "description": "The Meta-Llama-3-8B-Instruct model is designed for instruction-following tasks with optimized quantization for efficient computation.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "token_level_embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "description": "The grammar file used to restrict text generation (optional; default is to not use any grammar file). Examples: `json`, `list`"
        }
      ],
      "credit_costs": {
        "input_tokens": 1.5,
        "output_tokens": 1.1,
        "compute_cost": 0.5,
        "memory_cost": 0.5
      }
    },
    {
      "model_name": "swiss_army_llama-Lexi-Llama-3-8B-Uncensored_Q5_K_M",
      "model_url": "https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored_Q5_K_M.gguf",
      "description": "The Lexi-Llama-3-8B-Uncensored, based on the powerful Meta Llama-3-8B model, is designed for uncensored text generation with a focus on creative and innovative content.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "token_level_embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "description": "The grammar file used to restrict text generation (optional; default is to not use any grammar file). Examples: `json`, `list`"
        }
      ],
      "credit_costs": {
        "input_tokens": 1.5,
        "output_tokens": 1.1,
        "compute_cost": 0.8,
        "memory_cost": 0.8
      }
    },    
    {
      "model_name": "swiss_army_llama-bge-m3-q8_0",
      "model_url": "https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf",
      "description": "BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It performs dense retrieval, multi-vector retrieval, and sparse retrieval, providing a unified model foundation for real-world IR applications.",
      "supported_inference_type_strings": [
        "embedding",
        "token_level_embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "embeddings",
          "file_type": "json",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "embedding_size",
          "type": "int",
          "default": 512,
          "description": "The size of the generated embeddings"
        }
      ],
      "credit_costs": {
        "input_tokens": 0.8,
        "output_tokens": 0.0,
        "compute_cost": 0.1,
        "memory_cost": 0.1
      }
    },
    {
      "model_name": "openai-gpt-4-0125-preview",
      "model_url": "",
      "description": "OpenAI's GPT-4 model with a preview API. Capable of handling more complex tasks and instructions compared to GPT-3.5.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openai-gpt-4-turbo-preview",
      "model_url": "",
      "description": "OpenAI's GPT-4 model with a turbo preview API. Capable of generating high-quality text at a faster speed compared to the standard GPT-4 preview model.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openai-gpt-4-vision-preview",
      "model_url": "",
      "description": "OpenAI's GPT-4 model with vision capabilities. Capable of analyzing and generating responses based on image inputs.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_image",
          "file_type": "image",
          "optional": false
        },
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": true
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openai-gpt-3.5-turbo-0125",
      "model_url": "",
      "description": "OpenAI's GPT-3.5 Turbo model with a specific version (0125). Provides a good balance between performance and cost-effectiveness.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openai-text-embedding-ada-002",
      "model_url": "",
      "description": "OpenAI's text embedding model. Provides high-quality embeddings for semantic search and clustering tasks.",
      "supported_inference_type_strings": [
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "embedding",
          "file_type": "embedding",
          "optional": false
        }
      ],
      "model_parameters": [],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "claude3-haiku",
      "model_url": "",
      "description": "The Claude3 Haiku model is a powerful language model developed by Anthropic. It offers advanced natural language processing capabilities and can be accessed through the Claude API.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "claude3-opus",
      "model_url": "",
      "description": "The Claude3 Opus model is a powerful language model developed by Anthropic. It offers advanced natural language processing capabilities and can be accessed through the Claude API.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "claude3-sonnet",
      "model_url": "",
      "description": "The Claude3 Sonnet model is a powerful language model developed by Anthropic. It offers advanced natural language processing capabilities and can be accessed through the Claude API.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama3-70b-8192",
      "model_url": "",
      "description": "LLaMA3 70b is a powerful language model developed by Meta with a context window of 8,192 tokens.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama3-8b-8192",
      "model_url": "",
      "description": "LLaMA3 8b is a powerful but compact language model developed by Meta with a context window of 8,192 tokens.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-mixtral-8x7b-32768",
      "model_url": "",
      "description": "Mixtral 8x7b is a powerful language model developed by Mistral with a context window of 32,768 tokens.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-gemma-7b-it",
      "model_url": "",
      "description": "Gemma 7b is a powerful language model developed by Google with a context window of 8,192 tokens.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-mistral-small-latest",
      "model_url": "",
      "description": "Mistral small is a powerful language model developed by Mistral AI.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-mistral-medium-latest",
      "model_url": "",
      "description": "Mistral medium is a powerful language model developed by Mistral AI.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-mistral-large-latest",
      "model_url": "",
      "description": "Mistral large is a powerful language model developed by Mistral AI.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-core",
      "model_url": "",
      "description": "Stable Image Core is the best quality achievable at high speed for text-to-image generation.",
      "supported_inference_type_strings": [
        "text_to_image",
        "creative_upscale"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "aspect_ratio",
          "type": "string",
          "default": "1:1",
          "description": "The aspect ratio of the generated image"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "description": "Random seed to use for generation"
        },
        {
          "name": "style_preset",
          "type": "string",
          "default": null,
          "description": "Guides the image model towards a particular style (e.g., 3d-model, analog-film, anime, etc.)"
        },
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "description": "The format of the generated image"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "description": "A blurb of text describing what you do not wish to see in the output image"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-sdxl-1.0",
      "model_url": "",
      "description": "The standard base model for image generation from Stability AI.",
      "supported_inference_type_strings": [
        "text_to_image"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "height",
          "type": "int",
          "default": 512,
          "description": "The height of the generated image"
        },
        {
          "name": "width",
          "type": "int",
          "default": 512,
          "description": "The width of the generated image"
        },
        {
          "name": "cfg_scale",
          "type": "float",
          "default": 7,
          "description": "CFG scale factor"
        },
        {
          "name": "sampler",
          "type": "string",
          "default": null,
          "description": "Sampler to use for generation (e.g., ddim, plms, k_euler, etc.)"
        },
        {
          "name": "steps",
          "type": "int",
          "default": 50,
          "description": "Number of steps for generation"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "description": "Random seed to use for generation"
        },
        {
          "name": "num_samples",
          "type": "int",
          "default": 1,
          "description": "Number of samples to generate"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "description": "A blurb of text describing what you do not wish to see in the output image"
        },
        {
          "name": "style_preset",
          "type": "string",
          "default": null,
          "description": "Guides the image model towards a particular style (e.g., 3d-model, analog-film, anime, etc.)"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-sd-1.6",
      "model_url": "",
      "description": "Flexible-resolution base model for image generation from Stability AI.",
      "supported_inference_type_strings": [
        "text_to_image"
      ],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "height",
          "type": "int",
          "default": 512,
          "description": "The height of the generated image"
        },
        {
          "name": "width",
          "type": "int",
          "default": 512,
          "description": "The width of the generated image"
        },
        {
          "name": "cfg_scale",
          "type": "float",
          "default": 7,
          "description": "CFG scale factor"
        },
        {
          "name": "sampler",
          "type": "string",
          "default": null,
          "description": "Sampler to use for generation (e.g., ddim, plms, k_euler, etc.)"
        },
        {
          "name": "steps",
          "type": "int",
          "default": 50,
          "description": "Number of steps for generation"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "description": "Random seed to use for generation"
        },
        {
          "name": "num_samples",
          "type": "int",
          "default": 1,
          "description": "Number of samples to generate"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "description": "A blurb of text describing what you do not wish to see in the output image"
        },
        {
          "name": "style_preset",
          "type": "string",
          "default": null,
          "description": "Guides the image model towards a particular style (e.g., 3d-model, analog-film, anime, etc.)"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/auto",
      "model_url": "",
      "description": "OpenRouter's auto-routing model that automatically selects the best available model for the given task.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/openai/gpt-4-turbo",
      "model_url": "",
      "description": "OpenAI's GPT-4 Turbo model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/google/palm-2-chat-bison",
      "model_url": "",
      "description": "Google's PaLM 2 Chat Bison model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/anthropic/claude-3-haiku",
      "model_url": "",
      "description": "Anthropic's Claude 3 Haiku model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/meta-llama/llama-2-70b-chat",
      "model_url": "",
      "description": "Meta's LLaMA v2 70B Chat model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/mistralai/mistral-7b-instruct",
      "model_url": "",
      "description": "Mistral's Mistral 7B Instruct model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/gryphe/mythomax-l2-13b",
      "model_url": "",
      "description": "Gryphe's MythoMax 13B model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/openchat/openchat-7b",
      "model_url": "",
      "description": "OpenChat 3.5 model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/google/palm-2-codechat-bison",
      "model_url": "",
      "description": "Google's PaLM 2 Code Chat Bison model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/anthropic/claude-3-sonnet",
      "model_url": "",
      "description": "Anthropic's Claude 3 Sonnet model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/anthropic/claude-3-opus",
      "model_url": "",
      "description": "Anthropic's Claude 3 Opus model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/nousresearch/nous-hermes-llama2-13b",
      "model_url": "",
      "description": "Nous Research's Hermes 13B model based on LLaMA 2 accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/nousresearch/nous-capybara-7b",
      "model_url": "",
      "description": "Nous Research's Capybara 7B model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/meta-llama/codellama-34b-instruct",
      "model_url": "",
      "description": "Meta's CodeLLaMA 34B Instruct model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/codellama/codellama-70b-instruct",
      "model_url": "",
      "description": "CodeLLaMA 70B Instruct model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/phind/phind-codellama-34b",
      "model_url": "",
      "description": "Phind's CodeLLaMA 34B v2 model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/teknium/openhermes-2-mistral-7b",
      "model_url": "",
      "description": "OpenHermes 2 Mistral 7B model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/teknium/openhermes-2.5-mistral-7b",
      "model_url": "",
      "description": "OpenHermes 2.5 Mistral 7B model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/undi95/remm-slerp-l2-13b",
      "model_url": "",
      "description": "ReMM SLERP 13B model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/undi95/toppy-m-7b",
      "model_url": "",
      "description": "Toppy M 7B model accessed through OpenRouter.",
      "supported_inference_type_strings": [
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "description": "The temperature for sampling"
        },
        {
          "name": "number_of_completions_to_generate",
          "type": "int",
          "default": 1,
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "whisper-large-v2",
      "model_url": "https://huggingface.co/openai/whisper-large-v2",
      "description": "Whisper large-v2 model for audio transcription",
      "input_fields": [
        {
          "name": "audio",
          "file_type": "audio",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "language",
          "type": "string",
          "default": "en",
          "description": "The target language for transcription"
        },
        {
          "name": "task",
          "type": "string",
          "default": "transcribe",
          "description": "The task to perform (transcribe or translate)"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.0,
          "description": "The temperature for sampling"
        }
      ],
      "credit_costs": {
        "input_tokens": 0,
        "output_tokens": 1.5,
        "compute_cost": 1.0,
        "memory_cost": 0.5
      }
    },
    {
      "model_name": "clip-interrogator",
      "model_url": "https://huggingface.co/pharma/ci-preprocess",
      "description": "CLIP Interrogator model for image-to-text generation",
      "input_fields": [
        {
          "name": "image",
          "file_type": "image",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "min_length",
          "type": "int",
          "default": 10,
          "description": "The minimum length of the generated text"
        },
        {
          "name": "max_length",
          "type": "int",
          "default": 100,
          "description": "The maximum length of the generated text"
        },
        {
          "name": "num_beams",
          "type": "int",
          "default": 4,
          "description": "The number of beams for beam search"
        }
      ],
      "credit_costs": {
        "input_tokens": 0,
        "output_tokens": 1.2,
        "compute_cost": 0.8,
        "memory_cost": 0.3
      }
    },
    {
      "model_name": "videocap-transformer",
      "model_url": "https://huggingface.co/ArhanK005/videocap-transformer",
      "description": "Videocap Transformer model for video captioning",
      "input_fields": [
        {
          "name": "video",
          "file_type": "video",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "max_length",
          "type": "int",
          "default": 100,
          "description": "The maximum length of the generated caption"
        },
        {
          "name": "num_beams",
          "type": "int",
          "default": 4,
          "description": "The number of beams for beam search"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 1.0,
          "description": "The temperature for sampling"
        }
      ],
      "credit_costs": {
        "input_tokens": 0,
        "output_tokens": 1.8,
        "compute_cost": 1.2,
        "memory_cost": 0.6
      }
    }
  ]
}