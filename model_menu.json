{
  "models": [
    {
      "model_name": "claude3.5-sonnet",
      "model_url": "",
      "description": "Most intelligent Claude model optimized for complex tasks. Features 200K context window, support for images, PDFs, JSON output, structured responses, prompt caching, and embeddings.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "ask_question_about_an_image",
        "analyze_document"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "inference_types_parameter_applies_to": [
            "text_completion",
            "embedding"
          ],
          "optional": false
        },
        {
          "name": "image",
          "file_type": "base64",
          "inference_types_parameter_applies_to": [
            "ask_question_about_an_image"
          ],
          "optional": false,
          "constraints": {
            "max_file_size_mb": 100,
            "supported_formats": ["png", "jpeg", "jpg", "gif", "webp", "heic"],
            "max_dimension": 4096,
            "min_dimension": 16
          }
        },
        {
          "name": "question",
          "file_type": "text",
          "inference_types_parameter_applies_to": [
            "ask_question_about_an_image"
          ],
          "optional": false
        },
        {
          "name": "document",
          "file_type": "base64",
          "inference_types_parameter_applies_to": ["analyze_document"],
          "optional": false,
          "constraints": {
            "max_file_size_mb": 32,
            "supported_formats": ["pdf"],
            "max_pages": 100
          }
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        },
        {
          "name": "embedding",
          "file_type": "embedding",
          "optional": false,
          "dimensions": 1536
        }
      ],
      "model_parameters": [
        {
          "name": "system_message",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image",
            "analyze_document"
          ],
          "description": "System message to set context and behavior for the model"
        },
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "minimum": 1,
          "maximum": 8192,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image",
            "analyze_document"
          ],
          "description": "Maximum number of tokens to generate in response"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "minimum": 0.0,
          "maximum": 1.0,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image",
            "analyze_document"
          ],
          "description": "Controls randomness in output. Higher values make output more random, lower values more focused"
        },
        {
          "name": "top_p",
          "type": "float",
          "default": 1.0,
          "minimum": 0.0,
          "maximum": 1.0,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image",
            "analyze_document"
          ],
          "description": "Nucleus sampling parameter. Lower values lead to more focused output"
        },
        {
          "name": "top_k",
          "type": "integer",
          "default": null,
          "minimum": 1,
          "maximum": 500,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image",
            "analyze_document"
          ],
          "description": "Only sample from the top K options for each token. Lower values lead to more focused output"
        },
        {
          "name": "response_format",
          "type": "object",
          "default": null,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image",
            "analyze_document"
          ],
          "description": "Controls the format of model responses",
          "properties": {
            "type": {
              "type": "string",
              "enum": ["text", "json_object", "json_schema"],
              "description": "Response format type"
            },
            "schema": {
              "type": "object",
              "description": "JSON Schema for structured output validation"
            }
          }
        },
        {
          "name": "cache_control",
          "type": "object",
          "default": null,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image",
            "analyze_document"
          ],
          "description": "Controls prompt caching behavior",
          "properties": {
            "type": {
              "type": "string",
              "enum": ["ephemeral"],
              "description": "Cache type - currently only 'ephemeral' (5-minute cache) is supported"
            }
          }
        },
        {
          "name": "metadata",
          "type": "object",
          "default": null,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image",
            "analyze_document"
          ],
          "description": "Additional metadata to include with the request"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "claude3.5-haiku",
      "model_url": "",
      "description": "Fastest Claude model optimized for rapid responses. Features 200K context window, support for prompt caching, JSON output, and structured responses. Does not support image or PDF analysis.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "inference_types_parameter_applies_to": [
            "text_completion",
            "embedding"
          ],
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        },
        {
          "name": "embedding",
          "file_type": "embedding",
          "optional": false,
          "dimensions": 1536
        }
      ],
      "model_parameters": [
        {
          "name": "system_message",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "System message to set context and behavior for the model"
        },
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "minimum": 1,
          "maximum": 8192,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Maximum number of tokens to generate in response"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "minimum": 0.0,
          "maximum": 1.0,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls randomness in output. Higher values make output more random, lower values more focused"
        },
        {
          "name": "top_p",
          "type": "float",
          "default": 1.0,
          "minimum": 0.0,
          "maximum": 1.0,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Nucleus sampling parameter. Lower values lead to more focused output"
        },
        {
          "name": "top_k",
          "type": "integer",
          "default": null,
          "minimum": 1,
          "maximum": 500,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Only sample from the top K options for each token. Lower values lead to more focused output"
        },
        {
          "name": "response_format",
          "type": "object",
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls the format of model responses",
          "properties": {
            "type": {
              "type": "string",
              "enum": ["text", "json_object", "json_schema"],
              "description": "Response format type"
            },
            "schema": {
              "type": "object",
              "description": "JSON Schema for structured output validation"
            }
          }
        },
        {
          "name": "cache_control",
          "type": "object",
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls prompt caching behavior",
          "properties": {
            "type": {
              "type": "string",
              "enum": ["ephemeral"],
              "description": "Cache type - currently only 'ephemeral' (5-minute cache) is supported"
            }
          }
        },
        {
          "name": "metadata",
          "type": "object",
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Additional metadata to include with the request"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "claude3-opus",
      "model_url": "",
      "description": "Most powerful Claude model with top-tier intelligence and multilingual capabilities. Features 200K context window, support for images, PDFs, JSON output, structured responses, prompt caching, and embeddings.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "ask_question_about_an_image",
        "analyze_document"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "inference_types_parameter_applies_to": [
            "text_completion",
            "embedding"
          ],
          "optional": false
        },
        {
          "name": "image",
          "file_type": "base64",
          "inference_types_parameter_applies_to": [
            "ask_question_about_an_image"
          ],
          "optional": false,
          "constraints": {
            "max_file_size_mb": 100,
            "supported_formats": ["png", "jpeg", "jpg", "gif", "webp", "heic"],
            "max_dimension": 4096,
            "min_dimension": 16
          }
        },
        {
          "name": "question",
          "file_type": "text",
          "inference_types_parameter_applies_to": [
            "ask_question_about_an_image"
          ],
          "optional": false
        },
        {
          "name": "document",
          "file_type": "base64",
          "inference_types_parameter_applies_to": ["analyze_document"],
          "optional": false,
          "constraints": {
            "max_file_size_mb": 32,
            "supported_formats": ["pdf"],
            "max_pages": 100
          }
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        },
        {
          "name": "embedding",
          "file_type": "embedding",
          "optional": false,
          "dimensions": 1536
        }
      ],
      "model_parameters": [
        {
          "name": "system_message",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image",
            "analyze_document"
          ],
          "description": "System message to set context and behavior for the model"
        },
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "minimum": 1,
          "maximum": 4096,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image",
            "analyze_document"
          ],
          "description": "Maximum number of tokens to generate in response"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "minimum": 0.0,
          "maximum": 1.0,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image",
            "analyze_document"
          ],
          "description": "Controls randomness in output. Higher values make output more random, lower values more focused"
        },
        {
          "name": "top_p",
          "type": "float",
          "default": 1.0,
          "minimum": 0.0,
          "maximum": 1.0,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image",
            "analyze_document"
          ],
          "description": "Nucleus sampling parameter. Lower values lead to more focused output"
        },
        {
          "name": "top_k",
          "type": "integer",
          "default": null,
          "minimum": 1,
          "maximum": 500,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image",
            "analyze_document"
          ],
          "description": "Only sample from the top K options for each token. Lower values lead to more focused output"
        },
        {
          "name": "response_format",
          "type": "object",
          "default": null,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image",
            "analyze_document"
          ],
          "description": "Controls the format of model responses",
          "properties": {
            "type": {
              "type": "string",
              "enum": ["text", "json_object", "json_schema"],
              "description": "Response format type"
            },
            "schema": {
              "type": "object",
              "description": "JSON Schema for structured output validation"
            }
          }
        },
        {
          "name": "cache_control",
          "type": "object",
          "default": null,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image",
            "analyze_document"
          ],
          "description": "Controls prompt caching behavior",
          "properties": {
            "type": {
              "type": "string",
              "enum": ["ephemeral"],
              "description": "Cache type - currently only 'ephemeral' (5-minute cache) is supported"
            }
          }
        },
        {
          "name": "metadata",
          "type": "object",
          "default": null,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image",
            "analyze_document"
          ],
          "description": "Additional metadata to include with the request"
        },
        {
          "name": "stop_sequences",
          "type": "array",
          "items": {
            "type": "string"
          },
          "default": null,
          "maxItems": 4,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image",
            "analyze_document"
          ],
          "description": "Custom stop sequences to end generation"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "deepseek-chat",
      "model_url": "",
      "description": "DeepSeek-V3 chat model with 671B MoE parameters (37B activated). Features context caching, JSON output, and function calling capabilities. Maximum context window of 128K tokens.",
      "supported_inference_type_strings": ["text_completion"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "max_tokens",
          "type": "int",
          "default": 4096,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate, between 1 and 8192 [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 1.0,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "What sampling temperature to use, between 0 and 2 [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        },
        {
          "name": "response_format",
          "type": "string",
          "default": "text",
          "options": ["text", "json_object"],
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The format of the response [Optional]"
        },
        {
          "name": "tools",
          "type": "object",
          "default": null,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Function calling definitions [Optional]"
        },
        {
          "name": "tool_choice",
          "type": "string",
          "default": "none",
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Controls function calling behavior [Optional]"
        },
        {
          "name": "system_message",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "System message for setting context [Optional]"
        },
        {
          "name": "frequency_penalty",
          "type": "float",
          "default": 0,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Number between -2.0 and 2.0 to penalize token frequency [Optional]"
        },
        {
          "name": "presence_penalty",
          "type": "float",
          "default": 0,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Number between -2.0 and 2.0 to penalize token presence [Optional]"
        },
        {
          "name": "top_p",
          "type": "float",
          "default": 1.0,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Alternative to temperature for nucleus sampling, between 0 and 1 [Optional]"
        },
        {
          "name": "logprobs",
          "type": "boolean",
          "default": false,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Whether to return log probabilities of output tokens [Optional]"
        },
        {
          "name": "top_logprobs",
          "type": "int",
          "default": null,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Number of most likely tokens (0-20) to return with probabilities [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "swiss_army_llama-Llama-3.1-8B-Lexi-Uncensored_V2_Q5",
      "model_url": "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/blob/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf",
      "description": "The Llama-3.1-8B-Lexi-Uncensored V2, based on the powerful Meta Llama-3.1-8B model, is designed for uncensored text generation with a focus on creative and innovative content.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Grammar File String: The grammar file used to restrict text generation [Optional] (Default is to not use any grammar file. Examples: `json`, `list`)"
        },
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "text_completion": {
          "input_tokens": 1.6,
          "output_tokens": 1.2,
          "compute_cost": 0.9,
          "memory_cost": 0.8
        },
        "embedding": {
          "input_tokens": 0.6,
          "compute_cost": 0.7,
          "memory_cost": 0.5
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.15,
          "total_sentences": 0.08,
          "query_string_included": 0.35,
          "compute_cost": 1.0,
          "memory_cost": 0.7
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.03,
          "query_string_included": 0.35,
          "compute_cost": 1.1,
          "memory_cost": 0.9
        }
      }
    },
    {
      "model_name": "swiss_army_llama-Hermes-3-Llama-3.1-8B.Q4_K_M",
      "model_url": "https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/blob/main/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf",
      "description": "Hermes 2 is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house. This version excels at Function Calling, JSON Structured Outputs, and has improved on several other metrics, scoring 90% on function calling evaluation and 84% on structured JSON Output evaluation.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Grammar File String: The grammar file used to restrict text generation [Optional] (Default is to not use any grammar file. Examples: `json`, `list`)"
        },
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "text_completion": {
          "input_tokens": 1.8,
          "output_tokens": 1.4,
          "compute_cost": 1.0,
          "memory_cost": 0.9
        },
        "embedding": {
          "input_tokens": 0.7,
          "compute_cost": 0.8,
          "memory_cost": 0.5
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.2,
          "total_sentences": 0.1,
          "query_string_included": 0.4,
          "compute_cost": 1.2,
          "memory_cost": 0.8
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.5,
          "query_string_included": 1.5,
          "compute_cost": 1.2,
          "memory_cost": 1.0
        }
      }
    },
    {
      "model_name": "swiss_army_llama-Meta-Llama-3.1-8B-Instruct-Q5_K_M",
      "model_url": "https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
      "description": "The Meta-Llama-3.1-8B-Instruct model is designed for instruction-following tasks with optimized quantization for efficient computation.",
      "supported_inference_type_strings": [
        "text_completion",
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        },
        {
          "name": "grammar_file_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Grammar File String: The grammar file used to restrict text generation [Optional] (Default is to not use any grammar file. Examples: `json`, `list`)"
        },
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "text_completion": {
          "input_tokens": 1.5,
          "output_tokens": 1.1,
          "compute_cost": 0.8,
          "memory_cost": 0.7
        },
        "embedding": {
          "input_tokens": 0.5,
          "compute_cost": 0.6,
          "memory_cost": 0.4
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.1,
          "total_sentences": 0.05,
          "query_string_included": 0.3,
          "compute_cost": 0.9,
          "memory_cost": 0.6
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.02,
          "query_string_included": 0.3,
          "compute_cost": 1.0,
          "memory_cost": 0.8
        }
      }
    },
    {
      "model_name": "swiss_army_llama-llava-llama-3-8b-v1_1-int4",
      "model_url": "https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf/resolve/main/llava-llama-3-8b-v1_1-int4.gguf",
      "description": "The LLaVA-LLaMA-3-8B-v1.1 model is a powerful language model developed by XTuner; it allows you to ask questions about images and get answers in a conversational manner.",
      "supported_inference_type_strings": ["ask_question_about_an_image"],
      "input_fields": [
        {
          "name": "image_data",
          "file_type": "base64",
          "optional": false
        },
        {
          "name": "question",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "ask_question_about_an_image": {
          "input_tokens": 1.5,
          "output_tokens": 1.1,
          "compute_cost": 1.2,
          "memory_cost": 1.0
        }
      }
    },
    {
      "model_name": "swiss_army_llama-bge-m3-q8_0",
      "model_url": "https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf",
      "description": "BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It performs dense retrieval, multi-vector retrieval, and sparse retrieval, providing a unified model foundation for real-world IR applications.",
      "supported_inference_type_strings": [
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "embeddings",
          "file_type": "json",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "embedding": {
          "input_tokens": 0.4,
          "compute_cost": 0.5,
          "memory_cost": 0.3
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.05,
          "total_sentences": 0.03,
          "query_string_included": 0.2,
          "compute_cost": 0.6,
          "memory_cost": 0.4
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.01,
          "query_string_included": 0.2,
          "compute_cost": 0.6,
          "memory_cost": 0.5
        }
      }
    },
    {
      "model_name": "swiss_army_llama-nomic-embed-text-v1.5.Q6_K",
      "model_url": "https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF/resolve/main/nomic-embed-text-v1.5.Q6_K.gguf",
      "description": "Nomic Embed Text v1.5 is a powerful language model developed by Nomic AI. It provides high-quality embeddings for semantic search and clustering tasks.",
      "supported_inference_type_strings": [
        "embedding",
        "embedding_document",
        "embedding_audio"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "embeddings",
          "file_type": "json",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "corpus_identifier_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['embedding_document', 'embedding_audio']",
          "description": "Corpus Identifier String: Identifies the subset of the stored embeddings to use in the semantic search [Optional] (Default is to use just the embeddings from the uploaded document or audio file transcript)."
        }
      ],
      "credit_costs": {
        "embedding": {
          "input_tokens": 0.2,
          "compute_cost": 0.2,
          "memory_cost": 0.1
        },
        "embedding_document": {
          "average_tokens_per_sentence": 0.05,
          "total_sentences": 0.02,
          "query_string_included": 0.2,
          "compute_cost": 0.4,
          "memory_cost": 0.3
        },
        "embedding_audio": {
          "audio_file_length_in_seconds": 0.01,
          "query_string_included": 0.2,
          "compute_cost": 0.5,
          "memory_cost": 0.4
        }
      }
    },
    {
      "model_name": "openai-gpt-4o",
      "model_url": "",
      "description": "OpenAI's most advanced multimodal model that's faster and cheaper than GPT-4 Turbo with stronger vision capabilities. The model has 128K context and an October 2023 knowledge cutoff.",
      "supported_inference_type_strings": [
        "text_completion",
        "ask_question_about_an_image"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "tools",
          "type": "array",
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Array of functions the model may call",
          "items": {
            "type": "object",
            "required": ["type", "function"],
            "properties": {
              "type": {
                "type": "string",
                "enum": ["function"],
                "description": "Type of tool - currently only 'function' is supported"
              },
              "function": {
                "type": "object",
                "required": ["name", "parameters"],
                "properties": {
                  "name": {
                    "type": "string",
                    "description": "Name of the function to be called"
                  },
                  "description": {
                    "type": "string",
                    "description": "Description of when the function should be used"
                  },
                  "parameters": {
                    "type": "object",
                    "description": "JSON Schema defining the function parameters",
                    "required": ["type", "properties"],
                    "properties": {
                      "type": {
                        "type": "string",
                        "enum": ["object"],
                        "description": "Must be 'object' for function parameters"
                      },
                      "properties": {
                        "type": "object",
                        "description": "Function parameters schema"
                      },
                      "required": {
                        "type": "array",
                        "items": {
                          "type": "string"
                        },
                        "description": "Array of required parameter names"
                      }
                    }
                  },
                  "strict": {
                    "type": "boolean",
                    "default": false,
                    "description": "Enable Structured Outputs for strict JSON schema validation"
                  }
                }
              }
            }
          }
        },
        {
          "name": "tool_choice",
          "type": "object",
          "default": "auto",
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls which (if any) tool is called by the model",
          "oneOf": [
            {
              "type": "string",
              "enum": ["none", "auto", "required"]
            },
            {
              "type": "object",
              "required": ["type", "function"],
              "properties": {
                "type": {
                  "type": "string",
                  "enum": ["function"]
                },
                "function": {
                  "type": "object",
                  "required": ["name"],
                  "properties": {
                    "name": {
                      "type": "string",
                      "description": "Name of the function to force the model to use"
                    }
                  }
                }
              }
            }
          ]
        },
        {
          "name": "parallel_tool_calls",
          "type": "boolean",
          "default": true,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Whether to allow multiple tool calls in parallel. Set to false when using strict schema validation"
        },
        {
          "name": "response_format",
          "type": "object",
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls the format of model responses",
          "properties": {
            "type": {
              "type": "string",
              "enum": ["text", "json_object", "json_schema"],
              "description": "Response format type"
            },
            "schema": {
              "type": "object",
              "description": "JSON Schema for structured output validation"
            }
          }
        },
        {
          "name": "seed",
          "type": "int",
          "default": null,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image"
          ],
          "description": "Seed for deterministic sampling"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "minimum": 0,
          "maximum": 2,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image"
          ],
          "description": "What sampling temperature to use, between 0 and 2. Higher values make output more random, lower values more focused"
        },
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image"
          ],
          "description": "Maximum number of tokens to generate"
        },
        {
          "name": "user_defined_tool_definition_function_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image"
          ],
          "description": "Optional user-defined function code that can be validated, then used as a custom tool."
        },
        {
          "name": "presence_penalty",
          "type": "float",
          "default": 0,
          "minimum": -2.0,
          "maximum": 2.0,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far"
        },
        {
          "name": "frequency_penalty",
          "type": "float",
          "default": 0,
          "minimum": -2.0,
          "maximum": 2.0,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency"
        },
        {
          "name": "logit_bias",
          "type": "object",
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Modify the likelihood of specified tokens appearing in the completion",
          "additionalProperties": {
            "type": "number",
            "minimum": -100,
            "maximum": 100
          }
        },
        {
          "name": "top_p",
          "type": "float",
          "default": 1,
          "minimum": 0,
          "maximum": 1,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Alternative to temperature for nucleus sampling. Top-p probability mass to consider"
        },
        {
          "name": "stop",
          "type": "array",
          "items": {
            "type": "string"
          },
          "maxItems": 4,
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Up to 4 sequences where the API will stop generating tokens"
        },
        {
          "name": "logprobs",
          "type": "boolean",
          "default": false,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Whether to return log probabilities of the output tokens"
        },
        {
          "name": "top_logprobs",
          "type": "int",
          "default": null,
          "minimum": 0,
          "maximum": 20,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Number of most likely tokens to return with probabilities (0-20). Requires logprobs=true"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openai-gpt-4o-mini",
      "model_url": "",
      "description": "Cost-efficient small model that's smarter and cheaper than GPT-3.5 Turbo, with vision capabilities. Features full multimodal capabilities like GPT-4o at lower cost.",
      "supported_inference_type_strings": [
        "text_completion",
        "ask_question_about_an_image"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "system_message",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "System message to set context and behavior for the model"
        },
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion', 'ask_question_about_an_image']",
          "description": "The maximum number of tokens to generate"
        },
        {
          "name": "user_defined_tool_definition_function_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image"
          ],
          "description": "Optional user-defined function code that can be validated, then used as a custom tool."
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "minimum": 0,
          "maximum": 2,
          "inference_types_parameter_applies_to": "['text_completion', 'ask_question_about_an_image']",
          "description": "What sampling temperature to use. Higher values make output more random, lower values more focused"
        },
        {
          "name": "response_format",
          "type": "string",
          "default": "text",
          "options": ["text", "json"],
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Format for model responses. Use 'json' to force JSON output"
        },
        {
          "name": "tools",
          "type": "array",
          "default": null,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "List of function definitions that the model may call"
        },
        {
          "name": "tool_choice",
          "type": "string",
          "default": "none",
          "options": ["none", "auto", "required"],
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "Controls when the model is allowed to call functions"
        },
        {
          "name": "seed",
          "type": "int",
          "default": null,
          "inference_types_parameter_applies_to": "['text_completion', 'ask_question_about_an_image']",
          "description": "Seed for deterministic sampling"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openai-o1",
      "model_url": "",
      "description": "Most powerful reasoning model with 200K context, supporting tools, Structured Outputs, and vision. October 2023 knowledge cutoff. Specialized for complex reasoning tasks.",
      "supported_inference_type_strings": [
        "text_completion",
        "ask_question_about_an_image"
      ],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "tools",
          "type": "array",
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Array of functions the model may call",
          "items": {
            "type": "object",
            "required": ["type", "function"],
            "properties": {
              "type": {
                "type": "string",
                "enum": ["function"],
                "description": "Type of tool - currently only 'function' is supported"
              },
              "function": {
                "type": "object",
                "required": ["name", "parameters"],
                "properties": {
                  "name": {
                    "type": "string",
                    "description": "Name of the function to be called"
                  },
                  "description": {
                    "type": "string",
                    "description": "Description of when the function should be used"
                  },
                  "parameters": {
                    "type": "object",
                    "description": "JSON Schema defining the function parameters",
                    "required": ["type", "properties"],
                    "properties": {
                      "type": {
                        "type": "string",
                        "enum": ["object"],
                        "description": "Must be 'object' for function parameters"
                      },
                      "properties": {
                        "type": "object",
                        "description": "Function parameters schema"
                      },
                      "required": {
                        "type": "array",
                        "items": {
                          "type": "string"
                        },
                        "description": "Array of required parameter names"
                      }
                    }
                  },
                  "strict": {
                    "type": "boolean",
                    "default": false,
                    "description": "Enable Structured Outputs for strict JSON schema validation"
                  }
                }
              }
            }
          }
        },
        {
          "name": "tool_choice",
          "type": "object",
          "default": "auto",
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls which (if any) tool is called by the model",
          "oneOf": [
            {
              "type": "string",
              "enum": ["none", "auto", "required"]
            },
            {
              "type": "object",
              "required": ["type", "function"],
              "properties": {
                "type": {
                  "type": "string",
                  "enum": ["function"]
                },
                "function": {
                  "type": "object",
                  "required": ["name"],
                  "properties": {
                    "name": {
                      "type": "string",
                      "description": "Name of the function to force the model to use"
                    }
                  }
                }
              }
            }
          ]
        },
        {
          "name": "parallel_tool_calls",
          "type": "boolean",
          "default": true,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Whether to allow multiple tool calls in parallel. Set to false when using strict schema validation"
        },
        {
          "name": "response_format",
          "type": "object",
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls the format of model responses",
          "properties": {
            "type": {
              "type": "string",
              "enum": ["text", "json_object", "json_schema"],
              "description": "Response format type"
            },
            "schema": {
              "type": "object",
              "description": "JSON Schema for structured output validation"
            }
          }
        },
        {
          "name": "reasoning_effort",
          "type": "string",
          "default": "medium",
          "enum": ["low", "medium", "high"],
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls effort spent on reasoning. Lower values are faster but less thorough"
        },
        {
          "name": "seed",
          "type": "int",
          "default": null,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image"
          ],
          "description": "Seed for deterministic sampling"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "minimum": 0,
          "maximum": 2,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image"
          ],
          "description": "What sampling temperature to use, between 0 and 2. Higher values make output more random, lower values more focused"
        },
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image"
          ],
          "description": "Maximum number of tokens to generate"
        },
        {
          "name": "user_defined_tool_definition_function_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image"
          ],
          "description": "Optional user-defined function code that can be validated, then used as a custom tool."
        },
        {
          "name": "presence_penalty",
          "type": "float",
          "default": 0,
          "minimum": -2.0,
          "maximum": 2.0,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far"
        },
        {
          "name": "frequency_penalty",
          "type": "float",
          "default": 0,
          "minimum": -2.0,
          "maximum": 2.0,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openai-o1-mini",
      "model_url": "",
      "description": "Small reasoning model optimized for coding and math. Faster than o1 with specialized capabilities for technical tasks.",
      "supported_inference_type_strings": ["text_completion"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "tools",
          "type": "array",
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Array of functions the model may call",
          "items": {
            "type": "object",
            "required": ["type", "function"],
            "properties": {
              "type": {
                "type": "string",
                "enum": ["function"],
                "description": "Type of tool - currently only 'function' is supported"
              },
              "function": {
                "type": "object",
                "required": ["name", "parameters"],
                "properties": {
                  "name": {
                    "type": "string",
                    "description": "Name of the function to be called"
                  },
                  "description": {
                    "type": "string",
                    "description": "Description of when the function should be used"
                  },
                  "parameters": {
                    "type": "object",
                    "description": "JSON Schema defining the function parameters",
                    "required": ["type", "properties"],
                    "properties": {
                      "type": {
                        "type": "string",
                        "enum": ["object"],
                        "description": "Must be 'object' for function parameters"
                      },
                      "properties": {
                        "type": "object",
                        "description": "Function parameters schema"
                      },
                      "required": {
                        "type": "array",
                        "items": {
                          "type": "string"
                        },
                        "description": "Array of required parameter names"
                      }
                    }
                  },
                  "strict": {
                    "type": "boolean",
                    "default": false,
                    "description": "Enable Structured Outputs for strict JSON schema validation"
                  }
                }
              }
            }
          }
        },
        {
          "name": "tool_choice",
          "type": "object",
          "default": "auto",
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls which (if any) tool is called by the model",
          "oneOf": [
            {
              "type": "string",
              "enum": ["none", "auto", "required"]
            },
            {
              "type": "object",
              "required": ["type", "function"],
              "properties": {
                "type": {
                  "type": "string",
                  "enum": ["function"]
                },
                "function": {
                  "type": "object",
                  "required": ["name"],
                  "properties": {
                    "name": {
                      "type": "string",
                      "description": "Name of the function to force the model to use"
                    }
                  }
                }
              }
            }
          ]
        },
        {
          "name": "parallel_tool_calls",
          "type": "boolean",
          "default": true,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Whether to allow multiple tool calls in parallel. Set to false when using strict schema validation"
        },
        {
          "name": "response_format",
          "type": "object",
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls the format of model responses",
          "properties": {
            "type": {
              "type": "string",
              "enum": ["text", "json_object", "json_schema"],
              "description": "Response format type"
            },
            "schema": {
              "type": "object",
              "description": "JSON Schema for structured output validation"
            }
          }
        },
        {
          "name": "reasoning_effort",
          "type": "string",
          "default": "medium",
          "enum": ["low", "medium", "high"],
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls effort spent on reasoning. Lower values are faster but less thorough"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "minimum": 0,
          "maximum": 2,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "What sampling temperature to use, between 0 and 2. Higher values make output more random, lower values more focused"
        },
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Maximum number of tokens to generate"
        },
        {
          "name": "user_defined_tool_definition_function_string",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": [
            "text_completion",
            "ask_question_about_an_image"
          ],
          "description": "Optional user-defined function code that can be validated, then used as a custom tool."
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openai-text-embedding-3-large",
      "model_url": "",
      "description": "Most capable embedding model for maximum performance. Best choice for tasks requiring highest accuracy. 3072 dimensions by default.",
      "supported_inference_type_strings": ["embedding"],
      "input_fields": [
        {
          "name": "input_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "embedding",
          "file_type": "embedding",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "encoding_format",
          "type": "string",
          "default": "float",
          "options": ["float", "base64"],
          "inference_types_parameter_applies_to": ["embedding"],
          "description": "The format to return the embeddings in"
        },
        {
          "name": "dimensions",
          "type": "int",
          "default": 3072,
          "minimum": 1,
          "maximum": 3072,
          "inference_types_parameter_applies_to": ["embedding"],
          "description": "The number of dimensions the resulting output embeddings should have"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openai-text-embedding-3-small",
      "model_url": "",
      "description": "Efficient embedding model for search, clustering, and classification tasks. Best price-performance ratio. 1536 dimensions by default.",
      "supported_inference_type_strings": ["embedding"],
      "input_fields": [
        {
          "name": "input_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "embedding",
          "file_type": "embedding",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "encoding_format",
          "type": "string",
          "default": "float",
          "options": ["float", "base64"],
          "inference_types_parameter_applies_to": ["embedding"],
          "description": "The format to return the embeddings in"
        },
        {
          "name": "dimensions",
          "type": "int",
          "default": 1536,
          "minimum": 1,
          "maximum": 1536,
          "inference_types_parameter_applies_to": ["embedding"],
          "description": "The number of dimensions the resulting output embeddings should have"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openai-text-embedding-ada-002",
      "model_url": "",
      "description": "Legacy embedding model. 1536 dimensions. Fixed size only. Recommended to use text-embedding-3-small instead for better performance.",
      "supported_inference_type_strings": ["embedding"],
      "input_fields": [
        {
          "name": "input_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "embedding",
          "file_type": "embedding",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "encoding_format",
          "type": "string",
          "default": "float",
          "options": ["float", "base64"],
          "inference_types_parameter_applies_to": ["embedding"],
          "description": "The format to return the embeddings in"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama-3.1-70b-versatile",
      "model_url": "",
      "description": "LLaMA3.1 70b versatile is a powerful language model developed by Meta with a context window of 131,072 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama-3.1-8b-instant",
      "model_url": "",
      "description": "LLaMA3.1 8b instant is a powerful language model developed by Meta with a context window of 131,072 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama3-70b-8192",
      "model_url": "",
      "description": "LLaMA3 70b is a powerful language model developed by Meta with a context window of 8,192 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-llama3-8b-8192",
      "model_url": "",
      "description": "LLaMA3 8b is a powerful but compact language model developed by Meta with a context window of 8,192 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-mixtral-8x7b-32768",
      "model_url": "",
      "description": "Mixtral 8x7b is a powerful language model developed by Mistral with a context window of 32,768 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "groq-gemma2-9b-it",
      "model_url": "",
      "description": "Gemma 2 9b is an LLM developed by Google with a context window of 8,192 tokens.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-mistral-large-latest",
      "model_url": "",
      "description": "Most powerful Mistral model optimized for complex tasks. Features 32K context window, JSON output, function calling, tools, embeddings and advanced parameter controls.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "inference_types_parameter_applies_to": ["text_completion"],
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "system_message",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Optional system message to set context and behavior"
        },
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "minimum": 1,
          "maximum": 32768,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "minimum": 0.0,
          "maximum": 2.0,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls randomness. Higher values make output more random, lower values more deterministic"
        },
        {
          "name": "top_p",
          "type": "float",
          "default": 1.0,
          "minimum": 0.0,
          "maximum": 1.0,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Nucleus sampling parameter. Lower values lead to more focused output"
        },
        {
          "name": "presence_penalty",
          "type": "float",
          "default": 0.0,
          "minimum": -2.0,
          "maximum": 2.0,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Penalizes new tokens based on presence in text"
        },
        {
          "name": "frequency_penalty",
          "type": "float",
          "default": 0.0,
          "minimum": -2.0,
          "maximum": 2.0,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Penalizes new tokens based on their frequency"
        },
        {
          "name": "seed",
          "type": "integer",
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Random seed for reproducible outputs"
        },
        {
          "name": "safe_prompt",
          "type": "boolean",
          "default": false,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Enable content filtering"
        },
        {
          "name": "response_format",
          "type": "object",
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls response format",
          "properties": {
            "type": {
              "type": "string",
              "enum": ["text", "json_object"],
              "description": "Response format type"
            }
          }
        },
        {
          "name": "tools",
          "type": "array",
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "List of functions the model may call",
          "items": {
            "type": "object",
            "required": ["type", "function"],
            "properties": {
              "type": {
                "type": "string",
                "enum": ["function"],
                "description": "Tool type - only 'function' supported"
              },
              "function": {
                "type": "object",
                "required": ["name", "parameters"],
                "properties": {
                  "name": {
                    "type": "string",
                    "description": "Function name"
                  },
                  "description": {
                    "type": "string",
                    "description": "Function description"
                  },
                  "parameters": {
                    "type": "object",
                    "description": "JSON Schema for parameters"
                  }
                }
              }
            }
          }
        },
        {
          "name": "tool_choice",
          "type": "string",
          "default": "auto",
          "options": ["auto", "any", "none"],
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls tool usage"
        },
        {
          "name": "parallel_tool_calls",
          "type": "boolean",
          "default": true,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Allow parallel function calls"
        },
        {
          "name": "logprobs",
          "type": "boolean",
          "default": false,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Return log probabilities of tokens"
        },
        {
          "name": "top_logprobs",
          "type": "integer",
          "default": null,
          "minimum": 0,
          "maximum": 20,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Number of most likely tokens to return probabilities for"
        },
        {
          "name": "request_timeout",
          "type": "integer",
          "default": 60,
          "minimum": 1,
          "maximum": 300,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "embedding"
          ],
          "description": "Request timeout in seconds"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-mistral-medium-latest",
      "model_url": "",
      "description": "Balanced Mistral model with improved cost-performance ratio. Features 32K context window, JSON output, function calling, and efficient inference capabilities.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "inference_types_parameter_applies_to": ["text_completion"],
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "system_message",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Optional system message to set context and behavior"
        },
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "minimum": 1,
          "maximum": 32768,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "minimum": 0.0,
          "maximum": 2.0,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls randomness. Higher values make output more random, lower values more deterministic"
        },
        {
          "name": "top_p",
          "type": "float",
          "default": 1.0,
          "minimum": 0.0,
          "maximum": 1.0,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Nucleus sampling parameter. Lower values lead to more focused output"
        },
        {
          "name": "presence_penalty",
          "type": "float",
          "default": 0.0,
          "minimum": -2.0,
          "maximum": 2.0,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Penalizes new tokens based on presence in text"
        },
        {
          "name": "frequency_penalty",
          "type": "float",
          "default": 0.0,
          "minimum": -2.0,
          "maximum": 2.0,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Penalizes new tokens based on their frequency"
        },
        {
          "name": "seed",
          "type": "integer",
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Random seed for reproducible outputs"
        },
        {
          "name": "safe_prompt",
          "type": "boolean",
          "default": false,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Enable content filtering"
        },
        {
          "name": "response_format",
          "type": "object",
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls response format",
          "properties": {
            "type": {
              "type": "string",
              "enum": ["text", "json_object"],
              "description": "Response format type"
            }
          }
        },
        {
          "name": "tools",
          "type": "array",
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "List of functions the model may call",
          "items": {
            "type": "object",
            "required": ["type", "function"],
            "properties": {
              "type": {
                "type": "string",
                "enum": ["function"],
                "description": "Tool type - only 'function' supported"
              },
              "function": {
                "type": "object",
                "required": ["name", "parameters"],
                "properties": {
                  "name": {
                    "type": "string",
                    "description": "Function name"
                  },
                  "description": {
                    "type": "string",
                    "description": "Function description"
                  },
                  "parameters": {
                    "type": "object",
                    "description": "JSON Schema for parameters"
                  }
                }
              }
            }
          }
        },
        {
          "name": "tool_choice",
          "type": "string",
          "default": "auto",
          "options": ["auto", "any", "none"],
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls tool usage"
        },
        {
          "name": "parallel_tool_calls",
          "type": "boolean",
          "default": true,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Allow parallel function calls"
        },
        {
          "name": "logprobs",
          "type": "boolean",
          "default": false,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Return log probabilities of tokens"
        },
        {
          "name": "top_logprobs",
          "type": "integer",
          "default": null,
          "minimum": 0,
          "maximum": 20,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Number of most likely tokens to return probabilities for"
        },
        {
          "name": "request_timeout",
          "type": "integer",
          "default": 60,
          "minimum": 1,
          "maximum": 300,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "embedding"
          ],
          "description": "Request timeout in seconds"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-mistral-small-latest",
      "model_url": "",
      "description": "Fast, efficient Mistral model optimized for cost-effective inference. Features 32K context window, JSON output, function calling, and rapid response times.",
      "supported_inference_type_strings": ["text_completion", "embedding"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "inference_types_parameter_applies_to": ["text_completion"],
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "system_message",
          "type": "string",
          "default": "",
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Optional system message to set context and behavior"
        },
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "minimum": 1,
          "maximum": 32768,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "minimum": 0.0,
          "maximum": 2.0,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls randomness. Higher values make output more random, lower values more deterministic"
        },
        {
          "name": "top_p",
          "type": "float",
          "default": 1.0,
          "minimum": 0.0,
          "maximum": 1.0,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Nucleus sampling parameter. Lower values lead to more focused output"
        },
        {
          "name": "presence_penalty",
          "type": "float",
          "default": 0.0,
          "minimum": -2.0,
          "maximum": 2.0,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Penalizes new tokens based on presence in text"
        },
        {
          "name": "frequency_penalty",
          "type": "float",
          "default": 0.0,
          "minimum": -2.0,
          "maximum": 2.0,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Penalizes new tokens based on their frequency"
        },
        {
          "name": "seed",
          "type": "integer",
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Random seed for reproducible outputs"
        },
        {
          "name": "safe_prompt",
          "type": "boolean",
          "default": false,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Enable content filtering"
        },
        {
          "name": "response_format",
          "type": "object",
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls response format",
          "properties": {
            "type": {
              "type": "string",
              "enum": ["text", "json_object"],
              "description": "Response format type"
            }
          }
        },
        {
          "name": "tools",
          "type": "array",
          "default": null,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "List of functions the model may call",
          "items": {
            "type": "object",
            "required": ["type", "function"],
            "properties": {
              "type": {
                "type": "string",
                "enum": ["function"],
                "description": "Tool type - only 'function' supported"
              },
              "function": {
                "type": "object",
                "required": ["name", "parameters"],
                "properties": {
                  "name": {
                    "type": "string",
                    "description": "Function name"
                  },
                  "description": {
                    "type": "string",
                    "description": "Function description"
                  },
                  "parameters": {
                    "type": "object",
                    "description": "JSON Schema for parameters"
                  }
                }
              }
            }
          }
        },
        {
          "name": "tool_choice",
          "type": "string",
          "default": "auto",
          "options": ["auto", "any", "none"],
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Controls tool usage"
        },
        {
          "name": "parallel_tool_calls",
          "type": "boolean",
          "default": true,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Allow parallel function calls"
        },
        {
          "name": "logprobs",
          "type": "boolean",
          "default": false,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Return log probabilities of tokens"
        },
        {
          "name": "top_logprobs",
          "type": "integer",
          "default": null,
          "minimum": 0,
          "maximum": 20,
          "inference_types_parameter_applies_to": ["text_completion"],
          "description": "Number of most likely tokens to return probabilities for"
        },
        {
          "name": "request_timeout",
          "type": "integer",
          "default": 60,
          "minimum": 1,
          "maximum": 300,
          "inference_types_parameter_applies_to": [
            "text_completion",
            "embedding"
          ],
          "description": "Request timeout in seconds"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-pixtral-12b-2409",
      "model_url": "",
      "description": "Multimodal model combining advanced vision and language capabilities. Supports image analysis, OCR, chart interpretation, and multi-image comparison.",
      "supported_inference_type_strings": [
        "ask_question_about_an_image",
        "text_completion"
      ],
      "input_fields": [
        {
          "name": "image",
          "file_type": "base64",
          "inference_types_parameter_applies_to": [
            "ask_question_about_an_image"
          ],
          "optional": false,
          "constraints": {
            "max_file_size_mb": 10,
            "max_dimension": 1024,
            "supported_formats": ["png", "jpeg", "jpg", "webp", "gif"],
            "max_images_per_request": 8
          }
        },
        {
          "name": "question",
          "file_type": "text",
          "inference_types_parameter_applies_to": [
            "ask_question_about_an_image"
          ],
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 300,
          "minimum": 1,
          "maximum": 4096,
          "inference_types_parameter_applies_to": [
            "ask_question_about_an_image",
            "text_completion"
          ],
          "description": "Maximum tokens to generate"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "minimum": 0.0,
          "maximum": 2.0,
          "inference_types_parameter_applies_to": [
            "ask_question_about_an_image",
            "text_completion"
          ],
          "description": "Controls randomness in output"
        },
        {
          "name": "response_format",
          "type": "object",
          "default": null,
          "inference_types_parameter_applies_to": [
            "ask_question_about_an_image",
            "text_completion"
          ],
          "description": "Controls response format",
          "properties": {
            "type": {
              "type": "string",
              "enum": ["text", "json_object"],
              "description": "Response format type"
            }
          }
        },
        {
          "name": "seed",
          "type": "integer",
          "default": null,
          "inference_types_parameter_applies_to": [
            "ask_question_about_an_image",
            "text_completion"
          ],
          "description": "Random seed for reproducible outputs"
        },
        {
          "name": "safe_prompt",
          "type": "boolean",
          "default": false,
          "inference_types_parameter_applies_to": [
            "ask_question_about_an_image",
            "text_completion"
          ],
          "description": "Enable content filtering"
        },
        {
          "name": "task_type",
          "type": "string",
          "default": "general",
          "options": ["general", "ocr", "chart_analysis", "image_comparison"],
          "inference_types_parameter_applies_to": [
            "ask_question_about_an_image"
          ],
          "description": "Specify type of vision task"
        },
        {
          "name": "request_timeout",
          "type": "integer",
          "default": 60,
          "minimum": 1,
          "maximum": 300,
          "inference_types_parameter_applies_to": [
            "ask_question_about_an_image",
            "text_completion"
          ],
          "description": "Request timeout in seconds"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "mistralapi-mistral-embed",
      "model_url": "",
      "description": "State-of-the-art embedding model producing 1024-dimensional vectors. Optimized for semantic search, clustering, and similarity tasks.",
      "supported_inference_type_strings": ["embedding"],
      "input_fields": [
        {
          "name": "input_text",
          "file_type": "text",
          "optional": false,
          "constraints": {
            "max_length": 32768
          }
        }
      ],
      "output_fields": [
        {
          "name": "embedding",
          "file_type": "embedding",
          "optional": false,
          "properties": {
            "dimensions": 1024
          }
        }
      ],
      "model_parameters": [
        {
          "name": "encoding_format",
          "type": "string",
          "default": "float",
          "options": ["float", "base64"],
          "inference_types_parameter_applies_to": ["embedding"],
          "description": "Output encoding format"
        },
        {
          "name": "batch_size",
          "type": "integer",
          "default": 32,
          "minimum": 1,
          "maximum": 2048,
          "inference_types_parameter_applies_to": ["embedding"],
          "description": "Number of texts to embed in parallel"
        },
        {
          "name": "request_timeout",
          "type": "integer",
          "default": 60,
          "minimum": 1,
          "maximum": 300,
          "inference_types_parameter_applies_to": ["embedding"],
          "description": "Request timeout in seconds"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },

    {
      "model_name": "stability-core",
      "model_url": "",
      "description": "Stable Image Core is the best quality achievable at high speed for text-to-image generation.",
      "supported_inference_type_strings": ["text_to_image"],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "aspect_ratio",
          "type": "string",
          "default": "1:1",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The aspect ratio of the generated image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Random seed to use for generation [Optional]"
        },
        {
          "name": "style_preset",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Style Preset: Guides the image model towards a particular style (e.g., '3d-model', 'analog-film', 'anime', etc.) [Optional]"
        },
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The format of the generated image (e.g., 'png', 'jpeg', 'webp') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-sd3-large",
      "model_url": "",
      "description": "Stable Diffusion 3 Large model for high-quality image generation.",
      "supported_inference_type_strings": ["text_to_image"],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "aspect_ratio",
          "type": "string",
          "default": "1:1",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The aspect ratio of the generated image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Random seed to use for generation [Optional]"
        },
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The format of the generated image (e.g., 'png', 'jpeg') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-conservative-upscale",
      "model_url": "",
      "description": "Conservative upscaling model that preserves image details.",
      "supported_inference_type_strings": ["conservative_upscale"],
      "input_fields": [
        {
          "name": "image",
          "file_type": "image",
          "optional": false
        },
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "upscaled_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["conservative_upscale"],
          "description": "The format of the upscaled image (e.g., 'png', 'jpeg', 'webp') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["conservative_upscale"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["conservative_upscale"],
          "description": "Random seed to use for upscaling [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-creative-upscale",
      "model_url": "",
      "description": "Creative upscaling model that can enhance and add details to images.",
      "supported_inference_type_strings": ["creative_upscale"],
      "input_fields": [
        {
          "name": "image",
          "file_type": "image",
          "optional": false
        },
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "upscaled_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["creative_upscale"],
          "description": "The format of the upscaled image (e.g., 'png', 'jpeg', 'webp') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["creative_upscale"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["creative_upscale"],
          "description": "Random seed to use for upscaling [Optional]"
        },
        {
          "name": "creativity",
          "type": "float",
          "default": 0.3,
          "min": 0,
          "max": 0.35,
          "inference_types_parameter_applies_to": ["creative_upscale"],
          "description": "Controls the creativity level of the upscaling process [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-sd3-medium",
      "model_url": "",
      "description": "Stable Diffusion 3 Medium model for balanced performance and quality in image generation.",
      "supported_inference_type_strings": ["text_to_image"],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "aspect_ratio",
          "type": "string",
          "default": "1:1",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The aspect ratio of the generated image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Random seed to use for generation [Optional]"
        },
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The format of the generated image (e.g., 'png', 'jpeg') [Optional]"
        },
        {
          "name": "negative_prompt",
          "type": "string",
          "default": null,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Negative Prompt: A blurb of text describing what you do not wish to see in the output image [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "stability-sd3-large-turbo",
      "model_url": "",
      "description": "Stable Diffusion 3 Large Turbo model for faster inference with high-quality results.",
      "supported_inference_type_strings": ["text_to_image"],
      "input_fields": [
        {
          "name": "prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_image",
          "file_type": "base64_image",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "aspect_ratio",
          "type": "string",
          "default": "1:1",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The aspect ratio of the generated image [Optional]"
        },
        {
          "name": "seed",
          "type": "int",
          "default": 0,
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "Random seed to use for generation [Optional]"
        },
        {
          "name": "output_format",
          "type": "string",
          "default": "png",
          "inference_types_parameter_applies_to": ["text_to_image"],
          "description": "The format of the generated image (e.g., 'png', 'jpeg') [Optional]"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    },
    {
      "model_name": "openrouter/auto",
      "model_url": "",
      "description": "OpenRouter's auto-routing model that automatically selects the best available model for the given task.",
      "supported_inference_type_strings": ["text_completion"],
      "input_fields": [
        {
          "name": "input_prompt",
          "file_type": "text",
          "optional": false
        }
      ],
      "output_fields": [
        {
          "name": "generated_text",
          "file_type": "text",
          "optional": false
        }
      ],
      "model_parameters": [
        {
          "name": "number_of_tokens_to_generate",
          "type": "int",
          "default": 1000,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The maximum number of tokens to generate [Optional]"
        },
        {
          "name": "temperature",
          "type": "float",
          "default": 0.7,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The temperature for sampling [Optional]"
        },
        {
          "name": "number_of_completions_to_generate [Optional]",
          "type": "int",
          "default": 1,
          "inference_types_parameter_applies_to": "['text_completion']",
          "description": "The number of completions to generate"
        }
      ],
      "credit_costs": {
        "api_based_pricing": 1
      }
    }
  ]
}
